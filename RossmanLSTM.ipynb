{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation, LSTM, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilherme/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "Store = 67\n",
    "\n",
    "data = pd.read_csv('train.csv', index_col=\"Date\", parse_dates=[\"Date\"])\n",
    "data.drop([\"DayOfWeek\", \"Customers\", \"Open\", \"Promo\", \"StateHoliday\", \"SchoolHoliday\"], inplace=True, axis=1)\n",
    "store = data[data.Store == Store].Sales\n",
    "store_ts = store.resample(\"W\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilherme/.local/lib/python3.5/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "look_back = 3\n",
    "\n",
    "scaler = MinMaxScaler((-1,1))\n",
    "data_scaled = scaler.fit_transform(store_ts.values.reshape(-1, 1))\n",
    "\n",
    "convert_to_step = lambda interval: data_scaled[interval[0]:interval[1]]\n",
    "intervals = zip(range(len(data_scaled) - look_back), range(look_back, len(data_scaled)))\n",
    "\n",
    "train_set, test_set = train_test_split(np.array(list(map(convert_to_step, intervals))), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train_set[:-1], train_set[1:, -1]\n",
    "X_test, y_test = test_set[:-1], test_set[1:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (32, 3)                   60        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (32, 1)                   4         \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    LSTM(3, input_shape=(look_back, 1), batch_size=32, stateful=True),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(loss='mape', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96 samples, validate on 32 samples\n",
      "Epoch 1/10000\n",
      "96/96 [==============================] - 9s 94ms/step - loss: 205.1805 - val_loss: 92.0894\n",
      "Epoch 2/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 204.6343 - val_loss: 92.1204\n",
      "Epoch 3/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 181.9968 - val_loss: 96.9300\n",
      "Epoch 4/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 178.2444 - val_loss: 96.6344\n",
      "Epoch 5/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 172.0016 - val_loss: 91.4993\n",
      "Epoch 6/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 167.2361 - val_loss: 91.7365\n",
      "Epoch 7/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 156.3178 - val_loss: 92.3881\n",
      "Epoch 8/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 151.8232 - val_loss: 93.0511\n",
      "Epoch 9/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 128.3989 - val_loss: 92.6192\n",
      "Epoch 10/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 133.7626 - val_loss: 92.0561\n",
      "Epoch 11/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 132.8843 - val_loss: 89.9946\n",
      "Epoch 12/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 142.8485 - val_loss: 91.5247\n",
      "Epoch 13/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 129.3741 - val_loss: 93.7977\n",
      "Epoch 14/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 129.2618 - val_loss: 96.1693\n",
      "Epoch 15/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 125.2065 - val_loss: 94.7657\n",
      "Epoch 16/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 127.7992 - val_loss: 96.1053\n",
      "Epoch 17/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 127.3298 - val_loss: 98.6017\n",
      "Epoch 18/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 122.0177 - val_loss: 97.6293\n",
      "Epoch 19/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 121.8382 - val_loss: 101.6439\n",
      "Epoch 20/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 127.7913 - val_loss: 101.6479\n",
      "Epoch 21/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 127.6254 - val_loss: 103.2516\n",
      "Epoch 22/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 124.4596 - val_loss: 106.0481\n",
      "Epoch 23/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 125.6177 - val_loss: 103.1630\n",
      "Epoch 24/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 123.2787 - val_loss: 102.2321\n",
      "Epoch 25/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 126.1580 - val_loss: 104.2942\n",
      "Epoch 26/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 125.4173 - val_loss: 104.1457\n",
      "Epoch 27/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 125.4984 - val_loss: 104.5223\n",
      "Epoch 28/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 124.0460 - val_loss: 105.0399\n",
      "Epoch 29/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 125.1768 - val_loss: 101.3268\n",
      "Epoch 30/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 122.8012 - val_loss: 103.7872\n",
      "Epoch 31/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 126.5008 - val_loss: 102.4197\n",
      "Epoch 32/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 118.9553 - val_loss: 101.4945\n",
      "Epoch 33/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 123.7769 - val_loss: 102.5709\n",
      "Epoch 34/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 122.3717 - val_loss: 101.1853\n",
      "Epoch 35/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 121.5051 - val_loss: 100.2233\n",
      "Epoch 36/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 120.3458 - val_loss: 102.2431\n",
      "Epoch 37/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 124.2484 - val_loss: 100.7332\n",
      "Epoch 38/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 121.0698 - val_loss: 99.9675\n",
      "Epoch 39/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 119.3766 - val_loss: 101.3339\n",
      "Epoch 40/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 122.4971 - val_loss: 101.0080\n",
      "Epoch 41/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 118.3411 - val_loss: 100.8064\n",
      "Epoch 42/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 121.3751 - val_loss: 100.4545\n",
      "Epoch 43/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 120.0713 - val_loss: 101.0926\n",
      "Epoch 44/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 120.2503 - val_loss: 101.5772\n",
      "Epoch 45/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 122.8353 - val_loss: 101.2545\n",
      "Epoch 46/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 117.1040 - val_loss: 102.3042\n",
      "Epoch 47/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 116.6889 - val_loss: 102.2054\n",
      "Epoch 48/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 122.6073 - val_loss: 100.9505\n",
      "Epoch 49/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 120.0931 - val_loss: 101.1097\n",
      "Epoch 50/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 118.7473 - val_loss: 99.7182\n",
      "Epoch 51/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 119.0193 - val_loss: 100.0467\n",
      "Epoch 52/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 121.6227 - val_loss: 99.3325\n",
      "Epoch 53/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 115.4343 - val_loss: 99.3810\n",
      "Epoch 54/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 117.1159 - val_loss: 101.1243\n",
      "Epoch 55/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 116.4389 - val_loss: 101.1783\n",
      "Epoch 56/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 115.6272 - val_loss: 99.5993\n",
      "Epoch 57/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 117.2972 - val_loss: 100.8500\n",
      "Epoch 58/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 117.4536 - val_loss: 100.2685\n",
      "Epoch 59/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 117.9568 - val_loss: 99.8239\n",
      "Epoch 60/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 117.8525 - val_loss: 100.4633\n",
      "Epoch 61/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 114.9982 - val_loss: 99.2724\n",
      "Epoch 62/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 118.3447 - val_loss: 99.3670\n",
      "Epoch 63/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 117.0919 - val_loss: 98.3285\n",
      "Epoch 64/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 115.7997 - val_loss: 97.5480\n",
      "Epoch 65/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 114.2242 - val_loss: 99.5052\n",
      "Epoch 66/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 114.5932 - val_loss: 98.4108\n",
      "Epoch 67/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 115.0766 - val_loss: 98.7730\n",
      "Epoch 68/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 117.6334 - val_loss: 100.1915\n",
      "Epoch 69/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 114.7087 - val_loss: 100.3710\n",
      "Epoch 70/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 117.6754 - val_loss: 99.5978\n",
      "Epoch 71/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 115.6638 - val_loss: 99.7511\n",
      "Epoch 72/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 115.1108 - val_loss: 98.7542\n",
      "Epoch 73/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 115.6851 - val_loss: 97.7250\n",
      "Epoch 74/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 115.2785 - val_loss: 99.1584\n",
      "Epoch 75/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 115.2362 - val_loss: 97.0980\n",
      "Epoch 76/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 111.9610 - val_loss: 98.6143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 110.3199 - val_loss: 96.7422\n",
      "Epoch 78/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 112.4987 - val_loss: 98.3430\n",
      "Epoch 79/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 116.9160 - val_loss: 97.2419\n",
      "Epoch 80/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 115.1643 - val_loss: 98.0700\n",
      "Epoch 81/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 113.5347 - val_loss: 100.5595\n",
      "Epoch 82/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 109.9335 - val_loss: 99.6753\n",
      "Epoch 83/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 114.6914 - val_loss: 98.7422\n",
      "Epoch 84/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 109.0149 - val_loss: 98.2268\n",
      "Epoch 85/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 115.2619 - val_loss: 96.4099\n",
      "Epoch 86/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 112.7405 - val_loss: 98.7849\n",
      "Epoch 87/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 115.6281 - val_loss: 98.6504\n",
      "Epoch 88/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 113.2449 - val_loss: 99.0343\n",
      "Epoch 89/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 113.6132 - val_loss: 98.1702\n",
      "Epoch 90/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 110.0950 - val_loss: 97.2271\n",
      "Epoch 91/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 115.0445 - val_loss: 99.1810\n",
      "Epoch 92/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 109.5287 - val_loss: 96.6000\n",
      "Epoch 93/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 110.2890 - val_loss: 97.1996\n",
      "Epoch 94/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 111.4001 - val_loss: 96.8495\n",
      "Epoch 95/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 113.9827 - val_loss: 98.5720\n",
      "Epoch 96/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 107.3001 - val_loss: 97.7726\n",
      "Epoch 97/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 109.3577 - val_loss: 99.3860\n",
      "Epoch 98/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 108.7524 - val_loss: 97.6546\n",
      "Epoch 99/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 109.4497 - val_loss: 99.2161\n",
      "Epoch 100/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 107.0097 - val_loss: 96.3289\n",
      "Epoch 101/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 108.7044 - val_loss: 98.6670\n",
      "Epoch 102/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 107.2077 - val_loss: 99.2833\n",
      "Epoch 103/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 107.3522 - val_loss: 95.3085\n",
      "Epoch 104/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 109.1750 - val_loss: 98.5623\n",
      "Epoch 105/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 109.3501 - val_loss: 98.4515\n",
      "Epoch 106/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 105.6805 - val_loss: 98.3334\n",
      "Epoch 107/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 112.9037 - val_loss: 98.2930\n",
      "Epoch 108/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 113.0929 - val_loss: 98.0971\n",
      "Epoch 109/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 103.6930 - val_loss: 96.6741\n",
      "Epoch 110/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 108.8835 - val_loss: 100.9359\n",
      "Epoch 111/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 112.2999 - val_loss: 99.6523\n",
      "Epoch 112/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 108.1672 - val_loss: 100.9422\n",
      "Epoch 113/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 107.2694 - val_loss: 95.4335\n",
      "Epoch 114/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 109.9586 - val_loss: 96.1020\n",
      "Epoch 115/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 109.3405 - val_loss: 96.4712\n",
      "Epoch 116/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 106.2970 - val_loss: 99.2690\n",
      "Epoch 117/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 108.6946 - val_loss: 97.6752\n",
      "Epoch 118/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 106.3369 - val_loss: 97.4175\n",
      "Epoch 119/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 110.5818 - val_loss: 98.8765\n",
      "Epoch 120/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 104.9918 - val_loss: 96.6583\n",
      "Epoch 121/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 106.3069 - val_loss: 95.5420\n",
      "Epoch 122/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 106.5290 - val_loss: 96.6909\n",
      "Epoch 123/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 110.6605 - val_loss: 97.7090\n",
      "Epoch 124/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 104.5365 - val_loss: 100.5956\n",
      "Epoch 125/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 101.8756 - val_loss: 98.5954\n",
      "Epoch 126/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 103.9663 - val_loss: 98.0220\n",
      "Epoch 127/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 107.6565 - val_loss: 97.0617\n",
      "Epoch 128/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 103.9747 - val_loss: 100.1034\n",
      "Epoch 129/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 110.6946 - val_loss: 96.9125\n",
      "Epoch 130/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 113.0189 - val_loss: 98.1083\n",
      "Epoch 131/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 104.8172 - val_loss: 98.4427\n",
      "Epoch 132/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 106.3014 - val_loss: 102.4008\n",
      "Epoch 133/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 106.3046 - val_loss: 98.2530\n",
      "Epoch 134/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 105.1887 - val_loss: 99.4306\n",
      "Epoch 135/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 106.3627 - val_loss: 101.8692\n",
      "Epoch 136/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 106.4306 - val_loss: 97.6420\n",
      "Epoch 137/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 108.9023 - val_loss: 99.8742\n",
      "Epoch 138/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 104.4958 - val_loss: 102.7353\n",
      "Epoch 139/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 106.2932 - val_loss: 101.2596\n",
      "Epoch 140/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 105.8508 - val_loss: 97.1631\n",
      "Epoch 141/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 104.8862 - val_loss: 104.0275\n",
      "Epoch 142/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 109.5516 - val_loss: 97.9903\n",
      "Epoch 143/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 104.3044 - val_loss: 96.3669\n",
      "Epoch 144/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 105.2418 - val_loss: 97.1934\n",
      "Epoch 145/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 111.1466 - val_loss: 98.5951\n",
      "Epoch 146/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 107.7557 - val_loss: 99.1618\n",
      "Epoch 147/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 109.1893 - val_loss: 99.6475\n",
      "Epoch 148/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 103.5482 - val_loss: 100.8882\n",
      "Epoch 149/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 108.2255 - val_loss: 99.5475\n",
      "Epoch 150/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 107.1422 - val_loss: 98.8714\n",
      "Epoch 151/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.6001 - val_loss: 97.6041\n",
      "Epoch 152/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 103.8414 - val_loss: 94.4281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 109.0441 - val_loss: 95.8420\n",
      "Epoch 154/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 105.0620 - val_loss: 94.0030\n",
      "Epoch 155/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 107.3300 - val_loss: 94.3506\n",
      "Epoch 156/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 111.6812 - val_loss: 95.6886\n",
      "Epoch 157/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 100.6561 - val_loss: 97.2783\n",
      "Epoch 158/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 103.7776 - val_loss: 94.8903\n",
      "Epoch 159/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 113.7038 - val_loss: 95.4876\n",
      "Epoch 160/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 100.2315 - val_loss: 96.5058\n",
      "Epoch 161/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 102.7737 - val_loss: 94.9229\n",
      "Epoch 162/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 111.0354 - val_loss: 99.4757\n",
      "Epoch 163/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 106.7479 - val_loss: 98.4470\n",
      "Epoch 164/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 100.4238 - val_loss: 100.3511\n",
      "Epoch 165/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 107.3027 - val_loss: 97.0773\n",
      "Epoch 166/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 107.9368 - val_loss: 100.1007\n",
      "Epoch 167/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 104.7735 - val_loss: 102.5231\n",
      "Epoch 168/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 100.1175 - val_loss: 98.4700\n",
      "Epoch 169/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 107.3164 - val_loss: 105.7198\n",
      "Epoch 170/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 111.1018 - val_loss: 107.1326\n",
      "Epoch 171/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 109.7096 - val_loss: 96.9091\n",
      "Epoch 172/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 106.3258 - val_loss: 98.6187\n",
      "Epoch 173/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 107.0494 - val_loss: 99.5053\n",
      "Epoch 174/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 110.7192 - val_loss: 98.1174\n",
      "Epoch 175/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 106.5255 - val_loss: 100.9787\n",
      "Epoch 176/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 108.1587 - val_loss: 99.6269\n",
      "Epoch 177/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 105.0632 - val_loss: 97.4076\n",
      "Epoch 178/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 104.5998 - val_loss: 94.7441\n",
      "Epoch 179/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 104.9383 - val_loss: 96.0510\n",
      "Epoch 180/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 103.6055 - val_loss: 99.1510\n",
      "Epoch 181/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 104.3765 - val_loss: 95.7049\n",
      "Epoch 182/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 99.7269 - val_loss: 96.8112\n",
      "Epoch 183/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 102.3622 - val_loss: 94.8053\n",
      "Epoch 184/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 105.1573 - val_loss: 95.9839\n",
      "Epoch 185/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 110.5419 - val_loss: 98.3485\n",
      "Epoch 186/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 100.7071 - val_loss: 103.3265\n",
      "Epoch 187/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 101.4628 - val_loss: 96.3408\n",
      "Epoch 188/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.8960 - val_loss: 99.9798\n",
      "Epoch 189/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 99.5615 - val_loss: 100.3135\n",
      "Epoch 190/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 102.6739 - val_loss: 99.4606\n",
      "Epoch 191/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 107.7788 - val_loss: 97.9418\n",
      "Epoch 192/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 99.8419 - val_loss: 98.7775\n",
      "Epoch 193/10000\n",
      "96/96 [==============================] - 0s 111us/step - loss: 100.9459 - val_loss: 103.5330\n",
      "Epoch 194/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 106.4088 - val_loss: 100.3319\n",
      "Epoch 195/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 105.9595 - val_loss: 100.6127\n",
      "Epoch 196/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 104.8264 - val_loss: 99.7248\n",
      "Epoch 197/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 103.5245 - val_loss: 96.5210\n",
      "Epoch 198/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 103.6769 - val_loss: 95.9989\n",
      "Epoch 199/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 106.1398 - val_loss: 98.6780\n",
      "Epoch 200/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 100.8598 - val_loss: 97.1073\n",
      "Epoch 201/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.3718 - val_loss: 97.3143\n",
      "Epoch 202/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 104.9190 - val_loss: 95.7731\n",
      "Epoch 203/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 99.2441 - val_loss: 95.4146\n",
      "Epoch 204/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.7663 - val_loss: 96.8772\n",
      "Epoch 205/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 109.4342 - val_loss: 99.3532\n",
      "Epoch 206/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 99.4120 - val_loss: 97.2546\n",
      "Epoch 207/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 107.4375 - val_loss: 94.6229\n",
      "Epoch 208/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 106.7522 - val_loss: 96.9485\n",
      "Epoch 209/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 101.5200 - val_loss: 98.3936\n",
      "Epoch 210/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 102.0368 - val_loss: 97.4404\n",
      "Epoch 211/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 107.9673 - val_loss: 100.7513\n",
      "Epoch 212/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 102.7172 - val_loss: 99.9683\n",
      "Epoch 213/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 100.3711 - val_loss: 99.9661\n",
      "Epoch 214/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 104.4092 - val_loss: 98.7027\n",
      "Epoch 215/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 102.4563 - val_loss: 97.1571\n",
      "Epoch 216/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.6880 - val_loss: 97.2781\n",
      "Epoch 217/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 108.7055 - val_loss: 96.7805\n",
      "Epoch 218/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 97.3052 - val_loss: 98.2480\n",
      "Epoch 219/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 106.2277 - val_loss: 98.0643\n",
      "Epoch 220/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 101.4114 - val_loss: 97.4185\n",
      "Epoch 221/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 100.1336 - val_loss: 97.9411\n",
      "Epoch 222/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 99.8346 - val_loss: 98.6550\n",
      "Epoch 223/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 105.0112 - val_loss: 97.3663\n",
      "Epoch 224/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 100.5770 - val_loss: 95.5869\n",
      "Epoch 225/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 101.4756 - val_loss: 95.7628\n",
      "Epoch 226/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 100.1284 - val_loss: 98.2857\n",
      "Epoch 227/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 105.9272 - val_loss: 98.4735\n",
      "Epoch 228/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 95.4151 - val_loss: 96.9629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 102.7607 - val_loss: 96.7457\n",
      "Epoch 230/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 97.9602 - val_loss: 99.4024\n",
      "Epoch 231/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 104.7999 - val_loss: 98.8671\n",
      "Epoch 232/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 103.1682 - val_loss: 100.0163\n",
      "Epoch 233/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 102.9061 - val_loss: 100.3669\n",
      "Epoch 234/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 104.7820 - val_loss: 95.5334\n",
      "Epoch 235/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 107.8934 - val_loss: 100.7272\n",
      "Epoch 236/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 113.0063 - val_loss: 99.8104\n",
      "Epoch 237/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 101.1344 - val_loss: 98.3689\n",
      "Epoch 238/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 99.7873 - val_loss: 97.5340\n",
      "Epoch 239/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 99.3897 - val_loss: 102.7798\n",
      "Epoch 240/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 102.1486 - val_loss: 100.2661\n",
      "Epoch 241/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 102.1890 - val_loss: 99.9757\n",
      "Epoch 242/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 101.8703 - val_loss: 98.5565\n",
      "Epoch 243/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 99.4074 - val_loss: 99.1284\n",
      "Epoch 244/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 101.9824 - val_loss: 97.5567\n",
      "Epoch 245/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 100.4260 - val_loss: 99.3596\n",
      "Epoch 246/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 107.3619 - val_loss: 99.5697\n",
      "Epoch 247/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.3870 - val_loss: 99.0550\n",
      "Epoch 248/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.7720 - val_loss: 97.7888\n",
      "Epoch 249/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 102.6444 - val_loss: 96.2701\n",
      "Epoch 250/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 99.8976 - val_loss: 97.2126\n",
      "Epoch 251/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 100.9342 - val_loss: 96.4661\n",
      "Epoch 252/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.5327 - val_loss: 95.8889\n",
      "Epoch 253/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 102.2573 - val_loss: 96.2107\n",
      "Epoch 254/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 104.9697 - val_loss: 94.6804\n",
      "Epoch 255/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 99.4049 - val_loss: 94.7716\n",
      "Epoch 256/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 106.3748 - val_loss: 98.9202\n",
      "Epoch 257/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 101.7437 - val_loss: 97.0308\n",
      "Epoch 258/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 99.7816 - val_loss: 101.7892\n",
      "Epoch 259/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.7810 - val_loss: 99.2104\n",
      "Epoch 260/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 103.7046 - val_loss: 97.0888\n",
      "Epoch 261/10000\n",
      "96/96 [==============================] - 0s 111us/step - loss: 99.0477 - val_loss: 98.9305\n",
      "Epoch 262/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 100.4505 - val_loss: 97.2557\n",
      "Epoch 263/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 101.5741 - val_loss: 99.6737\n",
      "Epoch 264/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 99.7587 - val_loss: 100.2350\n",
      "Epoch 265/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 103.5265 - val_loss: 98.0039\n",
      "Epoch 266/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 98.8156 - val_loss: 101.1392\n",
      "Epoch 267/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 105.3964 - val_loss: 99.6755\n",
      "Epoch 268/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 99.9298 - val_loss: 95.9931\n",
      "Epoch 269/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 103.7219 - val_loss: 97.0526\n",
      "Epoch 270/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 101.0145 - val_loss: 99.5114\n",
      "Epoch 271/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 103.5248 - val_loss: 103.3804\n",
      "Epoch 272/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 105.0645 - val_loss: 100.4923\n",
      "Epoch 273/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 101.7386 - val_loss: 99.1392\n",
      "Epoch 274/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 103.3838 - val_loss: 96.1782\n",
      "Epoch 275/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 100.0445 - val_loss: 97.6072\n",
      "Epoch 276/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 103.5303 - val_loss: 100.0284\n",
      "Epoch 277/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 98.7962 - val_loss: 101.3263\n",
      "Epoch 278/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.3491 - val_loss: 98.1338\n",
      "Epoch 279/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 102.4492 - val_loss: 99.7001\n",
      "Epoch 280/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.8858 - val_loss: 98.7339\n",
      "Epoch 281/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 104.1008 - val_loss: 97.8689\n",
      "Epoch 282/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 99.0182 - val_loss: 97.0761\n",
      "Epoch 283/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 107.5413 - val_loss: 94.7229\n",
      "Epoch 284/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 99.9489 - val_loss: 94.7233\n",
      "Epoch 285/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 106.6146 - val_loss: 96.1932\n",
      "Epoch 286/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 102.8415 - val_loss: 98.4014\n",
      "Epoch 287/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.8680 - val_loss: 96.5963\n",
      "Epoch 288/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 101.7876 - val_loss: 96.3953\n",
      "Epoch 289/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 106.2518 - val_loss: 96.8410\n",
      "Epoch 290/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 101.1479 - val_loss: 97.6874\n",
      "Epoch 291/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 102.2123 - val_loss: 96.8393\n",
      "Epoch 292/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 101.3354 - val_loss: 99.0991\n",
      "Epoch 293/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 96.1087 - val_loss: 99.5240\n",
      "Epoch 294/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 104.4983 - val_loss: 97.4360\n",
      "Epoch 295/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 102.7503 - val_loss: 97.2145\n",
      "Epoch 296/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 100.9832 - val_loss: 97.9162\n",
      "Epoch 297/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 99.1634 - val_loss: 98.4417\n",
      "Epoch 298/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 101.2629 - val_loss: 97.8134\n",
      "Epoch 299/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.3337 - val_loss: 96.3135\n",
      "Epoch 300/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.6474 - val_loss: 96.9368\n",
      "Epoch 301/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 104.1050 - val_loss: 95.7910\n",
      "Epoch 302/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 100.3968 - val_loss: 99.3455\n",
      "Epoch 303/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 101.0392 - val_loss: 95.9339\n",
      "Epoch 304/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 101.5879 - val_loss: 97.7666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 305/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 99.9245 - val_loss: 96.6484\n",
      "Epoch 306/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 97.2620 - val_loss: 98.9746\n",
      "Epoch 307/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 101.7828 - val_loss: 100.4496\n",
      "Epoch 308/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 98.9394 - val_loss: 97.9384\n",
      "Epoch 309/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 100.3454 - val_loss: 101.1732\n",
      "Epoch 310/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 99.0927 - val_loss: 102.8415\n",
      "Epoch 311/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 104.5648 - val_loss: 100.0919\n",
      "Epoch 312/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.7482 - val_loss: 101.5034\n",
      "Epoch 313/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 100.9641 - val_loss: 97.2623\n",
      "Epoch 314/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 98.8723 - val_loss: 99.4606\n",
      "Epoch 315/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 97.0562 - val_loss: 98.3834\n",
      "Epoch 316/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 102.9153 - val_loss: 95.2906\n",
      "Epoch 317/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 99.4488 - val_loss: 96.1082\n",
      "Epoch 318/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 98.9183 - val_loss: 99.2788\n",
      "Epoch 319/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 101.8846 - val_loss: 96.8918\n",
      "Epoch 320/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 99.2323 - val_loss: 98.2407\n",
      "Epoch 321/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 103.7186 - val_loss: 97.1603\n",
      "Epoch 322/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.8079 - val_loss: 99.9311\n",
      "Epoch 323/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 103.4061 - val_loss: 97.0209\n",
      "Epoch 324/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 100.2938 - val_loss: 100.5014\n",
      "Epoch 325/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 100.2465 - val_loss: 96.9013\n",
      "Epoch 326/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 101.0162 - val_loss: 100.2621\n",
      "Epoch 327/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 100.5610 - val_loss: 98.8710\n",
      "Epoch 328/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 103.6118 - val_loss: 98.6566\n",
      "Epoch 329/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.7577 - val_loss: 99.9640\n",
      "Epoch 330/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 99.9475 - val_loss: 98.6336\n",
      "Epoch 331/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.0515 - val_loss: 99.6589\n",
      "Epoch 332/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 97.0800 - val_loss: 95.4175\n",
      "Epoch 333/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 98.1716 - val_loss: 96.9139\n",
      "Epoch 334/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 100.6012 - val_loss: 97.7851\n",
      "Epoch 335/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.4661 - val_loss: 98.6653\n",
      "Epoch 336/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 100.2968 - val_loss: 99.4256\n",
      "Epoch 337/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 106.6633 - val_loss: 96.3794\n",
      "Epoch 338/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 102.6503 - val_loss: 97.4937\n",
      "Epoch 339/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.4663 - val_loss: 101.1511\n",
      "Epoch 340/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 95.3015 - val_loss: 102.1841\n",
      "Epoch 341/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.4205 - val_loss: 94.9859\n",
      "Epoch 342/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 100.5352 - val_loss: 95.9204\n",
      "Epoch 343/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 100.7995 - val_loss: 95.3030\n",
      "Epoch 344/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 103.2458 - val_loss: 95.8973\n",
      "Epoch 345/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 102.9001 - val_loss: 98.4308\n",
      "Epoch 346/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 106.6576 - val_loss: 98.3834\n",
      "Epoch 347/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.2452 - val_loss: 96.9755\n",
      "Epoch 348/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 102.3993 - val_loss: 99.6985\n",
      "Epoch 349/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 100.1587 - val_loss: 98.6886\n",
      "Epoch 350/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 99.6809 - val_loss: 97.2005\n",
      "Epoch 351/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 101.6026 - val_loss: 99.1609\n",
      "Epoch 352/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 99.1991 - val_loss: 100.4065\n",
      "Epoch 353/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 100.7136 - val_loss: 101.3418\n",
      "Epoch 354/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 100.8873 - val_loss: 98.5096\n",
      "Epoch 355/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 105.6795 - val_loss: 98.7033\n",
      "Epoch 356/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 99.0793 - val_loss: 101.0641\n",
      "Epoch 357/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 100.1208 - val_loss: 99.3997\n",
      "Epoch 358/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 100.9087 - val_loss: 99.4473\n",
      "Epoch 359/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 99.9918 - val_loss: 97.6940\n",
      "Epoch 360/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 101.1509 - val_loss: 97.7199\n",
      "Epoch 361/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 101.9852 - val_loss: 100.5853\n",
      "Epoch 362/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.6228 - val_loss: 97.4943\n",
      "Epoch 363/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 96.6937 - val_loss: 97.8984\n",
      "Epoch 364/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 99.5136 - val_loss: 100.3367\n",
      "Epoch 365/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 100.9364 - val_loss: 101.4871\n",
      "Epoch 366/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.6780 - val_loss: 97.6441\n",
      "Epoch 367/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 100.6997 - val_loss: 98.9444\n",
      "Epoch 368/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.1706 - val_loss: 100.0470\n",
      "Epoch 369/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.0358 - val_loss: 98.4510\n",
      "Epoch 370/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 103.2463 - val_loss: 100.0407\n",
      "Epoch 371/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 102.0894 - val_loss: 98.3450\n",
      "Epoch 372/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 101.6442 - val_loss: 99.2485\n",
      "Epoch 373/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 98.3648 - val_loss: 99.0789\n",
      "Epoch 374/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 100.7697 - val_loss: 96.4093\n",
      "Epoch 375/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.3274 - val_loss: 95.9658\n",
      "Epoch 376/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 101.2767 - val_loss: 96.0737\n",
      "Epoch 377/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 97.5222 - val_loss: 97.9471\n",
      "Epoch 378/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 100.9618 - val_loss: 96.8543\n",
      "Epoch 379/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.7217 - val_loss: 95.4574\n",
      "Epoch 380/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 102.8841 - val_loss: 96.3333\n",
      "Epoch 381/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 125us/step - loss: 98.8874 - val_loss: 98.3563\n",
      "Epoch 382/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 102.4836 - val_loss: 97.6179\n",
      "Epoch 383/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.9546 - val_loss: 97.8401\n",
      "Epoch 384/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 100.4749 - val_loss: 98.6248\n",
      "Epoch 385/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 98.2449 - val_loss: 99.1277\n",
      "Epoch 386/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.8133 - val_loss: 99.1980\n",
      "Epoch 387/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 101.3772 - val_loss: 97.5532\n",
      "Epoch 388/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.2424 - val_loss: 97.9277\n",
      "Epoch 389/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.5657 - val_loss: 97.8799\n",
      "Epoch 390/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 103.2036 - val_loss: 99.3971\n",
      "Epoch 391/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 100.4675 - val_loss: 97.9058\n",
      "Epoch 392/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 102.8510 - val_loss: 99.0184\n",
      "Epoch 393/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 99.4509 - val_loss: 99.2794\n",
      "Epoch 394/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 100.4802 - val_loss: 96.1658\n",
      "Epoch 395/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.7218 - val_loss: 98.6817\n",
      "Epoch 396/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 100.0200 - val_loss: 98.8636\n",
      "Epoch 397/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 98.5867 - val_loss: 96.9390\n",
      "Epoch 398/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 98.1429 - val_loss: 97.5928\n",
      "Epoch 399/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.7073 - val_loss: 98.2836\n",
      "Epoch 400/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 101.5883 - val_loss: 99.9051\n",
      "Epoch 401/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 100.8939 - val_loss: 96.6584\n",
      "Epoch 402/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 98.3265 - val_loss: 101.6403\n",
      "Epoch 403/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 97.5473 - val_loss: 102.3037\n",
      "Epoch 404/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 99.1279 - val_loss: 98.9469\n",
      "Epoch 405/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.9367 - val_loss: 99.9997\n",
      "Epoch 406/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 99.9551 - val_loss: 99.2943\n",
      "Epoch 407/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 100.3362 - val_loss: 97.3069\n",
      "Epoch 408/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 100.2639 - val_loss: 96.4510\n",
      "Epoch 409/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 100.5845 - val_loss: 96.6612\n",
      "Epoch 410/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 97.2323 - val_loss: 97.3042\n",
      "Epoch 411/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 97.5775 - val_loss: 98.5809\n",
      "Epoch 412/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 100.7883 - val_loss: 96.9743\n",
      "Epoch 413/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.9756 - val_loss: 97.2168\n",
      "Epoch 414/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 98.7407 - val_loss: 98.2682\n",
      "Epoch 415/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 104.3109 - val_loss: 98.9461\n",
      "Epoch 416/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 100.0517 - val_loss: 99.5528\n",
      "Epoch 417/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 100.6859 - val_loss: 99.8600\n",
      "Epoch 418/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 100.5259 - val_loss: 99.0525\n",
      "Epoch 419/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 101.4066 - val_loss: 100.1753\n",
      "Epoch 420/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 99.6553 - val_loss: 97.4715\n",
      "Epoch 421/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 102.7816 - val_loss: 97.9243\n",
      "Epoch 422/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 100.2318 - val_loss: 99.3974\n",
      "Epoch 423/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.1921 - val_loss: 98.9485\n",
      "Epoch 424/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 99.4740 - val_loss: 97.4800\n",
      "Epoch 425/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.5203 - val_loss: 96.9881\n",
      "Epoch 426/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.0319 - val_loss: 98.5910\n",
      "Epoch 427/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.9379 - val_loss: 98.9291\n",
      "Epoch 428/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.7626 - val_loss: 97.7456\n",
      "Epoch 429/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 102.2210 - val_loss: 97.2855\n",
      "Epoch 430/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 99.2265 - val_loss: 99.0254\n",
      "Epoch 431/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.6342 - val_loss: 100.1614\n",
      "Epoch 432/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 96.5873 - val_loss: 100.4680\n",
      "Epoch 433/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.5393 - val_loss: 100.1194\n",
      "Epoch 434/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 97.6041 - val_loss: 96.4890\n",
      "Epoch 435/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 98.4790 - val_loss: 96.8325\n",
      "Epoch 436/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.6230 - val_loss: 98.4764\n",
      "Epoch 437/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 101.2077 - val_loss: 96.9942\n",
      "Epoch 438/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 102.6442 - val_loss: 98.6247\n",
      "Epoch 439/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 101.6638 - val_loss: 98.9484\n",
      "Epoch 440/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 99.2616 - val_loss: 97.5358\n",
      "Epoch 441/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 99.2557 - val_loss: 100.7444\n",
      "Epoch 442/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.2052 - val_loss: 97.1201\n",
      "Epoch 443/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.8001 - val_loss: 97.1939\n",
      "Epoch 444/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 102.5055 - val_loss: 96.1829\n",
      "Epoch 445/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 102.1767 - val_loss: 97.5604\n",
      "Epoch 446/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 100.3566 - val_loss: 96.9417\n",
      "Epoch 447/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.2457 - val_loss: 97.4273\n",
      "Epoch 448/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 100.0267 - val_loss: 97.7330\n",
      "Epoch 449/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.7539 - val_loss: 98.4454\n",
      "Epoch 450/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 101.1651 - val_loss: 98.2988\n",
      "Epoch 451/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 100.3300 - val_loss: 99.6249\n",
      "Epoch 452/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.7371 - val_loss: 99.9902\n",
      "Epoch 453/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.6290 - val_loss: 99.0630\n",
      "Epoch 454/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 100.6754 - val_loss: 98.3883\n",
      "Epoch 455/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.5439 - val_loss: 97.3691\n",
      "Epoch 456/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 99.9929 - val_loss: 98.5836\n",
      "Epoch 457/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 116us/step - loss: 98.0300 - val_loss: 98.3413\n",
      "Epoch 458/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.5594 - val_loss: 97.7181\n",
      "Epoch 459/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 97.8964 - val_loss: 98.2390\n",
      "Epoch 460/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 100.1895 - val_loss: 98.5536\n",
      "Epoch 461/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.8257 - val_loss: 99.4327\n",
      "Epoch 462/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 98.1146 - val_loss: 98.8847\n",
      "Epoch 463/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 100.7619 - val_loss: 97.1962\n",
      "Epoch 464/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 99.7018 - val_loss: 97.1606\n",
      "Epoch 465/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 99.4379 - val_loss: 97.3016\n",
      "Epoch 466/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 97.6902 - val_loss: 95.7647\n",
      "Epoch 467/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 99.3654 - val_loss: 97.8800\n",
      "Epoch 468/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 100.4163 - val_loss: 98.0892\n",
      "Epoch 469/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.2539 - val_loss: 96.9129\n",
      "Epoch 470/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.0708 - val_loss: 98.1087\n",
      "Epoch 471/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.5312 - val_loss: 97.0117\n",
      "Epoch 472/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 98.2534 - val_loss: 98.9113\n",
      "Epoch 473/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 100.1262 - val_loss: 98.0637\n",
      "Epoch 474/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 99.0108 - val_loss: 99.7083\n",
      "Epoch 475/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 101.4427 - val_loss: 99.5187\n",
      "Epoch 476/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.7364 - val_loss: 100.4536\n",
      "Epoch 477/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 99.2534 - val_loss: 98.1808\n",
      "Epoch 478/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 98.1719 - val_loss: 99.6312\n",
      "Epoch 479/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 100.7078 - val_loss: 99.9980\n",
      "Epoch 480/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 98.0192 - val_loss: 99.2080\n",
      "Epoch 481/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.4326 - val_loss: 101.8230\n",
      "Epoch 482/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 101.2695 - val_loss: 99.4303\n",
      "Epoch 483/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 98.4918 - val_loss: 98.9514\n",
      "Epoch 484/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 99.4817 - val_loss: 97.6888\n",
      "Epoch 485/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.4379 - val_loss: 96.3908\n",
      "Epoch 486/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 98.9579 - val_loss: 96.8508\n",
      "Epoch 487/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 99.7301 - val_loss: 97.6141\n",
      "Epoch 488/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 99.2319 - val_loss: 98.2939\n",
      "Epoch 489/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.4091 - val_loss: 98.6761\n",
      "Epoch 490/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.3015 - val_loss: 97.9791\n",
      "Epoch 491/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 98.9505 - val_loss: 98.3450\n",
      "Epoch 492/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.3680 - val_loss: 99.7924\n",
      "Epoch 493/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.6036 - val_loss: 98.3343\n",
      "Epoch 494/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.4678 - val_loss: 98.7786\n",
      "Epoch 495/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 99.2863 - val_loss: 98.3728\n",
      "Epoch 496/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 98.8440 - val_loss: 97.0475\n",
      "Epoch 497/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.8864 - val_loss: 99.5213\n",
      "Epoch 498/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.8322 - val_loss: 98.4794\n",
      "Epoch 499/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 99.5129 - val_loss: 97.5262\n",
      "Epoch 500/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.0462 - val_loss: 99.4562\n",
      "Epoch 501/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 100.9242 - val_loss: 99.7182\n",
      "Epoch 502/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 99.8428 - val_loss: 98.6505\n",
      "Epoch 503/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.7716 - val_loss: 98.0636\n",
      "Epoch 504/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 98.3536 - val_loss: 97.6616\n",
      "Epoch 505/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.4415 - val_loss: 98.2593\n",
      "Epoch 506/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 100.7544 - val_loss: 97.9404\n",
      "Epoch 507/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 100.7755 - val_loss: 97.3079\n",
      "Epoch 508/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 98.1807 - val_loss: 98.4577\n",
      "Epoch 509/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 101.8749 - val_loss: 96.8788\n",
      "Epoch 510/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 101.5857 - val_loss: 97.3583\n",
      "Epoch 511/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.3903 - val_loss: 95.8781\n",
      "Epoch 512/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 101.4645 - val_loss: 97.0764\n",
      "Epoch 513/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 99.1531 - val_loss: 95.9953\n",
      "Epoch 514/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 97.5425 - val_loss: 98.3027\n",
      "Epoch 515/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 99.6373 - val_loss: 96.9451\n",
      "Epoch 516/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 100.2432 - val_loss: 98.1928\n",
      "Epoch 517/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.6762 - val_loss: 96.7988\n",
      "Epoch 518/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.4874 - val_loss: 96.0969\n",
      "Epoch 519/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.5967 - val_loss: 99.4628\n",
      "Epoch 520/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 99.3445 - val_loss: 98.7895\n",
      "Epoch 521/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.1931 - val_loss: 99.6410\n",
      "Epoch 522/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.4058 - val_loss: 98.9224\n",
      "Epoch 523/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.2386 - val_loss: 98.0497\n",
      "Epoch 524/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.0911 - val_loss: 98.4175\n",
      "Epoch 525/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 100.9685 - val_loss: 97.6579\n",
      "Epoch 526/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.1787 - val_loss: 95.5954\n",
      "Epoch 527/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 104.2791 - val_loss: 95.9568\n",
      "Epoch 528/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 99.9435 - val_loss: 95.6523\n",
      "Epoch 529/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 101.9080 - val_loss: 95.8524\n",
      "Epoch 530/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.7320 - val_loss: 97.7976\n",
      "Epoch 531/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 101.5207 - val_loss: 97.3141\n",
      "Epoch 532/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 100.5247 - val_loss: 98.8860\n",
      "Epoch 533/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 117us/step - loss: 99.0579 - val_loss: 99.0189\n",
      "Epoch 534/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.1972 - val_loss: 99.1392\n",
      "Epoch 535/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 97.2986 - val_loss: 98.8619\n",
      "Epoch 536/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.1516 - val_loss: 99.8170\n",
      "Epoch 537/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.6102 - val_loss: 98.2540\n",
      "Epoch 538/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.5604 - val_loss: 98.9176\n",
      "Epoch 539/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.6882 - val_loss: 98.1261\n",
      "Epoch 540/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 98.0851 - val_loss: 99.1690\n",
      "Epoch 541/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 98.2468 - val_loss: 98.0180\n",
      "Epoch 542/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 101.3536 - val_loss: 97.7781\n",
      "Epoch 543/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.8795 - val_loss: 95.9156\n",
      "Epoch 544/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.9328 - val_loss: 98.8327\n",
      "Epoch 545/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.9605 - val_loss: 97.5443\n",
      "Epoch 546/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.8482 - val_loss: 97.1492\n",
      "Epoch 547/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 97.3895 - val_loss: 97.4191\n",
      "Epoch 548/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 99.3025 - val_loss: 98.9809\n",
      "Epoch 549/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 100.6889 - val_loss: 98.4986\n",
      "Epoch 550/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 99.2110 - val_loss: 97.2878\n",
      "Epoch 551/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.6542 - val_loss: 97.5540\n",
      "Epoch 552/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 97.7637 - val_loss: 98.2262\n",
      "Epoch 553/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.8730 - val_loss: 98.2416\n",
      "Epoch 554/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 99.6465 - val_loss: 98.2547\n",
      "Epoch 555/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 101.5001 - val_loss: 98.0483\n",
      "Epoch 556/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 98.1288 - val_loss: 97.9345\n",
      "Epoch 557/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 96.8485 - val_loss: 98.8791\n",
      "Epoch 558/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 97.9328 - val_loss: 99.3381\n",
      "Epoch 559/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.0232 - val_loss: 97.3243\n",
      "Epoch 560/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.8291 - val_loss: 98.2437\n",
      "Epoch 561/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.5230 - val_loss: 98.8032\n",
      "Epoch 562/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 99.0067 - val_loss: 98.9814\n",
      "Epoch 563/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.9323 - val_loss: 99.2986\n",
      "Epoch 564/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.8466 - val_loss: 98.1979\n",
      "Epoch 565/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.3956 - val_loss: 98.7124\n",
      "Epoch 566/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.6128 - val_loss: 100.0776\n",
      "Epoch 567/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.8478 - val_loss: 97.9733\n",
      "Epoch 568/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 99.1641 - val_loss: 98.6605\n",
      "Epoch 569/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 98.7342 - val_loss: 98.2572\n",
      "Epoch 570/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.8831 - val_loss: 99.2500\n",
      "Epoch 571/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 99.3686 - val_loss: 98.8126\n",
      "Epoch 572/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.7411 - val_loss: 98.0526\n",
      "Epoch 573/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 100.5916 - val_loss: 97.9946\n",
      "Epoch 574/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.8651 - val_loss: 96.7888\n",
      "Epoch 575/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.7353 - val_loss: 97.0144\n",
      "Epoch 576/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 98.1063 - val_loss: 98.3668\n",
      "Epoch 577/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.1287 - val_loss: 97.3565\n",
      "Epoch 578/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.0540 - val_loss: 99.1637\n",
      "Epoch 579/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.8028 - val_loss: 97.8773\n",
      "Epoch 580/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 98.7672 - val_loss: 97.6838\n",
      "Epoch 581/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 98.6098 - val_loss: 97.1687\n",
      "Epoch 582/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 99.0351 - val_loss: 98.6635\n",
      "Epoch 583/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 99.2315 - val_loss: 99.0127\n",
      "Epoch 584/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.2616 - val_loss: 98.7482\n",
      "Epoch 585/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 98.7553 - val_loss: 99.1932\n",
      "Epoch 586/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 98.2791 - val_loss: 99.7012\n",
      "Epoch 587/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.4309 - val_loss: 99.8156\n",
      "Epoch 588/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 99.0115 - val_loss: 98.1323\n",
      "Epoch 589/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 98.9337 - val_loss: 99.0993\n",
      "Epoch 590/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.3633 - val_loss: 97.2740\n",
      "Epoch 591/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 99.1793 - val_loss: 95.4884\n",
      "Epoch 592/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 99.9560 - val_loss: 97.9600\n",
      "Epoch 593/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.9107 - val_loss: 98.2851\n",
      "Epoch 594/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.2555 - val_loss: 98.6865\n",
      "Epoch 595/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.1398 - val_loss: 98.6138\n",
      "Epoch 596/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 98.5026 - val_loss: 98.6099\n",
      "Epoch 597/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 98.4410 - val_loss: 98.1506\n",
      "Epoch 598/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.8079 - val_loss: 99.5134\n",
      "Epoch 599/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.1299 - val_loss: 98.5037\n",
      "Epoch 600/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.9959 - val_loss: 97.7318\n",
      "Epoch 601/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 98.1606 - val_loss: 97.6622\n",
      "Epoch 602/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 102.2203 - val_loss: 97.7697\n",
      "Epoch 603/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 98.0055 - val_loss: 98.4733\n",
      "Epoch 604/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 98.1038 - val_loss: 96.9080\n",
      "Epoch 605/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 98.2053 - val_loss: 98.5801\n",
      "Epoch 606/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 97.3181 - val_loss: 98.7939\n",
      "Epoch 607/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 99.2291 - val_loss: 98.8740\n",
      "Epoch 608/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.9092 - val_loss: 98.8431\n",
      "Epoch 609/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 117us/step - loss: 99.6846 - val_loss: 98.2973\n",
      "Epoch 610/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 99.1585 - val_loss: 97.6559\n",
      "Epoch 611/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 99.0472 - val_loss: 98.5212\n",
      "Epoch 612/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 98.3053 - val_loss: 98.4069\n",
      "Epoch 613/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.2752 - val_loss: 98.4806\n",
      "Epoch 614/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 99.1137 - val_loss: 98.7952\n",
      "Epoch 615/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.7316 - val_loss: 98.4319\n",
      "Epoch 616/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.5696 - val_loss: 98.8911\n",
      "Epoch 617/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.3652 - val_loss: 99.1246\n",
      "Epoch 618/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 98.3642 - val_loss: 96.4905\n",
      "Epoch 619/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 101.3001 - val_loss: 97.3981\n",
      "Epoch 620/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 96.9490 - val_loss: 96.7620\n",
      "Epoch 621/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 98.5188 - val_loss: 97.3626\n",
      "Epoch 622/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.2325 - val_loss: 98.3269\n",
      "Epoch 623/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.6168 - val_loss: 98.8252\n",
      "Epoch 624/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.7559 - val_loss: 99.3276\n",
      "Epoch 625/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 98.2187 - val_loss: 98.7686\n",
      "Epoch 626/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 98.5031 - val_loss: 99.5755\n",
      "Epoch 627/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 97.8437 - val_loss: 99.2710\n",
      "Epoch 628/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.0093 - val_loss: 99.3669\n",
      "Epoch 629/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 101.5848 - val_loss: 97.8551\n",
      "Epoch 630/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.5012 - val_loss: 96.1941\n",
      "Epoch 631/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 98.6754 - val_loss: 98.5769\n",
      "Epoch 632/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.0680 - val_loss: 97.1820\n",
      "Epoch 633/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.5522 - val_loss: 98.7664\n",
      "Epoch 634/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.0861 - val_loss: 99.0047\n",
      "Epoch 635/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 100.3900 - val_loss: 97.8756\n",
      "Epoch 636/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.7035 - val_loss: 99.1372\n",
      "Epoch 637/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 99.4318 - val_loss: 97.9423\n",
      "Epoch 638/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.7734 - val_loss: 97.3660\n",
      "Epoch 639/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 98.8070 - val_loss: 98.4206\n",
      "Epoch 640/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 96.7553 - val_loss: 99.1495\n",
      "Epoch 641/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 97.3243 - val_loss: 97.8801\n",
      "Epoch 642/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 97.1395 - val_loss: 97.8797\n",
      "Epoch 643/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 99.4741 - val_loss: 96.8526\n",
      "Epoch 644/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 102.6689 - val_loss: 97.2515\n",
      "Epoch 645/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.8397 - val_loss: 97.2629\n",
      "Epoch 646/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 99.3252 - val_loss: 97.9500\n",
      "Epoch 647/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.6666 - val_loss: 98.3097\n",
      "Epoch 648/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.1758 - val_loss: 99.4229\n",
      "Epoch 649/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 97.4186 - val_loss: 98.7927\n",
      "Epoch 650/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 101.1227 - val_loss: 98.0780\n",
      "Epoch 651/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 99.1373 - val_loss: 97.8789\n",
      "Epoch 652/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.8308 - val_loss: 97.4314\n",
      "Epoch 653/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 99.1272 - val_loss: 96.8817\n",
      "Epoch 654/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 97.7397 - val_loss: 97.9736\n",
      "Epoch 655/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.3186 - val_loss: 97.8585\n",
      "Epoch 656/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.1295 - val_loss: 98.0613\n",
      "Epoch 657/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 96.8583 - val_loss: 98.2855\n",
      "Epoch 658/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 97.4865 - val_loss: 97.5314\n",
      "Epoch 659/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 99.0273 - val_loss: 97.9538\n",
      "Epoch 660/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 98.2036 - val_loss: 97.1553\n",
      "Epoch 661/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 100.1545 - val_loss: 98.5609\n",
      "Epoch 662/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 99.3402 - val_loss: 98.9609\n",
      "Epoch 663/10000\n",
      "96/96 [==============================] - 0s 149us/step - loss: 97.8458 - val_loss: 99.2290\n",
      "Epoch 664/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 98.8421 - val_loss: 98.8435\n",
      "Epoch 665/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 98.3675 - val_loss: 98.2526\n",
      "Epoch 666/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 99.3136 - val_loss: 98.5545\n",
      "Epoch 667/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 97.9881 - val_loss: 97.6083\n",
      "Epoch 668/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 101.0950 - val_loss: 97.6299\n",
      "Epoch 669/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 97.7179 - val_loss: 97.5073\n",
      "Epoch 670/10000\n",
      "96/96 [==============================] - 0s 156us/step - loss: 100.9887 - val_loss: 98.3705\n",
      "Epoch 671/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 98.9968 - val_loss: 99.0961\n",
      "Epoch 672/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 97.4721 - val_loss: 98.4869\n",
      "Epoch 673/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 96.7931 - val_loss: 98.4702\n",
      "Epoch 674/10000\n",
      "96/96 [==============================] - 0s 146us/step - loss: 99.4331 - val_loss: 99.2955\n",
      "Epoch 675/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 97.0835 - val_loss: 99.7094\n",
      "Epoch 676/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 99.1066 - val_loss: 99.8883\n",
      "Epoch 677/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 98.4721 - val_loss: 98.8826\n",
      "Epoch 678/10000\n",
      "96/96 [==============================] - 0s 157us/step - loss: 98.0155 - val_loss: 99.5634\n",
      "Epoch 679/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 98.3979 - val_loss: 99.6923\n",
      "Epoch 680/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 98.9883 - val_loss: 99.6387\n",
      "Epoch 681/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 98.8598 - val_loss: 98.3272\n",
      "Epoch 682/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 99.0460 - val_loss: 98.8283\n",
      "Epoch 683/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 97.5999 - val_loss: 98.4442\n",
      "Epoch 684/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.8505 - val_loss: 97.5249\n",
      "Epoch 685/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 126us/step - loss: 98.6208 - val_loss: 97.9090\n",
      "Epoch 686/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 98.6565 - val_loss: 97.5123\n",
      "Epoch 687/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.0517 - val_loss: 98.2514\n",
      "Epoch 688/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.9550 - val_loss: 98.0294\n",
      "Epoch 689/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 98.7103 - val_loss: 99.1538\n",
      "Epoch 690/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 98.3191 - val_loss: 100.0732\n",
      "Epoch 691/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 98.5145 - val_loss: 100.4389\n",
      "Epoch 692/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 97.5637 - val_loss: 97.9927\n",
      "Epoch 693/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 97.9841 - val_loss: 98.3841\n",
      "Epoch 694/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.7808 - val_loss: 98.5085\n",
      "Epoch 695/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.7542 - val_loss: 98.0169\n",
      "Epoch 696/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 97.5053 - val_loss: 97.8256\n",
      "Epoch 697/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 97.2405 - val_loss: 98.0073\n",
      "Epoch 698/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 98.6508 - val_loss: 97.4852\n",
      "Epoch 699/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.8307 - val_loss: 98.3555\n",
      "Epoch 700/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.1548 - val_loss: 98.5934\n",
      "Epoch 701/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 99.3835 - val_loss: 98.6253\n",
      "Epoch 702/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.8452 - val_loss: 98.1268\n",
      "Epoch 703/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 99.3177 - val_loss: 97.9082\n",
      "Epoch 704/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.8600 - val_loss: 100.1017\n",
      "Epoch 705/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 99.4516 - val_loss: 99.5147\n",
      "Epoch 706/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.0908 - val_loss: 99.6349\n",
      "Epoch 707/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.1243 - val_loss: 99.0089\n",
      "Epoch 708/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 98.8566 - val_loss: 98.0111\n",
      "Epoch 709/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.3515 - val_loss: 99.3345\n",
      "Epoch 710/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 97.1595 - val_loss: 98.5379\n",
      "Epoch 711/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.9332 - val_loss: 97.3354\n",
      "Epoch 712/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.3026 - val_loss: 95.7485\n",
      "Epoch 713/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.3560 - val_loss: 97.4896\n",
      "Epoch 714/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 99.8429 - val_loss: 98.3903\n",
      "Epoch 715/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 101.2898 - val_loss: 98.8532\n",
      "Epoch 716/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.1299 - val_loss: 98.3204\n",
      "Epoch 717/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.0308 - val_loss: 98.5273\n",
      "Epoch 718/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 96.6632 - val_loss: 99.4148\n",
      "Epoch 719/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.2697 - val_loss: 98.2332\n",
      "Epoch 720/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.1122 - val_loss: 98.3314\n",
      "Epoch 721/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 97.7082 - val_loss: 97.3273\n",
      "Epoch 722/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.5081 - val_loss: 97.6501\n",
      "Epoch 723/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 98.1194 - val_loss: 96.8895\n",
      "Epoch 724/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.0850 - val_loss: 97.4080\n",
      "Epoch 725/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.7530 - val_loss: 97.4095\n",
      "Epoch 726/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.3196 - val_loss: 97.8631\n",
      "Epoch 727/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.1643 - val_loss: 98.3577\n",
      "Epoch 728/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.0626 - val_loss: 98.0407\n",
      "Epoch 729/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.4854 - val_loss: 99.3026\n",
      "Epoch 730/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.0425 - val_loss: 99.8399\n",
      "Epoch 731/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.3622 - val_loss: 97.9891\n",
      "Epoch 732/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.0550 - val_loss: 98.0947\n",
      "Epoch 733/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 99.5225 - val_loss: 97.0688\n",
      "Epoch 734/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.9294 - val_loss: 98.4735\n",
      "Epoch 735/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 97.5115 - val_loss: 97.7192\n",
      "Epoch 736/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 98.9089 - val_loss: 97.7147\n",
      "Epoch 737/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 99.2550 - val_loss: 99.1539\n",
      "Epoch 738/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 97.9249 - val_loss: 99.9462\n",
      "Epoch 739/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.0296 - val_loss: 99.9580\n",
      "Epoch 740/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 99.7960 - val_loss: 100.1876\n",
      "Epoch 741/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.5238 - val_loss: 100.9332\n",
      "Epoch 742/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 100.8100 - val_loss: 99.8850\n",
      "Epoch 743/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 99.7684 - val_loss: 99.9713\n",
      "Epoch 744/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.5243 - val_loss: 97.0812\n",
      "Epoch 745/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.4352 - val_loss: 98.0832\n",
      "Epoch 746/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 99.2921 - val_loss: 96.9129\n",
      "Epoch 747/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.5514 - val_loss: 96.5911\n",
      "Epoch 748/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.9522 - val_loss: 97.5231\n",
      "Epoch 749/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 98.2459 - val_loss: 98.5122\n",
      "Epoch 750/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 98.6616 - val_loss: 99.0436\n",
      "Epoch 751/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.3047 - val_loss: 98.8749\n",
      "Epoch 752/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.0249 - val_loss: 100.1001\n",
      "Epoch 753/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.3514 - val_loss: 98.0284\n",
      "Epoch 754/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.5539 - val_loss: 97.4478\n",
      "Epoch 755/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.4593 - val_loss: 97.2836\n",
      "Epoch 756/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.7281 - val_loss: 98.0063\n",
      "Epoch 757/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.9560 - val_loss: 97.8300\n",
      "Epoch 758/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.5098 - val_loss: 98.2301\n",
      "Epoch 759/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 99.2603 - val_loss: 98.1152\n",
      "Epoch 760/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 98.4638 - val_loss: 99.4185\n",
      "Epoch 761/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 126us/step - loss: 98.6349 - val_loss: 98.8012\n",
      "Epoch 762/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.2052 - val_loss: 99.5187\n",
      "Epoch 763/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 98.4596 - val_loss: 99.1793\n",
      "Epoch 764/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.6323 - val_loss: 98.4970\n",
      "Epoch 765/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.2277 - val_loss: 98.2332\n",
      "Epoch 766/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 99.1786 - val_loss: 97.7614\n",
      "Epoch 767/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 98.3588 - val_loss: 99.5185\n",
      "Epoch 768/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.7568 - val_loss: 98.0884\n",
      "Epoch 769/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.3518 - val_loss: 98.6443\n",
      "Epoch 770/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.4702 - val_loss: 99.9109\n",
      "Epoch 771/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.8456 - val_loss: 99.0946\n",
      "Epoch 772/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.5692 - val_loss: 98.0846\n",
      "Epoch 773/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.4709 - val_loss: 99.1267\n",
      "Epoch 774/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.7257 - val_loss: 98.5819\n",
      "Epoch 775/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.5048 - val_loss: 98.0949\n",
      "Epoch 776/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 101.3763 - val_loss: 98.8131\n",
      "Epoch 777/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 98.0288 - val_loss: 97.9887\n",
      "Epoch 778/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 97.0559 - val_loss: 98.9095\n",
      "Epoch 779/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.6822 - val_loss: 97.4937\n",
      "Epoch 780/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 100.8323 - val_loss: 98.4552\n",
      "Epoch 781/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.3720 - val_loss: 98.6458\n",
      "Epoch 782/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.5992 - val_loss: 98.1505\n",
      "Epoch 783/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 97.7863 - val_loss: 98.0630\n",
      "Epoch 784/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 96.5860 - val_loss: 98.5304\n",
      "Epoch 785/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.6544 - val_loss: 98.4736\n",
      "Epoch 786/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.1642 - val_loss: 98.5028\n",
      "Epoch 787/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.7439 - val_loss: 98.8004\n",
      "Epoch 788/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.5585 - val_loss: 99.1525\n",
      "Epoch 789/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.2346 - val_loss: 98.6280\n",
      "Epoch 790/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.2886 - val_loss: 98.8526\n",
      "Epoch 791/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.8854 - val_loss: 99.9576\n",
      "Epoch 792/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 99.4706 - val_loss: 97.9770\n",
      "Epoch 793/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.0728 - val_loss: 98.6924\n",
      "Epoch 794/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 97.5983 - val_loss: 97.8702\n",
      "Epoch 795/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.3157 - val_loss: 98.3700\n",
      "Epoch 796/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.9743 - val_loss: 98.6381\n",
      "Epoch 797/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.3836 - val_loss: 97.2677\n",
      "Epoch 798/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 97.9290 - val_loss: 97.9098\n",
      "Epoch 799/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 99.4748 - val_loss: 97.9524\n",
      "Epoch 800/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 98.5405 - val_loss: 98.0059\n",
      "Epoch 801/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.8263 - val_loss: 99.0408\n",
      "Epoch 802/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.3724 - val_loss: 98.6915\n",
      "Epoch 803/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.2652 - val_loss: 98.7124\n",
      "Epoch 804/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.5698 - val_loss: 98.4567\n",
      "Epoch 805/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.4948 - val_loss: 99.7761\n",
      "Epoch 806/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.1508 - val_loss: 99.5823\n",
      "Epoch 807/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.0184 - val_loss: 97.4011\n",
      "Epoch 808/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.8128 - val_loss: 98.2294\n",
      "Epoch 809/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 98.0363 - val_loss: 97.1679\n",
      "Epoch 810/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.3510 - val_loss: 96.9121\n",
      "Epoch 811/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.4833 - val_loss: 96.8752\n",
      "Epoch 812/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.6930 - val_loss: 98.0926\n",
      "Epoch 813/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.7160 - val_loss: 97.6288\n",
      "Epoch 814/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 97.5600 - val_loss: 97.5082\n",
      "Epoch 815/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.5459 - val_loss: 98.1829\n",
      "Epoch 816/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.3713 - val_loss: 99.1582\n",
      "Epoch 817/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 97.3894 - val_loss: 99.1920\n",
      "Epoch 818/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.6027 - val_loss: 98.9280\n",
      "Epoch 819/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.7982 - val_loss: 98.9662\n",
      "Epoch 820/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.4765 - val_loss: 98.2706\n",
      "Epoch 821/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.9858 - val_loss: 98.4833\n",
      "Epoch 822/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 99.7396 - val_loss: 97.9979\n",
      "Epoch 823/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 99.3934 - val_loss: 99.2901\n",
      "Epoch 824/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.7714 - val_loss: 99.6355\n",
      "Epoch 825/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 99.7198 - val_loss: 100.0418\n",
      "Epoch 826/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.9364 - val_loss: 99.3387\n",
      "Epoch 827/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.8697 - val_loss: 98.0632\n",
      "Epoch 828/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.5911 - val_loss: 98.2821\n",
      "Epoch 829/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.0740 - val_loss: 97.4007\n",
      "Epoch 830/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 98.3539 - val_loss: 97.5358\n",
      "Epoch 831/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 99.7027 - val_loss: 98.2354\n",
      "Epoch 832/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.9175 - val_loss: 98.0421\n",
      "Epoch 833/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.6847 - val_loss: 98.1313\n",
      "Epoch 834/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.1989 - val_loss: 99.0130\n",
      "Epoch 835/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 97.3948 - val_loss: 99.0534\n",
      "Epoch 836/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.6641 - val_loss: 100.1347\n",
      "Epoch 837/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 129us/step - loss: 98.4895 - val_loss: 99.5394\n",
      "Epoch 838/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 98.7028 - val_loss: 99.2001\n",
      "Epoch 839/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.7949 - val_loss: 99.2061\n",
      "Epoch 840/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.1303 - val_loss: 97.8314\n",
      "Epoch 841/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 98.0121 - val_loss: 97.7792\n",
      "Epoch 842/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 99.1695 - val_loss: 97.7472\n",
      "Epoch 843/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.4908 - val_loss: 97.9177\n",
      "Epoch 844/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.2470 - val_loss: 98.0827\n",
      "Epoch 845/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 96.7878 - val_loss: 98.7441\n",
      "Epoch 846/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.5587 - val_loss: 97.8772\n",
      "Epoch 847/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 96.4036 - val_loss: 98.2221\n",
      "Epoch 848/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 97.9252 - val_loss: 98.2826\n",
      "Epoch 849/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.1482 - val_loss: 98.5746\n",
      "Epoch 850/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.2556 - val_loss: 99.5963\n",
      "Epoch 851/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.1764 - val_loss: 99.2825\n",
      "Epoch 852/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.7081 - val_loss: 98.2104\n",
      "Epoch 853/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.0205 - val_loss: 98.2541\n",
      "Epoch 854/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.2036 - val_loss: 98.6171\n",
      "Epoch 855/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.1427 - val_loss: 98.0335\n",
      "Epoch 856/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.8376 - val_loss: 97.5880\n",
      "Epoch 857/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.1224 - val_loss: 96.9052\n",
      "Epoch 858/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 100.0469 - val_loss: 97.6863\n",
      "Epoch 859/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.0406 - val_loss: 98.7249\n",
      "Epoch 860/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.2275 - val_loss: 98.4251\n",
      "Epoch 861/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.9634 - val_loss: 99.9536\n",
      "Epoch 862/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 98.1721 - val_loss: 98.5634\n",
      "Epoch 863/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 98.3095 - val_loss: 99.1155\n",
      "Epoch 864/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.9464 - val_loss: 98.7330\n",
      "Epoch 865/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 98.1989 - val_loss: 98.2759\n",
      "Epoch 866/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.5347 - val_loss: 98.1067\n",
      "Epoch 867/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.6794 - val_loss: 98.5657\n",
      "Epoch 868/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.4729 - val_loss: 98.1871\n",
      "Epoch 869/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 98.7824 - val_loss: 98.1069\n",
      "Epoch 870/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.8303 - val_loss: 98.8880\n",
      "Epoch 871/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.1635 - val_loss: 98.4756\n",
      "Epoch 872/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 97.5225 - val_loss: 98.5082\n",
      "Epoch 873/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.9112 - val_loss: 98.8984\n",
      "Epoch 874/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.7462 - val_loss: 98.7193\n",
      "Epoch 875/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 97.8321 - val_loss: 98.9739\n",
      "Epoch 876/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.7815 - val_loss: 99.3410\n",
      "Epoch 877/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.4739 - val_loss: 98.4816\n",
      "Epoch 878/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.3807 - val_loss: 99.3330\n",
      "Epoch 879/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.5667 - val_loss: 98.5953\n",
      "Epoch 880/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.9542 - val_loss: 98.6494\n",
      "Epoch 881/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.8166 - val_loss: 98.3838\n",
      "Epoch 882/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.1356 - val_loss: 99.1219\n",
      "Epoch 883/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.5250 - val_loss: 99.3719\n",
      "Epoch 884/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.3269 - val_loss: 100.8051\n",
      "Epoch 885/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.6122 - val_loss: 99.6495\n",
      "Epoch 886/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.7112 - val_loss: 99.8187\n",
      "Epoch 887/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 97.5287 - val_loss: 99.5556\n",
      "Epoch 888/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.3952 - val_loss: 97.9461\n",
      "Epoch 889/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.3691 - val_loss: 97.1971\n",
      "Epoch 890/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 98.1114 - val_loss: 97.2369\n",
      "Epoch 891/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.1809 - val_loss: 97.0985\n",
      "Epoch 892/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.6739 - val_loss: 97.7929\n",
      "Epoch 893/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 98.6805 - val_loss: 98.2718\n",
      "Epoch 894/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.5718 - val_loss: 98.0689\n",
      "Epoch 895/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 97.7191 - val_loss: 98.5370\n",
      "Epoch 896/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 101.1958 - val_loss: 99.0850\n",
      "Epoch 897/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.6435 - val_loss: 99.6075\n",
      "Epoch 898/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.1512 - val_loss: 99.5562\n",
      "Epoch 899/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 100.0710 - val_loss: 100.7288\n",
      "Epoch 900/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.4768 - val_loss: 99.1667\n",
      "Epoch 901/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.6172 - val_loss: 98.5199\n",
      "Epoch 902/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.1187 - val_loss: 98.0545\n",
      "Epoch 903/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 98.1984 - val_loss: 97.6761\n",
      "Epoch 904/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.4162 - val_loss: 98.7281\n",
      "Epoch 905/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 98.2703 - val_loss: 99.1065\n",
      "Epoch 906/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.6009 - val_loss: 99.0991\n",
      "Epoch 907/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.5525 - val_loss: 99.3892\n",
      "Epoch 908/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 99.7400 - val_loss: 99.9086\n",
      "Epoch 909/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.7525 - val_loss: 99.4859\n",
      "Epoch 910/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.5697 - val_loss: 99.7440\n",
      "Epoch 911/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 97.4978 - val_loss: 98.3507\n",
      "Epoch 912/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.3672 - val_loss: 97.1429\n",
      "Epoch 913/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 116us/step - loss: 98.1364 - val_loss: 97.1351\n",
      "Epoch 914/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 98.2639 - val_loss: 97.7348\n",
      "Epoch 915/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.2521 - val_loss: 97.1885\n",
      "Epoch 916/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 98.0555 - val_loss: 97.8960\n",
      "Epoch 917/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.9210 - val_loss: 97.9905\n",
      "Epoch 918/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 97.9202 - val_loss: 98.4842\n",
      "Epoch 919/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.9198 - val_loss: 98.1123\n",
      "Epoch 920/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.9283 - val_loss: 98.5587\n",
      "Epoch 921/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.8184 - val_loss: 98.6493\n",
      "Epoch 922/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.6787 - val_loss: 98.8246\n",
      "Epoch 923/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.0784 - val_loss: 98.7235\n",
      "Epoch 924/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.5158 - val_loss: 98.6393\n",
      "Epoch 925/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.2820 - val_loss: 98.8504\n",
      "Epoch 926/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.6177 - val_loss: 99.2988\n",
      "Epoch 927/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.4846 - val_loss: 98.5420\n",
      "Epoch 928/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.3688 - val_loss: 99.6756\n",
      "Epoch 929/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.5542 - val_loss: 98.9318\n",
      "Epoch 930/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.0792 - val_loss: 98.5832\n",
      "Epoch 931/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 96.4057 - val_loss: 98.3850\n",
      "Epoch 932/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.8448 - val_loss: 97.9118\n",
      "Epoch 933/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 97.4279 - val_loss: 97.5793\n",
      "Epoch 934/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.8375 - val_loss: 98.5266\n",
      "Epoch 935/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.5679 - val_loss: 98.6576\n",
      "Epoch 936/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.8885 - val_loss: 99.0408\n",
      "Epoch 937/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.1902 - val_loss: 99.1003\n",
      "Epoch 938/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.8063 - val_loss: 98.9472\n",
      "Epoch 939/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.4279 - val_loss: 97.8265\n",
      "Epoch 940/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.7813 - val_loss: 97.3537\n",
      "Epoch 941/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.5503 - val_loss: 97.3950\n",
      "Epoch 942/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 100.3661 - val_loss: 96.9449\n",
      "Epoch 943/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 99.0619 - val_loss: 97.1426\n",
      "Epoch 944/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 98.0666 - val_loss: 97.8849\n",
      "Epoch 945/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 98.0653 - val_loss: 99.1087\n",
      "Epoch 946/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.6294 - val_loss: 98.9011\n",
      "Epoch 947/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.1642 - val_loss: 98.9147\n",
      "Epoch 948/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.6615 - val_loss: 99.1031\n",
      "Epoch 949/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 98.8342 - val_loss: 98.6183\n",
      "Epoch 950/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.2107 - val_loss: 97.6285\n",
      "Epoch 951/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.5432 - val_loss: 98.0867\n",
      "Epoch 952/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.7656 - val_loss: 98.6714\n",
      "Epoch 953/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.3806 - val_loss: 99.1046\n",
      "Epoch 954/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.0351 - val_loss: 98.2649\n",
      "Epoch 955/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.3836 - val_loss: 98.4840\n",
      "Epoch 956/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.8107 - val_loss: 98.9930\n",
      "Epoch 957/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 95.8917 - val_loss: 98.6623\n",
      "Epoch 958/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.4762 - val_loss: 99.3772\n",
      "Epoch 959/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.4584 - val_loss: 99.6883\n",
      "Epoch 960/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.4476 - val_loss: 98.7098\n",
      "Epoch 961/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.7806 - val_loss: 98.8960\n",
      "Epoch 962/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.1362 - val_loss: 99.1604\n",
      "Epoch 963/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.7345 - val_loss: 98.9023\n",
      "Epoch 964/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.5358 - val_loss: 99.2428\n",
      "Epoch 965/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.5475 - val_loss: 99.1465\n",
      "Epoch 966/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 99.0266 - val_loss: 98.3488\n",
      "Epoch 967/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.5360 - val_loss: 98.7596\n",
      "Epoch 968/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 97.2406 - val_loss: 98.7860\n",
      "Epoch 969/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.7743 - val_loss: 98.0040\n",
      "Epoch 970/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.3798 - val_loss: 98.8134\n",
      "Epoch 971/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.3505 - val_loss: 97.9225\n",
      "Epoch 972/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.9433 - val_loss: 99.1860\n",
      "Epoch 973/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.2065 - val_loss: 100.2278\n",
      "Epoch 974/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.5418 - val_loss: 99.1754\n",
      "Epoch 975/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 97.2926 - val_loss: 99.5467\n",
      "Epoch 976/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.3831 - val_loss: 99.2661\n",
      "Epoch 977/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.2996 - val_loss: 98.6920\n",
      "Epoch 978/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 96.9090 - val_loss: 98.6365\n",
      "Epoch 979/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 99.4336 - val_loss: 97.7528\n",
      "Epoch 980/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.8799 - val_loss: 97.4345\n",
      "Epoch 981/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.6656 - val_loss: 96.8975\n",
      "Epoch 982/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.1608 - val_loss: 98.3776\n",
      "Epoch 983/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.6968 - val_loss: 98.7900\n",
      "Epoch 984/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.6393 - val_loss: 99.6458\n",
      "Epoch 985/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 97.6252 - val_loss: 99.1177\n",
      "Epoch 986/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.1956 - val_loss: 97.8638\n",
      "Epoch 987/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.0419 - val_loss: 98.1715\n",
      "Epoch 988/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.7241 - val_loss: 98.0439\n",
      "Epoch 989/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 119us/step - loss: 98.3557 - val_loss: 97.7470\n",
      "Epoch 990/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.1856 - val_loss: 98.8471\n",
      "Epoch 991/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.5329 - val_loss: 98.8988\n",
      "Epoch 992/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 98.9283 - val_loss: 99.1363\n",
      "Epoch 993/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 97.8732 - val_loss: 99.4339\n",
      "Epoch 994/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.2146 - val_loss: 98.2636\n",
      "Epoch 995/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.5083 - val_loss: 98.8931\n",
      "Epoch 996/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.5634 - val_loss: 97.9424\n",
      "Epoch 997/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.7068 - val_loss: 97.6836\n",
      "Epoch 998/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.9196 - val_loss: 99.0278\n",
      "Epoch 999/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.9088 - val_loss: 99.6034\n",
      "Epoch 1000/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.0183 - val_loss: 99.7681\n",
      "Epoch 1001/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.6865 - val_loss: 99.9176\n",
      "Epoch 1002/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.3429 - val_loss: 99.0125\n",
      "Epoch 1003/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.8298 - val_loss: 99.2514\n",
      "Epoch 1004/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.3644 - val_loss: 98.6776\n",
      "Epoch 1005/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 98.0067 - val_loss: 96.9095\n",
      "Epoch 1006/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.6467 - val_loss: 97.9719\n",
      "Epoch 1007/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.9229 - val_loss: 97.1709\n",
      "Epoch 1008/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.3791 - val_loss: 97.8259\n",
      "Epoch 1009/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 98.0284 - val_loss: 98.5448\n",
      "Epoch 1010/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 98.3416 - val_loss: 99.1336\n",
      "Epoch 1011/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.5807 - val_loss: 98.3004\n",
      "Epoch 1012/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 98.2129 - val_loss: 98.7585\n",
      "Epoch 1013/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.3729 - val_loss: 98.0793\n",
      "Epoch 1014/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 97.1774 - val_loss: 98.3431\n",
      "Epoch 1015/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.6426 - val_loss: 97.7609\n",
      "Epoch 1016/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.0921 - val_loss: 96.9260\n",
      "Epoch 1017/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.7300 - val_loss: 96.9432\n",
      "Epoch 1018/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 97.5562 - val_loss: 98.6512\n",
      "Epoch 1019/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.5529 - val_loss: 99.2646\n",
      "Epoch 1020/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.5851 - val_loss: 100.0408\n",
      "Epoch 1021/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 98.4613 - val_loss: 99.4169\n",
      "Epoch 1022/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.5353 - val_loss: 99.8597\n",
      "Epoch 1023/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.0156 - val_loss: 99.2820\n",
      "Epoch 1024/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 98.4473 - val_loss: 98.0512\n",
      "Epoch 1025/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.1467 - val_loss: 98.1907\n",
      "Epoch 1026/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 100.1761 - val_loss: 98.0979\n",
      "Epoch 1027/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.1516 - val_loss: 98.4519\n",
      "Epoch 1028/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.1470 - val_loss: 98.9424\n",
      "Epoch 1029/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.4521 - val_loss: 99.7385\n",
      "Epoch 1030/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.9851 - val_loss: 99.8241\n",
      "Epoch 1031/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 98.5532 - val_loss: 98.5196\n",
      "Epoch 1032/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.0676 - val_loss: 99.1313\n",
      "Epoch 1033/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.2854 - val_loss: 97.6567\n",
      "Epoch 1034/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.0222 - val_loss: 98.2908\n",
      "Epoch 1035/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.0641 - val_loss: 97.9419\n",
      "Epoch 1036/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.7978 - val_loss: 98.1198\n",
      "Epoch 1037/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.9620 - val_loss: 99.9999\n",
      "Epoch 1038/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 100.1475 - val_loss: 99.3709\n",
      "Epoch 1039/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.9213 - val_loss: 99.5725\n",
      "Epoch 1040/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.5544 - val_loss: 98.3560\n",
      "Epoch 1041/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.8698 - val_loss: 98.6780\n",
      "Epoch 1042/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.8797 - val_loss: 98.8870\n",
      "Epoch 1043/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.6503 - val_loss: 97.9351\n",
      "Epoch 1044/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.5170 - val_loss: 98.2879\n",
      "Epoch 1045/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.2213 - val_loss: 97.6131\n",
      "Epoch 1046/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.6588 - val_loss: 98.7755\n",
      "Epoch 1047/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.9140 - val_loss: 98.3478\n",
      "Epoch 1048/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.3045 - val_loss: 98.7688\n",
      "Epoch 1049/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 97.5004 - val_loss: 98.9938\n",
      "Epoch 1050/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.7424 - val_loss: 98.8655\n",
      "Epoch 1051/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.5958 - val_loss: 99.6466\n",
      "Epoch 1052/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.8042 - val_loss: 98.7701\n",
      "Epoch 1053/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 99.2115 - val_loss: 98.8681\n",
      "Epoch 1054/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.7025 - val_loss: 99.3533\n",
      "Epoch 1055/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 97.4076 - val_loss: 98.6177\n",
      "Epoch 1056/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.3341 - val_loss: 97.9084\n",
      "Epoch 1057/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.8645 - val_loss: 98.0313\n",
      "Epoch 1058/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.1680 - val_loss: 96.6049\n",
      "Epoch 1059/10000\n",
      "96/96 [==============================] - 0s 166us/step - loss: 99.2544 - val_loss: 97.4997\n",
      "Epoch 1060/10000\n",
      "96/96 [==============================] - 0s 190us/step - loss: 97.2256 - val_loss: 97.8357\n",
      "Epoch 1061/10000\n",
      "96/96 [==============================] - 0s 185us/step - loss: 97.4632 - val_loss: 98.0070\n",
      "Epoch 1062/10000\n",
      "96/96 [==============================] - 0s 190us/step - loss: 97.5905 - val_loss: 99.5972\n",
      "Epoch 1063/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 98.9115 - val_loss: 99.9248\n",
      "Epoch 1064/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 98.1669 - val_loss: 99.6954\n",
      "Epoch 1065/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 146us/step - loss: 97.8611 - val_loss: 99.4794\n",
      "Epoch 1066/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 96.9159 - val_loss: 98.8215\n",
      "Epoch 1067/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 96.8697 - val_loss: 98.9788\n",
      "Epoch 1068/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 97.0001 - val_loss: 99.6580\n",
      "Epoch 1069/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 98.5230 - val_loss: 99.2535\n",
      "Epoch 1070/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 97.2125 - val_loss: 96.7950\n",
      "Epoch 1071/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.2387 - val_loss: 98.2731\n",
      "Epoch 1072/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 98.0095 - val_loss: 97.1370\n",
      "Epoch 1073/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.9846 - val_loss: 96.9418\n",
      "Epoch 1074/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.6203 - val_loss: 98.4870\n",
      "Epoch 1075/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.1823 - val_loss: 99.0100\n",
      "Epoch 1076/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.5069 - val_loss: 99.2774\n",
      "Epoch 1077/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.8781 - val_loss: 99.1926\n",
      "Epoch 1078/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.0901 - val_loss: 99.3688\n",
      "Epoch 1079/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.8596 - val_loss: 98.8827\n",
      "Epoch 1080/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.5528 - val_loss: 99.2073\n",
      "Epoch 1081/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.3762 - val_loss: 98.8885\n",
      "Epoch 1082/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 98.3240 - val_loss: 98.8663\n",
      "Epoch 1083/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 98.4708 - val_loss: 98.8960\n",
      "Epoch 1084/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.6000 - val_loss: 99.2868\n",
      "Epoch 1085/10000\n",
      "96/96 [==============================] - 0s 155us/step - loss: 97.6259 - val_loss: 99.6503\n",
      "Epoch 1086/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 98.2446 - val_loss: 99.6400\n",
      "Epoch 1087/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 97.8447 - val_loss: 98.4993\n",
      "Epoch 1088/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 97.5339 - val_loss: 97.7886\n",
      "Epoch 1089/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 96.5878 - val_loss: 98.0617\n",
      "Epoch 1090/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 97.8284 - val_loss: 98.9500\n",
      "Epoch 1091/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 98.1511 - val_loss: 98.0616\n",
      "Epoch 1092/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 97.4361 - val_loss: 97.4370\n",
      "Epoch 1093/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 100.3211 - val_loss: 98.7926\n",
      "Epoch 1094/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 98.7333 - val_loss: 98.2902\n",
      "Epoch 1095/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 98.5291 - val_loss: 100.1650\n",
      "Epoch 1096/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 96.9158 - val_loss: 98.5826\n",
      "Epoch 1097/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 97.8949 - val_loss: 98.2968\n",
      "Epoch 1098/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.9499 - val_loss: 99.0860\n",
      "Epoch 1099/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.9022 - val_loss: 99.3052\n",
      "Epoch 1100/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.7459 - val_loss: 98.9133\n",
      "Epoch 1101/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 97.7157 - val_loss: 100.2977\n",
      "Epoch 1102/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 98.1964 - val_loss: 99.2492\n",
      "Epoch 1103/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 96.4515 - val_loss: 97.6030\n",
      "Epoch 1104/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 97.7660 - val_loss: 96.5930\n",
      "Epoch 1105/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 99.8076 - val_loss: 96.8344\n",
      "Epoch 1106/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 97.7586 - val_loss: 97.5004\n",
      "Epoch 1107/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 98.0155 - val_loss: 98.0246\n",
      "Epoch 1108/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 97.4735 - val_loss: 97.7170\n",
      "Epoch 1109/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 96.7095 - val_loss: 98.9008\n",
      "Epoch 1110/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 98.2364 - val_loss: 98.7025\n",
      "Epoch 1111/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 96.7541 - val_loss: 98.8016\n",
      "Epoch 1112/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 98.2909 - val_loss: 98.9622\n",
      "Epoch 1113/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 97.3831 - val_loss: 98.9845\n",
      "Epoch 1114/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 97.3895 - val_loss: 98.0895\n",
      "Epoch 1115/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 99.7954 - val_loss: 97.9993\n",
      "Epoch 1116/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 97.3215 - val_loss: 98.3647\n",
      "Epoch 1117/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 97.1177 - val_loss: 97.9708\n",
      "Epoch 1118/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 98.5912 - val_loss: 98.3956\n",
      "Epoch 1119/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 96.6335 - val_loss: 99.3885\n",
      "Epoch 1120/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 95.6445 - val_loss: 98.9617\n",
      "Epoch 1121/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.9360 - val_loss: 98.7400\n",
      "Epoch 1122/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 97.9939 - val_loss: 98.8690\n",
      "Epoch 1123/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 98.7382 - val_loss: 98.1874\n",
      "Epoch 1124/10000\n",
      "96/96 [==============================] - 0s 150us/step - loss: 97.7005 - val_loss: 98.4713\n",
      "Epoch 1125/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 97.9041 - val_loss: 98.6845\n",
      "Epoch 1126/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 96.6514 - val_loss: 98.6756\n",
      "Epoch 1127/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 97.2028 - val_loss: 99.4018\n",
      "Epoch 1128/10000\n",
      "96/96 [==============================] - 0s 148us/step - loss: 98.0863 - val_loss: 99.0202\n",
      "Epoch 1129/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 98.9723 - val_loss: 99.2133\n",
      "Epoch 1130/10000\n",
      "96/96 [==============================] - 0s 153us/step - loss: 97.6863 - val_loss: 98.5543\n",
      "Epoch 1131/10000\n",
      "96/96 [==============================] - 0s 147us/step - loss: 98.4469 - val_loss: 98.3143\n",
      "Epoch 1132/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 98.7303 - val_loss: 98.2090\n",
      "Epoch 1133/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 97.3092 - val_loss: 99.1074\n",
      "Epoch 1134/10000\n",
      "96/96 [==============================] - 0s 150us/step - loss: 96.7643 - val_loss: 98.4287\n",
      "Epoch 1135/10000\n",
      "96/96 [==============================] - 0s 150us/step - loss: 97.2678 - val_loss: 98.7324\n",
      "Epoch 1136/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 96.8855 - val_loss: 99.4534\n",
      "Epoch 1137/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 97.1709 - val_loss: 98.8962\n",
      "Epoch 1138/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 97.3621 - val_loss: 98.7699\n",
      "Epoch 1139/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 97.5606 - val_loss: 99.5889\n",
      "Epoch 1140/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 98.4823 - val_loss: 98.4167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1141/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 97.7781 - val_loss: 99.4773\n",
      "Epoch 1142/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 97.9305 - val_loss: 98.9723\n",
      "Epoch 1143/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 97.9566 - val_loss: 99.3740\n",
      "Epoch 1144/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 97.7472 - val_loss: 98.4506\n",
      "Epoch 1145/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 96.4049 - val_loss: 98.5105\n",
      "Epoch 1146/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 99.0096 - val_loss: 99.1831\n",
      "Epoch 1147/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 97.2865 - val_loss: 98.7619\n",
      "Epoch 1148/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 98.6249 - val_loss: 99.1134\n",
      "Epoch 1149/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 96.5726 - val_loss: 98.7211\n",
      "Epoch 1150/10000\n",
      "96/96 [==============================] - 0s 163us/step - loss: 97.4404 - val_loss: 98.4493\n",
      "Epoch 1151/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 98.0795 - val_loss: 97.8121\n",
      "Epoch 1152/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 97.2680 - val_loss: 97.8496\n",
      "Epoch 1153/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 97.0625 - val_loss: 98.8265\n",
      "Epoch 1154/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 98.2132 - val_loss: 99.5331\n",
      "Epoch 1155/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 98.4224 - val_loss: 99.2082\n",
      "Epoch 1156/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 97.2972 - val_loss: 98.8924\n",
      "Epoch 1157/10000\n",
      "96/96 [==============================] - 0s 146us/step - loss: 97.9892 - val_loss: 98.1127\n",
      "Epoch 1158/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 99.0100 - val_loss: 97.7024\n",
      "Epoch 1159/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 97.5761 - val_loss: 98.4015\n",
      "Epoch 1160/10000\n",
      "96/96 [==============================] - 0s 175us/step - loss: 97.1182 - val_loss: 98.6116\n",
      "Epoch 1161/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 98.0537 - val_loss: 99.5541\n",
      "Epoch 1162/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 96.0355 - val_loss: 98.8205\n",
      "Epoch 1163/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.3982 - val_loss: 98.6111\n",
      "Epoch 1164/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.2486 - val_loss: 99.2629\n",
      "Epoch 1165/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 97.3874 - val_loss: 99.0648\n",
      "Epoch 1166/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.4041 - val_loss: 99.1731\n",
      "Epoch 1167/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 97.4929 - val_loss: 99.2046\n",
      "Epoch 1168/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.3324 - val_loss: 98.8837\n",
      "Epoch 1169/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 97.4078 - val_loss: 97.5805\n",
      "Epoch 1170/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 96.6205 - val_loss: 98.6000\n",
      "Epoch 1171/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 96.7694 - val_loss: 98.3723\n",
      "Epoch 1172/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 96.8663 - val_loss: 98.0550\n",
      "Epoch 1173/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 99.1169 - val_loss: 99.0355\n",
      "Epoch 1174/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 96.7789 - val_loss: 98.8367\n",
      "Epoch 1175/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.0131 - val_loss: 98.8271\n",
      "Epoch 1176/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 98.3028 - val_loss: 99.3119\n",
      "Epoch 1177/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 97.7364 - val_loss: 97.6147\n",
      "Epoch 1178/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 97.4731 - val_loss: 99.6550\n",
      "Epoch 1179/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.1299 - val_loss: 97.5404\n",
      "Epoch 1180/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 97.3195 - val_loss: 98.5923\n",
      "Epoch 1181/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.7856 - val_loss: 98.2268\n",
      "Epoch 1182/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 96.8818 - val_loss: 97.9820\n",
      "Epoch 1183/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 98.2957 - val_loss: 97.9802\n",
      "Epoch 1184/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 97.7461 - val_loss: 98.4936\n",
      "Epoch 1185/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.6902 - val_loss: 98.8178\n",
      "Epoch 1186/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 96.9483 - val_loss: 99.1219\n",
      "Epoch 1187/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 95.7605 - val_loss: 98.6483\n",
      "Epoch 1188/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 97.7563 - val_loss: 99.3292\n",
      "Epoch 1189/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 98.1535 - val_loss: 98.6300\n",
      "Epoch 1190/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 99.1839 - val_loss: 98.2561\n",
      "Epoch 1191/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 97.5571 - val_loss: 98.1422\n",
      "Epoch 1192/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 98.3343 - val_loss: 97.9658\n",
      "Epoch 1193/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 97.2743 - val_loss: 98.0316\n",
      "Epoch 1194/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 97.6406 - val_loss: 98.2376\n",
      "Epoch 1195/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 97.6259 - val_loss: 98.5014\n",
      "Epoch 1196/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 96.7277 - val_loss: 98.6682\n",
      "Epoch 1197/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 97.1746 - val_loss: 96.0193\n",
      "Epoch 1198/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 97.4751 - val_loss: 98.1824\n",
      "Epoch 1199/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 96.3729 - val_loss: 96.4958\n",
      "Epoch 1200/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 97.4612 - val_loss: 98.7513\n",
      "Epoch 1201/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 97.1423 - val_loss: 98.5079\n",
      "Epoch 1202/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 96.7195 - val_loss: 97.9081\n",
      "Epoch 1203/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 98.6528 - val_loss: 98.5051\n",
      "Epoch 1204/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 97.0286 - val_loss: 98.8429\n",
      "Epoch 1205/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 97.7384 - val_loss: 98.3624\n",
      "Epoch 1206/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 97.1989 - val_loss: 99.6679\n",
      "Epoch 1207/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 98.8634 - val_loss: 98.8533\n",
      "Epoch 1208/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.1921 - val_loss: 97.9681\n",
      "Epoch 1209/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 97.0850 - val_loss: 98.7874\n",
      "Epoch 1210/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.8369 - val_loss: 99.3338\n",
      "Epoch 1211/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 98.3282 - val_loss: 99.5069\n",
      "Epoch 1212/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 98.1101 - val_loss: 99.2153\n",
      "Epoch 1213/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 98.3141 - val_loss: 98.2993\n",
      "Epoch 1214/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 97.8529 - val_loss: 98.5953\n",
      "Epoch 1215/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 97.9696 - val_loss: 98.6720\n",
      "Epoch 1216/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 98.9774 - val_loss: 97.5506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1217/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.3693 - val_loss: 98.2893\n",
      "Epoch 1218/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 97.5482 - val_loss: 98.5838\n",
      "Epoch 1219/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 98.2615 - val_loss: 99.6019\n",
      "Epoch 1220/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 97.7768 - val_loss: 98.9105\n",
      "Epoch 1221/10000\n",
      "96/96 [==============================] - ETA: 0s - loss: 101.165 - 0s 126us/step - loss: 98.6703 - val_loss: 98.5737\n",
      "Epoch 1222/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 97.7178 - val_loss: 98.7054\n",
      "Epoch 1223/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 96.8006 - val_loss: 99.3990\n",
      "Epoch 1224/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 96.9275 - val_loss: 98.3250\n",
      "Epoch 1225/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 97.8225 - val_loss: 97.9511\n",
      "Epoch 1226/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 99.7923 - val_loss: 98.4607\n",
      "Epoch 1227/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 98.5161 - val_loss: 98.9890\n",
      "Epoch 1228/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 97.3946 - val_loss: 98.4640\n",
      "Epoch 1229/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 97.5860 - val_loss: 97.4852\n",
      "Epoch 1230/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 96.2744 - val_loss: 97.4172\n",
      "Epoch 1231/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.6681 - val_loss: 98.1170\n",
      "Epoch 1232/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.5222 - val_loss: 98.0911\n",
      "Epoch 1233/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 97.3120 - val_loss: 98.6336\n",
      "Epoch 1234/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.2447 - val_loss: 99.0869\n",
      "Epoch 1235/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.9255 - val_loss: 99.4902\n",
      "Epoch 1236/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.0114 - val_loss: 98.7834\n",
      "Epoch 1237/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 97.7283 - val_loss: 97.7384\n",
      "Epoch 1238/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 97.8445 - val_loss: 97.4484\n",
      "Epoch 1239/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 99.4108 - val_loss: 97.6906\n",
      "Epoch 1240/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.9737 - val_loss: 97.8831\n",
      "Epoch 1241/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.5797 - val_loss: 99.2429\n",
      "Epoch 1242/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 96.9601 - val_loss: 98.9557\n",
      "Epoch 1243/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.1617 - val_loss: 98.9398\n",
      "Epoch 1244/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 98.3744 - val_loss: 98.7341\n",
      "Epoch 1245/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 96.5215 - val_loss: 98.8383\n",
      "Epoch 1246/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 96.5977 - val_loss: 97.9444\n",
      "Epoch 1247/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.1864 - val_loss: 98.0988\n",
      "Epoch 1248/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.9514 - val_loss: 98.3151\n",
      "Epoch 1249/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 97.5147 - val_loss: 99.1295\n",
      "Epoch 1250/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.4782 - val_loss: 98.7629\n",
      "Epoch 1251/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.0995 - val_loss: 99.4404\n",
      "Epoch 1252/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.1812 - val_loss: 99.0837\n",
      "Epoch 1253/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.8649 - val_loss: 98.4451\n",
      "Epoch 1254/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.9221 - val_loss: 97.4751\n",
      "Epoch 1255/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 98.0485 - val_loss: 97.3881\n",
      "Epoch 1256/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 97.8813 - val_loss: 97.7443\n",
      "Epoch 1257/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.4029 - val_loss: 97.8174\n",
      "Epoch 1258/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.2644 - val_loss: 98.9815\n",
      "Epoch 1259/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 100.5378 - val_loss: 99.2716\n",
      "Epoch 1260/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.8719 - val_loss: 98.7156\n",
      "Epoch 1261/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.8915 - val_loss: 99.4213\n",
      "Epoch 1262/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 96.5759 - val_loss: 99.4015\n",
      "Epoch 1263/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.7663 - val_loss: 99.3351\n",
      "Epoch 1264/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.1541 - val_loss: 98.0497\n",
      "Epoch 1265/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 97.2937 - val_loss: 97.8939\n",
      "Epoch 1266/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 99.3896 - val_loss: 98.5991\n",
      "Epoch 1267/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.0508 - val_loss: 97.6346\n",
      "Epoch 1268/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 97.3170 - val_loss: 98.2624\n",
      "Epoch 1269/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 96.7062 - val_loss: 98.3731\n",
      "Epoch 1270/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.9188 - val_loss: 98.6581\n",
      "Epoch 1271/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 97.7667 - val_loss: 98.8831\n",
      "Epoch 1272/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.7011 - val_loss: 98.0093\n",
      "Epoch 1273/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.4635 - val_loss: 98.1026\n",
      "Epoch 1274/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 97.3878 - val_loss: 98.0173\n",
      "Epoch 1275/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 97.8476 - val_loss: 97.8987\n",
      "Epoch 1276/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.3026 - val_loss: 97.3960\n",
      "Epoch 1277/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.1246 - val_loss: 98.5902\n",
      "Epoch 1278/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.2579 - val_loss: 99.0442\n",
      "Epoch 1279/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.1959 - val_loss: 98.3792\n",
      "Epoch 1280/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.5371 - val_loss: 98.0074\n",
      "Epoch 1281/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 97.5437 - val_loss: 97.2969\n",
      "Epoch 1282/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.9566 - val_loss: 97.2533\n",
      "Epoch 1283/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.6370 - val_loss: 98.6487\n",
      "Epoch 1284/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.7076 - val_loss: 98.3851\n",
      "Epoch 1285/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.8384 - val_loss: 98.2250\n",
      "Epoch 1286/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.8709 - val_loss: 99.2884\n",
      "Epoch 1287/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.6177 - val_loss: 99.2468\n",
      "Epoch 1288/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.2317 - val_loss: 99.5721\n",
      "Epoch 1289/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 97.4187 - val_loss: 97.9970\n",
      "Epoch 1290/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.0585 - val_loss: 98.5318\n",
      "Epoch 1291/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 100.5161 - val_loss: 98.4262\n",
      "Epoch 1292/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 123us/step - loss: 98.1349 - val_loss: 98.7666\n",
      "Epoch 1293/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.1353 - val_loss: 98.4796\n",
      "Epoch 1294/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.8555 - val_loss: 98.1557\n",
      "Epoch 1295/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.8688 - val_loss: 97.5392\n",
      "Epoch 1296/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.7845 - val_loss: 97.3452\n",
      "Epoch 1297/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 97.9494 - val_loss: 98.3200\n",
      "Epoch 1298/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 97.2523 - val_loss: 99.1712\n",
      "Epoch 1299/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 99.2558 - val_loss: 99.1821\n",
      "Epoch 1300/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.3854 - val_loss: 99.5750\n",
      "Epoch 1301/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.5430 - val_loss: 99.2862\n",
      "Epoch 1302/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.8324 - val_loss: 98.1913\n",
      "Epoch 1303/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 96.4560 - val_loss: 98.5465\n",
      "Epoch 1304/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 96.5449 - val_loss: 98.3662\n",
      "Epoch 1305/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.4815 - val_loss: 98.0442\n",
      "Epoch 1306/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 97.3562 - val_loss: 98.4393\n",
      "Epoch 1307/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.4324 - val_loss: 97.9379\n",
      "Epoch 1308/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 98.1178 - val_loss: 98.4879\n",
      "Epoch 1309/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.0562 - val_loss: 98.4655\n",
      "Epoch 1310/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.9142 - val_loss: 98.8233\n",
      "Epoch 1311/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 99.0530 - val_loss: 98.7588\n",
      "Epoch 1312/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.3352 - val_loss: 98.5522\n",
      "Epoch 1313/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.7385 - val_loss: 98.4948\n",
      "Epoch 1314/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 96.8720 - val_loss: 98.4498\n",
      "Epoch 1315/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.0016 - val_loss: 98.4173\n",
      "Epoch 1316/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.9894 - val_loss: 97.9486\n",
      "Epoch 1317/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.6237 - val_loss: 98.5016\n",
      "Epoch 1318/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.3482 - val_loss: 97.0782\n",
      "Epoch 1319/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.5367 - val_loss: 98.5628\n",
      "Epoch 1320/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.2095 - val_loss: 99.2994\n",
      "Epoch 1321/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.9136 - val_loss: 98.2271\n",
      "Epoch 1322/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.7092 - val_loss: 97.5535\n",
      "Epoch 1323/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.9904 - val_loss: 98.3132\n",
      "Epoch 1324/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.3163 - val_loss: 98.7250\n",
      "Epoch 1325/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 96.4071 - val_loss: 98.2995\n",
      "Epoch 1326/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.8263 - val_loss: 98.2601\n",
      "Epoch 1327/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.2596 - val_loss: 99.1750\n",
      "Epoch 1328/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.1174 - val_loss: 98.0965\n",
      "Epoch 1329/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 96.3678 - val_loss: 98.7238\n",
      "Epoch 1330/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.8504 - val_loss: 97.6011\n",
      "Epoch 1331/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.9172 - val_loss: 97.9313\n",
      "Epoch 1332/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.2053 - val_loss: 98.1560\n",
      "Epoch 1333/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 97.2316 - val_loss: 98.0073\n",
      "Epoch 1334/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 97.2209 - val_loss: 98.3024\n",
      "Epoch 1335/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.4588 - val_loss: 99.5764\n",
      "Epoch 1336/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.9380 - val_loss: 98.7663\n",
      "Epoch 1337/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.4818 - val_loss: 99.2781\n",
      "Epoch 1338/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.7616 - val_loss: 99.1494\n",
      "Epoch 1339/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 98.5851 - val_loss: 99.4862\n",
      "Epoch 1340/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.5622 - val_loss: 98.9360\n",
      "Epoch 1341/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.2300 - val_loss: 98.0423\n",
      "Epoch 1342/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.7925 - val_loss: 98.4538\n",
      "Epoch 1343/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.5316 - val_loss: 98.4617\n",
      "Epoch 1344/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.5570 - val_loss: 99.1408\n",
      "Epoch 1345/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.7529 - val_loss: 98.7072\n",
      "Epoch 1346/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 97.7412 - val_loss: 98.2018\n",
      "Epoch 1347/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.9098 - val_loss: 98.1157\n",
      "Epoch 1348/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.1545 - val_loss: 98.5873\n",
      "Epoch 1349/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.2062 - val_loss: 98.7585\n",
      "Epoch 1350/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.2800 - val_loss: 98.5440\n",
      "Epoch 1351/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 98.1264 - val_loss: 97.4242\n",
      "Epoch 1352/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.1987 - val_loss: 98.0509\n",
      "Epoch 1353/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.1550 - val_loss: 98.7251\n",
      "Epoch 1354/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 97.5984 - val_loss: 98.9408\n",
      "Epoch 1355/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.0121 - val_loss: 99.1777\n",
      "Epoch 1356/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.4439 - val_loss: 99.8184\n",
      "Epoch 1357/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.8423 - val_loss: 100.1284\n",
      "Epoch 1358/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.4099 - val_loss: 98.8597\n",
      "Epoch 1359/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 98.2052 - val_loss: 97.3971\n",
      "Epoch 1360/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 102.1241 - val_loss: 97.2508\n",
      "Epoch 1361/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.9706 - val_loss: 97.2169\n",
      "Epoch 1362/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.1213 - val_loss: 98.2057\n",
      "Epoch 1363/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.5891 - val_loss: 99.1474\n",
      "Epoch 1364/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.9099 - val_loss: 98.9318\n",
      "Epoch 1365/10000\n",
      "96/96 [==============================] - 0s 152us/step - loss: 97.6188 - val_loss: 98.6008\n",
      "Epoch 1366/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 97.4747 - val_loss: 98.5713\n",
      "Epoch 1367/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 96.6121 - val_loss: 98.4519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1368/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 96.1708 - val_loss: 98.6215\n",
      "Epoch 1369/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 96.5742 - val_loss: 98.5055\n",
      "Epoch 1370/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 98.5476 - val_loss: 99.1563\n",
      "Epoch 1371/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 97.1283 - val_loss: 99.6264\n",
      "Epoch 1372/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 97.5947 - val_loss: 99.0675\n",
      "Epoch 1373/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 96.5290 - val_loss: 98.0810\n",
      "Epoch 1374/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 96.6875 - val_loss: 97.9994\n",
      "Epoch 1375/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 96.3366 - val_loss: 98.0630\n",
      "Epoch 1376/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 96.9176 - val_loss: 98.5521\n",
      "Epoch 1377/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 97.4456 - val_loss: 97.8885\n",
      "Epoch 1378/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 97.4738 - val_loss: 99.4748\n",
      "Epoch 1379/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 98.1175 - val_loss: 99.6472\n",
      "Epoch 1380/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 98.5545 - val_loss: 99.7758\n",
      "Epoch 1381/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 96.4840 - val_loss: 98.9265\n",
      "Epoch 1382/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 97.2836 - val_loss: 97.7688\n",
      "Epoch 1383/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 98.2180 - val_loss: 98.4909\n",
      "Epoch 1384/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 98.9655 - val_loss: 98.4254\n",
      "Epoch 1385/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 97.2251 - val_loss: 98.0312\n",
      "Epoch 1386/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 99.6694 - val_loss: 99.2565\n",
      "Epoch 1387/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 96.5156 - val_loss: 99.0485\n",
      "Epoch 1388/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 97.3461 - val_loss: 98.9889\n",
      "Epoch 1389/10000\n",
      "96/96 [==============================] - 0s 147us/step - loss: 97.3946 - val_loss: 99.0853\n",
      "Epoch 1390/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 96.9713 - val_loss: 98.0473\n",
      "Epoch 1391/10000\n",
      "96/96 [==============================] - 0s 146us/step - loss: 97.6229 - val_loss: 97.1985\n",
      "Epoch 1392/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 98.8403 - val_loss: 97.4091\n",
      "Epoch 1393/10000\n",
      "96/96 [==============================] - 0s 147us/step - loss: 98.1209 - val_loss: 97.6840\n",
      "Epoch 1394/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 96.1460 - val_loss: 98.4456\n",
      "Epoch 1395/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 97.4517 - val_loss: 99.2064\n",
      "Epoch 1396/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 97.8618 - val_loss: 99.4045\n",
      "Epoch 1397/10000\n",
      "96/96 [==============================] - 0s 152us/step - loss: 97.4058 - val_loss: 99.0481\n",
      "Epoch 1398/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 97.4650 - val_loss: 97.9887\n",
      "Epoch 1399/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 98.3098 - val_loss: 98.0652\n",
      "Epoch 1400/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 97.9095 - val_loss: 97.6709\n",
      "Epoch 1401/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 98.0112 - val_loss: 97.9896\n",
      "Epoch 1402/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 99.3985 - val_loss: 97.4891\n",
      "Epoch 1403/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 95.8771 - val_loss: 98.2159\n",
      "Epoch 1404/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 97.2945 - val_loss: 97.7833\n",
      "Epoch 1405/10000\n",
      "96/96 [==============================] - 0s 146us/step - loss: 98.4167 - val_loss: 98.2949\n",
      "Epoch 1406/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 97.4199 - val_loss: 99.0036\n",
      "Epoch 1407/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 97.8530 - val_loss: 98.3517\n",
      "Epoch 1408/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 99.4670 - val_loss: 98.5563\n",
      "Epoch 1409/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 97.8873 - val_loss: 98.8893\n",
      "Epoch 1410/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 96.1247 - val_loss: 98.8555\n",
      "Epoch 1411/10000\n",
      "96/96 [==============================] - 0s 147us/step - loss: 97.9621 - val_loss: 98.0127\n",
      "Epoch 1412/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 96.6498 - val_loss: 98.1528\n",
      "Epoch 1413/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 98.4265 - val_loss: 97.6093\n",
      "Epoch 1414/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 96.5684 - val_loss: 98.0424\n",
      "Epoch 1415/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 96.4514 - val_loss: 97.6304\n",
      "Epoch 1416/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 98.3499 - val_loss: 98.0395\n",
      "Epoch 1417/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 97.1341 - val_loss: 97.6590\n",
      "Epoch 1418/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 97.9348 - val_loss: 98.5920\n",
      "Epoch 1419/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 97.7677 - val_loss: 98.9540\n",
      "Epoch 1420/10000\n",
      "96/96 [==============================] - 0s 149us/step - loss: 97.7246 - val_loss: 98.2004\n",
      "Epoch 1421/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 97.6428 - val_loss: 98.3216\n",
      "Epoch 1422/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 97.5175 - val_loss: 97.6133\n",
      "Epoch 1423/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 97.8540 - val_loss: 97.4438\n",
      "Epoch 1424/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 97.1269 - val_loss: 98.1508\n",
      "Epoch 1425/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 95.9955 - val_loss: 97.8297\n",
      "Epoch 1426/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.0991 - val_loss: 97.8387\n",
      "Epoch 1427/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 98.0213 - val_loss: 98.0270\n",
      "Epoch 1428/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.3163 - val_loss: 98.3475\n",
      "Epoch 1429/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 96.5541 - val_loss: 99.0835\n",
      "Epoch 1430/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 96.9669 - val_loss: 99.1389\n",
      "Epoch 1431/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 97.0538 - val_loss: 98.5521\n",
      "Epoch 1432/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 97.9599 - val_loss: 98.3404\n",
      "Epoch 1433/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 97.5673 - val_loss: 98.3409\n",
      "Epoch 1434/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 97.9567 - val_loss: 98.6614\n",
      "Epoch 1435/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.1972 - val_loss: 98.4564\n",
      "Epoch 1436/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.6558 - val_loss: 99.0403\n",
      "Epoch 1437/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 97.5742 - val_loss: 98.3482\n",
      "Epoch 1438/10000\n",
      "96/96 [==============================] - 0s 257us/step - loss: 96.4942 - val_loss: 97.9370\n",
      "Epoch 1439/10000\n",
      "96/96 [==============================] - 0s 276us/step - loss: 97.9603 - val_loss: 98.5630\n",
      "Epoch 1440/10000\n",
      "96/96 [==============================] - 0s 228us/step - loss: 97.5521 - val_loss: 98.8113\n",
      "Epoch 1441/10000\n",
      "96/96 [==============================] - 0s 198us/step - loss: 98.0528 - val_loss: 99.2690\n",
      "Epoch 1442/10000\n",
      "96/96 [==============================] - 0s 236us/step - loss: 98.3611 - val_loss: 99.0335\n",
      "Epoch 1443/10000\n",
      "96/96 [==============================] - 0s 236us/step - loss: 97.6335 - val_loss: 98.2042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1444/10000\n",
      "96/96 [==============================] - 0s 149us/step - loss: 96.8566 - val_loss: 98.3302\n",
      "Epoch 1445/10000\n",
      "96/96 [==============================] - 0s 173us/step - loss: 96.6183 - val_loss: 98.4322\n",
      "Epoch 1446/10000\n",
      "96/96 [==============================] - 0s 164us/step - loss: 96.6952 - val_loss: 97.7845\n",
      "Epoch 1447/10000\n",
      "96/96 [==============================] - 0s 155us/step - loss: 97.9742 - val_loss: 97.5320\n",
      "Epoch 1448/10000\n",
      "96/96 [==============================] - 0s 160us/step - loss: 96.1805 - val_loss: 98.0382\n",
      "Epoch 1449/10000\n",
      "96/96 [==============================] - 0s 146us/step - loss: 98.1032 - val_loss: 97.9848\n",
      "Epoch 1450/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 98.1695 - val_loss: 98.6489\n",
      "Epoch 1451/10000\n",
      "96/96 [==============================] - 0s 172us/step - loss: 97.5358 - val_loss: 98.3004\n",
      "Epoch 1452/10000\n",
      "96/96 [==============================] - 0s 158us/step - loss: 97.8979 - val_loss: 99.0871\n",
      "Epoch 1453/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 97.1651 - val_loss: 99.2173\n",
      "Epoch 1454/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 95.8239 - val_loss: 98.9193\n",
      "Epoch 1455/10000\n",
      "96/96 [==============================] - 0s 153us/step - loss: 97.4269 - val_loss: 97.8883\n",
      "Epoch 1456/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 95.7474 - val_loss: 98.0068\n",
      "Epoch 1457/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 97.0100 - val_loss: 97.7058\n",
      "Epoch 1458/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 97.8420 - val_loss: 98.2099\n",
      "Epoch 1459/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 97.4144 - val_loss: 98.0992\n",
      "Epoch 1460/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 96.7836 - val_loss: 98.7211\n",
      "Epoch 1461/10000\n",
      "96/96 [==============================] - 0s 162us/step - loss: 96.6606 - val_loss: 97.3489\n",
      "Epoch 1462/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 98.4218 - val_loss: 97.1985\n",
      "Epoch 1463/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 98.8622 - val_loss: 97.5007\n",
      "Epoch 1464/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 97.8137 - val_loss: 98.3480\n",
      "Epoch 1465/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 96.4444 - val_loss: 98.1096\n",
      "Epoch 1466/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 95.8508 - val_loss: 99.0390\n",
      "Epoch 1467/10000\n",
      "96/96 [==============================] - 0s 146us/step - loss: 97.9763 - val_loss: 99.0760\n",
      "Epoch 1468/10000\n",
      "96/96 [==============================] - 0s 148us/step - loss: 97.1353 - val_loss: 98.8244\n",
      "Epoch 1469/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 96.7626 - val_loss: 98.1567\n",
      "Epoch 1470/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 96.3471 - val_loss: 98.4714\n",
      "Epoch 1471/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 96.6244 - val_loss: 98.3660\n",
      "Epoch 1472/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 96.4193 - val_loss: 98.3481\n",
      "Epoch 1473/10000\n",
      "96/96 [==============================] - 0s 146us/step - loss: 97.1284 - val_loss: 97.7099\n",
      "Epoch 1474/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 97.1575 - val_loss: 98.2248\n",
      "Epoch 1475/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 98.6508 - val_loss: 96.3901\n",
      "Epoch 1476/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 97.6235 - val_loss: 96.5740\n",
      "Epoch 1477/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 97.3193 - val_loss: 98.5830\n",
      "Epoch 1478/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 96.4906 - val_loss: 97.7156\n",
      "Epoch 1479/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 97.0547 - val_loss: 97.7299\n",
      "Epoch 1480/10000\n",
      "96/96 [==============================] - 0s 149us/step - loss: 97.2072 - val_loss: 97.8227\n",
      "Epoch 1481/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 97.4212 - val_loss: 97.5778\n",
      "Epoch 1482/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 96.2242 - val_loss: 97.8101\n",
      "Epoch 1483/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 98.0494 - val_loss: 98.1686\n",
      "Epoch 1484/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 98.1167 - val_loss: 98.3865\n",
      "Epoch 1485/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 98.4993 - val_loss: 98.5756\n",
      "Epoch 1486/10000\n",
      "96/96 [==============================] - 0s 147us/step - loss: 98.5728 - val_loss: 98.3839\n",
      "Epoch 1487/10000\n",
      "96/96 [==============================] - 0s 165us/step - loss: 96.5669 - val_loss: 98.7964\n",
      "Epoch 1488/10000\n",
      "96/96 [==============================] - 0s 164us/step - loss: 97.0444 - val_loss: 99.0740\n",
      "Epoch 1489/10000\n",
      "96/96 [==============================] - 0s 156us/step - loss: 97.9920 - val_loss: 99.8299\n",
      "Epoch 1490/10000\n",
      "96/96 [==============================] - 0s 152us/step - loss: 96.4840 - val_loss: 99.4251\n",
      "Epoch 1491/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 98.4282 - val_loss: 98.7343\n",
      "Epoch 1492/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 96.3611 - val_loss: 98.2880\n",
      "Epoch 1493/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 96.4869 - val_loss: 97.7511\n",
      "Epoch 1494/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 100.1377 - val_loss: 97.5411\n",
      "Epoch 1495/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.0834 - val_loss: 98.2082\n",
      "Epoch 1496/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 96.3312 - val_loss: 98.0935\n",
      "Epoch 1497/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 97.1885 - val_loss: 97.6176\n",
      "Epoch 1498/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 95.6730 - val_loss: 98.1833\n",
      "Epoch 1499/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 100.2286 - val_loss: 98.5151\n",
      "Epoch 1500/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 97.1344 - val_loss: 98.7988\n",
      "Epoch 1501/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 97.3854 - val_loss: 98.4716\n",
      "Epoch 1502/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.9987 - val_loss: 97.5575\n",
      "Epoch 1503/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 98.2649 - val_loss: 97.5349\n",
      "Epoch 1504/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.1881 - val_loss: 97.5886\n",
      "Epoch 1505/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.1541 - val_loss: 98.0112\n",
      "Epoch 1506/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.4095 - val_loss: 98.1257\n",
      "Epoch 1507/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 95.7161 - val_loss: 99.0215\n",
      "Epoch 1508/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.7094 - val_loss: 98.1467\n",
      "Epoch 1509/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 97.7164 - val_loss: 98.1826\n",
      "Epoch 1510/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 97.7464 - val_loss: 98.7631\n",
      "Epoch 1511/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 98.2470 - val_loss: 98.3575\n",
      "Epoch 1512/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 96.9658 - val_loss: 97.9183\n",
      "Epoch 1513/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.9920 - val_loss: 98.5364\n",
      "Epoch 1514/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 100.9031 - val_loss: 98.0164\n",
      "Epoch 1515/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 97.0306 - val_loss: 98.1767\n",
      "Epoch 1516/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.6253 - val_loss: 97.7428\n",
      "Epoch 1517/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 98.0340 - val_loss: 98.2744\n",
      "Epoch 1518/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.2525 - val_loss: 98.7677\n",
      "Epoch 1519/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.1373 - val_loss: 98.9409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1520/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.7351 - val_loss: 98.3161\n",
      "Epoch 1521/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.1221 - val_loss: 97.6572\n",
      "Epoch 1522/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 96.7979 - val_loss: 97.8655\n",
      "Epoch 1523/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.9063 - val_loss: 98.0930\n",
      "Epoch 1524/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.4157 - val_loss: 98.3704\n",
      "Epoch 1525/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.8937 - val_loss: 98.8669\n",
      "Epoch 1526/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.1465 - val_loss: 98.8013\n",
      "Epoch 1527/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 97.1912 - val_loss: 98.5313\n",
      "Epoch 1528/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.4130 - val_loss: 97.9992\n",
      "Epoch 1529/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.9750 - val_loss: 97.0123\n",
      "Epoch 1530/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.2710 - val_loss: 96.5683\n",
      "Epoch 1531/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.1052 - val_loss: 97.1595\n",
      "Epoch 1532/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.1234 - val_loss: 97.9680\n",
      "Epoch 1533/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 99.4122 - val_loss: 98.1705\n",
      "Epoch 1534/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 97.3801 - val_loss: 98.9017\n",
      "Epoch 1535/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.5412 - val_loss: 98.1699\n",
      "Epoch 1536/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.6768 - val_loss: 98.0525\n",
      "Epoch 1537/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.5201 - val_loss: 97.4207\n",
      "Epoch 1538/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.6814 - val_loss: 97.8327\n",
      "Epoch 1539/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 97.6164 - val_loss: 97.7791\n",
      "Epoch 1540/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.0518 - val_loss: 98.3035\n",
      "Epoch 1541/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 97.3324 - val_loss: 98.9057\n",
      "Epoch 1542/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.2078 - val_loss: 99.3498\n",
      "Epoch 1543/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.5501 - val_loss: 99.4119\n",
      "Epoch 1544/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 98.1291 - val_loss: 99.5516\n",
      "Epoch 1545/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 98.5074 - val_loss: 99.2621\n",
      "Epoch 1546/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.3573 - val_loss: 99.2406\n",
      "Epoch 1547/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.9233 - val_loss: 97.8707\n",
      "Epoch 1548/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.0984 - val_loss: 97.6707\n",
      "Epoch 1549/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.1218 - val_loss: 97.3539\n",
      "Epoch 1550/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 97.2732 - val_loss: 97.1412\n",
      "Epoch 1551/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 97.3361 - val_loss: 97.6236\n",
      "Epoch 1552/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 96.4635 - val_loss: 97.7851\n",
      "Epoch 1553/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 96.7732 - val_loss: 98.3284\n",
      "Epoch 1554/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.7387 - val_loss: 98.5060\n",
      "Epoch 1555/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.8779 - val_loss: 97.6523\n",
      "Epoch 1556/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.3966 - val_loss: 97.3390\n",
      "Epoch 1557/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.1042 - val_loss: 97.5674\n",
      "Epoch 1558/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.9096 - val_loss: 97.9409\n",
      "Epoch 1559/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 96.0551 - val_loss: 97.6829\n",
      "Epoch 1560/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.8044 - val_loss: 97.6528\n",
      "Epoch 1561/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.8877 - val_loss: 99.2462\n",
      "Epoch 1562/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.3909 - val_loss: 99.3032\n",
      "Epoch 1563/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.7529 - val_loss: 98.6906\n",
      "Epoch 1564/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 97.7974 - val_loss: 97.1989\n",
      "Epoch 1565/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.8791 - val_loss: 97.3112\n",
      "Epoch 1566/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.9771 - val_loss: 97.4918\n",
      "Epoch 1567/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.5286 - val_loss: 97.2717\n",
      "Epoch 1568/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.3583 - val_loss: 97.6812\n",
      "Epoch 1569/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.0694 - val_loss: 99.0505\n",
      "Epoch 1570/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.8962 - val_loss: 98.8234\n",
      "Epoch 1571/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.0891 - val_loss: 98.9577\n",
      "Epoch 1572/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.7024 - val_loss: 97.7765\n",
      "Epoch 1573/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.8949 - val_loss: 98.3707\n",
      "Epoch 1574/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.6371 - val_loss: 97.5349\n",
      "Epoch 1575/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 97.2227 - val_loss: 97.4232\n",
      "Epoch 1576/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.7302 - val_loss: 97.6822\n",
      "Epoch 1577/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.8789 - val_loss: 97.8810\n",
      "Epoch 1578/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.5590 - val_loss: 98.4002\n",
      "Epoch 1579/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.4638 - val_loss: 98.3246\n",
      "Epoch 1580/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.8937 - val_loss: 98.7882\n",
      "Epoch 1581/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 97.7595 - val_loss: 98.7656\n",
      "Epoch 1582/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 98.3249 - val_loss: 97.9259\n",
      "Epoch 1583/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.3960 - val_loss: 97.7519\n",
      "Epoch 1584/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.4766 - val_loss: 97.3637\n",
      "Epoch 1585/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.0883 - val_loss: 97.8629\n",
      "Epoch 1586/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.7064 - val_loss: 98.0475\n",
      "Epoch 1587/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 96.2275 - val_loss: 98.1270\n",
      "Epoch 1588/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.7585 - val_loss: 98.5991\n",
      "Epoch 1589/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 95.5591 - val_loss: 98.3418\n",
      "Epoch 1590/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.4722 - val_loss: 98.4333\n",
      "Epoch 1591/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.9634 - val_loss: 98.4838\n",
      "Epoch 1592/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 95.7308 - val_loss: 98.5579\n",
      "Epoch 1593/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 95.8491 - val_loss: 98.5743\n",
      "Epoch 1594/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.0061 - val_loss: 97.9152\n",
      "Epoch 1595/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.1748 - val_loss: 98.1944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1596/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.9202 - val_loss: 98.0058\n",
      "Epoch 1597/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.2537 - val_loss: 98.9450\n",
      "Epoch 1598/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 96.5292 - val_loss: 99.2445\n",
      "Epoch 1599/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.8014 - val_loss: 98.8295\n",
      "Epoch 1600/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.7000 - val_loss: 98.2696\n",
      "Epoch 1601/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.4113 - val_loss: 98.7289\n",
      "Epoch 1602/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.7200 - val_loss: 97.1987\n",
      "Epoch 1603/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.1903 - val_loss: 97.7623\n",
      "Epoch 1604/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.7564 - val_loss: 98.2195\n",
      "Epoch 1605/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 96.4235 - val_loss: 98.3445\n",
      "Epoch 1606/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 98.7259 - val_loss: 98.3852\n",
      "Epoch 1607/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.1702 - val_loss: 99.1374\n",
      "Epoch 1608/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.4884 - val_loss: 98.3717\n",
      "Epoch 1609/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.8830 - val_loss: 97.6948\n",
      "Epoch 1610/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.7973 - val_loss: 96.9556\n",
      "Epoch 1611/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 96.7143 - val_loss: 97.7615\n",
      "Epoch 1612/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 96.4100 - val_loss: 98.1337\n",
      "Epoch 1613/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 96.6690 - val_loss: 98.6963\n",
      "Epoch 1614/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.6282 - val_loss: 98.9128\n",
      "Epoch 1615/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.5754 - val_loss: 98.6207\n",
      "Epoch 1616/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.4962 - val_loss: 98.1932\n",
      "Epoch 1617/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.2012 - val_loss: 97.9000\n",
      "Epoch 1618/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.5208 - val_loss: 97.7371\n",
      "Epoch 1619/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.6030 - val_loss: 97.7911\n",
      "Epoch 1620/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.3185 - val_loss: 98.5817\n",
      "Epoch 1621/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.1482 - val_loss: 97.2301\n",
      "Epoch 1622/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.9237 - val_loss: 98.0246\n",
      "Epoch 1623/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.2796 - val_loss: 98.1112\n",
      "Epoch 1624/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.4743 - val_loss: 97.8128\n",
      "Epoch 1625/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 96.4899 - val_loss: 98.0646\n",
      "Epoch 1626/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.3442 - val_loss: 98.3137\n",
      "Epoch 1627/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.3449 - val_loss: 97.4509\n",
      "Epoch 1628/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.0862 - val_loss: 97.9096\n",
      "Epoch 1629/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 96.2031 - val_loss: 98.5185\n",
      "Epoch 1630/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 96.8511 - val_loss: 99.1075\n",
      "Epoch 1631/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.3854 - val_loss: 98.2118\n",
      "Epoch 1632/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.9469 - val_loss: 98.2099\n",
      "Epoch 1633/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.7401 - val_loss: 97.2501\n",
      "Epoch 1634/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 96.6800 - val_loss: 97.4758\n",
      "Epoch 1635/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.0042 - val_loss: 97.9330\n",
      "Epoch 1636/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.0486 - val_loss: 97.5103\n",
      "Epoch 1637/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 95.9687 - val_loss: 98.4543\n",
      "Epoch 1638/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.8089 - val_loss: 97.3610\n",
      "Epoch 1639/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.2430 - val_loss: 98.3810\n",
      "Epoch 1640/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.6147 - val_loss: 98.0440\n",
      "Epoch 1641/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 97.6714 - val_loss: 98.1572\n",
      "Epoch 1642/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.5089 - val_loss: 96.9492\n",
      "Epoch 1643/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.4072 - val_loss: 97.5456\n",
      "Epoch 1644/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.5955 - val_loss: 98.3947\n",
      "Epoch 1645/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.6182 - val_loss: 98.2223\n",
      "Epoch 1646/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.5613 - val_loss: 97.5942\n",
      "Epoch 1647/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.2424 - val_loss: 96.2762\n",
      "Epoch 1648/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.9678 - val_loss: 97.5112\n",
      "Epoch 1649/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.1258 - val_loss: 97.8748\n",
      "Epoch 1650/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.5071 - val_loss: 97.4156\n",
      "Epoch 1651/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 97.0503 - val_loss: 97.5256\n",
      "Epoch 1652/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.5622 - val_loss: 98.5751\n",
      "Epoch 1653/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.8109 - val_loss: 99.1441\n",
      "Epoch 1654/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.2671 - val_loss: 97.3803\n",
      "Epoch 1655/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 95.5387 - val_loss: 97.9206\n",
      "Epoch 1656/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.2304 - val_loss: 97.1559\n",
      "Epoch 1657/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.8388 - val_loss: 97.5898\n",
      "Epoch 1658/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.7919 - val_loss: 98.2802\n",
      "Epoch 1659/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.6845 - val_loss: 98.2027\n",
      "Epoch 1660/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.4183 - val_loss: 98.7789\n",
      "Epoch 1661/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.2010 - val_loss: 98.7662\n",
      "Epoch 1662/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.5189 - val_loss: 98.8326\n",
      "Epoch 1663/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.9317 - val_loss: 98.5625\n",
      "Epoch 1664/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.1693 - val_loss: 97.3386\n",
      "Epoch 1665/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.8844 - val_loss: 97.0422\n",
      "Epoch 1666/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.3711 - val_loss: 96.8225\n",
      "Epoch 1667/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 98.9790 - val_loss: 97.3699\n",
      "Epoch 1668/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.1195 - val_loss: 97.8901\n",
      "Epoch 1669/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.7688 - val_loss: 98.5964\n",
      "Epoch 1670/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.9532 - val_loss: 98.6491\n",
      "Epoch 1671/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.5563 - val_loss: 98.6689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1672/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.4756 - val_loss: 98.0117\n",
      "Epoch 1673/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 101.1027 - val_loss: 97.6684\n",
      "Epoch 1674/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 100.9494 - val_loss: 97.8213\n",
      "Epoch 1675/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.3102 - val_loss: 97.7988\n",
      "Epoch 1676/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.7085 - val_loss: 98.5194\n",
      "Epoch 1677/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.3972 - val_loss: 98.5745\n",
      "Epoch 1678/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 97.8800 - val_loss: 98.3396\n",
      "Epoch 1679/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.8398 - val_loss: 97.2616\n",
      "Epoch 1680/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.7560 - val_loss: 96.5936\n",
      "Epoch 1681/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.2043 - val_loss: 96.5995\n",
      "Epoch 1682/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.2571 - val_loss: 97.3643\n",
      "Epoch 1683/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.2779 - val_loss: 97.6880\n",
      "Epoch 1684/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.7053 - val_loss: 98.1772\n",
      "Epoch 1685/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.5331 - val_loss: 98.7833\n",
      "Epoch 1686/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.5927 - val_loss: 98.0786\n",
      "Epoch 1687/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.3702 - val_loss: 97.3284\n",
      "Epoch 1688/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 96.6632 - val_loss: 97.5717\n",
      "Epoch 1689/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.1186 - val_loss: 97.2144\n",
      "Epoch 1690/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.0263 - val_loss: 97.4085\n",
      "Epoch 1691/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.9701 - val_loss: 98.0016\n",
      "Epoch 1692/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.4864 - val_loss: 98.2196\n",
      "Epoch 1693/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.3188 - val_loss: 97.1421\n",
      "Epoch 1694/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.8488 - val_loss: 97.9678\n",
      "Epoch 1695/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.7409 - val_loss: 97.7960\n",
      "Epoch 1696/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 95.4939 - val_loss: 96.5858\n",
      "Epoch 1697/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.0521 - val_loss: 97.7370\n",
      "Epoch 1698/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.4109 - val_loss: 98.2169\n",
      "Epoch 1699/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.2738 - val_loss: 98.6551\n",
      "Epoch 1700/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 95.6541 - val_loss: 98.5571\n",
      "Epoch 1701/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.2608 - val_loss: 98.0441\n",
      "Epoch 1702/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.2417 - val_loss: 97.9739\n",
      "Epoch 1703/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 96.7256 - val_loss: 97.8462\n",
      "Epoch 1704/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 96.1086 - val_loss: 97.4085\n",
      "Epoch 1705/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.5912 - val_loss: 96.8909\n",
      "Epoch 1706/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.1762 - val_loss: 96.9899\n",
      "Epoch 1707/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 95.5439 - val_loss: 97.7659\n",
      "Epoch 1708/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.3156 - val_loss: 98.2845\n",
      "Epoch 1709/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.6890 - val_loss: 98.8257\n",
      "Epoch 1710/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 96.3604 - val_loss: 98.2204\n",
      "Epoch 1711/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 96.6169 - val_loss: 97.6464\n",
      "Epoch 1712/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.2507 - val_loss: 97.4595\n",
      "Epoch 1713/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.8749 - val_loss: 97.1348\n",
      "Epoch 1714/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 97.5742 - val_loss: 97.7045\n",
      "Epoch 1715/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 95.9813 - val_loss: 97.7069\n",
      "Epoch 1716/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 96.6379 - val_loss: 97.3592\n",
      "Epoch 1717/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.7587 - val_loss: 97.6467\n",
      "Epoch 1718/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 96.9033 - val_loss: 98.1191\n",
      "Epoch 1719/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.8288 - val_loss: 96.9700\n",
      "Epoch 1720/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 96.2298 - val_loss: 97.6652\n",
      "Epoch 1721/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 98.9281 - val_loss: 97.2988\n",
      "Epoch 1722/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.6707 - val_loss: 97.5595\n",
      "Epoch 1723/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.3049 - val_loss: 98.4513\n",
      "Epoch 1724/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.1985 - val_loss: 97.8293\n",
      "Epoch 1725/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.6308 - val_loss: 97.7747\n",
      "Epoch 1726/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.0425 - val_loss: 97.3664\n",
      "Epoch 1727/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.4026 - val_loss: 97.6384\n",
      "Epoch 1728/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.9440 - val_loss: 97.6970\n",
      "Epoch 1729/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.2799 - val_loss: 97.5862\n",
      "Epoch 1730/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 100.9181 - val_loss: 98.0671\n",
      "Epoch 1731/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.0034 - val_loss: 97.6792\n",
      "Epoch 1732/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.3547 - val_loss: 97.8400\n",
      "Epoch 1733/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.4243 - val_loss: 97.0588\n",
      "Epoch 1734/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.2676 - val_loss: 97.9189\n",
      "Epoch 1735/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.2770 - val_loss: 97.9310\n",
      "Epoch 1736/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.8542 - val_loss: 98.0281\n",
      "Epoch 1737/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.4476 - val_loss: 98.1932\n",
      "Epoch 1738/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.3016 - val_loss: 98.0004\n",
      "Epoch 1739/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.2778 - val_loss: 97.9976\n",
      "Epoch 1740/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 96.3164 - val_loss: 97.6468\n",
      "Epoch 1741/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.6770 - val_loss: 97.9901\n",
      "Epoch 1742/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.1933 - val_loss: 98.0977\n",
      "Epoch 1743/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.9356 - val_loss: 97.1566\n",
      "Epoch 1744/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.4795 - val_loss: 97.1069\n",
      "Epoch 1745/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.4896 - val_loss: 97.5803\n",
      "Epoch 1746/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.6944 - val_loss: 98.2344\n",
      "Epoch 1747/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.0160 - val_loss: 98.2060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1748/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 100.1100 - val_loss: 96.1015\n",
      "Epoch 1749/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.8538 - val_loss: 97.9693\n",
      "Epoch 1750/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.1498 - val_loss: 97.4521\n",
      "Epoch 1751/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.6572 - val_loss: 97.6792\n",
      "Epoch 1752/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.2437 - val_loss: 98.0741\n",
      "Epoch 1753/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 96.2550 - val_loss: 98.0947\n",
      "Epoch 1754/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.4344 - val_loss: 98.4928\n",
      "Epoch 1755/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.8892 - val_loss: 98.1010\n",
      "Epoch 1756/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.8447 - val_loss: 97.2627\n",
      "Epoch 1757/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.1586 - val_loss: 97.5683\n",
      "Epoch 1758/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 96.8538 - val_loss: 97.9917\n",
      "Epoch 1759/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 96.1700 - val_loss: 97.9152\n",
      "Epoch 1760/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.5502 - val_loss: 97.8710\n",
      "Epoch 1761/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.9572 - val_loss: 96.9548\n",
      "Epoch 1762/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.8610 - val_loss: 96.7187\n",
      "Epoch 1763/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 98.2727 - val_loss: 96.5174\n",
      "Epoch 1764/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 96.2741 - val_loss: 97.0083\n",
      "Epoch 1765/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.3867 - val_loss: 97.3895\n",
      "Epoch 1766/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.1675 - val_loss: 98.1019\n",
      "Epoch 1767/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.0533 - val_loss: 98.2917\n",
      "Epoch 1768/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.0120 - val_loss: 96.8985\n",
      "Epoch 1769/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.4407 - val_loss: 97.0779\n",
      "Epoch 1770/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.9172 - val_loss: 97.3982\n",
      "Epoch 1771/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.9683 - val_loss: 97.9831\n",
      "Epoch 1772/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.9464 - val_loss: 98.4809\n",
      "Epoch 1773/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.2988 - val_loss: 98.6084\n",
      "Epoch 1774/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.4123 - val_loss: 98.1916\n",
      "Epoch 1775/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.9725 - val_loss: 97.2302\n",
      "Epoch 1776/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.2675 - val_loss: 96.5722\n",
      "Epoch 1777/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 97.8149 - val_loss: 97.2479\n",
      "Epoch 1778/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 95.6366 - val_loss: 97.6112\n",
      "Epoch 1779/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.7961 - val_loss: 97.8862\n",
      "Epoch 1780/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.9416 - val_loss: 98.1154\n",
      "Epoch 1781/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.7792 - val_loss: 98.2894\n",
      "Epoch 1782/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.4863 - val_loss: 97.9834\n",
      "Epoch 1783/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 99.8926 - val_loss: 98.2188\n",
      "Epoch 1784/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 96.7771 - val_loss: 98.2124\n",
      "Epoch 1785/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.2425 - val_loss: 98.0953\n",
      "Epoch 1786/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.9876 - val_loss: 97.3849\n",
      "Epoch 1787/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.8544 - val_loss: 97.5126\n",
      "Epoch 1788/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.9561 - val_loss: 96.9019\n",
      "Epoch 1789/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.2096 - val_loss: 97.4932\n",
      "Epoch 1790/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.9616 - val_loss: 97.8755\n",
      "Epoch 1791/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.0322 - val_loss: 97.4238\n",
      "Epoch 1792/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.6359 - val_loss: 97.4259\n",
      "Epoch 1793/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.0855 - val_loss: 97.3981\n",
      "Epoch 1794/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 96.1201 - val_loss: 97.0144\n",
      "Epoch 1795/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.4842 - val_loss: 97.0936\n",
      "Epoch 1796/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 97.4453 - val_loss: 97.6189\n",
      "Epoch 1797/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 96.1235 - val_loss: 97.9513\n",
      "Epoch 1798/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.5528 - val_loss: 97.5038\n",
      "Epoch 1799/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 96.7220 - val_loss: 98.8532\n",
      "Epoch 1800/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.8825 - val_loss: 97.4210\n",
      "Epoch 1801/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 95.7198 - val_loss: 96.5953\n",
      "Epoch 1802/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.3587 - val_loss: 94.9715\n",
      "Epoch 1803/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.9447 - val_loss: 94.9723\n",
      "Epoch 1804/10000\n",
      "96/96 [==============================] - ETA: 0s - loss: 94.81 - 0s 133us/step - loss: 98.0185 - val_loss: 95.6025\n",
      "Epoch 1805/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 101.7965 - val_loss: 96.4944\n",
      "Epoch 1806/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.0357 - val_loss: 97.2348\n",
      "Epoch 1807/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.2954 - val_loss: 97.5246\n",
      "Epoch 1808/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.5958 - val_loss: 98.0221\n",
      "Epoch 1809/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 98.5407 - val_loss: 97.6553\n",
      "Epoch 1810/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.9384 - val_loss: 98.0446\n",
      "Epoch 1811/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.7197 - val_loss: 97.9507\n",
      "Epoch 1812/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.1597 - val_loss: 97.9973\n",
      "Epoch 1813/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.6138 - val_loss: 97.7299\n",
      "Epoch 1814/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.0224 - val_loss: 96.6337\n",
      "Epoch 1815/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.4418 - val_loss: 97.0047\n",
      "Epoch 1816/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.2607 - val_loss: 96.1843\n",
      "Epoch 1817/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.4520 - val_loss: 97.2848\n",
      "Epoch 1818/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.6366 - val_loss: 97.5491\n",
      "Epoch 1819/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 95.5896 - val_loss: 97.0961\n",
      "Epoch 1820/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 95.4725 - val_loss: 97.0965\n",
      "Epoch 1821/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 94.8649 - val_loss: 97.7433\n",
      "Epoch 1822/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.3136 - val_loss: 96.9556\n",
      "Epoch 1823/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 117us/step - loss: 96.0933 - val_loss: 97.5773\n",
      "Epoch 1824/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.9876 - val_loss: 97.5712\n",
      "Epoch 1825/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 97.6684 - val_loss: 96.5974\n",
      "Epoch 1826/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.2133 - val_loss: 96.3520\n",
      "Epoch 1827/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.0442 - val_loss: 96.7986\n",
      "Epoch 1828/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 99.3622 - val_loss: 97.6938\n",
      "Epoch 1829/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.6612 - val_loss: 97.6726\n",
      "Epoch 1830/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 99.9315 - val_loss: 97.7527\n",
      "Epoch 1831/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.3785 - val_loss: 97.7160\n",
      "Epoch 1832/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 100.6474 - val_loss: 98.0043\n",
      "Epoch 1833/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.8175 - val_loss: 98.2628\n",
      "Epoch 1834/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.8230 - val_loss: 98.2770\n",
      "Epoch 1835/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.3749 - val_loss: 97.3595\n",
      "Epoch 1836/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.5640 - val_loss: 96.5528\n",
      "Epoch 1837/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 96.2958 - val_loss: 96.5442\n",
      "Epoch 1838/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.5549 - val_loss: 96.8954\n",
      "Epoch 1839/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.0044 - val_loss: 96.7252\n",
      "Epoch 1840/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 96.7105 - val_loss: 97.4713\n",
      "Epoch 1841/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.3882 - val_loss: 97.6117\n",
      "Epoch 1842/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 95.8868 - val_loss: 97.7128\n",
      "Epoch 1843/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 96.2214 - val_loss: 97.0131\n",
      "Epoch 1844/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.8508 - val_loss: 96.8345\n",
      "Epoch 1845/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 96.5834 - val_loss: 97.2018\n",
      "Epoch 1846/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 98.1190 - val_loss: 97.6256\n",
      "Epoch 1847/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 95.6501 - val_loss: 97.9978\n",
      "Epoch 1848/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.2086 - val_loss: 97.7145\n",
      "Epoch 1849/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.8336 - val_loss: 96.8806\n",
      "Epoch 1850/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.4680 - val_loss: 97.3800\n",
      "Epoch 1851/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.1523 - val_loss: 97.5272\n",
      "Epoch 1852/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.2613 - val_loss: 97.9165\n",
      "Epoch 1853/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.0574 - val_loss: 98.2280\n",
      "Epoch 1854/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.4381 - val_loss: 97.9804\n",
      "Epoch 1855/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.6592 - val_loss: 97.9977\n",
      "Epoch 1856/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.5055 - val_loss: 97.5934\n",
      "Epoch 1857/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 97.1777 - val_loss: 96.7915\n",
      "Epoch 1858/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.0298 - val_loss: 96.9910\n",
      "Epoch 1859/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 95.8562 - val_loss: 97.5048\n",
      "Epoch 1860/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.6322 - val_loss: 97.3600\n",
      "Epoch 1861/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 96.1031 - val_loss: 98.0305\n",
      "Epoch 1862/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.7758 - val_loss: 97.2332\n",
      "Epoch 1863/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.4590 - val_loss: 97.4436\n",
      "Epoch 1864/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.4056 - val_loss: 95.7798\n",
      "Epoch 1865/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.5458 - val_loss: 96.6119\n",
      "Epoch 1866/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.3347 - val_loss: 96.9396\n",
      "Epoch 1867/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 95.6970 - val_loss: 97.5186\n",
      "Epoch 1868/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.7142 - val_loss: 98.1128\n",
      "Epoch 1869/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.2810 - val_loss: 98.0587\n",
      "Epoch 1870/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.7802 - val_loss: 97.5135\n",
      "Epoch 1871/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 96.1349 - val_loss: 96.4002\n",
      "Epoch 1872/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.9932 - val_loss: 96.9712\n",
      "Epoch 1873/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.5253 - val_loss: 96.5113\n",
      "Epoch 1874/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 100.0670 - val_loss: 97.2695\n",
      "Epoch 1875/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.5988 - val_loss: 97.5645\n",
      "Epoch 1876/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 96.8167 - val_loss: 97.0403\n",
      "Epoch 1877/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 95.3489 - val_loss: 97.1019\n",
      "Epoch 1878/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 97.1282 - val_loss: 96.6164\n",
      "Epoch 1879/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.5840 - val_loss: 96.9181\n",
      "Epoch 1880/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.4575 - val_loss: 97.0582\n",
      "Epoch 1881/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.6401 - val_loss: 97.3236\n",
      "Epoch 1882/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.6080 - val_loss: 96.9607\n",
      "Epoch 1883/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 95.9736 - val_loss: 97.5060\n",
      "Epoch 1884/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.3874 - val_loss: 97.1909\n",
      "Epoch 1885/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.0286 - val_loss: 97.2655\n",
      "Epoch 1886/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.3580 - val_loss: 97.0455\n",
      "Epoch 1887/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.1558 - val_loss: 96.8978\n",
      "Epoch 1888/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.2514 - val_loss: 96.0066\n",
      "Epoch 1889/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.4154 - val_loss: 97.6443\n",
      "Epoch 1890/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 96.2762 - val_loss: 97.5511\n",
      "Epoch 1891/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.0890 - val_loss: 97.3597\n",
      "Epoch 1892/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.0963 - val_loss: 97.9395\n",
      "Epoch 1893/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.5574 - val_loss: 98.0179\n",
      "Epoch 1894/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.0873 - val_loss: 96.9712\n",
      "Epoch 1895/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.0045 - val_loss: 95.6757\n",
      "Epoch 1896/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.2116 - val_loss: 95.9659\n",
      "Epoch 1897/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.9719 - val_loss: 95.6555\n",
      "Epoch 1898/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.6678 - val_loss: 96.3010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1899/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 96.0586 - val_loss: 96.5580\n",
      "Epoch 1900/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.2730 - val_loss: 97.4941\n",
      "Epoch 1901/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 97.3135 - val_loss: 97.2059\n",
      "Epoch 1902/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.9775 - val_loss: 97.2282\n",
      "Epoch 1903/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.8615 - val_loss: 96.2801\n",
      "Epoch 1904/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 95.3257 - val_loss: 97.3073\n",
      "Epoch 1905/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.5487 - val_loss: 97.3397\n",
      "Epoch 1906/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.5745 - val_loss: 96.6490\n",
      "Epoch 1907/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.0654 - val_loss: 96.6776\n",
      "Epoch 1908/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.5317 - val_loss: 96.3458\n",
      "Epoch 1909/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 96.3823 - val_loss: 97.2794\n",
      "Epoch 1910/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.9844 - val_loss: 97.3779\n",
      "Epoch 1911/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.9453 - val_loss: 97.1440\n",
      "Epoch 1912/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.6921 - val_loss: 97.0186\n",
      "Epoch 1913/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 96.9231 - val_loss: 97.2435\n",
      "Epoch 1914/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.4862 - val_loss: 97.0344\n",
      "Epoch 1915/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.4777 - val_loss: 96.9220\n",
      "Epoch 1916/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 95.9953 - val_loss: 96.1493\n",
      "Epoch 1917/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 96.4979 - val_loss: 95.8110\n",
      "Epoch 1918/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.0912 - val_loss: 95.8077\n",
      "Epoch 1919/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 95.4286 - val_loss: 96.3202\n",
      "Epoch 1920/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.3652 - val_loss: 97.0105\n",
      "Epoch 1921/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 95.9288 - val_loss: 97.0517\n",
      "Epoch 1922/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 95.0025 - val_loss: 97.1254\n",
      "Epoch 1923/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 95.1222 - val_loss: 96.9060\n",
      "Epoch 1924/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.7878 - val_loss: 97.5966\n",
      "Epoch 1925/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 95.1146 - val_loss: 97.7244\n",
      "Epoch 1926/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 95.2178 - val_loss: 95.0387\n",
      "Epoch 1927/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.8904 - val_loss: 95.9297\n",
      "Epoch 1928/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.8428 - val_loss: 95.7459\n",
      "Epoch 1929/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.7565 - val_loss: 96.5585\n",
      "Epoch 1930/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.9038 - val_loss: 97.5814\n",
      "Epoch 1931/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.0179 - val_loss: 97.8704\n",
      "Epoch 1932/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.5423 - val_loss: 97.4646\n",
      "Epoch 1933/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.7196 - val_loss: 97.9367\n",
      "Epoch 1934/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.1963 - val_loss: 96.8784\n",
      "Epoch 1935/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.2965 - val_loss: 96.5832\n",
      "Epoch 1936/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 97.1567 - val_loss: 96.2049\n",
      "Epoch 1937/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.1091 - val_loss: 96.2539\n",
      "Epoch 1938/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.2187 - val_loss: 95.5694\n",
      "Epoch 1939/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.9112 - val_loss: 95.8566\n",
      "Epoch 1940/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.2985 - val_loss: 96.0299\n",
      "Epoch 1941/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.0266 - val_loss: 96.8881\n",
      "Epoch 1942/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 96.9256 - val_loss: 97.5424\n",
      "Epoch 1943/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.7299 - val_loss: 96.7436\n",
      "Epoch 1944/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.7839 - val_loss: 96.6068\n",
      "Epoch 1945/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.1423 - val_loss: 95.4469\n",
      "Epoch 1946/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.8278 - val_loss: 96.2063\n",
      "Epoch 1947/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 95.4868 - val_loss: 96.3351\n",
      "Epoch 1948/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.2715 - val_loss: 95.7723\n",
      "Epoch 1949/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.7279 - val_loss: 97.6922\n",
      "Epoch 1950/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 95.6366 - val_loss: 97.5278\n",
      "Epoch 1951/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.7413 - val_loss: 95.9600\n",
      "Epoch 1952/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.1940 - val_loss: 95.2973\n",
      "Epoch 1953/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.6081 - val_loss: 95.7972\n",
      "Epoch 1954/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.3168 - val_loss: 96.5407\n",
      "Epoch 1955/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 100.1782 - val_loss: 97.0116\n",
      "Epoch 1956/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 95.2034 - val_loss: 96.7424\n",
      "Epoch 1957/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.8254 - val_loss: 96.8030\n",
      "Epoch 1958/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.0777 - val_loss: 95.0406\n",
      "Epoch 1959/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 95.7957 - val_loss: 95.4671\n",
      "Epoch 1960/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.3477 - val_loss: 96.5270\n",
      "Epoch 1961/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.6187 - val_loss: 95.9862\n",
      "Epoch 1962/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.8327 - val_loss: 97.6621\n",
      "Epoch 1963/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.2545 - val_loss: 97.0724\n",
      "Epoch 1964/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.5190 - val_loss: 96.8010\n",
      "Epoch 1965/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.0059 - val_loss: 96.7707\n",
      "Epoch 1966/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 94.8051 - val_loss: 96.4191\n",
      "Epoch 1967/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.8449 - val_loss: 96.5339\n",
      "Epoch 1968/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.0920 - val_loss: 95.9690\n",
      "Epoch 1969/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.1524 - val_loss: 95.5079\n",
      "Epoch 1970/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.3646 - val_loss: 96.4150\n",
      "Epoch 1971/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.5241 - val_loss: 96.1714\n",
      "Epoch 1972/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 95.5709 - val_loss: 96.7991\n",
      "Epoch 1973/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.8610 - val_loss: 97.7089\n",
      "Epoch 1974/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.7151 - val_loss: 97.6846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1975/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.2372 - val_loss: 97.3315\n",
      "Epoch 1976/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.1991 - val_loss: 96.1661\n",
      "Epoch 1977/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.3171 - val_loss: 95.3685\n",
      "Epoch 1978/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.3620 - val_loss: 95.6806\n",
      "Epoch 1979/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.4272 - val_loss: 94.7098\n",
      "Epoch 1980/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.8314 - val_loss: 95.6388\n",
      "Epoch 1981/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.0437 - val_loss: 95.2925\n",
      "Epoch 1982/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.4774 - val_loss: 97.3489\n",
      "Epoch 1983/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.6599 - val_loss: 96.9886\n",
      "Epoch 1984/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.3242 - val_loss: 97.1757\n",
      "Epoch 1985/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.4112 - val_loss: 96.3585\n",
      "Epoch 1986/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.0768 - val_loss: 96.1752\n",
      "Epoch 1987/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.0079 - val_loss: 95.3043\n",
      "Epoch 1988/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.5637 - val_loss: 95.3240\n",
      "Epoch 1989/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 97.2544 - val_loss: 95.8055\n",
      "Epoch 1990/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 95.6245 - val_loss: 95.0396\n",
      "Epoch 1991/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 94.9574 - val_loss: 97.1413\n",
      "Epoch 1992/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.7348 - val_loss: 96.6151\n",
      "Epoch 1993/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.7590 - val_loss: 96.5360\n",
      "Epoch 1994/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.4221 - val_loss: 96.5860\n",
      "Epoch 1995/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.4985 - val_loss: 95.4159\n",
      "Epoch 1996/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 95.7624 - val_loss: 95.0779\n",
      "Epoch 1997/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.5843 - val_loss: 95.5345\n",
      "Epoch 1998/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.4812 - val_loss: 96.7094\n",
      "Epoch 1999/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.8037 - val_loss: 97.5913\n",
      "Epoch 2000/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.6584 - val_loss: 96.9979\n",
      "Epoch 2001/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.3420 - val_loss: 97.2171\n",
      "Epoch 2002/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 95.8759 - val_loss: 96.9724\n",
      "Epoch 2003/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.2798 - val_loss: 97.4076\n",
      "Epoch 2004/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.7400 - val_loss: 97.3873\n",
      "Epoch 2005/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.4632 - val_loss: 97.6597\n",
      "Epoch 2006/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.3334 - val_loss: 95.5485\n",
      "Epoch 2007/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.1029 - val_loss: 96.7414\n",
      "Epoch 2008/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 94.6361 - val_loss: 96.8018\n",
      "Epoch 2009/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.2759 - val_loss: 95.2132\n",
      "Epoch 2010/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.0031 - val_loss: 95.1259\n",
      "Epoch 2011/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 96.6507 - val_loss: 95.1642\n",
      "Epoch 2012/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.0017 - val_loss: 96.3705\n",
      "Epoch 2013/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.7088 - val_loss: 95.0360\n",
      "Epoch 2014/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 96.0029 - val_loss: 97.2613\n",
      "Epoch 2015/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.4894 - val_loss: 96.3257\n",
      "Epoch 2016/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 94.7366 - val_loss: 97.0708\n",
      "Epoch 2017/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.8348 - val_loss: 95.7793\n",
      "Epoch 2018/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.1103 - val_loss: 96.1820\n",
      "Epoch 2019/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.2343 - val_loss: 95.4249\n",
      "Epoch 2020/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.9060 - val_loss: 95.1551\n",
      "Epoch 2021/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 95.2805 - val_loss: 96.1663\n",
      "Epoch 2022/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.1224 - val_loss: 96.2444\n",
      "Epoch 2023/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.2701 - val_loss: 96.7389\n",
      "Epoch 2024/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.7355 - val_loss: 97.2676\n",
      "Epoch 2025/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.7082 - val_loss: 95.8804\n",
      "Epoch 2026/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.4418 - val_loss: 96.0851\n",
      "Epoch 2027/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.3743 - val_loss: 96.1028\n",
      "Epoch 2028/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.6408 - val_loss: 95.9938\n",
      "Epoch 2029/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.6232 - val_loss: 96.5682\n",
      "Epoch 2030/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.4626 - val_loss: 95.5019\n",
      "Epoch 2031/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 95.2807 - val_loss: 95.2695\n",
      "Epoch 2032/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.1877 - val_loss: 96.7517\n",
      "Epoch 2033/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.9871 - val_loss: 96.3482\n",
      "Epoch 2034/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.1102 - val_loss: 95.9126\n",
      "Epoch 2035/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 95.7713 - val_loss: 96.5910\n",
      "Epoch 2036/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.4233 - val_loss: 96.7249\n",
      "Epoch 2037/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.1914 - val_loss: 97.2009\n",
      "Epoch 2038/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 96.4428 - val_loss: 95.4248\n",
      "Epoch 2039/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 95.7265 - val_loss: 95.6825\n",
      "Epoch 2040/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 95.8588 - val_loss: 95.8199\n",
      "Epoch 2041/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.3392 - val_loss: 96.3797\n",
      "Epoch 2042/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.8797 - val_loss: 96.0237\n",
      "Epoch 2043/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.8625 - val_loss: 95.6910\n",
      "Epoch 2044/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.1022 - val_loss: 94.9617\n",
      "Epoch 2045/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.3164 - val_loss: 95.4787\n",
      "Epoch 2046/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.0654 - val_loss: 94.1106\n",
      "Epoch 2047/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 94.7832 - val_loss: 95.8951\n",
      "Epoch 2048/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.5685 - val_loss: 96.3864\n",
      "Epoch 2049/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 95.5524 - val_loss: 96.3974\n",
      "Epoch 2050/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.1392 - val_loss: 96.1634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2051/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.5412 - val_loss: 95.4788\n",
      "Epoch 2052/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.5491 - val_loss: 95.4385\n",
      "Epoch 2053/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.5971 - val_loss: 95.4072\n",
      "Epoch 2054/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.2677 - val_loss: 94.7076\n",
      "Epoch 2055/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.0399 - val_loss: 95.9308\n",
      "Epoch 2056/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.3145 - val_loss: 95.8085\n",
      "Epoch 2057/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.7589 - val_loss: 95.2448\n",
      "Epoch 2058/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.2567 - val_loss: 95.0348\n",
      "Epoch 2059/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 94.2184 - val_loss: 96.1617\n",
      "Epoch 2060/10000\n",
      "96/96 [==============================] - ETA: 0s - loss: 93.65 - 0s 117us/step - loss: 96.4519 - val_loss: 96.2307\n",
      "Epoch 2061/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.4933 - val_loss: 96.6379\n",
      "Epoch 2062/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 95.4075 - val_loss: 96.4698\n",
      "Epoch 2063/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.1560 - val_loss: 95.4057\n",
      "Epoch 2064/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.2573 - val_loss: 95.7740\n",
      "Epoch 2065/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.1027 - val_loss: 95.1720\n",
      "Epoch 2066/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.2930 - val_loss: 95.3460\n",
      "Epoch 2067/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.0924 - val_loss: 95.6998\n",
      "Epoch 2068/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.3717 - val_loss: 96.2634\n",
      "Epoch 2069/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.4531 - val_loss: 95.9914\n",
      "Epoch 2070/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.2899 - val_loss: 96.5934\n",
      "Epoch 2071/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.9602 - val_loss: 97.3910\n",
      "Epoch 2072/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.8653 - val_loss: 97.2064\n",
      "Epoch 2073/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 95.5070 - val_loss: 95.8437\n",
      "Epoch 2074/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 94.3122 - val_loss: 95.7007\n",
      "Epoch 2075/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.7535 - val_loss: 95.4698\n",
      "Epoch 2076/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.4876 - val_loss: 96.0946\n",
      "Epoch 2077/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.3261 - val_loss: 95.7706\n",
      "Epoch 2078/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.0986 - val_loss: 95.7288\n",
      "Epoch 2079/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 95.1425 - val_loss: 96.3054\n",
      "Epoch 2080/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.0864 - val_loss: 95.5490\n",
      "Epoch 2081/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.9674 - val_loss: 95.2187\n",
      "Epoch 2082/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.5864 - val_loss: 95.8016\n",
      "Epoch 2083/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.6373 - val_loss: 95.3461\n",
      "Epoch 2084/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.9224 - val_loss: 95.9033\n",
      "Epoch 2085/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.2273 - val_loss: 96.5110\n",
      "Epoch 2086/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.7091 - val_loss: 95.9912\n",
      "Epoch 2087/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.2807 - val_loss: 96.0369\n",
      "Epoch 2088/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.6743 - val_loss: 95.6770\n",
      "Epoch 2089/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.9608 - val_loss: 96.0348\n",
      "Epoch 2090/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.5898 - val_loss: 95.7701\n",
      "Epoch 2091/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.5289 - val_loss: 95.6351\n",
      "Epoch 2092/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 96.3965 - val_loss: 96.7389\n",
      "Epoch 2093/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.8119 - val_loss: 96.0431\n",
      "Epoch 2094/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.0708 - val_loss: 95.4018\n",
      "Epoch 2095/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.6212 - val_loss: 95.7644\n",
      "Epoch 2096/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 95.8946 - val_loss: 95.6581\n",
      "Epoch 2097/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 98.0201 - val_loss: 95.6002\n",
      "Epoch 2098/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.3028 - val_loss: 96.0270\n",
      "Epoch 2099/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.1536 - val_loss: 96.6361\n",
      "Epoch 2100/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 99.4104 - val_loss: 96.0509\n",
      "Epoch 2101/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.2096 - val_loss: 97.0856\n",
      "Epoch 2102/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.4886 - val_loss: 95.9045\n",
      "Epoch 2103/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.2066 - val_loss: 95.5050\n",
      "Epoch 2104/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 96.4012 - val_loss: 95.3842\n",
      "Epoch 2105/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.2012 - val_loss: 94.4822\n",
      "Epoch 2106/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.6243 - val_loss: 96.6687\n",
      "Epoch 2107/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.4300 - val_loss: 96.3934\n",
      "Epoch 2108/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.2462 - val_loss: 96.1388\n",
      "Epoch 2109/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.2851 - val_loss: 95.8878\n",
      "Epoch 2110/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 95.6587 - val_loss: 95.0311\n",
      "Epoch 2111/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 94.8140 - val_loss: 95.8708\n",
      "Epoch 2112/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.4087 - val_loss: 95.4179\n",
      "Epoch 2113/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 99.8250 - val_loss: 95.3836\n",
      "Epoch 2114/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.6067 - val_loss: 96.6582\n",
      "Epoch 2115/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 98.5518 - val_loss: 95.5351\n",
      "Epoch 2116/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 96.0205 - val_loss: 95.8376\n",
      "Epoch 2117/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.2685 - val_loss: 94.9870\n",
      "Epoch 2118/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.9472 - val_loss: 95.9749\n",
      "Epoch 2119/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.2955 - val_loss: 95.9596\n",
      "Epoch 2120/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.5564 - val_loss: 95.8248\n",
      "Epoch 2121/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.8571 - val_loss: 95.6601\n",
      "Epoch 2122/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.9923 - val_loss: 94.9324\n",
      "Epoch 2123/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.2847 - val_loss: 95.9025\n",
      "Epoch 2124/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.4446 - val_loss: 94.3875\n",
      "Epoch 2125/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.7311 - val_loss: 94.9059\n",
      "Epoch 2126/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 117us/step - loss: 94.0334 - val_loss: 95.0127\n",
      "Epoch 2127/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.5309 - val_loss: 95.0120\n",
      "Epoch 2128/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.3413 - val_loss: 95.1682\n",
      "Epoch 2129/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.1480 - val_loss: 95.9322\n",
      "Epoch 2130/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.1760 - val_loss: 94.8337\n",
      "Epoch 2131/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.9809 - val_loss: 96.5212\n",
      "Epoch 2132/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.2644 - val_loss: 96.5151\n",
      "Epoch 2133/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 96.1950 - val_loss: 96.1565\n",
      "Epoch 2134/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.5871 - val_loss: 96.6238\n",
      "Epoch 2135/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.3737 - val_loss: 96.4373\n",
      "Epoch 2136/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.2716 - val_loss: 95.2682\n",
      "Epoch 2137/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.2419 - val_loss: 95.2210\n",
      "Epoch 2138/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 98.9462 - val_loss: 95.6751\n",
      "Epoch 2139/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.1352 - val_loss: 94.5545\n",
      "Epoch 2140/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.5811 - val_loss: 95.4229\n",
      "Epoch 2141/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 94.0940 - val_loss: 96.1334\n",
      "Epoch 2142/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 95.2568 - val_loss: 96.0493\n",
      "Epoch 2143/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 95.4800 - val_loss: 96.7775\n",
      "Epoch 2144/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 96.5292 - val_loss: 95.9555\n",
      "Epoch 2145/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.6766 - val_loss: 96.6510\n",
      "Epoch 2146/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.5973 - val_loss: 96.6945\n",
      "Epoch 2147/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 93.5620 - val_loss: 94.3971\n",
      "Epoch 2148/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.4816 - val_loss: 95.0561\n",
      "Epoch 2149/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.2203 - val_loss: 94.7288\n",
      "Epoch 2150/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.6664 - val_loss: 95.5237\n",
      "Epoch 2151/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.5032 - val_loss: 95.1302\n",
      "Epoch 2152/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 94.7701 - val_loss: 96.8348\n",
      "Epoch 2153/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.7331 - val_loss: 95.3876\n",
      "Epoch 2154/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.6806 - val_loss: 96.2533\n",
      "Epoch 2155/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.9043 - val_loss: 96.9088\n",
      "Epoch 2156/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 93.0656 - val_loss: 94.9392\n",
      "Epoch 2157/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.9177 - val_loss: 95.8113\n",
      "Epoch 2158/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.4172 - val_loss: 95.3664\n",
      "Epoch 2159/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.1101 - val_loss: 95.1130\n",
      "Epoch 2160/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 98.2059 - val_loss: 94.8596\n",
      "Epoch 2161/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.1798 - val_loss: 95.3280\n",
      "Epoch 2162/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.0486 - val_loss: 96.3521\n",
      "Epoch 2163/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.5986 - val_loss: 94.9174\n",
      "Epoch 2164/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6823 - val_loss: 95.6105\n",
      "Epoch 2165/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.1783 - val_loss: 95.4105\n",
      "Epoch 2166/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.6140 - val_loss: 95.1797\n",
      "Epoch 2167/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.4743 - val_loss: 96.2104\n",
      "Epoch 2168/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 94.1940 - val_loss: 96.0477\n",
      "Epoch 2169/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.3754 - val_loss: 96.3965\n",
      "Epoch 2170/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.5956 - val_loss: 95.9002\n",
      "Epoch 2171/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.8883 - val_loss: 95.3544\n",
      "Epoch 2172/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.7983 - val_loss: 96.6995\n",
      "Epoch 2173/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 94.0328 - val_loss: 95.4977\n",
      "Epoch 2174/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.0343 - val_loss: 94.8466\n",
      "Epoch 2175/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 95.6378 - val_loss: 95.2789\n",
      "Epoch 2176/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 94.3468 - val_loss: 95.9857\n",
      "Epoch 2177/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.7722 - val_loss: 95.0681\n",
      "Epoch 2178/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.9102 - val_loss: 95.9668\n",
      "Epoch 2179/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 96.5450 - val_loss: 95.5591\n",
      "Epoch 2180/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 95.7139 - val_loss: 96.2274\n",
      "Epoch 2181/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 95.1008 - val_loss: 95.2558\n",
      "Epoch 2182/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.0385 - val_loss: 95.4881\n",
      "Epoch 2183/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.8139 - val_loss: 95.0352\n",
      "Epoch 2184/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.4192 - val_loss: 95.9421\n",
      "Epoch 2185/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.9218 - val_loss: 95.1696\n",
      "Epoch 2186/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 94.8463 - val_loss: 95.6117\n",
      "Epoch 2187/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 96.0345 - val_loss: 95.8677\n",
      "Epoch 2188/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.5274 - val_loss: 95.6424\n",
      "Epoch 2189/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.7311 - val_loss: 96.7368\n",
      "Epoch 2190/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.0048 - val_loss: 96.5985\n",
      "Epoch 2191/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.5046 - val_loss: 97.0308\n",
      "Epoch 2192/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.6908 - val_loss: 96.8571\n",
      "Epoch 2193/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 97.4598 - val_loss: 95.9971\n",
      "Epoch 2194/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 95.5583 - val_loss: 97.0070\n",
      "Epoch 2195/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.1340 - val_loss: 96.1943\n",
      "Epoch 2196/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.4250 - val_loss: 95.6631\n",
      "Epoch 2197/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.0098 - val_loss: 95.4755\n",
      "Epoch 2198/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.2032 - val_loss: 95.0938\n",
      "Epoch 2199/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.0674 - val_loss: 96.0748\n",
      "Epoch 2200/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.5744 - val_loss: 95.3229\n",
      "Epoch 2201/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.3332 - val_loss: 95.6821\n",
      "Epoch 2202/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 120us/step - loss: 94.8601 - val_loss: 94.2615\n",
      "Epoch 2203/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 95.2146 - val_loss: 95.8392\n",
      "Epoch 2204/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.9298 - val_loss: 96.4753\n",
      "Epoch 2205/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 96.2275 - val_loss: 97.2246\n",
      "Epoch 2206/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.5318 - val_loss: 96.4568\n",
      "Epoch 2207/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.4446 - val_loss: 96.0172\n",
      "Epoch 2208/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 93.2587 - val_loss: 93.9831\n",
      "Epoch 2209/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.3056 - val_loss: 94.7074\n",
      "Epoch 2210/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.9702 - val_loss: 95.2185\n",
      "Epoch 2211/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.8178 - val_loss: 95.1376\n",
      "Epoch 2212/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.8717 - val_loss: 96.6144\n",
      "Epoch 2213/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.5037 - val_loss: 95.3790\n",
      "Epoch 2214/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.9509 - val_loss: 94.8285\n",
      "Epoch 2215/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.1683 - val_loss: 95.1721\n",
      "Epoch 2216/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 96.6651 - val_loss: 94.7524\n",
      "Epoch 2217/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.9160 - val_loss: 95.4481\n",
      "Epoch 2218/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.9200 - val_loss: 96.5741\n",
      "Epoch 2219/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 94.9278 - val_loss: 95.6610\n",
      "Epoch 2220/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.6745 - val_loss: 97.0860\n",
      "Epoch 2221/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.8032 - val_loss: 95.9072\n",
      "Epoch 2222/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 94.7085 - val_loss: 96.6216\n",
      "Epoch 2223/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.4217 - val_loss: 97.1276\n",
      "Epoch 2224/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 96.6143 - val_loss: 97.0372\n",
      "Epoch 2225/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.3122 - val_loss: 96.5828\n",
      "Epoch 2226/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 98.3938 - val_loss: 96.1319\n",
      "Epoch 2227/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 94.9827 - val_loss: 96.1230\n",
      "Epoch 2228/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.6041 - val_loss: 94.8041\n",
      "Epoch 2229/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.7411 - val_loss: 95.5956\n",
      "Epoch 2230/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.7761 - val_loss: 95.7779\n",
      "Epoch 2231/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 97.1187 - val_loss: 95.6142\n",
      "Epoch 2232/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.7399 - val_loss: 96.4026\n",
      "Epoch 2233/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.6915 - val_loss: 95.3521\n",
      "Epoch 2234/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.2237 - val_loss: 95.9681\n",
      "Epoch 2235/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.4524 - val_loss: 94.9903\n",
      "Epoch 2236/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.9258 - val_loss: 96.1915\n",
      "Epoch 2237/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.3282 - val_loss: 96.9862\n",
      "Epoch 2238/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 95.0030 - val_loss: 96.8214\n",
      "Epoch 2239/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.5314 - val_loss: 96.1900\n",
      "Epoch 2240/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.1338 - val_loss: 95.2646\n",
      "Epoch 2241/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 94.2002 - val_loss: 97.5662\n",
      "Epoch 2242/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.5629 - val_loss: 96.4656\n",
      "Epoch 2243/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.3433 - val_loss: 95.8194\n",
      "Epoch 2244/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.1809 - val_loss: 94.9119\n",
      "Epoch 2245/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 95.5179 - val_loss: 95.6742\n",
      "Epoch 2246/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.6618 - val_loss: 96.8366\n",
      "Epoch 2247/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.5847 - val_loss: 94.5634\n",
      "Epoch 2248/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.4506 - val_loss: 94.3038\n",
      "Epoch 2249/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.6206 - val_loss: 97.3327\n",
      "Epoch 2250/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.5323 - val_loss: 95.4136\n",
      "Epoch 2251/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 95.0883 - val_loss: 95.6447\n",
      "Epoch 2252/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.0554 - val_loss: 96.7429\n",
      "Epoch 2253/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.7934 - val_loss: 96.0721\n",
      "Epoch 2254/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.2414 - val_loss: 94.7662\n",
      "Epoch 2255/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.8807 - val_loss: 96.1477\n",
      "Epoch 2256/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 95.1953 - val_loss: 96.7257\n",
      "Epoch 2257/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.0287 - val_loss: 96.1414\n",
      "Epoch 2258/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.0164 - val_loss: 96.3826\n",
      "Epoch 2259/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.9811 - val_loss: 95.8672\n",
      "Epoch 2260/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.8226 - val_loss: 96.4657\n",
      "Epoch 2261/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.2621 - val_loss: 96.1215\n",
      "Epoch 2262/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.8092 - val_loss: 95.5838\n",
      "Epoch 2263/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.4606 - val_loss: 95.8690\n",
      "Epoch 2264/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.9845 - val_loss: 97.0987\n",
      "Epoch 2265/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.1896 - val_loss: 94.1614\n",
      "Epoch 2266/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.6409 - val_loss: 94.9877\n",
      "Epoch 2267/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.1018 - val_loss: 95.3006\n",
      "Epoch 2268/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.4183 - val_loss: 96.8264\n",
      "Epoch 2269/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.7523 - val_loss: 94.9680\n",
      "Epoch 2270/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.9834 - val_loss: 94.6058\n",
      "Epoch 2271/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.0008 - val_loss: 95.4633\n",
      "Epoch 2272/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.6365 - val_loss: 95.7011\n",
      "Epoch 2273/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.2280 - val_loss: 95.8738\n",
      "Epoch 2274/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.2570 - val_loss: 95.1972\n",
      "Epoch 2275/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.8896 - val_loss: 94.3546\n",
      "Epoch 2276/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.9673 - val_loss: 95.7331\n",
      "Epoch 2277/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.6655 - val_loss: 95.7508\n",
      "Epoch 2278/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 117us/step - loss: 93.5829 - val_loss: 96.5459\n",
      "Epoch 2279/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 95.1529 - val_loss: 96.1700\n",
      "Epoch 2280/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.1697 - val_loss: 95.3187\n",
      "Epoch 2281/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.4965 - val_loss: 95.3639\n",
      "Epoch 2282/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.5202 - val_loss: 94.7218\n",
      "Epoch 2283/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 99.9275 - val_loss: 95.0497\n",
      "Epoch 2284/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.9799 - val_loss: 94.5920\n",
      "Epoch 2285/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.6629 - val_loss: 94.8534\n",
      "Epoch 2286/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 95.2795 - val_loss: 95.0997\n",
      "Epoch 2287/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.4698 - val_loss: 95.0567\n",
      "Epoch 2288/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.3893 - val_loss: 94.4873\n",
      "Epoch 2289/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.0394 - val_loss: 94.7904\n",
      "Epoch 2290/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.1850 - val_loss: 95.2383\n",
      "Epoch 2291/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.4782 - val_loss: 95.5296\n",
      "Epoch 2292/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.6620 - val_loss: 97.2915\n",
      "Epoch 2293/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 95.2242 - val_loss: 96.5848\n",
      "Epoch 2294/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.6570 - val_loss: 95.6176\n",
      "Epoch 2295/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.3542 - val_loss: 95.1587\n",
      "Epoch 2296/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.2649 - val_loss: 95.2799\n",
      "Epoch 2297/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.4405 - val_loss: 95.5468\n",
      "Epoch 2298/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.0664 - val_loss: 95.2432\n",
      "Epoch 2299/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.2871 - val_loss: 96.5450\n",
      "Epoch 2300/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.4436 - val_loss: 95.3120\n",
      "Epoch 2301/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.4511 - val_loss: 95.1445\n",
      "Epoch 2302/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.8494 - val_loss: 94.6226\n",
      "Epoch 2303/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.2410 - val_loss: 96.0743\n",
      "Epoch 2304/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 95.4336 - val_loss: 95.2000\n",
      "Epoch 2305/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 95.5268 - val_loss: 95.1630\n",
      "Epoch 2306/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 96.1545 - val_loss: 95.0427\n",
      "Epoch 2307/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.9343 - val_loss: 95.7460\n",
      "Epoch 2308/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.5392 - val_loss: 96.2568\n",
      "Epoch 2309/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 95.0372 - val_loss: 94.8493\n",
      "Epoch 2310/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.5300 - val_loss: 96.1319\n",
      "Epoch 2311/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 94.3671 - val_loss: 94.6895\n",
      "Epoch 2312/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 93.3158 - val_loss: 95.5251\n",
      "Epoch 2313/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.6810 - val_loss: 95.4205\n",
      "Epoch 2314/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 97.5521 - val_loss: 95.2942\n",
      "Epoch 2315/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.0905 - val_loss: 95.0258\n",
      "Epoch 2316/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.4812 - val_loss: 96.6855\n",
      "Epoch 2317/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 95.6077 - val_loss: 95.6962\n",
      "Epoch 2318/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.8198 - val_loss: 96.5215\n",
      "Epoch 2319/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 97.0278 - val_loss: 95.7044\n",
      "Epoch 2320/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 95.8127 - val_loss: 96.3499\n",
      "Epoch 2321/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 95.2169 - val_loss: 94.5658\n",
      "Epoch 2322/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.6697 - val_loss: 95.2331\n",
      "Epoch 2323/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 95.9163 - val_loss: 94.5374\n",
      "Epoch 2324/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.1087 - val_loss: 94.4937\n",
      "Epoch 2325/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.3096 - val_loss: 94.7590\n",
      "Epoch 2326/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.3855 - val_loss: 95.7999\n",
      "Epoch 2327/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 95.3134 - val_loss: 96.7581\n",
      "Epoch 2328/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.9816 - val_loss: 96.8109\n",
      "Epoch 2329/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 94.0715 - val_loss: 96.5706\n",
      "Epoch 2330/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.8008 - val_loss: 97.3582\n",
      "Epoch 2331/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.3287 - val_loss: 95.8374\n",
      "Epoch 2332/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.9372 - val_loss: 96.1289\n",
      "Epoch 2333/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.9470 - val_loss: 95.6409\n",
      "Epoch 2334/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.3420 - val_loss: 95.4747\n",
      "Epoch 2335/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.9480 - val_loss: 93.9117\n",
      "Epoch 2336/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.4901 - val_loss: 96.2292\n",
      "Epoch 2337/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.3395 - val_loss: 95.5423\n",
      "Epoch 2338/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.8483 - val_loss: 94.3704\n",
      "Epoch 2339/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.8666 - val_loss: 94.9617\n",
      "Epoch 2340/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.3452 - val_loss: 95.4074\n",
      "Epoch 2341/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.6563 - val_loss: 96.5286\n",
      "Epoch 2342/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.9433 - val_loss: 95.2159\n",
      "Epoch 2343/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.3234 - val_loss: 95.2007\n",
      "Epoch 2344/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.3547 - val_loss: 94.4345\n",
      "Epoch 2345/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.6059 - val_loss: 96.6832\n",
      "Epoch 2346/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.0250 - val_loss: 97.2967\n",
      "Epoch 2347/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.8386 - val_loss: 95.5418\n",
      "Epoch 2348/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.4276 - val_loss: 95.8047\n",
      "Epoch 2349/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 94.1807 - val_loss: 95.0924\n",
      "Epoch 2350/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.9239 - val_loss: 95.0175\n",
      "Epoch 2351/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.0302 - val_loss: 95.1283\n",
      "Epoch 2352/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 97.2051 - val_loss: 96.3850\n",
      "Epoch 2353/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 95.5422 - val_loss: 95.9341\n",
      "Epoch 2354/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 116us/step - loss: 94.9949 - val_loss: 95.2028\n",
      "Epoch 2355/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.4501 - val_loss: 96.1485\n",
      "Epoch 2356/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 94.0213 - val_loss: 96.9163\n",
      "Epoch 2357/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.3994 - val_loss: 95.1184\n",
      "Epoch 2358/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.9288 - val_loss: 94.7444\n",
      "Epoch 2359/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.4565 - val_loss: 96.6175\n",
      "Epoch 2360/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.9392 - val_loss: 95.9900\n",
      "Epoch 2361/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 95.4339 - val_loss: 96.1288\n",
      "Epoch 2362/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 95.5713 - val_loss: 96.4610\n",
      "Epoch 2363/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.2185 - val_loss: 97.1233\n",
      "Epoch 2364/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.9703 - val_loss: 96.4575\n",
      "Epoch 2365/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 95.7071 - val_loss: 96.1668\n",
      "Epoch 2366/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.7105 - val_loss: 94.8463\n",
      "Epoch 2367/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 94.7107 - val_loss: 94.9067\n",
      "Epoch 2368/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.6794 - val_loss: 94.9042\n",
      "Epoch 2369/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 96.5912 - val_loss: 94.7479\n",
      "Epoch 2370/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.7820 - val_loss: 94.7551\n",
      "Epoch 2371/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 94.2137 - val_loss: 95.0766\n",
      "Epoch 2372/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.9941 - val_loss: 96.2532\n",
      "Epoch 2373/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.3434 - val_loss: 95.8285\n",
      "Epoch 2374/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.2296 - val_loss: 96.7349\n",
      "Epoch 2375/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 96.0442 - val_loss: 96.9326\n",
      "Epoch 2376/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.6620 - val_loss: 96.4642\n",
      "Epoch 2377/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.5927 - val_loss: 96.0690\n",
      "Epoch 2378/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.2548 - val_loss: 94.8651\n",
      "Epoch 2379/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 95.8877 - val_loss: 95.1447\n",
      "Epoch 2380/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.0760 - val_loss: 95.1368\n",
      "Epoch 2381/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.5103 - val_loss: 95.2925\n",
      "Epoch 2382/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 95.8935 - val_loss: 95.7064\n",
      "Epoch 2383/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.5887 - val_loss: 96.4633\n",
      "Epoch 2384/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.0564 - val_loss: 96.9908\n",
      "Epoch 2385/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.6879 - val_loss: 96.0453\n",
      "Epoch 2386/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 96.8172 - val_loss: 97.5205\n",
      "Epoch 2387/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 95.8585 - val_loss: 96.7773\n",
      "Epoch 2388/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.7976 - val_loss: 96.0330\n",
      "Epoch 2389/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.7492 - val_loss: 94.3764\n",
      "Epoch 2390/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.2078 - val_loss: 94.9374\n",
      "Epoch 2391/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.9383 - val_loss: 95.1279\n",
      "Epoch 2392/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 94.8042 - val_loss: 96.0282\n",
      "Epoch 2393/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.1284 - val_loss: 94.8677\n",
      "Epoch 2394/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.6997 - val_loss: 95.7986\n",
      "Epoch 2395/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 94.7347 - val_loss: 95.8552\n",
      "Epoch 2396/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.2212 - val_loss: 96.2906\n",
      "Epoch 2397/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.1408 - val_loss: 96.5772\n",
      "Epoch 2398/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 95.9054 - val_loss: 94.7481\n",
      "Epoch 2399/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.7441 - val_loss: 94.9980\n",
      "Epoch 2400/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 94.2067 - val_loss: 95.1321\n",
      "Epoch 2401/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.0032 - val_loss: 96.2788\n",
      "Epoch 2402/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.1624 - val_loss: 96.1499\n",
      "Epoch 2403/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 95.2784 - val_loss: 97.2633\n",
      "Epoch 2404/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.1207 - val_loss: 96.9523\n",
      "Epoch 2405/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.9976 - val_loss: 95.5374\n",
      "Epoch 2406/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.1987 - val_loss: 96.4376\n",
      "Epoch 2407/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.4687 - val_loss: 96.2254\n",
      "Epoch 2408/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.8507 - val_loss: 96.2045\n",
      "Epoch 2409/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.1669 - val_loss: 96.0716\n",
      "Epoch 2410/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.8068 - val_loss: 95.7909\n",
      "Epoch 2411/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 94.1349 - val_loss: 95.2029\n",
      "Epoch 2412/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.6961 - val_loss: 95.2634\n",
      "Epoch 2413/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 94.6486 - val_loss: 95.3523\n",
      "Epoch 2414/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.6671 - val_loss: 95.1831\n",
      "Epoch 2415/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.3348 - val_loss: 95.0305\n",
      "Epoch 2416/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.0810 - val_loss: 95.3278\n",
      "Epoch 2417/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.8725 - val_loss: 95.2869\n",
      "Epoch 2418/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.5622 - val_loss: 95.5000\n",
      "Epoch 2419/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.8355 - val_loss: 96.7243\n",
      "Epoch 2420/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.6215 - val_loss: 96.0705\n",
      "Epoch 2421/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 95.0203 - val_loss: 97.0505\n",
      "Epoch 2422/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 94.7931 - val_loss: 95.4333\n",
      "Epoch 2423/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.4686 - val_loss: 94.6891\n",
      "Epoch 2424/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.7499 - val_loss: 96.2163\n",
      "Epoch 2425/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.7518 - val_loss: 95.7175\n",
      "Epoch 2426/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.8742 - val_loss: 96.1355\n",
      "Epoch 2427/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.6754 - val_loss: 96.7939\n",
      "Epoch 2428/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.8247 - val_loss: 95.9028\n",
      "Epoch 2429/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 96.3414 - val_loss: 96.2574\n",
      "Epoch 2430/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/step - loss: 95.7044 - val_loss: 95.8082\n",
      "Epoch 2431/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 97.0018 - val_loss: 95.4701\n",
      "Epoch 2432/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.7970 - val_loss: 95.6316\n",
      "Epoch 2433/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.8295 - val_loss: 94.7657\n",
      "Epoch 2434/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.9966 - val_loss: 95.7020\n",
      "Epoch 2435/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.8304 - val_loss: 95.9782\n",
      "Epoch 2436/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.9274 - val_loss: 95.9137\n",
      "Epoch 2437/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.5552 - val_loss: 96.5525\n",
      "Epoch 2438/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 95.5202 - val_loss: 94.4306\n",
      "Epoch 2439/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 95.5732 - val_loss: 96.3761\n",
      "Epoch 2440/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 94.6854 - val_loss: 95.8882\n",
      "Epoch 2441/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 96.0249 - val_loss: 96.2398\n",
      "Epoch 2442/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 95.0297 - val_loss: 95.8430\n",
      "Epoch 2443/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 95.9004 - val_loss: 95.7288\n",
      "Epoch 2444/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 96.5583 - val_loss: 95.4795\n",
      "Epoch 2445/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.2181 - val_loss: 95.3317\n",
      "Epoch 2446/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.5833 - val_loss: 96.0139\n",
      "Epoch 2447/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.5694 - val_loss: 95.9966\n",
      "Epoch 2448/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.9183 - val_loss: 95.1029\n",
      "Epoch 2449/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.0685 - val_loss: 96.0287\n",
      "Epoch 2450/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 93.1255 - val_loss: 96.5520\n",
      "Epoch 2451/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.5639 - val_loss: 95.6965\n",
      "Epoch 2452/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 94.2207 - val_loss: 96.4691\n",
      "Epoch 2453/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.3832 - val_loss: 95.0938\n",
      "Epoch 2454/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.5740 - val_loss: 95.5912\n",
      "Epoch 2455/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 95.7024 - val_loss: 94.9263\n",
      "Epoch 2456/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.0426 - val_loss: 94.7964\n",
      "Epoch 2457/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.4016 - val_loss: 95.6358\n",
      "Epoch 2458/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.3869 - val_loss: 96.0007\n",
      "Epoch 2459/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 94.8712 - val_loss: 97.1053\n",
      "Epoch 2460/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.2340 - val_loss: 96.3205\n",
      "Epoch 2461/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 97.2414 - val_loss: 95.6384\n",
      "Epoch 2462/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.5997 - val_loss: 94.7489\n",
      "Epoch 2463/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.1873 - val_loss: 95.0300\n",
      "Epoch 2464/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.2299 - val_loss: 95.5538\n",
      "Epoch 2465/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.5785 - val_loss: 95.5315\n",
      "Epoch 2466/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.6280 - val_loss: 95.8495\n",
      "Epoch 2467/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.5395 - val_loss: 95.9114\n",
      "Epoch 2468/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.5401 - val_loss: 96.2676\n",
      "Epoch 2469/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.5679 - val_loss: 96.4824\n",
      "Epoch 2470/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.1494 - val_loss: 96.9764\n",
      "Epoch 2471/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.3685 - val_loss: 95.0420\n",
      "Epoch 2472/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.0665 - val_loss: 94.5568\n",
      "Epoch 2473/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.4710 - val_loss: 94.9246\n",
      "Epoch 2474/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.6048 - val_loss: 95.5897\n",
      "Epoch 2475/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.0462 - val_loss: 95.7065\n",
      "Epoch 2476/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.3227 - val_loss: 96.5880\n",
      "Epoch 2477/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.7101 - val_loss: 95.8884\n",
      "Epoch 2478/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.8132 - val_loss: 95.0811\n",
      "Epoch 2479/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.1023 - val_loss: 95.1762\n",
      "Epoch 2480/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.6317 - val_loss: 95.1862\n",
      "Epoch 2481/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.7423 - val_loss: 96.6348\n",
      "Epoch 2482/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 95.1079 - val_loss: 95.2501\n",
      "Epoch 2483/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.4176 - val_loss: 95.7557\n",
      "Epoch 2484/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.2655 - val_loss: 96.4582\n",
      "Epoch 2485/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.7555 - val_loss: 96.1260\n",
      "Epoch 2486/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 94.7887 - val_loss: 95.8096\n",
      "Epoch 2487/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 96.3513 - val_loss: 96.5473\n",
      "Epoch 2488/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.8283 - val_loss: 95.3927\n",
      "Epoch 2489/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 98.3788 - val_loss: 95.7250\n",
      "Epoch 2490/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.9279 - val_loss: 96.2849\n",
      "Epoch 2491/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.4774 - val_loss: 95.7007\n",
      "Epoch 2492/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.5798 - val_loss: 94.6837\n",
      "Epoch 2493/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.1671 - val_loss: 94.6303\n",
      "Epoch 2494/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.6390 - val_loss: 96.1022\n",
      "Epoch 2495/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.6110 - val_loss: 95.6569\n",
      "Epoch 2496/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.5533 - val_loss: 96.5531\n",
      "Epoch 2497/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.3578 - val_loss: 95.2457\n",
      "Epoch 2498/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.3049 - val_loss: 95.8823\n",
      "Epoch 2499/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.1188 - val_loss: 96.1469\n",
      "Epoch 2500/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.6172 - val_loss: 96.2221\n",
      "Epoch 2501/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 92.7642 - val_loss: 95.5388\n",
      "Epoch 2502/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.6981 - val_loss: 95.7530\n",
      "Epoch 2503/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.0507 - val_loss: 95.1197\n",
      "Epoch 2504/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.3205 - val_loss: 94.9295\n",
      "Epoch 2505/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.7635 - val_loss: 95.3889\n",
      "Epoch 2506/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 117us/step - loss: 95.9978 - val_loss: 95.1543\n",
      "Epoch 2507/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 93.3614 - val_loss: 95.2331\n",
      "Epoch 2508/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.1943 - val_loss: 96.2142\n",
      "Epoch 2509/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.5626 - val_loss: 96.5102\n",
      "Epoch 2510/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.2860 - val_loss: 95.8966\n",
      "Epoch 2511/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.4325 - val_loss: 95.2920\n",
      "Epoch 2512/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 93.3894 - val_loss: 95.1855\n",
      "Epoch 2513/10000\n",
      "96/96 [==============================] - 0s 150us/step - loss: 93.6198 - val_loss: 95.4457\n",
      "Epoch 2514/10000\n",
      "96/96 [==============================] - 0s 149us/step - loss: 94.2803 - val_loss: 94.5780\n",
      "Epoch 2515/10000\n",
      "96/96 [==============================] - 0s 147us/step - loss: 96.0494 - val_loss: 96.3543\n",
      "Epoch 2516/10000\n",
      "96/96 [==============================] - 0s 146us/step - loss: 94.2391 - val_loss: 95.5179\n",
      "Epoch 2517/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 94.0203 - val_loss: 96.3693\n",
      "Epoch 2518/10000\n",
      "96/96 [==============================] - 0s 146us/step - loss: 93.8626 - val_loss: 95.6690\n",
      "Epoch 2519/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 95.2444 - val_loss: 95.5261\n",
      "Epoch 2520/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 94.1130 - val_loss: 95.3960\n",
      "Epoch 2521/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 94.3403 - val_loss: 95.3899\n",
      "Epoch 2522/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 97.1676 - val_loss: 96.0608\n",
      "Epoch 2523/10000\n",
      "96/96 [==============================] - 0s 149us/step - loss: 94.1308 - val_loss: 95.4514\n",
      "Epoch 2524/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 93.1079 - val_loss: 95.5073\n",
      "Epoch 2525/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.7964 - val_loss: 95.9433\n",
      "Epoch 2526/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 93.7333 - val_loss: 95.7050\n",
      "Epoch 2527/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 94.0301 - val_loss: 95.3524\n",
      "Epoch 2528/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 95.4828 - val_loss: 96.3620\n",
      "Epoch 2529/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 95.4485 - val_loss: 96.2715\n",
      "Epoch 2530/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 95.4889 - val_loss: 97.0884\n",
      "Epoch 2531/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 95.0577 - val_loss: 97.0099\n",
      "Epoch 2532/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 95.2286 - val_loss: 96.0696\n",
      "Epoch 2533/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 94.1644 - val_loss: 96.1149\n",
      "Epoch 2534/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 95.7098 - val_loss: 95.0192\n",
      "Epoch 2535/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 95.0994 - val_loss: 95.6070\n",
      "Epoch 2536/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 94.4231 - val_loss: 95.6876\n",
      "Epoch 2537/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.8129 - val_loss: 95.7411\n",
      "Epoch 2538/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 94.6499 - val_loss: 95.7747\n",
      "Epoch 2539/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 94.2176 - val_loss: 96.3703\n",
      "Epoch 2540/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 94.6999 - val_loss: 95.1600\n",
      "Epoch 2541/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 94.0355 - val_loss: 95.8176\n",
      "Epoch 2542/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 94.4895 - val_loss: 94.9894\n",
      "Epoch 2543/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 94.6934 - val_loss: 96.0542\n",
      "Epoch 2544/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 95.8456 - val_loss: 96.4857\n",
      "Epoch 2545/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 95.0480 - val_loss: 96.1942\n",
      "Epoch 2546/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 94.9486 - val_loss: 97.0727\n",
      "Epoch 2547/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 94.9030 - val_loss: 95.4353\n",
      "Epoch 2548/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 95.9918 - val_loss: 96.7080\n",
      "Epoch 2549/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 94.5990 - val_loss: 94.8397\n",
      "Epoch 2550/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.0410 - val_loss: 95.1874\n",
      "Epoch 2551/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 94.0617 - val_loss: 96.2999\n",
      "Epoch 2552/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.3518 - val_loss: 95.1593\n",
      "Epoch 2553/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 94.3018 - val_loss: 94.9307\n",
      "Epoch 2554/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 93.3818 - val_loss: 95.7358\n",
      "Epoch 2555/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 95.0226 - val_loss: 95.8810\n",
      "Epoch 2556/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 93.8632 - val_loss: 95.4141\n",
      "Epoch 2557/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 94.0082 - val_loss: 96.3406\n",
      "Epoch 2558/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 94.1420 - val_loss: 95.5050\n",
      "Epoch 2559/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 93.6074 - val_loss: 95.3911\n",
      "Epoch 2560/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 94.5434 - val_loss: 95.9018\n",
      "Epoch 2561/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 95.1986 - val_loss: 95.8974\n",
      "Epoch 2562/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 94.4921 - val_loss: 95.4696\n",
      "Epoch 2563/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 95.4462 - val_loss: 95.4379\n",
      "Epoch 2564/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 95.1662 - val_loss: 95.6817\n",
      "Epoch 2565/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 94.6592 - val_loss: 94.6410\n",
      "Epoch 2566/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 93.5401 - val_loss: 95.6710\n",
      "Epoch 2567/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 94.4248 - val_loss: 94.7296\n",
      "Epoch 2568/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 93.1363 - val_loss: 96.5902\n",
      "Epoch 2569/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 93.5031 - val_loss: 94.9065\n",
      "Epoch 2570/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 94.6031 - val_loss: 93.8319\n",
      "Epoch 2571/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 94.0568 - val_loss: 95.7389\n",
      "Epoch 2572/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 97.2246 - val_loss: 94.6838\n",
      "Epoch 2573/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 93.8579 - val_loss: 94.8050\n",
      "Epoch 2574/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 95.1047 - val_loss: 95.0845\n",
      "Epoch 2575/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 94.1142 - val_loss: 96.5363\n",
      "Epoch 2576/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.0452 - val_loss: 96.4448\n",
      "Epoch 2577/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 95.1874 - val_loss: 96.4136\n",
      "Epoch 2578/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.6477 - val_loss: 95.5043\n",
      "Epoch 2579/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 93.2526 - val_loss: 95.5412\n",
      "Epoch 2580/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.3784 - val_loss: 94.9305\n",
      "Epoch 2581/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.6302 - val_loss: 96.1523\n",
      "Epoch 2582/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 117us/step - loss: 95.6382 - val_loss: 95.4657\n",
      "Epoch 2583/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.7682 - val_loss: 95.4370\n",
      "Epoch 2584/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.2911 - val_loss: 95.8677\n",
      "Epoch 2585/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.4106 - val_loss: 95.5174\n",
      "Epoch 2586/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.7677 - val_loss: 97.5054\n",
      "Epoch 2587/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 94.4302 - val_loss: 96.3624\n",
      "Epoch 2588/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.7283 - val_loss: 95.9151\n",
      "Epoch 2589/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.5902 - val_loss: 95.5413\n",
      "Epoch 2590/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.2538 - val_loss: 96.8035\n",
      "Epoch 2591/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.6148 - val_loss: 95.4883\n",
      "Epoch 2592/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.4406 - val_loss: 96.1742\n",
      "Epoch 2593/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.2511 - val_loss: 95.1235\n",
      "Epoch 2594/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.4757 - val_loss: 95.3222\n",
      "Epoch 2595/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.3499 - val_loss: 95.6746\n",
      "Epoch 2596/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.7454 - val_loss: 94.9358\n",
      "Epoch 2597/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.2447 - val_loss: 96.7323\n",
      "Epoch 2598/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.1365 - val_loss: 96.1553\n",
      "Epoch 2599/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.2405 - val_loss: 96.2260\n",
      "Epoch 2600/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.7306 - val_loss: 96.8338\n",
      "Epoch 2601/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.7634 - val_loss: 95.7945\n",
      "Epoch 2602/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.9192 - val_loss: 96.2426\n",
      "Epoch 2603/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.9944 - val_loss: 95.2429\n",
      "Epoch 2604/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.7424 - val_loss: 95.3271\n",
      "Epoch 2605/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 95.1482 - val_loss: 96.1652\n",
      "Epoch 2606/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.9744 - val_loss: 95.6858\n",
      "Epoch 2607/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 94.4930 - val_loss: 95.1986\n",
      "Epoch 2608/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.2952 - val_loss: 95.2263\n",
      "Epoch 2609/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.1813 - val_loss: 95.4565\n",
      "Epoch 2610/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.3347 - val_loss: 95.3229\n",
      "Epoch 2611/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.1574 - val_loss: 94.5344\n",
      "Epoch 2612/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.6279 - val_loss: 95.6681\n",
      "Epoch 2613/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.4799 - val_loss: 95.6356\n",
      "Epoch 2614/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.2254 - val_loss: 96.5339\n",
      "Epoch 2615/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 95.9271 - val_loss: 96.1694\n",
      "Epoch 2616/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 93.2595 - val_loss: 97.0252\n",
      "Epoch 2617/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 95.7245 - val_loss: 95.8827\n",
      "Epoch 2618/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.3223 - val_loss: 96.2798\n",
      "Epoch 2619/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.0565 - val_loss: 94.7267\n",
      "Epoch 2620/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.6212 - val_loss: 95.4784\n",
      "Epoch 2621/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.8688 - val_loss: 96.3838\n",
      "Epoch 2622/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.0914 - val_loss: 95.5667\n",
      "Epoch 2623/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.8970 - val_loss: 96.5216\n",
      "Epoch 2624/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.8321 - val_loss: 95.2172\n",
      "Epoch 2625/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.0325 - val_loss: 95.7634\n",
      "Epoch 2626/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.9626 - val_loss: 94.7626\n",
      "Epoch 2627/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.0240 - val_loss: 95.7438\n",
      "Epoch 2628/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.6261 - val_loss: 94.8209\n",
      "Epoch 2629/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.8471 - val_loss: 94.6292\n",
      "Epoch 2630/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.6006 - val_loss: 94.7770\n",
      "Epoch 2631/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 94.9356 - val_loss: 95.6078\n",
      "Epoch 2632/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7196 - val_loss: 95.9514\n",
      "Epoch 2633/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.0374 - val_loss: 96.2191\n",
      "Epoch 2634/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.8604 - val_loss: 96.2174\n",
      "Epoch 2635/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.2242 - val_loss: 95.9627\n",
      "Epoch 2636/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.5807 - val_loss: 95.8851\n",
      "Epoch 2637/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.7030 - val_loss: 97.0257\n",
      "Epoch 2638/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.8598 - val_loss: 95.5656\n",
      "Epoch 2639/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 94.6679 - val_loss: 96.6999\n",
      "Epoch 2640/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.5995 - val_loss: 96.0609\n",
      "Epoch 2641/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 95.7924 - val_loss: 95.4578\n",
      "Epoch 2642/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.4658 - val_loss: 95.4474\n",
      "Epoch 2643/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.5910 - val_loss: 95.6012\n",
      "Epoch 2644/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.9460 - val_loss: 95.2297\n",
      "Epoch 2645/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.7774 - val_loss: 94.8190\n",
      "Epoch 2646/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.4772 - val_loss: 95.8700\n",
      "Epoch 2647/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.6761 - val_loss: 95.8639\n",
      "Epoch 2648/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.5559 - val_loss: 95.2451\n",
      "Epoch 2649/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.3333 - val_loss: 95.6239\n",
      "Epoch 2650/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.4942 - val_loss: 96.1911\n",
      "Epoch 2651/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 95.1156 - val_loss: 96.2511\n",
      "Epoch 2652/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 95.8120 - val_loss: 96.0236\n",
      "Epoch 2653/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.8507 - val_loss: 94.3884\n",
      "Epoch 2654/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 95.8234 - val_loss: 94.3362\n",
      "Epoch 2655/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8869 - val_loss: 94.3801\n",
      "Epoch 2656/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.4840 - val_loss: 94.8592\n",
      "Epoch 2657/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.3873 - val_loss: 96.7859\n",
      "Epoch 2658/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 118us/step - loss: 92.9154 - val_loss: 96.1587\n",
      "Epoch 2659/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 95.7371 - val_loss: 96.1798\n",
      "Epoch 2660/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.4298 - val_loss: 95.8681\n",
      "Epoch 2661/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.7658 - val_loss: 95.3180\n",
      "Epoch 2662/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.8510 - val_loss: 95.1692\n",
      "Epoch 2663/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.1015 - val_loss: 95.1062\n",
      "Epoch 2664/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.6753 - val_loss: 95.2357\n",
      "Epoch 2665/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.2196 - val_loss: 95.2067\n",
      "Epoch 2666/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.9570 - val_loss: 96.2584\n",
      "Epoch 2667/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.2367 - val_loss: 96.0776\n",
      "Epoch 2668/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 96.9617 - val_loss: 94.7124\n",
      "Epoch 2669/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.5868 - val_loss: 95.2438\n",
      "Epoch 2670/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.6422 - val_loss: 95.4248\n",
      "Epoch 2671/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.0540 - val_loss: 95.7195\n",
      "Epoch 2672/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.9881 - val_loss: 95.8534\n",
      "Epoch 2673/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.4083 - val_loss: 94.4434\n",
      "Epoch 2674/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.0476 - val_loss: 94.7605\n",
      "Epoch 2675/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.2035 - val_loss: 96.4153\n",
      "Epoch 2676/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.5206 - val_loss: 95.4567\n",
      "Epoch 2677/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.0173 - val_loss: 96.2561\n",
      "Epoch 2678/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.7888 - val_loss: 95.8498\n",
      "Epoch 2679/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 94.9790 - val_loss: 95.1519\n",
      "Epoch 2680/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.6932 - val_loss: 95.8890\n",
      "Epoch 2681/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.8303 - val_loss: 95.0410\n",
      "Epoch 2682/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.3885 - val_loss: 95.3408\n",
      "Epoch 2683/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.6186 - val_loss: 94.7510\n",
      "Epoch 2684/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.9347 - val_loss: 94.3398\n",
      "Epoch 2685/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.9009 - val_loss: 96.1300\n",
      "Epoch 2686/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.5868 - val_loss: 96.7008\n",
      "Epoch 2687/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.9424 - val_loss: 97.4018\n",
      "Epoch 2688/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 95.5613 - val_loss: 96.0881\n",
      "Epoch 2689/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.2828 - val_loss: 96.2325\n",
      "Epoch 2690/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.3017 - val_loss: 95.4312\n",
      "Epoch 2691/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.7320 - val_loss: 94.6475\n",
      "Epoch 2692/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.5841 - val_loss: 94.6898\n",
      "Epoch 2693/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.9182 - val_loss: 94.9493\n",
      "Epoch 2694/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.1187 - val_loss: 95.7342\n",
      "Epoch 2695/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.5135 - val_loss: 95.1349\n",
      "Epoch 2696/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 96.0648 - val_loss: 95.7087\n",
      "Epoch 2697/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.9924 - val_loss: 96.0639\n",
      "Epoch 2698/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.7942 - val_loss: 95.4770\n",
      "Epoch 2699/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.9380 - val_loss: 95.9767\n",
      "Epoch 2700/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 100.6394 - val_loss: 95.5477\n",
      "Epoch 2701/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.4649 - val_loss: 95.2095\n",
      "Epoch 2702/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.0374 - val_loss: 95.7352\n",
      "Epoch 2703/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.2330 - val_loss: 96.4004\n",
      "Epoch 2704/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.7349 - val_loss: 95.9181\n",
      "Epoch 2705/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.9190 - val_loss: 96.1735\n",
      "Epoch 2706/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 94.2733 - val_loss: 95.2683\n",
      "Epoch 2707/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.7079 - val_loss: 94.0931\n",
      "Epoch 2708/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 96.5556 - val_loss: 95.6919\n",
      "Epoch 2709/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.2516 - val_loss: 94.6562\n",
      "Epoch 2710/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.9116 - val_loss: 95.8230\n",
      "Epoch 2711/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.0613 - val_loss: 95.1701\n",
      "Epoch 2712/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.5616 - val_loss: 95.7950\n",
      "Epoch 2713/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.8536 - val_loss: 95.0280\n",
      "Epoch 2714/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.1563 - val_loss: 95.3587\n",
      "Epoch 2715/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.1542 - val_loss: 95.7705\n",
      "Epoch 2716/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.4598 - val_loss: 94.4268\n",
      "Epoch 2717/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.3986 - val_loss: 94.6587\n",
      "Epoch 2718/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.6380 - val_loss: 95.2132\n",
      "Epoch 2719/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 95.5915 - val_loss: 95.3147\n",
      "Epoch 2720/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.9679 - val_loss: 95.7155\n",
      "Epoch 2721/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.5242 - val_loss: 95.9650\n",
      "Epoch 2722/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 94.4571 - val_loss: 96.0457\n",
      "Epoch 2723/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.4930 - val_loss: 95.6244\n",
      "Epoch 2724/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 94.3737 - val_loss: 95.2734\n",
      "Epoch 2725/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.8715 - val_loss: 95.2642\n",
      "Epoch 2726/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.2645 - val_loss: 95.9529\n",
      "Epoch 2727/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 96.6844 - val_loss: 96.2728\n",
      "Epoch 2728/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.8059 - val_loss: 94.7153\n",
      "Epoch 2729/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.6003 - val_loss: 95.2143\n",
      "Epoch 2730/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.6520 - val_loss: 96.1688\n",
      "Epoch 2731/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 94.7297 - val_loss: 95.6339\n",
      "Epoch 2732/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.0148 - val_loss: 95.8548\n",
      "Epoch 2733/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.1393 - val_loss: 95.1104\n",
      "Epoch 2734/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 118us/step - loss: 94.4419 - val_loss: 94.6303\n",
      "Epoch 2735/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.1715 - val_loss: 94.7044\n",
      "Epoch 2736/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.8170 - val_loss: 96.2486\n",
      "Epoch 2737/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.8939 - val_loss: 96.7880\n",
      "Epoch 2738/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 94.9348 - val_loss: 95.2613\n",
      "Epoch 2739/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.7968 - val_loss: 95.0198\n",
      "Epoch 2740/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.9562 - val_loss: 95.3417\n",
      "Epoch 2741/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.9833 - val_loss: 94.7477\n",
      "Epoch 2742/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 94.4018 - val_loss: 94.6434\n",
      "Epoch 2743/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.2872 - val_loss: 95.6670\n",
      "Epoch 2744/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 94.4995 - val_loss: 95.0717\n",
      "Epoch 2745/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.4374 - val_loss: 95.7977\n",
      "Epoch 2746/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 93.8169 - val_loss: 95.3450\n",
      "Epoch 2747/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.2511 - val_loss: 94.4938\n",
      "Epoch 2748/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 94.8103 - val_loss: 95.6129\n",
      "Epoch 2749/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.4024 - val_loss: 95.2689\n",
      "Epoch 2750/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.3984 - val_loss: 96.5235\n",
      "Epoch 2751/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.2077 - val_loss: 95.9342\n",
      "Epoch 2752/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.4033 - val_loss: 96.0140\n",
      "Epoch 2753/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.2365 - val_loss: 95.7611\n",
      "Epoch 2754/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.2093 - val_loss: 96.0224\n",
      "Epoch 2755/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.7897 - val_loss: 95.8308\n",
      "Epoch 2756/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.6326 - val_loss: 96.0175\n",
      "Epoch 2757/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 94.5466 - val_loss: 95.4929\n",
      "Epoch 2758/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 94.5124 - val_loss: 95.8922\n",
      "Epoch 2759/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.2055 - val_loss: 94.6035\n",
      "Epoch 2760/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.1911 - val_loss: 96.5378\n",
      "Epoch 2761/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.6834 - val_loss: 95.0362\n",
      "Epoch 2762/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 95.8843 - val_loss: 95.6489\n",
      "Epoch 2763/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.2318 - val_loss: 95.2329\n",
      "Epoch 2764/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 95.7598 - val_loss: 97.3931\n",
      "Epoch 2765/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.6830 - val_loss: 95.4986\n",
      "Epoch 2766/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.8836 - val_loss: 95.3944\n",
      "Epoch 2767/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.4833 - val_loss: 95.0971\n",
      "Epoch 2768/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.7121 - val_loss: 95.4465\n",
      "Epoch 2769/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.8168 - val_loss: 96.1890\n",
      "Epoch 2770/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.0394 - val_loss: 95.2218\n",
      "Epoch 2771/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.1241 - val_loss: 96.4708\n",
      "Epoch 2772/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.4788 - val_loss: 95.2966\n",
      "Epoch 2773/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.5664 - val_loss: 95.4235\n",
      "Epoch 2774/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.7975 - val_loss: 96.9601\n",
      "Epoch 2775/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.9820 - val_loss: 96.4850\n",
      "Epoch 2776/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.7830 - val_loss: 96.0370\n",
      "Epoch 2777/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.7397 - val_loss: 95.0823\n",
      "Epoch 2778/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.1089 - val_loss: 95.7049\n",
      "Epoch 2779/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.3414 - val_loss: 95.5815\n",
      "Epoch 2780/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.3686 - val_loss: 96.7664\n",
      "Epoch 2781/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.4374 - val_loss: 94.7748\n",
      "Epoch 2782/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.8558 - val_loss: 95.0117\n",
      "Epoch 2783/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.8846 - val_loss: 95.7915\n",
      "Epoch 2784/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.4620 - val_loss: 96.3778\n",
      "Epoch 2785/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 95.1991 - val_loss: 95.5685\n",
      "Epoch 2786/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.2957 - val_loss: 95.5522\n",
      "Epoch 2787/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.9802 - val_loss: 95.9852\n",
      "Epoch 2788/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.0302 - val_loss: 96.0676\n",
      "Epoch 2789/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.3187 - val_loss: 94.2495\n",
      "Epoch 2790/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.9933 - val_loss: 94.8421\n",
      "Epoch 2791/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.1340 - val_loss: 95.5932\n",
      "Epoch 2792/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.2724 - val_loss: 95.2472\n",
      "Epoch 2793/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.9678 - val_loss: 95.9932\n",
      "Epoch 2794/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.3036 - val_loss: 95.6780\n",
      "Epoch 2795/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 95.3537 - val_loss: 95.5473\n",
      "Epoch 2796/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.4136 - val_loss: 95.5106\n",
      "Epoch 2797/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 93.8857 - val_loss: 94.4032\n",
      "Epoch 2798/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.9899 - val_loss: 94.7739\n",
      "Epoch 2799/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.5081 - val_loss: 94.0099\n",
      "Epoch 2800/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.1576 - val_loss: 95.0237\n",
      "Epoch 2801/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.3272 - val_loss: 95.1169\n",
      "Epoch 2802/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 95.9024 - val_loss: 95.9750\n",
      "Epoch 2803/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.8340 - val_loss: 95.8944\n",
      "Epoch 2804/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.9086 - val_loss: 95.1881\n",
      "Epoch 2805/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.1312 - val_loss: 96.9143\n",
      "Epoch 2806/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.1519 - val_loss: 95.9107\n",
      "Epoch 2807/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.7071 - val_loss: 96.3993\n",
      "Epoch 2808/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.9340 - val_loss: 95.9071\n",
      "Epoch 2809/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.1594 - val_loss: 95.3635\n",
      "Epoch 2810/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 122us/step - loss: 94.5874 - val_loss: 94.8535\n",
      "Epoch 2811/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.0943 - val_loss: 94.4278\n",
      "Epoch 2812/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.5883 - val_loss: 94.9147\n",
      "Epoch 2813/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 96.5315 - val_loss: 95.0810\n",
      "Epoch 2814/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.9351 - val_loss: 95.4303\n",
      "Epoch 2815/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.6883 - val_loss: 96.0550\n",
      "Epoch 2816/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.2175 - val_loss: 95.3291\n",
      "Epoch 2817/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 94.3833 - val_loss: 95.3945\n",
      "Epoch 2818/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.9564 - val_loss: 95.4218\n",
      "Epoch 2819/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.6235 - val_loss: 95.9659\n",
      "Epoch 2820/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.2443 - val_loss: 95.8173\n",
      "Epoch 2821/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 94.6627 - val_loss: 96.0557\n",
      "Epoch 2822/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 95.2932 - val_loss: 94.4769\n",
      "Epoch 2823/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.2314 - val_loss: 95.4457\n",
      "Epoch 2824/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.9085 - val_loss: 95.4300\n",
      "Epoch 2825/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.7562 - val_loss: 95.5759\n",
      "Epoch 2826/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.1573 - val_loss: 95.9269\n",
      "Epoch 2827/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.7607 - val_loss: 94.6971\n",
      "Epoch 2828/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.0305 - val_loss: 94.6019\n",
      "Epoch 2829/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.9659 - val_loss: 94.9378\n",
      "Epoch 2830/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.4837 - val_loss: 96.5477\n",
      "Epoch 2831/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.4176 - val_loss: 94.4695\n",
      "Epoch 2832/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 96.2125 - val_loss: 95.3228\n",
      "Epoch 2833/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.8670 - val_loss: 94.8265\n",
      "Epoch 2834/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.5621 - val_loss: 96.2833\n",
      "Epoch 2835/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.7580 - val_loss: 95.6283\n",
      "Epoch 2836/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.0860 - val_loss: 95.3451\n",
      "Epoch 2837/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.4232 - val_loss: 95.1461\n",
      "Epoch 2838/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 94.0046 - val_loss: 95.0904\n",
      "Epoch 2839/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.7446 - val_loss: 95.3222\n",
      "Epoch 2840/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.6651 - val_loss: 95.5307\n",
      "Epoch 2841/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.4205 - val_loss: 95.1819\n",
      "Epoch 2842/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.4195 - val_loss: 95.3865\n",
      "Epoch 2843/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 95.3597 - val_loss: 95.4273\n",
      "Epoch 2844/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.2383 - val_loss: 94.8377\n",
      "Epoch 2845/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 95.9702 - val_loss: 94.8370\n",
      "Epoch 2846/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.1392 - val_loss: 96.6009\n",
      "Epoch 2847/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 98.1603 - val_loss: 95.7280\n",
      "Epoch 2848/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.5165 - val_loss: 95.4689\n",
      "Epoch 2849/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.7325 - val_loss: 94.6622\n",
      "Epoch 2850/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.7935 - val_loss: 95.2414\n",
      "Epoch 2851/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 94.2081 - val_loss: 95.0447\n",
      "Epoch 2852/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.9636 - val_loss: 95.5915\n",
      "Epoch 2853/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 94.8370 - val_loss: 96.6304\n",
      "Epoch 2854/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.0609 - val_loss: 95.5252\n",
      "Epoch 2855/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.9375 - val_loss: 94.8311\n",
      "Epoch 2856/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 94.6281 - val_loss: 94.7527\n",
      "Epoch 2857/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.4392 - val_loss: 94.9890\n",
      "Epoch 2858/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 95.8247 - val_loss: 95.5894\n",
      "Epoch 2859/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.4196 - val_loss: 94.6991\n",
      "Epoch 2860/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.7946 - val_loss: 95.2354\n",
      "Epoch 2861/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 94.5487 - val_loss: 95.1894\n",
      "Epoch 2862/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 95.9558 - val_loss: 95.4960\n",
      "Epoch 2863/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.1332 - val_loss: 95.9546\n",
      "Epoch 2864/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.9436 - val_loss: 95.9577\n",
      "Epoch 2865/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 93.5883 - val_loss: 94.8485\n",
      "Epoch 2866/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.7676 - val_loss: 95.8240\n",
      "Epoch 2867/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.0540 - val_loss: 95.0861\n",
      "Epoch 2868/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.0016 - val_loss: 95.6798\n",
      "Epoch 2869/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.3229 - val_loss: 94.2613\n",
      "Epoch 2870/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.8339 - val_loss: 95.1704\n",
      "Epoch 2871/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.4728 - val_loss: 95.3280\n",
      "Epoch 2872/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 95.2225 - val_loss: 95.7162\n",
      "Epoch 2873/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 94.7496 - val_loss: 94.6139\n",
      "Epoch 2874/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.7175 - val_loss: 94.8974\n",
      "Epoch 2875/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7242 - val_loss: 94.6674\n",
      "Epoch 2876/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.3023 - val_loss: 94.4964\n",
      "Epoch 2877/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.8904 - val_loss: 95.4802\n",
      "Epoch 2878/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.6182 - val_loss: 95.7258\n",
      "Epoch 2879/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.1026 - val_loss: 95.3753\n",
      "Epoch 2880/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.0176 - val_loss: 95.5134\n",
      "Epoch 2881/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.2945 - val_loss: 95.8635\n",
      "Epoch 2882/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.4894 - val_loss: 94.5555\n",
      "Epoch 2883/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.9551 - val_loss: 94.9105\n",
      "Epoch 2884/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.0164 - val_loss: 96.7566\n",
      "Epoch 2885/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 97.6378 - val_loss: 96.5809\n",
      "Epoch 2886/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/step - loss: 95.0133 - val_loss: 95.2253\n",
      "Epoch 2887/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.4922 - val_loss: 95.1157\n",
      "Epoch 2888/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.8743 - val_loss: 96.2087\n",
      "Epoch 2889/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.4377 - val_loss: 95.2864\n",
      "Epoch 2890/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.8823 - val_loss: 94.9670\n",
      "Epoch 2891/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.1489 - val_loss: 95.0218\n",
      "Epoch 2892/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 93.9017 - val_loss: 95.6115\n",
      "Epoch 2893/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.8780 - val_loss: 95.3142\n",
      "Epoch 2894/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 95.8447 - val_loss: 95.4069\n",
      "Epoch 2895/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.9751 - val_loss: 93.9390\n",
      "Epoch 2896/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.9608 - val_loss: 94.5197\n",
      "Epoch 2897/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.8414 - val_loss: 94.8021\n",
      "Epoch 2898/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.5375 - val_loss: 94.3692\n",
      "Epoch 2899/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.6167 - val_loss: 95.1064\n",
      "Epoch 2900/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.3723 - val_loss: 95.5342\n",
      "Epoch 2901/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.2972 - val_loss: 95.9976\n",
      "Epoch 2902/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 95.9479 - val_loss: 95.4657\n",
      "Epoch 2903/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 96.2777 - val_loss: 96.0622\n",
      "Epoch 2904/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.6441 - val_loss: 95.4488\n",
      "Epoch 2905/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.7552 - val_loss: 95.5767\n",
      "Epoch 2906/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.0445 - val_loss: 95.7599\n",
      "Epoch 2907/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.1537 - val_loss: 94.8967\n",
      "Epoch 2908/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.4886 - val_loss: 96.1126\n",
      "Epoch 2909/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.0622 - val_loss: 94.4745\n",
      "Epoch 2910/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.6907 - val_loss: 95.0702\n",
      "Epoch 2911/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.4385 - val_loss: 96.2521\n",
      "Epoch 2912/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.9987 - val_loss: 95.0078\n",
      "Epoch 2913/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.6288 - val_loss: 95.9598\n",
      "Epoch 2914/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.3874 - val_loss: 95.4809\n",
      "Epoch 2915/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.1104 - val_loss: 95.0658\n",
      "Epoch 2916/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.3479 - val_loss: 94.8480\n",
      "Epoch 2917/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.2999 - val_loss: 94.6558\n",
      "Epoch 2918/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 95.8165 - val_loss: 96.5170\n",
      "Epoch 2919/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.5643 - val_loss: 95.3913\n",
      "Epoch 2920/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.3040 - val_loss: 95.5335\n",
      "Epoch 2921/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.8907 - val_loss: 95.8980\n",
      "Epoch 2922/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.3724 - val_loss: 96.3883\n",
      "Epoch 2923/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 94.2952 - val_loss: 96.1861\n",
      "Epoch 2924/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.1889 - val_loss: 95.3933\n",
      "Epoch 2925/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.0489 - val_loss: 95.6118\n",
      "Epoch 2926/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.7077 - val_loss: 95.4690\n",
      "Epoch 2927/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.7943 - val_loss: 95.5856\n",
      "Epoch 2928/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.3189 - val_loss: 95.5279\n",
      "Epoch 2929/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.3367 - val_loss: 95.2238\n",
      "Epoch 2930/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.5561 - val_loss: 96.4135\n",
      "Epoch 2931/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.7937 - val_loss: 95.0316\n",
      "Epoch 2932/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 94.0084 - val_loss: 95.3102\n",
      "Epoch 2933/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.1197 - val_loss: 96.0527\n",
      "Epoch 2934/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.5379 - val_loss: 95.6464\n",
      "Epoch 2935/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 94.1147 - val_loss: 95.3941\n",
      "Epoch 2936/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.3655 - val_loss: 94.3231\n",
      "Epoch 2937/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.0267 - val_loss: 95.4148\n",
      "Epoch 2938/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.1393 - val_loss: 96.1820\n",
      "Epoch 2939/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.6767 - val_loss: 96.1828\n",
      "Epoch 2940/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.2601 - val_loss: 95.1839\n",
      "Epoch 2941/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.7059 - val_loss: 95.3220\n",
      "Epoch 2942/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.0393 - val_loss: 95.2643\n",
      "Epoch 2943/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.8010 - val_loss: 94.8865\n",
      "Epoch 2944/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.3755 - val_loss: 95.7341\n",
      "Epoch 2945/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.5243 - val_loss: 95.0633\n",
      "Epoch 2946/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.0479 - val_loss: 95.8250\n",
      "Epoch 2947/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.2027 - val_loss: 95.3303\n",
      "Epoch 2948/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.9798 - val_loss: 95.3393\n",
      "Epoch 2949/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.7448 - val_loss: 96.1320\n",
      "Epoch 2950/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.9876 - val_loss: 95.8312\n",
      "Epoch 2951/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 94.3979 - val_loss: 94.7064\n",
      "Epoch 2952/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.7097 - val_loss: 95.8148\n",
      "Epoch 2953/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.0532 - val_loss: 95.8254\n",
      "Epoch 2954/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.5998 - val_loss: 95.5774\n",
      "Epoch 2955/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.4444 - val_loss: 95.4713\n",
      "Epoch 2956/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.3675 - val_loss: 96.2930\n",
      "Epoch 2957/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.2736 - val_loss: 94.7320\n",
      "Epoch 2958/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.0893 - val_loss: 94.7247\n",
      "Epoch 2959/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 94.6060 - val_loss: 95.5557\n",
      "Epoch 2960/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.6981 - val_loss: 94.9647\n",
      "Epoch 2961/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.1534 - val_loss: 96.3639\n",
      "Epoch 2962/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 130us/step - loss: 93.8111 - val_loss: 95.1561\n",
      "Epoch 2963/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.6767 - val_loss: 95.1832\n",
      "Epoch 2964/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 95.4370 - val_loss: 96.3934\n",
      "Epoch 2965/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 95.8843 - val_loss: 96.1489\n",
      "Epoch 2966/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.2757 - val_loss: 95.7002\n",
      "Epoch 2967/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.3602 - val_loss: 95.7112\n",
      "Epoch 2968/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.2248 - val_loss: 95.8246\n",
      "Epoch 2969/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.9805 - val_loss: 95.8545\n",
      "Epoch 2970/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.0297 - val_loss: 95.4597\n",
      "Epoch 2971/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 94.3330 - val_loss: 95.5889\n",
      "Epoch 2972/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.6079 - val_loss: 95.5537\n",
      "Epoch 2973/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.9196 - val_loss: 95.2314\n",
      "Epoch 2974/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.8592 - val_loss: 95.5509\n",
      "Epoch 2975/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.0373 - val_loss: 96.0663\n",
      "Epoch 2976/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.5046 - val_loss: 95.5059\n",
      "Epoch 2977/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 94.2419 - val_loss: 94.7671\n",
      "Epoch 2978/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.3953 - val_loss: 95.4031\n",
      "Epoch 2979/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.1604 - val_loss: 95.9646\n",
      "Epoch 2980/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.0203 - val_loss: 95.6917\n",
      "Epoch 2981/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.5652 - val_loss: 95.5004\n",
      "Epoch 2982/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.5603 - val_loss: 95.8618\n",
      "Epoch 2983/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.1897 - val_loss: 95.1351\n",
      "Epoch 2984/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 94.5547 - val_loss: 94.6134\n",
      "Epoch 2985/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.3748 - val_loss: 95.4313\n",
      "Epoch 2986/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.0782 - val_loss: 94.3830\n",
      "Epoch 2987/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.1693 - val_loss: 95.1435\n",
      "Epoch 2988/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.7659 - val_loss: 95.4480\n",
      "Epoch 2989/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.2312 - val_loss: 95.7341\n",
      "Epoch 2990/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 94.5168 - val_loss: 95.4450\n",
      "Epoch 2991/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 95.0262 - val_loss: 96.6837\n",
      "Epoch 2992/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.2887 - val_loss: 94.7251\n",
      "Epoch 2993/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.1338 - val_loss: 94.2943\n",
      "Epoch 2994/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.9626 - val_loss: 94.6889\n",
      "Epoch 2995/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.3391 - val_loss: 95.4068\n",
      "Epoch 2996/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.1533 - val_loss: 96.3143\n",
      "Epoch 2997/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.3024 - val_loss: 94.8536\n",
      "Epoch 2998/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.0944 - val_loss: 96.0496\n",
      "Epoch 2999/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.6942 - val_loss: 95.6070\n",
      "Epoch 3000/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.6922 - val_loss: 94.7227\n",
      "Epoch 3001/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.9045 - val_loss: 95.0679\n",
      "Epoch 3002/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.4059 - val_loss: 94.4900\n",
      "Epoch 3003/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 98.4181 - val_loss: 95.7779\n",
      "Epoch 3004/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.5876 - val_loss: 96.4348\n",
      "Epoch 3005/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 97.8767 - val_loss: 95.3826\n",
      "Epoch 3006/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.4073 - val_loss: 95.6291\n",
      "Epoch 3007/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.7267 - val_loss: 95.9606\n",
      "Epoch 3008/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.7453 - val_loss: 96.0132\n",
      "Epoch 3009/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.3419 - val_loss: 94.8846\n",
      "Epoch 3010/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.4717 - val_loss: 94.8672\n",
      "Epoch 3011/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 94.0621 - val_loss: 95.1336\n",
      "Epoch 3012/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.0935 - val_loss: 94.8470\n",
      "Epoch 3013/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.8985 - val_loss: 96.1528\n",
      "Epoch 3014/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.5919 - val_loss: 95.9031\n",
      "Epoch 3015/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.2886 - val_loss: 95.3519\n",
      "Epoch 3016/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.8827 - val_loss: 95.2884\n",
      "Epoch 3017/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.2561 - val_loss: 96.1305\n",
      "Epoch 3018/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.5483 - val_loss: 96.0229\n",
      "Epoch 3019/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 94.8349 - val_loss: 96.0817\n",
      "Epoch 3020/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.3157 - val_loss: 95.8710\n",
      "Epoch 3021/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6960 - val_loss: 95.8534\n",
      "Epoch 3022/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.2379 - val_loss: 94.2793\n",
      "Epoch 3023/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.1829 - val_loss: 94.6022\n",
      "Epoch 3024/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.0597 - val_loss: 96.1844\n",
      "Epoch 3025/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 95.8927 - val_loss: 95.2778\n",
      "Epoch 3026/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 95.2735 - val_loss: 95.8241\n",
      "Epoch 3027/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.8978 - val_loss: 95.7498\n",
      "Epoch 3028/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.4808 - val_loss: 95.8101\n",
      "Epoch 3029/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 95.5604 - val_loss: 95.1524\n",
      "Epoch 3030/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.0092 - val_loss: 93.8044\n",
      "Epoch 3031/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 94.4708 - val_loss: 95.5928\n",
      "Epoch 3032/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.8930 - val_loss: 96.5247\n",
      "Epoch 3033/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.4148 - val_loss: 94.4971\n",
      "Epoch 3034/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.2171 - val_loss: 94.5636\n",
      "Epoch 3035/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.6348 - val_loss: 95.7437\n",
      "Epoch 3036/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 96.2138 - val_loss: 96.3765\n",
      "Epoch 3037/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.6055 - val_loss: 95.2556\n",
      "Epoch 3038/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 122us/step - loss: 93.1338 - val_loss: 95.1253\n",
      "Epoch 3039/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.0523 - val_loss: 96.3859\n",
      "Epoch 3040/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.5256 - val_loss: 95.6288\n",
      "Epoch 3041/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.6250 - val_loss: 95.1331\n",
      "Epoch 3042/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.7287 - val_loss: 95.2409\n",
      "Epoch 3043/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.3907 - val_loss: 95.9460\n",
      "Epoch 3044/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.5269 - val_loss: 95.4094\n",
      "Epoch 3045/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.5981 - val_loss: 96.0825\n",
      "Epoch 3046/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.0270 - val_loss: 96.6822\n",
      "Epoch 3047/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.6340 - val_loss: 95.5112\n",
      "Epoch 3048/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.4954 - val_loss: 95.7587\n",
      "Epoch 3049/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.5492 - val_loss: 96.2245\n",
      "Epoch 3050/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 94.0645 - val_loss: 96.0102\n",
      "Epoch 3051/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.9042 - val_loss: 95.5086\n",
      "Epoch 3052/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.3075 - val_loss: 94.8359\n",
      "Epoch 3053/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 94.0245 - val_loss: 95.2137\n",
      "Epoch 3054/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.8011 - val_loss: 95.2081\n",
      "Epoch 3055/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.1307 - val_loss: 95.4614\n",
      "Epoch 3056/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.0308 - val_loss: 95.5714\n",
      "Epoch 3057/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.0707 - val_loss: 96.1780\n",
      "Epoch 3058/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 94.5144 - val_loss: 95.7122\n",
      "Epoch 3059/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 95.3442 - val_loss: 95.2450\n",
      "Epoch 3060/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.8861 - val_loss: 95.3974\n",
      "Epoch 3061/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.6238 - val_loss: 94.4217\n",
      "Epoch 3062/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 97.2341 - val_loss: 94.0395\n",
      "Epoch 3063/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.9504 - val_loss: 95.1184\n",
      "Epoch 3064/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.1261 - val_loss: 95.9628\n",
      "Epoch 3065/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.3217 - val_loss: 95.9535\n",
      "Epoch 3066/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.8572 - val_loss: 96.1653\n",
      "Epoch 3067/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.2258 - val_loss: 95.7738\n",
      "Epoch 3068/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.5116 - val_loss: 95.1254\n",
      "Epoch 3069/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.7156 - val_loss: 95.0441\n",
      "Epoch 3070/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 94.0507 - val_loss: 96.0182\n",
      "Epoch 3071/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.6202 - val_loss: 94.7766\n",
      "Epoch 3072/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.0603 - val_loss: 94.7048\n",
      "Epoch 3073/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.6049 - val_loss: 96.1499\n",
      "Epoch 3074/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.3069 - val_loss: 94.7434\n",
      "Epoch 3075/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.9933 - val_loss: 94.7550\n",
      "Epoch 3076/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.4752 - val_loss: 95.5978\n",
      "Epoch 3077/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 94.4646 - val_loss: 95.4838\n",
      "Epoch 3078/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.2489 - val_loss: 95.7247\n",
      "Epoch 3079/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 92.9403 - val_loss: 95.1063\n",
      "Epoch 3080/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.9369 - val_loss: 96.4900\n",
      "Epoch 3081/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.2962 - val_loss: 95.6817\n",
      "Epoch 3082/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.9424 - val_loss: 95.3958\n",
      "Epoch 3083/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.9733 - val_loss: 94.6470\n",
      "Epoch 3084/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.9476 - val_loss: 94.1084\n",
      "Epoch 3085/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.0462 - val_loss: 95.8716\n",
      "Epoch 3086/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.6587 - val_loss: 95.1351\n",
      "Epoch 3087/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.3243 - val_loss: 95.7245\n",
      "Epoch 3088/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.5858 - val_loss: 95.7813\n",
      "Epoch 3089/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.0696 - val_loss: 96.2449\n",
      "Epoch 3090/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.2301 - val_loss: 95.4200\n",
      "Epoch 3091/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 93.4654 - val_loss: 94.8950\n",
      "Epoch 3092/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.5278 - val_loss: 94.3643\n",
      "Epoch 3093/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.2140 - val_loss: 94.7672\n",
      "Epoch 3094/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.7165 - val_loss: 94.8473\n",
      "Epoch 3095/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.4230 - val_loss: 94.6168\n",
      "Epoch 3096/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 97.4809 - val_loss: 95.7730\n",
      "Epoch 3097/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 93.9344 - val_loss: 96.2177\n",
      "Epoch 3098/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.6599 - val_loss: 95.4049\n",
      "Epoch 3099/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.7969 - val_loss: 95.0745\n",
      "Epoch 3100/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.2799 - val_loss: 95.0757\n",
      "Epoch 3101/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.0203 - val_loss: 95.1736\n",
      "Epoch 3102/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.1635 - val_loss: 94.8730\n",
      "Epoch 3103/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.3974 - val_loss: 93.9983\n",
      "Epoch 3104/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.4705 - val_loss: 94.4412\n",
      "Epoch 3105/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9144 - val_loss: 95.0570\n",
      "Epoch 3106/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.2457 - val_loss: 95.0055\n",
      "Epoch 3107/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.8706 - val_loss: 95.6073\n",
      "Epoch 3108/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.1018 - val_loss: 95.7198\n",
      "Epoch 3109/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.3293 - val_loss: 96.2314\n",
      "Epoch 3110/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.7349 - val_loss: 95.9061\n",
      "Epoch 3111/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.1250 - val_loss: 94.5831\n",
      "Epoch 3112/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 93.3349 - val_loss: 94.3247\n",
      "Epoch 3113/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.7312 - val_loss: 94.8266\n",
      "Epoch 3114/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 121us/step - loss: 93.5308 - val_loss: 96.6413\n",
      "Epoch 3115/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.1661 - val_loss: 96.2537\n",
      "Epoch 3116/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.8613 - val_loss: 95.8380\n",
      "Epoch 3117/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.9695 - val_loss: 95.3157\n",
      "Epoch 3118/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.4602 - val_loss: 96.5030\n",
      "Epoch 3119/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.1100 - val_loss: 95.3503\n",
      "Epoch 3120/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.2950 - val_loss: 95.6688\n",
      "Epoch 3121/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.9820 - val_loss: 94.7169\n",
      "Epoch 3122/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.9954 - val_loss: 94.2248\n",
      "Epoch 3123/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.8667 - val_loss: 95.0475\n",
      "Epoch 3124/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.6634 - val_loss: 95.0646\n",
      "Epoch 3125/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.3802 - val_loss: 95.9835\n",
      "Epoch 3126/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.4099 - val_loss: 94.6650\n",
      "Epoch 3127/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 95.0164 - val_loss: 96.3168\n",
      "Epoch 3128/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.3277 - val_loss: 95.8367\n",
      "Epoch 3129/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.4136 - val_loss: 96.0919\n",
      "Epoch 3130/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 94.3195 - val_loss: 95.0121\n",
      "Epoch 3131/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.6076 - val_loss: 94.0881\n",
      "Epoch 3132/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.2067 - val_loss: 96.0181\n",
      "Epoch 3133/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.2665 - val_loss: 96.0911\n",
      "Epoch 3134/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.2130 - val_loss: 95.1597\n",
      "Epoch 3135/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.3035 - val_loss: 95.5460\n",
      "Epoch 3136/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.5233 - val_loss: 95.7811\n",
      "Epoch 3137/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.0204 - val_loss: 95.8533\n",
      "Epoch 3138/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 97.4650 - val_loss: 95.4856\n",
      "Epoch 3139/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.3192 - val_loss: 94.9017\n",
      "Epoch 3140/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.3412 - val_loss: 95.7716\n",
      "Epoch 3141/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.0690 - val_loss: 95.3325\n",
      "Epoch 3142/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.8022 - val_loss: 95.0484\n",
      "Epoch 3143/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.9001 - val_loss: 95.3644\n",
      "Epoch 3144/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.1650 - val_loss: 95.1383\n",
      "Epoch 3145/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.4392 - val_loss: 95.0747\n",
      "Epoch 3146/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.9999 - val_loss: 93.9323\n",
      "Epoch 3147/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.3862 - val_loss: 95.7581\n",
      "Epoch 3148/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.7085 - val_loss: 94.3148\n",
      "Epoch 3149/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.3766 - val_loss: 95.3523\n",
      "Epoch 3150/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.9384 - val_loss: 95.5301\n",
      "Epoch 3151/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.4856 - val_loss: 95.4411\n",
      "Epoch 3152/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.1521 - val_loss: 95.0797\n",
      "Epoch 3153/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 96.0197 - val_loss: 95.6040\n",
      "Epoch 3154/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.2597 - val_loss: 95.5184\n",
      "Epoch 3155/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.7598 - val_loss: 96.2423\n",
      "Epoch 3156/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.1808 - val_loss: 95.2307\n",
      "Epoch 3157/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.8920 - val_loss: 94.5391\n",
      "Epoch 3158/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.7418 - val_loss: 94.7886\n",
      "Epoch 3159/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.3393 - val_loss: 94.8737\n",
      "Epoch 3160/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 93.2991 - val_loss: 95.0256\n",
      "Epoch 3161/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5082 - val_loss: 95.9830\n",
      "Epoch 3162/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.9128 - val_loss: 95.8766\n",
      "Epoch 3163/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.4509 - val_loss: 95.7659\n",
      "Epoch 3164/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 95.9530 - val_loss: 94.8127\n",
      "Epoch 3165/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.4333 - val_loss: 95.2759\n",
      "Epoch 3166/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 94.4473 - val_loss: 95.1436\n",
      "Epoch 3167/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 95.2181 - val_loss: 94.8473\n",
      "Epoch 3168/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.9392 - val_loss: 94.6401\n",
      "Epoch 3169/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.1196 - val_loss: 94.3574\n",
      "Epoch 3170/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 94.0817 - val_loss: 94.7053\n",
      "Epoch 3171/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.5475 - val_loss: 95.5143\n",
      "Epoch 3172/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.1710 - val_loss: 97.2756\n",
      "Epoch 3173/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.7978 - val_loss: 94.9949\n",
      "Epoch 3174/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.7260 - val_loss: 95.4781\n",
      "Epoch 3175/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.9844 - val_loss: 96.0058\n",
      "Epoch 3176/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.3952 - val_loss: 96.2515\n",
      "Epoch 3177/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.8095 - val_loss: 95.4387\n",
      "Epoch 3178/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.1947 - val_loss: 95.9019\n",
      "Epoch 3179/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.6683 - val_loss: 94.5793\n",
      "Epoch 3180/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.1637 - val_loss: 95.1984\n",
      "Epoch 3181/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.0163 - val_loss: 96.2361\n",
      "Epoch 3182/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.8262 - val_loss: 95.2522\n",
      "Epoch 3183/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.3216 - val_loss: 95.5056\n",
      "Epoch 3184/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8080 - val_loss: 94.6942\n",
      "Epoch 3185/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.8584 - val_loss: 95.1202\n",
      "Epoch 3186/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.6028 - val_loss: 94.7855\n",
      "Epoch 3187/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.3337 - val_loss: 94.5209\n",
      "Epoch 3188/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.9195 - val_loss: 94.4305\n",
      "Epoch 3189/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.0297 - val_loss: 95.9644\n",
      "Epoch 3190/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 119us/step - loss: 93.7443 - val_loss: 96.3118\n",
      "Epoch 3191/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.6401 - val_loss: 95.6238\n",
      "Epoch 3192/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.0201 - val_loss: 95.4165\n",
      "Epoch 3193/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.7939 - val_loss: 95.1689\n",
      "Epoch 3194/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.6564 - val_loss: 95.3819\n",
      "Epoch 3195/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.2933 - val_loss: 95.2967\n",
      "Epoch 3196/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 95.1908 - val_loss: 95.1535\n",
      "Epoch 3197/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 93.6142 - val_loss: 94.8219\n",
      "Epoch 3198/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.4125 - val_loss: 94.6564\n",
      "Epoch 3199/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.2703 - val_loss: 94.8783\n",
      "Epoch 3200/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.0864 - val_loss: 94.1071\n",
      "Epoch 3201/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 97.8650 - val_loss: 95.1004\n",
      "Epoch 3202/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.6086 - val_loss: 95.0800\n",
      "Epoch 3203/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6456 - val_loss: 94.8645\n",
      "Epoch 3204/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.7991 - val_loss: 95.0796\n",
      "Epoch 3205/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.0363 - val_loss: 95.5915\n",
      "Epoch 3206/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.5966 - val_loss: 97.1493\n",
      "Epoch 3207/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.8656 - val_loss: 96.0372\n",
      "Epoch 3208/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 97.1352 - val_loss: 95.8022\n",
      "Epoch 3209/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 94.7011 - val_loss: 95.4799\n",
      "Epoch 3210/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.9136 - val_loss: 95.2360\n",
      "Epoch 3211/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.5766 - val_loss: 94.8665\n",
      "Epoch 3212/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.4011 - val_loss: 95.3345\n",
      "Epoch 3213/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 98.0485 - val_loss: 94.5238\n",
      "Epoch 3214/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.7316 - val_loss: 95.3716\n",
      "Epoch 3215/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.9977 - val_loss: 95.0161\n",
      "Epoch 3216/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 96.1753 - val_loss: 95.3211\n",
      "Epoch 3217/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.2206 - val_loss: 94.9877\n",
      "Epoch 3218/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.5713 - val_loss: 94.8053\n",
      "Epoch 3219/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.4661 - val_loss: 94.5666\n",
      "Epoch 3220/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 95.0848 - val_loss: 94.9144\n",
      "Epoch 3221/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.8659 - val_loss: 94.5854\n",
      "Epoch 3222/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.6977 - val_loss: 95.5628\n",
      "Epoch 3223/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.6067 - val_loss: 95.0196\n",
      "Epoch 3224/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.4617 - val_loss: 95.4321\n",
      "Epoch 3225/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.8950 - val_loss: 95.7264\n",
      "Epoch 3226/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 94.2837 - val_loss: 95.3960\n",
      "Epoch 3227/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.6159 - val_loss: 96.5649\n",
      "Epoch 3228/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.5960 - val_loss: 96.6188\n",
      "Epoch 3229/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.0962 - val_loss: 94.8225\n",
      "Epoch 3230/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.7064 - val_loss: 94.5884\n",
      "Epoch 3231/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3309 - val_loss: 95.3464\n",
      "Epoch 3232/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.0212 - val_loss: 95.6160\n",
      "Epoch 3233/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.2334 - val_loss: 95.7119\n",
      "Epoch 3234/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.4659 - val_loss: 95.1396\n",
      "Epoch 3235/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.2388 - val_loss: 96.1255\n",
      "Epoch 3236/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.7736 - val_loss: 95.9192\n",
      "Epoch 3237/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.9861 - val_loss: 94.6110\n",
      "Epoch 3238/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.0776 - val_loss: 96.5111\n",
      "Epoch 3239/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.3148 - val_loss: 95.8276\n",
      "Epoch 3240/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.4815 - val_loss: 95.7702\n",
      "Epoch 3241/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.2056 - val_loss: 94.4315\n",
      "Epoch 3242/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8945 - val_loss: 94.9394\n",
      "Epoch 3243/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.9388 - val_loss: 95.1698\n",
      "Epoch 3244/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.7736 - val_loss: 95.3489\n",
      "Epoch 3245/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.8150 - val_loss: 95.2252\n",
      "Epoch 3246/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.8966 - val_loss: 95.7937\n",
      "Epoch 3247/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.8961 - val_loss: 94.6382\n",
      "Epoch 3248/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 96.9243 - val_loss: 95.2081\n",
      "Epoch 3249/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.0139 - val_loss: 95.0174\n",
      "Epoch 3250/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 94.5505 - val_loss: 95.8877\n",
      "Epoch 3251/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.2135 - val_loss: 95.4237\n",
      "Epoch 3252/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.8166 - val_loss: 96.8883\n",
      "Epoch 3253/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.4829 - val_loss: 94.8395\n",
      "Epoch 3254/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.4467 - val_loss: 95.5213\n",
      "Epoch 3255/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.7240 - val_loss: 95.6613\n",
      "Epoch 3256/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.3432 - val_loss: 95.0510\n",
      "Epoch 3257/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.9424 - val_loss: 94.4378\n",
      "Epoch 3258/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.6932 - val_loss: 95.4067\n",
      "Epoch 3259/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.9276 - val_loss: 95.5849\n",
      "Epoch 3260/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.0194 - val_loss: 95.7789\n",
      "Epoch 3261/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.5001 - val_loss: 95.6219\n",
      "Epoch 3262/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.8345 - val_loss: 95.8959\n",
      "Epoch 3263/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.5054 - val_loss: 95.6829\n",
      "Epoch 3264/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.7578 - val_loss: 95.0593\n",
      "Epoch 3265/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.5820 - val_loss: 94.5295\n",
      "Epoch 3266/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 126us/step - loss: 94.5536 - val_loss: 93.9181\n",
      "Epoch 3267/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.1546 - val_loss: 94.6752\n",
      "Epoch 3268/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.6361 - val_loss: 94.5238\n",
      "Epoch 3269/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.5001 - val_loss: 95.3377\n",
      "Epoch 3270/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.2327 - val_loss: 95.4193\n",
      "Epoch 3271/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.9591 - val_loss: 95.1247\n",
      "Epoch 3272/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.5102 - val_loss: 95.7484\n",
      "Epoch 3273/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.5725 - val_loss: 95.3921\n",
      "Epoch 3274/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.6432 - val_loss: 94.4769\n",
      "Epoch 3275/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.3445 - val_loss: 94.8022\n",
      "Epoch 3276/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 93.5716 - val_loss: 95.4014\n",
      "Epoch 3277/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 95.2772 - val_loss: 95.7428\n",
      "Epoch 3278/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 95.6417 - val_loss: 95.5893\n",
      "Epoch 3279/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.9568 - val_loss: 95.1161\n",
      "Epoch 3280/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.1018 - val_loss: 95.5876\n",
      "Epoch 3281/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.3360 - val_loss: 95.1559\n",
      "Epoch 3282/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 96.0105 - val_loss: 94.7800\n",
      "Epoch 3283/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.1212 - val_loss: 95.3906\n",
      "Epoch 3284/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.5235 - val_loss: 95.0093\n",
      "Epoch 3285/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.5995 - val_loss: 95.6876\n",
      "Epoch 3286/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.9175 - val_loss: 95.3169\n",
      "Epoch 3287/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.9745 - val_loss: 95.3302\n",
      "Epoch 3288/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.4807 - val_loss: 95.4864\n",
      "Epoch 3289/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.4085 - val_loss: 95.0219\n",
      "Epoch 3290/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.0659 - val_loss: 95.1015\n",
      "Epoch 3291/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.7045 - val_loss: 95.4053\n",
      "Epoch 3292/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 93.5946 - val_loss: 95.5678\n",
      "Epoch 3293/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.0848 - val_loss: 95.1530\n",
      "Epoch 3294/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.0498 - val_loss: 95.6886\n",
      "Epoch 3295/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.3813 - val_loss: 95.0657\n",
      "Epoch 3296/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.0087 - val_loss: 95.5025\n",
      "Epoch 3297/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.9280 - val_loss: 95.1444\n",
      "Epoch 3298/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.0286 - val_loss: 94.9922\n",
      "Epoch 3299/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.4165 - val_loss: 95.1117\n",
      "Epoch 3300/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 96.5704 - val_loss: 94.8830\n",
      "Epoch 3301/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 94.0383 - val_loss: 96.9819\n",
      "Epoch 3302/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.9613 - val_loss: 95.2904\n",
      "Epoch 3303/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.3505 - val_loss: 95.5017\n",
      "Epoch 3304/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.0427 - val_loss: 94.6551\n",
      "Epoch 3305/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 97.5016 - val_loss: 95.4039\n",
      "Epoch 3306/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.8350 - val_loss: 95.3635\n",
      "Epoch 3307/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.5039 - val_loss: 96.1393\n",
      "Epoch 3308/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 93.5179 - val_loss: 95.6045\n",
      "Epoch 3309/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.9356 - val_loss: 94.1615\n",
      "Epoch 3310/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.6325 - val_loss: 96.0877\n",
      "Epoch 3311/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 95.1558 - val_loss: 96.1438\n",
      "Epoch 3312/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.9981 - val_loss: 95.4986\n",
      "Epoch 3313/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.5220 - val_loss: 96.1757\n",
      "Epoch 3314/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.6103 - val_loss: 96.0853\n",
      "Epoch 3315/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 94.6133 - val_loss: 95.8730\n",
      "Epoch 3316/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.8256 - val_loss: 95.4210\n",
      "Epoch 3317/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.8605 - val_loss: 95.9249\n",
      "Epoch 3318/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.9971 - val_loss: 95.5433\n",
      "Epoch 3319/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.9427 - val_loss: 95.4896\n",
      "Epoch 3320/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.1292 - val_loss: 95.4232\n",
      "Epoch 3321/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.5963 - val_loss: 94.9884\n",
      "Epoch 3322/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.4351 - val_loss: 95.2473\n",
      "Epoch 3323/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.3716 - val_loss: 95.0579\n",
      "Epoch 3324/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.4426 - val_loss: 94.8617\n",
      "Epoch 3325/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.2459 - val_loss: 95.4799\n",
      "Epoch 3326/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 94.2838 - val_loss: 95.6167\n",
      "Epoch 3327/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.0819 - val_loss: 95.1962\n",
      "Epoch 3328/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.3341 - val_loss: 95.2934\n",
      "Epoch 3329/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.8931 - val_loss: 95.5148\n",
      "Epoch 3330/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.6328 - val_loss: 95.2292\n",
      "Epoch 3331/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.8169 - val_loss: 96.1838\n",
      "Epoch 3332/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 93.6786 - val_loss: 95.8061\n",
      "Epoch 3333/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.0643 - val_loss: 96.0868\n",
      "Epoch 3334/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 92.5560 - val_loss: 94.4482\n",
      "Epoch 3335/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.0757 - val_loss: 94.0408\n",
      "Epoch 3336/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.8969 - val_loss: 95.7465\n",
      "Epoch 3337/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 94.7370 - val_loss: 94.5650\n",
      "Epoch 3338/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.1311 - val_loss: 94.6356\n",
      "Epoch 3339/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.7694 - val_loss: 95.2153\n",
      "Epoch 3340/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.3510 - val_loss: 95.4532\n",
      "Epoch 3341/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.5419 - val_loss: 95.5356\n",
      "Epoch 3342/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 114us/step - loss: 94.0349 - val_loss: 95.7633\n",
      "Epoch 3343/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.7249 - val_loss: 94.8590\n",
      "Epoch 3344/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.4814 - val_loss: 95.4072\n",
      "Epoch 3345/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.3430 - val_loss: 94.8938\n",
      "Epoch 3346/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.4600 - val_loss: 95.0678\n",
      "Epoch 3347/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.9396 - val_loss: 94.0723\n",
      "Epoch 3348/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.9695 - val_loss: 94.9325\n",
      "Epoch 3349/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.7705 - val_loss: 94.6312\n",
      "Epoch 3350/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.6843 - val_loss: 95.6218\n",
      "Epoch 3351/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 94.3883 - val_loss: 95.6417\n",
      "Epoch 3352/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.7774 - val_loss: 95.3580\n",
      "Epoch 3353/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.9535 - val_loss: 95.2253\n",
      "Epoch 3354/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.9905 - val_loss: 94.5269\n",
      "Epoch 3355/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 95.1721 - val_loss: 95.2376\n",
      "Epoch 3356/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.4316 - val_loss: 95.3707\n",
      "Epoch 3357/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.1872 - val_loss: 95.6582\n",
      "Epoch 3358/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.4074 - val_loss: 94.3351\n",
      "Epoch 3359/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 96.4375 - val_loss: 94.3511\n",
      "Epoch 3360/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.6200 - val_loss: 95.7367\n",
      "Epoch 3361/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.8929 - val_loss: 95.5299\n",
      "Epoch 3362/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6018 - val_loss: 94.8197\n",
      "Epoch 3363/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.3115 - val_loss: 94.8556\n",
      "Epoch 3364/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.7084 - val_loss: 95.1565\n",
      "Epoch 3365/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.2176 - val_loss: 94.9871\n",
      "Epoch 3366/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.2978 - val_loss: 95.8991\n",
      "Epoch 3367/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.4597 - val_loss: 96.2106\n",
      "Epoch 3368/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.8932 - val_loss: 95.5012\n",
      "Epoch 3369/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7191 - val_loss: 94.9621\n",
      "Epoch 3370/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.5187 - val_loss: 95.1859\n",
      "Epoch 3371/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.6015 - val_loss: 95.0590\n",
      "Epoch 3372/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.0208 - val_loss: 94.6000\n",
      "Epoch 3373/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.9260 - val_loss: 94.9269\n",
      "Epoch 3374/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.6907 - val_loss: 95.2719\n",
      "Epoch 3375/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.8303 - val_loss: 95.1536\n",
      "Epoch 3376/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.8093 - val_loss: 95.3699\n",
      "Epoch 3377/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 92.1074 - val_loss: 95.4024\n",
      "Epoch 3378/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.0099 - val_loss: 95.9038\n",
      "Epoch 3379/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 93.4079 - val_loss: 95.7068\n",
      "Epoch 3380/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.6329 - val_loss: 95.4036\n",
      "Epoch 3381/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.8106 - val_loss: 95.5042\n",
      "Epoch 3382/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.8258 - val_loss: 95.8274\n",
      "Epoch 3383/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.4136 - val_loss: 95.1097\n",
      "Epoch 3384/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.1323 - val_loss: 94.9173\n",
      "Epoch 3385/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.9911 - val_loss: 95.3180\n",
      "Epoch 3386/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.4052 - val_loss: 94.7524\n",
      "Epoch 3387/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.9322 - val_loss: 95.6195\n",
      "Epoch 3388/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 94.9470 - val_loss: 96.1404\n",
      "Epoch 3389/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 94.5030 - val_loss: 96.1221\n",
      "Epoch 3390/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.2952 - val_loss: 95.2380\n",
      "Epoch 3391/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.6625 - val_loss: 95.3162\n",
      "Epoch 3392/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.0519 - val_loss: 95.2495\n",
      "Epoch 3393/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.9939 - val_loss: 95.8501\n",
      "Epoch 3394/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.4640 - val_loss: 95.1685\n",
      "Epoch 3395/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.8411 - val_loss: 95.2011\n",
      "Epoch 3396/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.0795 - val_loss: 95.6401\n",
      "Epoch 3397/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.1589 - val_loss: 95.5001\n",
      "Epoch 3398/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 94.8397 - val_loss: 95.8432\n",
      "Epoch 3399/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 96.4966 - val_loss: 95.6025\n",
      "Epoch 3400/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.1371 - val_loss: 94.7915\n",
      "Epoch 3401/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.2610 - val_loss: 95.1712\n",
      "Epoch 3402/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.6842 - val_loss: 94.5030\n",
      "Epoch 3403/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.2993 - val_loss: 95.8239\n",
      "Epoch 3404/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.7317 - val_loss: 95.2485\n",
      "Epoch 3405/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.4316 - val_loss: 94.8833\n",
      "Epoch 3406/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.5437 - val_loss: 95.2685\n",
      "Epoch 3407/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 93.5293 - val_loss: 94.8381\n",
      "Epoch 3408/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 95.1864 - val_loss: 95.1067\n",
      "Epoch 3409/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.0607 - val_loss: 94.7606\n",
      "Epoch 3410/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.4645 - val_loss: 95.8002\n",
      "Epoch 3411/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.7223 - val_loss: 95.5500\n",
      "Epoch 3412/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.2941 - val_loss: 95.2806\n",
      "Epoch 3413/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.2814 - val_loss: 95.6873\n",
      "Epoch 3414/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.6109 - val_loss: 95.6810\n",
      "Epoch 3415/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.8579 - val_loss: 94.9787\n",
      "Epoch 3416/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.8241 - val_loss: 95.2892\n",
      "Epoch 3417/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.6570 - val_loss: 95.3277\n",
      "Epoch 3418/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 115us/step - loss: 92.7487 - val_loss: 95.5526\n",
      "Epoch 3419/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.8151 - val_loss: 95.4975\n",
      "Epoch 3420/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.2934 - val_loss: 95.2816\n",
      "Epoch 3421/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.4234 - val_loss: 96.2756\n",
      "Epoch 3422/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.4526 - val_loss: 95.4692\n",
      "Epoch 3423/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.6257 - val_loss: 95.5433\n",
      "Epoch 3424/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.8092 - val_loss: 95.4115\n",
      "Epoch 3425/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.9246 - val_loss: 95.5316\n",
      "Epoch 3426/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.5442 - val_loss: 94.5185\n",
      "Epoch 3427/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.9025 - val_loss: 94.9831\n",
      "Epoch 3428/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.0310 - val_loss: 96.3397\n",
      "Epoch 3429/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.6846 - val_loss: 95.7740\n",
      "Epoch 3430/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.1564 - val_loss: 94.8795\n",
      "Epoch 3431/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.9297 - val_loss: 95.9958\n",
      "Epoch 3432/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.8563 - val_loss: 96.6212\n",
      "Epoch 3433/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 94.3011 - val_loss: 94.4941\n",
      "Epoch 3434/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.8834 - val_loss: 96.4402\n",
      "Epoch 3435/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.9542 - val_loss: 95.3466\n",
      "Epoch 3436/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.1347 - val_loss: 95.4288\n",
      "Epoch 3437/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.7738 - val_loss: 95.3479\n",
      "Epoch 3438/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1666 - val_loss: 94.8438\n",
      "Epoch 3439/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.4192 - val_loss: 94.4775\n",
      "Epoch 3440/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.4495 - val_loss: 95.3420\n",
      "Epoch 3441/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.8898 - val_loss: 95.0010\n",
      "Epoch 3442/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.3128 - val_loss: 94.9489\n",
      "Epoch 3443/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 93.8179 - val_loss: 94.1643\n",
      "Epoch 3444/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 94.6288 - val_loss: 95.1335\n",
      "Epoch 3445/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.7085 - val_loss: 95.1318\n",
      "Epoch 3446/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.6197 - val_loss: 95.2955\n",
      "Epoch 3447/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.2612 - val_loss: 95.6337\n",
      "Epoch 3448/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.4282 - val_loss: 94.5710\n",
      "Epoch 3449/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.2731 - val_loss: 94.1437\n",
      "Epoch 3450/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.6023 - val_loss: 94.6404\n",
      "Epoch 3451/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.4661 - val_loss: 95.5706\n",
      "Epoch 3452/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.9987 - val_loss: 96.6397\n",
      "Epoch 3453/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 95.1454 - val_loss: 95.5186\n",
      "Epoch 3454/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.0512 - val_loss: 95.9489\n",
      "Epoch 3455/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.1010 - val_loss: 94.2844\n",
      "Epoch 3456/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 94.9729 - val_loss: 95.7624\n",
      "Epoch 3457/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.9831 - val_loss: 94.5467\n",
      "Epoch 3458/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 95.2553 - val_loss: 95.1016\n",
      "Epoch 3459/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.3302 - val_loss: 95.4419\n",
      "Epoch 3460/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.4335 - val_loss: 95.2262\n",
      "Epoch 3461/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.6166 - val_loss: 95.1349\n",
      "Epoch 3462/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.0707 - val_loss: 94.2314\n",
      "Epoch 3463/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.4597 - val_loss: 95.7510\n",
      "Epoch 3464/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.2444 - val_loss: 94.9419\n",
      "Epoch 3465/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.7270 - val_loss: 95.0122\n",
      "Epoch 3466/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.3794 - val_loss: 95.7494\n",
      "Epoch 3467/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.2770 - val_loss: 95.8772\n",
      "Epoch 3468/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.4328 - val_loss: 95.5594\n",
      "Epoch 3469/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.7999 - val_loss: 94.9553\n",
      "Epoch 3470/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.8125 - val_loss: 94.2743\n",
      "Epoch 3471/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.0919 - val_loss: 94.6907\n",
      "Epoch 3472/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.4106 - val_loss: 94.5518\n",
      "Epoch 3473/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.6910 - val_loss: 95.5932\n",
      "Epoch 3474/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.0742 - val_loss: 94.9526\n",
      "Epoch 3475/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.2088 - val_loss: 95.6842\n",
      "Epoch 3476/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.6949 - val_loss: 95.2424\n",
      "Epoch 3477/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.1335 - val_loss: 95.2608\n",
      "Epoch 3478/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.3717 - val_loss: 94.9095\n",
      "Epoch 3479/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.1194 - val_loss: 95.0265\n",
      "Epoch 3480/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6615 - val_loss: 94.9772\n",
      "Epoch 3481/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.4074 - val_loss: 95.8056\n",
      "Epoch 3482/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.3119 - val_loss: 95.9971\n",
      "Epoch 3483/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.0834 - val_loss: 95.1843\n",
      "Epoch 3484/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.5124 - val_loss: 94.6673\n",
      "Epoch 3485/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.1900 - val_loss: 94.2469\n",
      "Epoch 3486/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.9277 - val_loss: 94.8872\n",
      "Epoch 3487/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3161 - val_loss: 94.4635\n",
      "Epoch 3488/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 94.5449 - val_loss: 94.7769\n",
      "Epoch 3489/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.7577 - val_loss: 95.9526\n",
      "Epoch 3490/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.4041 - val_loss: 95.4947\n",
      "Epoch 3491/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 94.6757 - val_loss: 94.6248\n",
      "Epoch 3492/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.3249 - val_loss: 95.0579\n",
      "Epoch 3493/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.8927 - val_loss: 95.2794\n",
      "Epoch 3494/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 121us/step - loss: 93.3284 - val_loss: 95.7747\n",
      "Epoch 3495/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.1564 - val_loss: 95.6010\n",
      "Epoch 3496/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.3721 - val_loss: 95.9342\n",
      "Epoch 3497/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.8614 - val_loss: 95.7164\n",
      "Epoch 3498/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 93.3636 - val_loss: 96.1094\n",
      "Epoch 3499/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.1918 - val_loss: 95.9127\n",
      "Epoch 3500/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.2278 - val_loss: 95.2156\n",
      "Epoch 3501/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.7485 - val_loss: 95.6866\n",
      "Epoch 3502/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.9316 - val_loss: 95.1361\n",
      "Epoch 3503/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.9760 - val_loss: 95.2689\n",
      "Epoch 3504/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.0793 - val_loss: 95.3999\n",
      "Epoch 3505/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.5558 - val_loss: 94.8040\n",
      "Epoch 3506/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.0522 - val_loss: 94.7384\n",
      "Epoch 3507/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.7249 - val_loss: 95.5462\n",
      "Epoch 3508/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.9062 - val_loss: 95.9132\n",
      "Epoch 3509/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.3770 - val_loss: 95.7555\n",
      "Epoch 3510/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.6756 - val_loss: 95.2764\n",
      "Epoch 3511/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 93.9516 - val_loss: 95.0011\n",
      "Epoch 3512/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.4999 - val_loss: 95.1340\n",
      "Epoch 3513/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.5456 - val_loss: 95.1937\n",
      "Epoch 3514/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.4384 - val_loss: 95.2441\n",
      "Epoch 3515/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.7386 - val_loss: 95.2218\n",
      "Epoch 3516/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.4907 - val_loss: 95.6803\n",
      "Epoch 3517/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.2207 - val_loss: 95.3787\n",
      "Epoch 3518/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.7402 - val_loss: 95.0607\n",
      "Epoch 3519/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.0934 - val_loss: 94.6686\n",
      "Epoch 3520/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 93.4588 - val_loss: 95.5377\n",
      "Epoch 3521/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.3596 - val_loss: 95.4521\n",
      "Epoch 3522/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.8998 - val_loss: 95.4598\n",
      "Epoch 3523/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.8309 - val_loss: 95.0558\n",
      "Epoch 3524/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.9536 - val_loss: 95.2614\n",
      "Epoch 3525/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.9951 - val_loss: 96.0688\n",
      "Epoch 3526/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.3688 - val_loss: 94.5344\n",
      "Epoch 3527/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 94.2637 - val_loss: 94.7389\n",
      "Epoch 3528/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 93.6726 - val_loss: 95.3610\n",
      "Epoch 3529/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.6072 - val_loss: 96.0556\n",
      "Epoch 3530/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.3238 - val_loss: 95.8565\n",
      "Epoch 3531/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.8174 - val_loss: 95.1008\n",
      "Epoch 3532/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.9254 - val_loss: 94.2732\n",
      "Epoch 3533/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.0631 - val_loss: 94.8356\n",
      "Epoch 3534/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.8537 - val_loss: 94.8961\n",
      "Epoch 3535/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.6734 - val_loss: 95.6057\n",
      "Epoch 3536/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.3029 - val_loss: 95.4233\n",
      "Epoch 3537/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.2352 - val_loss: 95.6373\n",
      "Epoch 3538/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.7502 - val_loss: 95.2255\n",
      "Epoch 3539/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.7120 - val_loss: 95.0135\n",
      "Epoch 3540/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.6909 - val_loss: 95.7182\n",
      "Epoch 3541/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2151 - val_loss: 95.4054\n",
      "Epoch 3542/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.3753 - val_loss: 94.8290\n",
      "Epoch 3543/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.4269 - val_loss: 95.0153\n",
      "Epoch 3544/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.4724 - val_loss: 94.9051\n",
      "Epoch 3545/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.0421 - val_loss: 95.1273\n",
      "Epoch 3546/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.7869 - val_loss: 96.1657\n",
      "Epoch 3547/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.3516 - val_loss: 95.6507\n",
      "Epoch 3548/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.9251 - val_loss: 95.1796\n",
      "Epoch 3549/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.4893 - val_loss: 95.7488\n",
      "Epoch 3550/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.2975 - val_loss: 94.6392\n",
      "Epoch 3551/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 94.5060 - val_loss: 95.3076\n",
      "Epoch 3552/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.5416 - val_loss: 94.7732\n",
      "Epoch 3553/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.4019 - val_loss: 95.9535\n",
      "Epoch 3554/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.6054 - val_loss: 95.0454\n",
      "Epoch 3555/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 94.3838 - val_loss: 94.9810\n",
      "Epoch 3556/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.6790 - val_loss: 95.0303\n",
      "Epoch 3557/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.5497 - val_loss: 95.2619\n",
      "Epoch 3558/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.9692 - val_loss: 94.6466\n",
      "Epoch 3559/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.8205 - val_loss: 95.2471\n",
      "Epoch 3560/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.0570 - val_loss: 95.0593\n",
      "Epoch 3561/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.8673 - val_loss: 94.8016\n",
      "Epoch 3562/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.9820 - val_loss: 95.1240\n",
      "Epoch 3563/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.6488 - val_loss: 95.5991\n",
      "Epoch 3564/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.4266 - val_loss: 94.5383\n",
      "Epoch 3565/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.7449 - val_loss: 95.1207\n",
      "Epoch 3566/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.8971 - val_loss: 95.8097\n",
      "Epoch 3567/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6619 - val_loss: 95.5096\n",
      "Epoch 3568/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1232 - val_loss: 95.2497\n",
      "Epoch 3569/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.9601 - val_loss: 95.1693\n",
      "Epoch 3570/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 123us/step - loss: 93.0801 - val_loss: 94.8118\n",
      "Epoch 3571/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 97.5556 - val_loss: 95.4007\n",
      "Epoch 3572/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.5940 - val_loss: 95.3836\n",
      "Epoch 3573/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.7832 - val_loss: 94.8449\n",
      "Epoch 3574/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 93.2130 - val_loss: 94.9469\n",
      "Epoch 3575/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.8735 - val_loss: 95.2855\n",
      "Epoch 3576/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.7343 - val_loss: 95.9558\n",
      "Epoch 3577/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 94.0452 - val_loss: 94.7357\n",
      "Epoch 3578/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.6939 - val_loss: 94.8322\n",
      "Epoch 3579/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.1974 - val_loss: 95.8703\n",
      "Epoch 3580/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.7463 - val_loss: 94.7608\n",
      "Epoch 3581/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.8207 - val_loss: 94.6120\n",
      "Epoch 3582/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.3500 - val_loss: 95.1277\n",
      "Epoch 3583/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.4416 - val_loss: 95.6393\n",
      "Epoch 3584/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.1353 - val_loss: 94.8077\n",
      "Epoch 3585/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.6637 - val_loss: 95.1398\n",
      "Epoch 3586/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.6093 - val_loss: 94.8041\n",
      "Epoch 3587/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 93.6070 - val_loss: 94.5442\n",
      "Epoch 3588/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.8564 - val_loss: 96.3412\n",
      "Epoch 3589/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5851 - val_loss: 94.6551\n",
      "Epoch 3590/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.1246 - val_loss: 95.1382\n",
      "Epoch 3591/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.8556 - val_loss: 95.1828\n",
      "Epoch 3592/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.5591 - val_loss: 95.7468\n",
      "Epoch 3593/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.6525 - val_loss: 95.3628\n",
      "Epoch 3594/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.7315 - val_loss: 96.5401\n",
      "Epoch 3595/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.3618 - val_loss: 95.8912\n",
      "Epoch 3596/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.5069 - val_loss: 94.5179\n",
      "Epoch 3597/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.7639 - val_loss: 96.0363\n",
      "Epoch 3598/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9791 - val_loss: 94.3593\n",
      "Epoch 3599/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 97.2333 - val_loss: 94.4901\n",
      "Epoch 3600/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 93.4283 - val_loss: 95.4600\n",
      "Epoch 3601/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.2830 - val_loss: 95.2671\n",
      "Epoch 3602/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.8348 - val_loss: 95.4361\n",
      "Epoch 3603/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.3980 - val_loss: 95.1734\n",
      "Epoch 3604/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.1708 - val_loss: 94.5860\n",
      "Epoch 3605/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.0895 - val_loss: 95.5497\n",
      "Epoch 3606/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.1579 - val_loss: 94.3481\n",
      "Epoch 3607/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.0082 - val_loss: 94.8303\n",
      "Epoch 3608/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.9638 - val_loss: 95.2845\n",
      "Epoch 3609/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.3011 - val_loss: 95.6165\n",
      "Epoch 3610/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.1619 - val_loss: 95.1112\n",
      "Epoch 3611/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 93.3392 - val_loss: 95.8293\n",
      "Epoch 3612/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.6130 - val_loss: 94.4804\n",
      "Epoch 3613/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.5711 - val_loss: 94.8566\n",
      "Epoch 3614/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.5649 - val_loss: 95.0974\n",
      "Epoch 3615/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.5928 - val_loss: 94.9579\n",
      "Epoch 3616/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.7140 - val_loss: 95.3497\n",
      "Epoch 3617/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.3482 - val_loss: 94.7498\n",
      "Epoch 3618/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 92.6816 - val_loss: 94.4429\n",
      "Epoch 3619/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.3733 - val_loss: 94.4825\n",
      "Epoch 3620/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 96.1502 - val_loss: 94.4327\n",
      "Epoch 3621/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.4578 - val_loss: 94.1540\n",
      "Epoch 3622/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.8268 - val_loss: 94.9670\n",
      "Epoch 3623/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.7433 - val_loss: 95.5014\n",
      "Epoch 3624/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 93.7056 - val_loss: 95.3852\n",
      "Epoch 3625/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.2746 - val_loss: 95.2534\n",
      "Epoch 3626/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.6909 - val_loss: 94.1609\n",
      "Epoch 3627/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 95.1580 - val_loss: 94.6262\n",
      "Epoch 3628/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.7276 - val_loss: 95.8988\n",
      "Epoch 3629/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.8009 - val_loss: 95.7421\n",
      "Epoch 3630/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.9931 - val_loss: 95.1535\n",
      "Epoch 3631/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.1212 - val_loss: 94.4742\n",
      "Epoch 3632/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.1264 - val_loss: 95.7907\n",
      "Epoch 3633/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.8962 - val_loss: 95.0501\n",
      "Epoch 3634/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.3553 - val_loss: 94.7046\n",
      "Epoch 3635/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.9184 - val_loss: 95.4975\n",
      "Epoch 3636/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.5092 - val_loss: 94.8337\n",
      "Epoch 3637/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.0821 - val_loss: 94.3231\n",
      "Epoch 3638/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.0085 - val_loss: 94.8764\n",
      "Epoch 3639/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.5029 - val_loss: 95.2370\n",
      "Epoch 3640/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 93.9501 - val_loss: 95.0143\n",
      "Epoch 3641/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.5095 - val_loss: 95.1032\n",
      "Epoch 3642/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.5322 - val_loss: 94.7182\n",
      "Epoch 3643/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.8275 - val_loss: 95.6434\n",
      "Epoch 3644/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.4365 - val_loss: 95.2198\n",
      "Epoch 3645/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.6650 - val_loss: 94.9118\n",
      "Epoch 3646/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 119us/step - loss: 93.3892 - val_loss: 95.1072\n",
      "Epoch 3647/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.4312 - val_loss: 95.9145\n",
      "Epoch 3648/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.7299 - val_loss: 95.1403\n",
      "Epoch 3649/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6683 - val_loss: 95.4318\n",
      "Epoch 3650/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.2602 - val_loss: 95.0391\n",
      "Epoch 3651/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.0519 - val_loss: 95.0952\n",
      "Epoch 3652/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.9467 - val_loss: 94.8859\n",
      "Epoch 3653/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6044 - val_loss: 94.9583\n",
      "Epoch 3654/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.3729 - val_loss: 94.6314\n",
      "Epoch 3655/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 94.3052 - val_loss: 94.5077\n",
      "Epoch 3656/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.9850 - val_loss: 94.7864\n",
      "Epoch 3657/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.3111 - val_loss: 95.5741\n",
      "Epoch 3658/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 94.1819 - val_loss: 95.5419\n",
      "Epoch 3659/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 94.1352 - val_loss: 95.4722\n",
      "Epoch 3660/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.4889 - val_loss: 94.4117\n",
      "Epoch 3661/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.0786 - val_loss: 94.4726\n",
      "Epoch 3662/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.9954 - val_loss: 94.4199\n",
      "Epoch 3663/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.9380 - val_loss: 94.6463\n",
      "Epoch 3664/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.7213 - val_loss: 94.9625\n",
      "Epoch 3665/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.1163 - val_loss: 94.9518\n",
      "Epoch 3666/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.1909 - val_loss: 95.2717\n",
      "Epoch 3667/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.6821 - val_loss: 95.1742\n",
      "Epoch 3668/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.3096 - val_loss: 95.3645\n",
      "Epoch 3669/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.0999 - val_loss: 94.8610\n",
      "Epoch 3670/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.3428 - val_loss: 94.2347\n",
      "Epoch 3671/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.0268 - val_loss: 95.1716\n",
      "Epoch 3672/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.3915 - val_loss: 94.9949\n",
      "Epoch 3673/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.9811 - val_loss: 95.0110\n",
      "Epoch 3674/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.7632 - val_loss: 95.6896\n",
      "Epoch 3675/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3543 - val_loss: 94.8643\n",
      "Epoch 3676/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.5389 - val_loss: 94.0091\n",
      "Epoch 3677/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.4658 - val_loss: 95.5453\n",
      "Epoch 3678/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 93.9471 - val_loss: 95.1696\n",
      "Epoch 3679/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.7404 - val_loss: 96.1085\n",
      "Epoch 3680/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.8053 - val_loss: 94.4265\n",
      "Epoch 3681/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.8844 - val_loss: 94.6107\n",
      "Epoch 3682/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.6246 - val_loss: 94.5775\n",
      "Epoch 3683/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3112 - val_loss: 94.7626\n",
      "Epoch 3684/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.8390 - val_loss: 95.2966\n",
      "Epoch 3685/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.5885 - val_loss: 94.8775\n",
      "Epoch 3686/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.9017 - val_loss: 95.1814\n",
      "Epoch 3687/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 97.3714 - val_loss: 95.2044\n",
      "Epoch 3688/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.6718 - val_loss: 95.2102\n",
      "Epoch 3689/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.2423 - val_loss: 95.3950\n",
      "Epoch 3690/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.3234 - val_loss: 95.1737\n",
      "Epoch 3691/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.4001 - val_loss: 94.5924\n",
      "Epoch 3692/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.9754 - val_loss: 95.1768\n",
      "Epoch 3693/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.5790 - val_loss: 95.6096\n",
      "Epoch 3694/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.2072 - val_loss: 95.0899\n",
      "Epoch 3695/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.5335 - val_loss: 95.1818\n",
      "Epoch 3696/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.9708 - val_loss: 95.5619\n",
      "Epoch 3697/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.3926 - val_loss: 95.2571\n",
      "Epoch 3698/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.5494 - val_loss: 95.0788\n",
      "Epoch 3699/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.5389 - val_loss: 94.5686\n",
      "Epoch 3700/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.0721 - val_loss: 94.8503\n",
      "Epoch 3701/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.4293 - val_loss: 94.8222\n",
      "Epoch 3702/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.2793 - val_loss: 95.8722\n",
      "Epoch 3703/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3619 - val_loss: 94.7264\n",
      "Epoch 3704/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 91.5470 - val_loss: 95.3276\n",
      "Epoch 3705/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.5485 - val_loss: 95.3671\n",
      "Epoch 3706/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.7528 - val_loss: 94.5998\n",
      "Epoch 3707/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.4130 - val_loss: 94.7304\n",
      "Epoch 3708/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.2106 - val_loss: 95.1230\n",
      "Epoch 3709/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 93.8385 - val_loss: 94.9249\n",
      "Epoch 3710/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 93.9845 - val_loss: 94.5225\n",
      "Epoch 3711/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.8278 - val_loss: 95.0662\n",
      "Epoch 3712/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.1259 - val_loss: 94.6242\n",
      "Epoch 3713/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.3965 - val_loss: 95.0392\n",
      "Epoch 3714/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.6072 - val_loss: 95.1862\n",
      "Epoch 3715/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.8146 - val_loss: 94.4389\n",
      "Epoch 3716/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.4968 - val_loss: 94.7659\n",
      "Epoch 3717/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.4412 - val_loss: 96.4476\n",
      "Epoch 3718/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 92.7164 - val_loss: 95.2383\n",
      "Epoch 3719/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.4904 - val_loss: 94.9958\n",
      "Epoch 3720/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.6474 - val_loss: 96.2717\n",
      "Epoch 3721/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 93.1375 - val_loss: 95.6694\n",
      "Epoch 3722/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 117us/step - loss: 95.1554 - val_loss: 95.0610\n",
      "Epoch 3723/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.2154 - val_loss: 95.4151\n",
      "Epoch 3724/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.0703 - val_loss: 95.0682\n",
      "Epoch 3725/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.7216 - val_loss: 94.4577\n",
      "Epoch 3726/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.0855 - val_loss: 94.3732\n",
      "Epoch 3727/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.4269 - val_loss: 94.6152\n",
      "Epoch 3728/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.9776 - val_loss: 94.3542\n",
      "Epoch 3729/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.9397 - val_loss: 94.3420\n",
      "Epoch 3730/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.6028 - val_loss: 95.2626\n",
      "Epoch 3731/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.0357 - val_loss: 94.7517\n",
      "Epoch 3732/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.8502 - val_loss: 95.2040\n",
      "Epoch 3733/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.8278 - val_loss: 95.4385\n",
      "Epoch 3734/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.4502 - val_loss: 95.5779\n",
      "Epoch 3735/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.8532 - val_loss: 95.4065\n",
      "Epoch 3736/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.1499 - val_loss: 94.9494\n",
      "Epoch 3737/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.7881 - val_loss: 94.5973\n",
      "Epoch 3738/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.3415 - val_loss: 94.8996\n",
      "Epoch 3739/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.4756 - val_loss: 94.8752\n",
      "Epoch 3740/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.8770 - val_loss: 95.2403\n",
      "Epoch 3741/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.6880 - val_loss: 95.2115\n",
      "Epoch 3742/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.6242 - val_loss: 94.9077\n",
      "Epoch 3743/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.2565 - val_loss: 94.6972\n",
      "Epoch 3744/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 92.3403 - val_loss: 94.7751\n",
      "Epoch 3745/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.3079 - val_loss: 95.9961\n",
      "Epoch 3746/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6953 - val_loss: 94.4895\n",
      "Epoch 3747/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.6088 - val_loss: 96.4978\n",
      "Epoch 3748/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.8571 - val_loss: 95.3354\n",
      "Epoch 3749/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.0428 - val_loss: 95.0905\n",
      "Epoch 3750/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.6871 - val_loss: 95.4891\n",
      "Epoch 3751/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.3994 - val_loss: 95.7039\n",
      "Epoch 3752/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.7335 - val_loss: 94.7717\n",
      "Epoch 3753/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.9811 - val_loss: 95.3098\n",
      "Epoch 3754/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.4975 - val_loss: 95.3882\n",
      "Epoch 3755/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.7526 - val_loss: 96.1351\n",
      "Epoch 3756/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.9984 - val_loss: 94.3278\n",
      "Epoch 3757/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.7574 - val_loss: 94.7951\n",
      "Epoch 3758/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.8601 - val_loss: 95.5656\n",
      "Epoch 3759/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.8361 - val_loss: 93.7155\n",
      "Epoch 3760/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.5569 - val_loss: 94.8893\n",
      "Epoch 3761/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.8327 - val_loss: 94.5208\n",
      "Epoch 3762/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.9416 - val_loss: 94.2437\n",
      "Epoch 3763/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.3292 - val_loss: 94.8348\n",
      "Epoch 3764/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.2525 - val_loss: 94.8655\n",
      "Epoch 3765/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.3818 - val_loss: 95.4055\n",
      "Epoch 3766/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.9946 - val_loss: 94.8793\n",
      "Epoch 3767/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.6737 - val_loss: 95.3506\n",
      "Epoch 3768/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.7125 - val_loss: 95.1138\n",
      "Epoch 3769/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.2642 - val_loss: 94.9778\n",
      "Epoch 3770/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.2144 - val_loss: 94.5087\n",
      "Epoch 3771/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.9255 - val_loss: 94.9154\n",
      "Epoch 3772/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.4536 - val_loss: 94.4266\n",
      "Epoch 3773/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.8221 - val_loss: 95.2949\n",
      "Epoch 3774/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.8442 - val_loss: 94.8880\n",
      "Epoch 3775/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 93.8463 - val_loss: 94.8445\n",
      "Epoch 3776/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.2747 - val_loss: 94.7682\n",
      "Epoch 3777/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.3431 - val_loss: 94.6993\n",
      "Epoch 3778/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6162 - val_loss: 94.9260\n",
      "Epoch 3779/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.4728 - val_loss: 95.3428\n",
      "Epoch 3780/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.2633 - val_loss: 94.9679\n",
      "Epoch 3781/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.9485 - val_loss: 94.7425\n",
      "Epoch 3782/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.3158 - val_loss: 94.1680\n",
      "Epoch 3783/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.9418 - val_loss: 94.7047\n",
      "Epoch 3784/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2713 - val_loss: 94.6178\n",
      "Epoch 3785/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.6789 - val_loss: 94.6060\n",
      "Epoch 3786/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.0205 - val_loss: 95.0012\n",
      "Epoch 3787/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.8776 - val_loss: 94.6523\n",
      "Epoch 3788/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2881 - val_loss: 95.2334\n",
      "Epoch 3789/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.7136 - val_loss: 95.2805\n",
      "Epoch 3790/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.3204 - val_loss: 94.8038\n",
      "Epoch 3791/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.0790 - val_loss: 94.6895\n",
      "Epoch 3792/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.2664 - val_loss: 95.3180\n",
      "Epoch 3793/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.7063 - val_loss: 95.1408\n",
      "Epoch 3794/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 96.0149 - val_loss: 95.1381\n",
      "Epoch 3795/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.4826 - val_loss: 94.8533\n",
      "Epoch 3796/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.2733 - val_loss: 95.4807\n",
      "Epoch 3797/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.6921 - val_loss: 94.8519\n",
      "Epoch 3798/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 116us/step - loss: 95.2573 - val_loss: 96.3729\n",
      "Epoch 3799/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.1509 - val_loss: 95.5360\n",
      "Epoch 3800/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.8315 - val_loss: 95.1891\n",
      "Epoch 3801/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.2621 - val_loss: 95.2237\n",
      "Epoch 3802/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.1569 - val_loss: 94.4971\n",
      "Epoch 3803/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.2730 - val_loss: 95.2604\n",
      "Epoch 3804/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.7783 - val_loss: 95.1860\n",
      "Epoch 3805/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 93.0188 - val_loss: 94.7223\n",
      "Epoch 3806/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.2017 - val_loss: 95.6191\n",
      "Epoch 3807/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.1049 - val_loss: 95.2774\n",
      "Epoch 3808/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.0717 - val_loss: 94.9160\n",
      "Epoch 3809/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.6194 - val_loss: 95.1216\n",
      "Epoch 3810/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.8958 - val_loss: 95.2932\n",
      "Epoch 3811/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.6224 - val_loss: 95.4589\n",
      "Epoch 3812/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.1888 - val_loss: 95.3462\n",
      "Epoch 3813/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 93.2441 - val_loss: 94.7022\n",
      "Epoch 3814/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.7720 - val_loss: 94.5152\n",
      "Epoch 3815/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.6297 - val_loss: 93.8250\n",
      "Epoch 3816/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.9742 - val_loss: 94.6095\n",
      "Epoch 3817/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.5223 - val_loss: 95.0590\n",
      "Epoch 3818/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 92.6381 - val_loss: 95.3098\n",
      "Epoch 3819/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 92.1317 - val_loss: 94.9816\n",
      "Epoch 3820/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 94.5529 - val_loss: 95.5582\n",
      "Epoch 3821/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.8443 - val_loss: 94.9567\n",
      "Epoch 3822/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.0579 - val_loss: 95.8469\n",
      "Epoch 3823/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 92.4979 - val_loss: 94.8269\n",
      "Epoch 3824/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.1363 - val_loss: 94.5165\n",
      "Epoch 3825/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 92.9662 - val_loss: 94.3873\n",
      "Epoch 3826/10000\n",
      "96/96 [==============================] - 0s 111us/step - loss: 92.5757 - val_loss: 95.6307\n",
      "Epoch 3827/10000\n",
      "96/96 [==============================] - 0s 111us/step - loss: 91.5855 - val_loss: 95.0272\n",
      "Epoch 3828/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.7895 - val_loss: 94.4814\n",
      "Epoch 3829/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 93.8270 - val_loss: 94.1927\n",
      "Epoch 3830/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 95.0442 - val_loss: 94.5465\n",
      "Epoch 3831/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6368 - val_loss: 95.2218\n",
      "Epoch 3832/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.6486 - val_loss: 94.7767\n",
      "Epoch 3833/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 97.0728 - val_loss: 95.7587\n",
      "Epoch 3834/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.6695 - val_loss: 94.9672\n",
      "Epoch 3835/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.2966 - val_loss: 95.2201\n",
      "Epoch 3836/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.2422 - val_loss: 95.5435\n",
      "Epoch 3837/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.9199 - val_loss: 94.7643\n",
      "Epoch 3838/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.0930 - val_loss: 94.6556\n",
      "Epoch 3839/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.9226 - val_loss: 95.1428\n",
      "Epoch 3840/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.2411 - val_loss: 95.8369\n",
      "Epoch 3841/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.0304 - val_loss: 95.1116\n",
      "Epoch 3842/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.5738 - val_loss: 95.3466\n",
      "Epoch 3843/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.8100 - val_loss: 95.3056\n",
      "Epoch 3844/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.0960 - val_loss: 94.2910\n",
      "Epoch 3845/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.9585 - val_loss: 94.6699\n",
      "Epoch 3846/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.5705 - val_loss: 95.1618\n",
      "Epoch 3847/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.1744 - val_loss: 94.3215\n",
      "Epoch 3848/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.8713 - val_loss: 94.1929\n",
      "Epoch 3849/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.1438 - val_loss: 94.6375\n",
      "Epoch 3850/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.5741 - val_loss: 94.3431\n",
      "Epoch 3851/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2109 - val_loss: 94.6101\n",
      "Epoch 3852/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.7668 - val_loss: 95.6218\n",
      "Epoch 3853/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 93.7108 - val_loss: 95.5676\n",
      "Epoch 3854/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.9880 - val_loss: 95.6749\n",
      "Epoch 3855/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.5016 - val_loss: 95.9803\n",
      "Epoch 3856/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 93.8566 - val_loss: 94.9752\n",
      "Epoch 3857/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.6868 - val_loss: 94.8012\n",
      "Epoch 3858/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.3041 - val_loss: 95.4892\n",
      "Epoch 3859/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.8462 - val_loss: 94.0394\n",
      "Epoch 3860/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.4562 - val_loss: 94.8961\n",
      "Epoch 3861/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.9285 - val_loss: 95.5996\n",
      "Epoch 3862/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.3882 - val_loss: 95.5432\n",
      "Epoch 3863/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.8783 - val_loss: 95.2885\n",
      "Epoch 3864/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.1246 - val_loss: 94.7459\n",
      "Epoch 3865/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.6232 - val_loss: 94.5077\n",
      "Epoch 3866/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.9146 - val_loss: 94.2742\n",
      "Epoch 3867/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.1250 - val_loss: 94.6429\n",
      "Epoch 3868/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.7876 - val_loss: 95.2925\n",
      "Epoch 3869/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.0732 - val_loss: 96.3109\n",
      "Epoch 3870/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 93.6225 - val_loss: 95.8592\n",
      "Epoch 3871/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.9461 - val_loss: 94.7370\n",
      "Epoch 3872/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.0022 - val_loss: 94.6870\n",
      "Epoch 3873/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.1786 - val_loss: 94.9637\n",
      "Epoch 3874/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 116us/step - loss: 93.2269 - val_loss: 94.4013\n",
      "Epoch 3875/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.1273 - val_loss: 94.4427\n",
      "Epoch 3876/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.2213 - val_loss: 94.2642\n",
      "Epoch 3877/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.1948 - val_loss: 95.0990\n",
      "Epoch 3878/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.7032 - val_loss: 94.6025\n",
      "Epoch 3879/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6098 - val_loss: 94.7862\n",
      "Epoch 3880/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 92.9685 - val_loss: 94.5778\n",
      "Epoch 3881/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.3702 - val_loss: 95.3875\n",
      "Epoch 3882/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.2876 - val_loss: 94.0733\n",
      "Epoch 3883/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 93.5804 - val_loss: 94.7517\n",
      "Epoch 3884/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.3286 - val_loss: 94.6785\n",
      "Epoch 3885/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.9335 - val_loss: 94.9141\n",
      "Epoch 3886/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.2008 - val_loss: 95.5069\n",
      "Epoch 3887/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.2080 - val_loss: 95.3829\n",
      "Epoch 3888/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 94.7667 - val_loss: 94.5954\n",
      "Epoch 3889/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.6128 - val_loss: 95.1973\n",
      "Epoch 3890/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.0810 - val_loss: 94.5456\n",
      "Epoch 3891/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.5322 - val_loss: 94.6404\n",
      "Epoch 3892/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 93.1594 - val_loss: 94.9890\n",
      "Epoch 3893/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.7134 - val_loss: 95.3786\n",
      "Epoch 3894/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.6687 - val_loss: 94.3554\n",
      "Epoch 3895/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.6818 - val_loss: 94.6376\n",
      "Epoch 3896/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.4371 - val_loss: 94.7131\n",
      "Epoch 3897/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.3132 - val_loss: 95.0290\n",
      "Epoch 3898/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.5581 - val_loss: 94.9302\n",
      "Epoch 3899/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 94.1375 - val_loss: 95.5979\n",
      "Epoch 3900/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.4967 - val_loss: 95.5588\n",
      "Epoch 3901/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.9426 - val_loss: 95.5208\n",
      "Epoch 3902/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.7461 - val_loss: 94.8741\n",
      "Epoch 3903/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 93.0066 - val_loss: 94.6637\n",
      "Epoch 3904/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 94.7500 - val_loss: 94.7714\n",
      "Epoch 3905/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.8568 - val_loss: 94.8612\n",
      "Epoch 3906/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.2453 - val_loss: 95.0089\n",
      "Epoch 3907/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.5187 - val_loss: 95.1150\n",
      "Epoch 3908/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.9250 - val_loss: 95.2690\n",
      "Epoch 3909/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.8933 - val_loss: 95.7779\n",
      "Epoch 3910/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 96.0210 - val_loss: 95.0897\n",
      "Epoch 3911/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2944 - val_loss: 94.9727\n",
      "Epoch 3912/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1498 - val_loss: 95.0189\n",
      "Epoch 3913/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 94.7876 - val_loss: 94.4948\n",
      "Epoch 3914/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.3464 - val_loss: 94.4941\n",
      "Epoch 3915/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.2364 - val_loss: 94.9552\n",
      "Epoch 3916/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.7461 - val_loss: 94.5739\n",
      "Epoch 3917/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.1536 - val_loss: 94.8432\n",
      "Epoch 3918/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.5527 - val_loss: 94.7918\n",
      "Epoch 3919/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.9976 - val_loss: 94.6277\n",
      "Epoch 3920/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.4085 - val_loss: 94.5752\n",
      "Epoch 3921/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.1024 - val_loss: 94.7841\n",
      "Epoch 3922/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.6677 - val_loss: 94.5657\n",
      "Epoch 3923/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.4665 - val_loss: 93.7435\n",
      "Epoch 3924/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.4728 - val_loss: 94.7964\n",
      "Epoch 3925/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.6633 - val_loss: 95.2320\n",
      "Epoch 3926/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.9218 - val_loss: 94.0554\n",
      "Epoch 3927/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3186 - val_loss: 94.9388\n",
      "Epoch 3928/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.5947 - val_loss: 95.1888\n",
      "Epoch 3929/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.2150 - val_loss: 94.5288\n",
      "Epoch 3930/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.8124 - val_loss: 95.2757\n",
      "Epoch 3931/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.3707 - val_loss: 94.6828\n",
      "Epoch 3932/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.8410 - val_loss: 94.3459\n",
      "Epoch 3933/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.3381 - val_loss: 95.2449\n",
      "Epoch 3934/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.5608 - val_loss: 95.2722\n",
      "Epoch 3935/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.3519 - val_loss: 95.2885\n",
      "Epoch 3936/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.9360 - val_loss: 94.7377\n",
      "Epoch 3937/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.3305 - val_loss: 94.9618\n",
      "Epoch 3938/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 92.8780 - val_loss: 95.5798\n",
      "Epoch 3939/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.3955 - val_loss: 94.3211\n",
      "Epoch 3940/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.1383 - val_loss: 94.3375\n",
      "Epoch 3941/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.1825 - val_loss: 94.9528\n",
      "Epoch 3942/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.9765 - val_loss: 94.9807\n",
      "Epoch 3943/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.5047 - val_loss: 95.1211\n",
      "Epoch 3944/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.4968 - val_loss: 95.1836\n",
      "Epoch 3945/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.2524 - val_loss: 94.9133\n",
      "Epoch 3946/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.8790 - val_loss: 94.7306\n",
      "Epoch 3947/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.7225 - val_loss: 95.5037\n",
      "Epoch 3948/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 93.2858 - val_loss: 95.3306\n",
      "Epoch 3949/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.9633 - val_loss: 95.1012\n",
      "Epoch 3950/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 119us/step - loss: 92.0451 - val_loss: 95.0426\n",
      "Epoch 3951/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.8803 - val_loss: 94.8854\n",
      "Epoch 3952/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.3900 - val_loss: 95.0726\n",
      "Epoch 3953/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.4252 - val_loss: 95.3289\n",
      "Epoch 3954/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.7861 - val_loss: 94.9029\n",
      "Epoch 3955/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.8427 - val_loss: 95.3038\n",
      "Epoch 3956/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.8565 - val_loss: 94.3366\n",
      "Epoch 3957/10000\n",
      "96/96 [==============================] - ETA: 0s - loss: 86.93 - 0s 117us/step - loss: 93.7253 - val_loss: 94.2924\n",
      "Epoch 3958/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 94.3294 - val_loss: 94.1323\n",
      "Epoch 3959/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.0105 - val_loss: 95.0562\n",
      "Epoch 3960/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.9160 - val_loss: 94.4839\n",
      "Epoch 3961/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 93.4114 - val_loss: 95.4338\n",
      "Epoch 3962/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.8157 - val_loss: 95.1961\n",
      "Epoch 3963/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 93.4356 - val_loss: 94.5802\n",
      "Epoch 3964/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.4591 - val_loss: 94.6390\n",
      "Epoch 3965/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.7389 - val_loss: 94.4367\n",
      "Epoch 3966/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.3925 - val_loss: 95.5920\n",
      "Epoch 3967/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.0814 - val_loss: 95.0696\n",
      "Epoch 3968/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.1586 - val_loss: 94.4592\n",
      "Epoch 3969/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0000 - val_loss: 94.7452\n",
      "Epoch 3970/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.4993 - val_loss: 93.8785\n",
      "Epoch 3971/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.9314 - val_loss: 94.3536\n",
      "Epoch 3972/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.0817 - val_loss: 94.5683\n",
      "Epoch 3973/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.3144 - val_loss: 95.0638\n",
      "Epoch 3974/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.5356 - val_loss: 94.7523\n",
      "Epoch 3975/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.7236 - val_loss: 95.6991\n",
      "Epoch 3976/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.1044 - val_loss: 95.2029\n",
      "Epoch 3977/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.1359 - val_loss: 95.1899\n",
      "Epoch 3978/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 92.6875 - val_loss: 94.6940\n",
      "Epoch 3979/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.2572 - val_loss: 95.0770\n",
      "Epoch 3980/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.9788 - val_loss: 94.4100\n",
      "Epoch 3981/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.1150 - val_loss: 94.8955\n",
      "Epoch 3982/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.7294 - val_loss: 93.9733\n",
      "Epoch 3983/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.7879 - val_loss: 95.2261\n",
      "Epoch 3984/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.8626 - val_loss: 94.9672\n",
      "Epoch 3985/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.3295 - val_loss: 95.5013\n",
      "Epoch 3986/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.5141 - val_loss: 94.6241\n",
      "Epoch 3987/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 96.6113 - val_loss: 95.5674\n",
      "Epoch 3988/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.2421 - val_loss: 95.3914\n",
      "Epoch 3989/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.9234 - val_loss: 94.2947\n",
      "Epoch 3990/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.6338 - val_loss: 94.6730\n",
      "Epoch 3991/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.2681 - val_loss: 95.1657\n",
      "Epoch 3992/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.5431 - val_loss: 94.2690\n",
      "Epoch 3993/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.2600 - val_loss: 94.4304\n",
      "Epoch 3994/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.0616 - val_loss: 94.8796\n",
      "Epoch 3995/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 94.7876 - val_loss: 95.3148\n",
      "Epoch 3996/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2699 - val_loss: 94.0568\n",
      "Epoch 3997/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1937 - val_loss: 95.7848\n",
      "Epoch 3998/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 93.1751 - val_loss: 95.1509\n",
      "Epoch 3999/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.0756 - val_loss: 94.5158\n",
      "Epoch 4000/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.4996 - val_loss: 95.4630\n",
      "Epoch 4001/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.4156 - val_loss: 95.4143\n",
      "Epoch 4002/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 94.3618 - val_loss: 94.8559\n",
      "Epoch 4003/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.7549 - val_loss: 94.4953\n",
      "Epoch 4004/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.4860 - val_loss: 95.3131\n",
      "Epoch 4005/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.2609 - val_loss: 94.9078\n",
      "Epoch 4006/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.6063 - val_loss: 94.6368\n",
      "Epoch 4007/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.0210 - val_loss: 94.4322\n",
      "Epoch 4008/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.0328 - val_loss: 94.8822\n",
      "Epoch 4009/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.3024 - val_loss: 95.3504\n",
      "Epoch 4010/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.8326 - val_loss: 94.7526\n",
      "Epoch 4011/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.2693 - val_loss: 94.3037\n",
      "Epoch 4012/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6198 - val_loss: 94.7411\n",
      "Epoch 4013/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.8928 - val_loss: 94.9501\n",
      "Epoch 4014/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.6122 - val_loss: 95.2085\n",
      "Epoch 4015/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.9617 - val_loss: 95.3346\n",
      "Epoch 4016/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 94.1855 - val_loss: 94.8023\n",
      "Epoch 4017/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.4163 - val_loss: 95.3979\n",
      "Epoch 4018/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.9062 - val_loss: 94.9152\n",
      "Epoch 4019/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.4371 - val_loss: 95.1326\n",
      "Epoch 4020/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9941 - val_loss: 95.4434\n",
      "Epoch 4021/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.4501 - val_loss: 95.0800\n",
      "Epoch 4022/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.3863 - val_loss: 95.3922\n",
      "Epoch 4023/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 97.0125 - val_loss: 95.0732\n",
      "Epoch 4024/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.6944 - val_loss: 95.2801\n",
      "Epoch 4025/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.0250 - val_loss: 95.1436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4026/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6094 - val_loss: 94.9164\n",
      "Epoch 4027/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.0977 - val_loss: 94.5622\n",
      "Epoch 4028/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.0819 - val_loss: 95.3413\n",
      "Epoch 4029/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.4424 - val_loss: 96.0213\n",
      "Epoch 4030/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.8734 - val_loss: 95.7382\n",
      "Epoch 4031/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.5690 - val_loss: 94.7489\n",
      "Epoch 4032/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.0266 - val_loss: 94.7440\n",
      "Epoch 4033/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.1521 - val_loss: 94.2996\n",
      "Epoch 4034/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.7838 - val_loss: 94.7377\n",
      "Epoch 4035/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.3429 - val_loss: 94.1151\n",
      "Epoch 4036/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.7669 - val_loss: 94.0602\n",
      "Epoch 4037/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 94.0722 - val_loss: 94.7929\n",
      "Epoch 4038/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 93.0461 - val_loss: 94.5922\n",
      "Epoch 4039/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.6062 - val_loss: 94.5087\n",
      "Epoch 4040/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.5064 - val_loss: 94.5630\n",
      "Epoch 4041/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.3973 - val_loss: 95.0791\n",
      "Epoch 4042/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.0652 - val_loss: 95.3277\n",
      "Epoch 4043/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.3659 - val_loss: 94.6022\n",
      "Epoch 4044/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 92.5377 - val_loss: 94.9667\n",
      "Epoch 4045/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.8137 - val_loss: 94.2316\n",
      "Epoch 4046/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.6002 - val_loss: 94.1544\n",
      "Epoch 4047/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.0583 - val_loss: 95.3368\n",
      "Epoch 4048/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.5767 - val_loss: 95.4975\n",
      "Epoch 4049/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.2625 - val_loss: 95.1119\n",
      "Epoch 4050/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.7007 - val_loss: 95.2421\n",
      "Epoch 4051/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8792 - val_loss: 94.9754\n",
      "Epoch 4052/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.2943 - val_loss: 95.2745\n",
      "Epoch 4053/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1028 - val_loss: 94.3838\n",
      "Epoch 4054/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 92.7336 - val_loss: 94.6989\n",
      "Epoch 4055/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.9900 - val_loss: 94.9743\n",
      "Epoch 4056/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.1781 - val_loss: 95.4920\n",
      "Epoch 4057/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 93.9322 - val_loss: 95.5328\n",
      "Epoch 4058/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.3085 - val_loss: 95.2023\n",
      "Epoch 4059/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.8221 - val_loss: 94.7288\n",
      "Epoch 4060/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.5498 - val_loss: 94.9297\n",
      "Epoch 4061/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.8340 - val_loss: 95.1688\n",
      "Epoch 4062/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.5610 - val_loss: 95.0876\n",
      "Epoch 4063/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.8246 - val_loss: 94.4429\n",
      "Epoch 4064/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 93.0626 - val_loss: 94.5488\n",
      "Epoch 4065/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.0046 - val_loss: 94.7766\n",
      "Epoch 4066/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.5353 - val_loss: 95.2039\n",
      "Epoch 4067/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.6299 - val_loss: 95.1222\n",
      "Epoch 4068/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 93.8515 - val_loss: 94.6503\n",
      "Epoch 4069/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.5646 - val_loss: 95.1720\n",
      "Epoch 4070/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.4464 - val_loss: 95.2270\n",
      "Epoch 4071/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.5055 - val_loss: 94.7913\n",
      "Epoch 4072/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.5308 - val_loss: 94.6445\n",
      "Epoch 4073/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.7557 - val_loss: 94.9668\n",
      "Epoch 4074/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 91.7959 - val_loss: 94.6363\n",
      "Epoch 4075/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.7263 - val_loss: 94.4117\n",
      "Epoch 4076/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.1853 - val_loss: 94.1003\n",
      "Epoch 4077/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.6543 - val_loss: 94.7630\n",
      "Epoch 4078/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.6106 - val_loss: 94.8851\n",
      "Epoch 4079/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.2496 - val_loss: 95.1259\n",
      "Epoch 4080/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.1764 - val_loss: 94.4383\n",
      "Epoch 4081/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.4631 - val_loss: 94.7623\n",
      "Epoch 4082/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.3766 - val_loss: 94.5927\n",
      "Epoch 4083/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 93.0985 - val_loss: 94.8776\n",
      "Epoch 4084/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.5521 - val_loss: 94.5148\n",
      "Epoch 4085/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.0464 - val_loss: 94.8798\n",
      "Epoch 4086/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.0892 - val_loss: 94.8531\n",
      "Epoch 4087/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.0227 - val_loss: 94.7348\n",
      "Epoch 4088/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.3424 - val_loss: 95.2886\n",
      "Epoch 4089/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.7707 - val_loss: 95.3902\n",
      "Epoch 4090/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.6035 - val_loss: 93.9283\n",
      "Epoch 4091/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.9458 - val_loss: 94.1625\n",
      "Epoch 4092/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.7919 - val_loss: 95.3105\n",
      "Epoch 4093/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.1258 - val_loss: 94.7055\n",
      "Epoch 4094/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.1094 - val_loss: 94.6167\n",
      "Epoch 4095/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.5090 - val_loss: 94.2728\n",
      "Epoch 4096/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.1213 - val_loss: 95.2829\n",
      "Epoch 4097/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.3902 - val_loss: 95.1008\n",
      "Epoch 4098/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 91.8724 - val_loss: 94.6802\n",
      "Epoch 4099/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.4892 - val_loss: 94.2958\n",
      "Epoch 4100/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.4379 - val_loss: 95.5584\n",
      "Epoch 4101/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.5998 - val_loss: 94.5744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4102/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.1156 - val_loss: 95.4462\n",
      "Epoch 4103/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2650 - val_loss: 94.6751\n",
      "Epoch 4104/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.1692 - val_loss: 94.1790\n",
      "Epoch 4105/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.3246 - val_loss: 95.0914\n",
      "Epoch 4106/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.9670 - val_loss: 94.5751\n",
      "Epoch 4107/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.0973 - val_loss: 94.0997\n",
      "Epoch 4108/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.0847 - val_loss: 94.3706\n",
      "Epoch 4109/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3406 - val_loss: 94.8081\n",
      "Epoch 4110/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.5899 - val_loss: 94.4746\n",
      "Epoch 4111/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.8890 - val_loss: 94.9625\n",
      "Epoch 4112/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 93.2574 - val_loss: 94.4752\n",
      "Epoch 4113/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.8317 - val_loss: 94.6861\n",
      "Epoch 4114/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.5280 - val_loss: 95.3911\n",
      "Epoch 4115/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.7379 - val_loss: 94.1764\n",
      "Epoch 4116/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.2008 - val_loss: 95.0562\n",
      "Epoch 4117/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6900 - val_loss: 94.7761\n",
      "Epoch 4118/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.7073 - val_loss: 96.0460\n",
      "Epoch 4119/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.8061 - val_loss: 94.8925\n",
      "Epoch 4120/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5560 - val_loss: 95.5561\n",
      "Epoch 4121/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.4996 - val_loss: 94.3379\n",
      "Epoch 4122/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1264 - val_loss: 94.9848\n",
      "Epoch 4123/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.6815 - val_loss: 94.6983\n",
      "Epoch 4124/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.0592 - val_loss: 94.6644\n",
      "Epoch 4125/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.2446 - val_loss: 94.8118\n",
      "Epoch 4126/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.0495 - val_loss: 95.2108\n",
      "Epoch 4127/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.3652 - val_loss: 94.7039\n",
      "Epoch 4128/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.7727 - val_loss: 94.5590\n",
      "Epoch 4129/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.3263 - val_loss: 94.3358\n",
      "Epoch 4130/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.3062 - val_loss: 94.4701\n",
      "Epoch 4131/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.6770 - val_loss: 94.4257\n",
      "Epoch 4132/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.1119 - val_loss: 94.2561\n",
      "Epoch 4133/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.1344 - val_loss: 94.7413\n",
      "Epoch 4134/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.2220 - val_loss: 94.4010\n",
      "Epoch 4135/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.5033 - val_loss: 95.6459\n",
      "Epoch 4136/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.0728 - val_loss: 94.9815\n",
      "Epoch 4137/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.8558 - val_loss: 95.0712\n",
      "Epoch 4138/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2889 - val_loss: 94.6646\n",
      "Epoch 4139/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 92.8059 - val_loss: 94.2846\n",
      "Epoch 4140/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 94.5907 - val_loss: 94.4812\n",
      "Epoch 4141/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.3907 - val_loss: 94.6851\n",
      "Epoch 4142/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.8943 - val_loss: 94.1813\n",
      "Epoch 4143/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.3265 - val_loss: 93.6207\n",
      "Epoch 4144/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 93.1834 - val_loss: 94.1197\n",
      "Epoch 4145/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1864 - val_loss: 94.2181\n",
      "Epoch 4146/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.2617 - val_loss: 94.3837\n",
      "Epoch 4147/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1078 - val_loss: 94.6851\n",
      "Epoch 4148/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.6453 - val_loss: 94.8005\n",
      "Epoch 4149/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.0893 - val_loss: 95.2734\n",
      "Epoch 4150/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.5833 - val_loss: 94.5024\n",
      "Epoch 4151/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.4140 - val_loss: 94.9274\n",
      "Epoch 4152/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.3743 - val_loss: 94.3756\n",
      "Epoch 4153/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.6087 - val_loss: 93.8924\n",
      "Epoch 4154/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.0326 - val_loss: 94.3416\n",
      "Epoch 4155/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.9629 - val_loss: 94.2726\n",
      "Epoch 4156/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.6613 - val_loss: 94.3688\n",
      "Epoch 4157/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.6276 - val_loss: 94.9288\n",
      "Epoch 4158/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.2344 - val_loss: 94.7265\n",
      "Epoch 4159/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.6203 - val_loss: 94.1811\n",
      "Epoch 4160/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.8241 - val_loss: 95.2783\n",
      "Epoch 4161/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.2474 - val_loss: 94.3073\n",
      "Epoch 4162/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.4327 - val_loss: 95.4014\n",
      "Epoch 4163/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.4170 - val_loss: 94.4742\n",
      "Epoch 4164/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 94.0950 - val_loss: 94.5091\n",
      "Epoch 4165/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.4491 - val_loss: 94.5301\n",
      "Epoch 4166/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.4034 - val_loss: 94.5223\n",
      "Epoch 4167/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.5755 - val_loss: 94.5560\n",
      "Epoch 4168/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 95.4775 - val_loss: 94.6347\n",
      "Epoch 4169/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.0167 - val_loss: 94.2861\n",
      "Epoch 4170/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 93.0895 - val_loss: 94.7895\n",
      "Epoch 4171/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.9789 - val_loss: 95.6112\n",
      "Epoch 4172/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.5927 - val_loss: 95.2526\n",
      "Epoch 4173/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.6782 - val_loss: 95.5296\n",
      "Epoch 4174/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.5370 - val_loss: 94.6027\n",
      "Epoch 4175/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.6906 - val_loss: 94.8618\n",
      "Epoch 4176/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.2919 - val_loss: 95.4770\n",
      "Epoch 4177/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.6913 - val_loss: 95.2292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4178/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.3162 - val_loss: 94.8051\n",
      "Epoch 4179/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.4002 - val_loss: 95.1978\n",
      "Epoch 4180/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.6671 - val_loss: 94.3359\n",
      "Epoch 4181/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.5982 - val_loss: 95.2118\n",
      "Epoch 4182/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.9474 - val_loss: 94.7363\n",
      "Epoch 4183/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.8049 - val_loss: 94.6727\n",
      "Epoch 4184/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.2276 - val_loss: 94.8825\n",
      "Epoch 4185/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.3344 - val_loss: 94.5245\n",
      "Epoch 4186/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.5387 - val_loss: 95.0400\n",
      "Epoch 4187/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 93.1737 - val_loss: 94.4930\n",
      "Epoch 4188/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 92.5344 - val_loss: 95.1266\n",
      "Epoch 4189/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 92.5691 - val_loss: 95.2693\n",
      "Epoch 4190/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.3213 - val_loss: 95.1820\n",
      "Epoch 4191/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.8404 - val_loss: 94.4799\n",
      "Epoch 4192/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.8636 - val_loss: 95.3354\n",
      "Epoch 4193/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.3051 - val_loss: 94.5278\n",
      "Epoch 4194/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.3524 - val_loss: 94.0933\n",
      "Epoch 4195/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.2583 - val_loss: 94.3842\n",
      "Epoch 4196/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3345 - val_loss: 95.2128\n",
      "Epoch 4197/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.9870 - val_loss: 95.1360\n",
      "Epoch 4198/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.6183 - val_loss: 95.1785\n",
      "Epoch 4199/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.6043 - val_loss: 95.0811\n",
      "Epoch 4200/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.1028 - val_loss: 94.5663\n",
      "Epoch 4201/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.6776 - val_loss: 94.5510\n",
      "Epoch 4202/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.3782 - val_loss: 94.9321\n",
      "Epoch 4203/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.8059 - val_loss: 95.1311\n",
      "Epoch 4204/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2809 - val_loss: 94.7212\n",
      "Epoch 4205/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.9343 - val_loss: 94.4896\n",
      "Epoch 4206/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.2403 - val_loss: 94.4171\n",
      "Epoch 4207/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.6289 - val_loss: 94.4957\n",
      "Epoch 4208/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.0352 - val_loss: 94.8234\n",
      "Epoch 4209/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.2258 - val_loss: 94.5693\n",
      "Epoch 4210/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.0460 - val_loss: 94.9492\n",
      "Epoch 4211/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 93.1614 - val_loss: 94.9609\n",
      "Epoch 4212/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.1129 - val_loss: 94.6536\n",
      "Epoch 4213/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.7225 - val_loss: 94.6566\n",
      "Epoch 4214/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.4653 - val_loss: 94.4443\n",
      "Epoch 4215/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3095 - val_loss: 94.9823\n",
      "Epoch 4216/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.3740 - val_loss: 95.2128\n",
      "Epoch 4217/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.0106 - val_loss: 95.0092\n",
      "Epoch 4218/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.2369 - val_loss: 94.5284\n",
      "Epoch 4219/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2225 - val_loss: 95.0319\n",
      "Epoch 4220/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.0888 - val_loss: 96.3077\n",
      "Epoch 4221/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 94.5206 - val_loss: 95.4922\n",
      "Epoch 4222/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.3316 - val_loss: 95.0190\n",
      "Epoch 4223/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.5405 - val_loss: 94.1531\n",
      "Epoch 4224/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.3951 - val_loss: 93.9909\n",
      "Epoch 4225/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.0033 - val_loss: 94.5291\n",
      "Epoch 4226/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.3994 - val_loss: 93.5858\n",
      "Epoch 4227/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.8197 - val_loss: 94.5739\n",
      "Epoch 4228/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2261 - val_loss: 94.8269\n",
      "Epoch 4229/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.8251 - val_loss: 95.4558\n",
      "Epoch 4230/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1307 - val_loss: 94.6565\n",
      "Epoch 4231/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.2787 - val_loss: 94.2318\n",
      "Epoch 4232/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.0384 - val_loss: 94.3263\n",
      "Epoch 4233/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.2554 - val_loss: 95.1514\n",
      "Epoch 4234/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.9374 - val_loss: 94.9718\n",
      "Epoch 4235/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.8089 - val_loss: 94.6279\n",
      "Epoch 4236/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.0531 - val_loss: 95.1594\n",
      "Epoch 4237/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.3333 - val_loss: 94.5215\n",
      "Epoch 4238/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.3290 - val_loss: 94.8979\n",
      "Epoch 4239/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.4679 - val_loss: 93.9247\n",
      "Epoch 4240/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.5604 - val_loss: 94.7465\n",
      "Epoch 4241/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.0722 - val_loss: 94.3847\n",
      "Epoch 4242/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.6467 - val_loss: 94.4970\n",
      "Epoch 4243/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6844 - val_loss: 95.3695\n",
      "Epoch 4244/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.7534 - val_loss: 94.8008\n",
      "Epoch 4245/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.6045 - val_loss: 93.9450\n",
      "Epoch 4246/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.7218 - val_loss: 93.9601\n",
      "Epoch 4247/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.3928 - val_loss: 94.4300\n",
      "Epoch 4248/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2311 - val_loss: 94.7681\n",
      "Epoch 4249/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6585 - val_loss: 95.3323\n",
      "Epoch 4250/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.9086 - val_loss: 95.4370\n",
      "Epoch 4251/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.2524 - val_loss: 94.9567\n",
      "Epoch 4252/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.4091 - val_loss: 94.9437\n",
      "Epoch 4253/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.1548 - val_loss: 94.8049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4254/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.9707 - val_loss: 94.1697\n",
      "Epoch 4255/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 95.7658 - val_loss: 94.2345\n",
      "Epoch 4256/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.0834 - val_loss: 94.6207\n",
      "Epoch 4257/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.9553 - val_loss: 95.0222\n",
      "Epoch 4258/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.3166 - val_loss: 94.8309\n",
      "Epoch 4259/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.0802 - val_loss: 94.9937\n",
      "Epoch 4260/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.1397 - val_loss: 94.4051\n",
      "Epoch 4261/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.9300 - val_loss: 95.1133\n",
      "Epoch 4262/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.2874 - val_loss: 93.6965\n",
      "Epoch 4263/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.4369 - val_loss: 94.4652\n",
      "Epoch 4264/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.9949 - val_loss: 94.5852\n",
      "Epoch 4265/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.9374 - val_loss: 94.7244\n",
      "Epoch 4266/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.4460 - val_loss: 94.7045\n",
      "Epoch 4267/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.5132 - val_loss: 94.4933\n",
      "Epoch 4268/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.6111 - val_loss: 94.2089\n",
      "Epoch 4269/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.8824 - val_loss: 94.5760\n",
      "Epoch 4270/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.6369 - val_loss: 94.4887\n",
      "Epoch 4271/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.9821 - val_loss: 94.1650\n",
      "Epoch 4272/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.4636 - val_loss: 94.1774\n",
      "Epoch 4273/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.3311 - val_loss: 94.6655\n",
      "Epoch 4274/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.3770 - val_loss: 94.0340\n",
      "Epoch 4275/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.7391 - val_loss: 93.9749\n",
      "Epoch 4276/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.9367 - val_loss: 94.4603\n",
      "Epoch 4277/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.2464 - val_loss: 94.8021\n",
      "Epoch 4278/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 93.8352 - val_loss: 94.5948\n",
      "Epoch 4279/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 92.7273 - val_loss: 94.6353\n",
      "Epoch 4280/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.9912 - val_loss: 94.9332\n",
      "Epoch 4281/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.7419 - val_loss: 94.8270\n",
      "Epoch 4282/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.2057 - val_loss: 94.7222\n",
      "Epoch 4283/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.2452 - val_loss: 95.0301\n",
      "Epoch 4284/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.2210 - val_loss: 94.9219\n",
      "Epoch 4285/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.6985 - val_loss: 94.1913\n",
      "Epoch 4286/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 93.0084 - val_loss: 94.3745\n",
      "Epoch 4287/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.8192 - val_loss: 94.2066\n",
      "Epoch 4288/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.8192 - val_loss: 94.2826\n",
      "Epoch 4289/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.1242 - val_loss: 94.7597\n",
      "Epoch 4290/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.0892 - val_loss: 94.7292\n",
      "Epoch 4291/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.0114 - val_loss: 95.1065\n",
      "Epoch 4292/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6049 - val_loss: 95.5756\n",
      "Epoch 4293/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.6812 - val_loss: 94.7560\n",
      "Epoch 4294/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.4530 - val_loss: 93.9771\n",
      "Epoch 4295/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1895 - val_loss: 94.6998\n",
      "Epoch 4296/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 94.0475 - val_loss: 94.5537\n",
      "Epoch 4297/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.4679 - val_loss: 94.5017\n",
      "Epoch 4298/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.3957 - val_loss: 95.9556\n",
      "Epoch 4299/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.1673 - val_loss: 95.1023\n",
      "Epoch 4300/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.1105 - val_loss: 94.7241\n",
      "Epoch 4301/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.9276 - val_loss: 94.4673\n",
      "Epoch 4302/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.8836 - val_loss: 94.7918\n",
      "Epoch 4303/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.7086 - val_loss: 94.7809\n",
      "Epoch 4304/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.0267 - val_loss: 94.4710\n",
      "Epoch 4305/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 96.5923 - val_loss: 94.4135\n",
      "Epoch 4306/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.5597 - val_loss: 94.6257\n",
      "Epoch 4307/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.1907 - val_loss: 95.0062\n",
      "Epoch 4308/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.7703 - val_loss: 94.4584\n",
      "Epoch 4309/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.4124 - val_loss: 94.7011\n",
      "Epoch 4310/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5071 - val_loss: 95.5366\n",
      "Epoch 4311/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.1197 - val_loss: 94.7828\n",
      "Epoch 4312/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.5921 - val_loss: 94.7466\n",
      "Epoch 4313/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1062 - val_loss: 94.6280\n",
      "Epoch 4314/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.9803 - val_loss: 94.2616\n",
      "Epoch 4315/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.2067 - val_loss: 94.1853\n",
      "Epoch 4316/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.7278 - val_loss: 94.5050\n",
      "Epoch 4317/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.3766 - val_loss: 95.1207\n",
      "Epoch 4318/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.3213 - val_loss: 94.8145\n",
      "Epoch 4319/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.9948 - val_loss: 95.3581\n",
      "Epoch 4320/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.6043 - val_loss: 94.2588\n",
      "Epoch 4321/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.6027 - val_loss: 94.2246\n",
      "Epoch 4322/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.7279 - val_loss: 94.2438\n",
      "Epoch 4323/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.2266 - val_loss: 94.2020\n",
      "Epoch 4324/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.1465 - val_loss: 95.0345\n",
      "Epoch 4325/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.5572 - val_loss: 94.7135\n",
      "Epoch 4326/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.8637 - val_loss: 94.5688\n",
      "Epoch 4327/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.1867 - val_loss: 94.1424\n",
      "Epoch 4328/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.7972 - val_loss: 94.6115\n",
      "Epoch 4329/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.0175 - val_loss: 95.1032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4330/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.9017 - val_loss: 94.5690\n",
      "Epoch 4331/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.3183 - val_loss: 94.4295\n",
      "Epoch 4332/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.9385 - val_loss: 94.2922\n",
      "Epoch 4333/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.2619 - val_loss: 94.2064\n",
      "Epoch 4334/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8220 - val_loss: 93.8445\n",
      "Epoch 4335/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.4268 - val_loss: 94.8121\n",
      "Epoch 4336/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.7241 - val_loss: 95.0402\n",
      "Epoch 4337/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.5108 - val_loss: 94.6869\n",
      "Epoch 4338/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.6895 - val_loss: 94.4393\n",
      "Epoch 4339/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.2337 - val_loss: 94.5750\n",
      "Epoch 4340/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.3729 - val_loss: 94.9114\n",
      "Epoch 4341/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 94.2198 - val_loss: 94.8264\n",
      "Epoch 4342/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2831 - val_loss: 94.9236\n",
      "Epoch 4343/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.4084 - val_loss: 94.7461\n",
      "Epoch 4344/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.6903 - val_loss: 94.3830\n",
      "Epoch 4345/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.7614 - val_loss: 95.0191\n",
      "Epoch 4346/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.5374 - val_loss: 94.8477\n",
      "Epoch 4347/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.4361 - val_loss: 94.4599\n",
      "Epoch 4348/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.8029 - val_loss: 94.0317\n",
      "Epoch 4349/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 94.3547 - val_loss: 94.3373\n",
      "Epoch 4350/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 92.8882 - val_loss: 94.3382\n",
      "Epoch 4351/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.8450 - val_loss: 94.9446\n",
      "Epoch 4352/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.2483 - val_loss: 95.1143\n",
      "Epoch 4353/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.6238 - val_loss: 94.1882\n",
      "Epoch 4354/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.6728 - val_loss: 93.9791\n",
      "Epoch 4355/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.1985 - val_loss: 94.1311\n",
      "Epoch 4356/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 93.6738 - val_loss: 94.7143\n",
      "Epoch 4357/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.5164 - val_loss: 94.3610\n",
      "Epoch 4358/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.4847 - val_loss: 94.5707\n",
      "Epoch 4359/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.7148 - val_loss: 94.2919\n",
      "Epoch 4360/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6243 - val_loss: 94.0494\n",
      "Epoch 4361/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.4196 - val_loss: 94.4165\n",
      "Epoch 4362/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 94.3185 - val_loss: 94.8739\n",
      "Epoch 4363/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.0205 - val_loss: 94.4012\n",
      "Epoch 4364/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.5133 - val_loss: 94.7883\n",
      "Epoch 4365/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.7542 - val_loss: 94.9619\n",
      "Epoch 4366/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.7112 - val_loss: 94.7682\n",
      "Epoch 4367/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.8445 - val_loss: 95.0637\n",
      "Epoch 4368/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.3687 - val_loss: 94.0794\n",
      "Epoch 4369/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.0748 - val_loss: 94.4563\n",
      "Epoch 4370/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.6430 - val_loss: 94.6604\n",
      "Epoch 4371/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.7490 - val_loss: 95.6991\n",
      "Epoch 4372/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.6704 - val_loss: 94.4778\n",
      "Epoch 4373/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.4284 - val_loss: 95.0585\n",
      "Epoch 4374/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.2399 - val_loss: 94.9498\n",
      "Epoch 4375/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.8967 - val_loss: 94.7850\n",
      "Epoch 4376/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.1547 - val_loss: 94.9801\n",
      "Epoch 4377/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.5734 - val_loss: 95.1439\n",
      "Epoch 4378/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.6244 - val_loss: 94.7127\n",
      "Epoch 4379/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.0921 - val_loss: 94.3096\n",
      "Epoch 4380/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3101 - val_loss: 93.7650\n",
      "Epoch 4381/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.9116 - val_loss: 93.9662\n",
      "Epoch 4382/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.9841 - val_loss: 94.0926\n",
      "Epoch 4383/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.4901 - val_loss: 94.5203\n",
      "Epoch 4384/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 91.8191 - val_loss: 95.0431\n",
      "Epoch 4385/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.5740 - val_loss: 95.8001\n",
      "Epoch 4386/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.5570 - val_loss: 94.6125\n",
      "Epoch 4387/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.5831 - val_loss: 93.8997\n",
      "Epoch 4388/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.0797 - val_loss: 95.0750\n",
      "Epoch 4389/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6435 - val_loss: 94.5997\n",
      "Epoch 4390/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.0464 - val_loss: 94.5572\n",
      "Epoch 4391/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.2506 - val_loss: 93.8604\n",
      "Epoch 4392/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.3653 - val_loss: 94.6678\n",
      "Epoch 4393/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.0541 - val_loss: 95.0154\n",
      "Epoch 4394/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.8682 - val_loss: 94.8075\n",
      "Epoch 4395/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.9821 - val_loss: 94.6156\n",
      "Epoch 4396/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 92.4375 - val_loss: 94.4563\n",
      "Epoch 4397/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.8673 - val_loss: 94.6355\n",
      "Epoch 4398/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.3650 - val_loss: 94.3875\n",
      "Epoch 4399/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.3577 - val_loss: 94.6987\n",
      "Epoch 4400/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.7853 - val_loss: 94.1967\n",
      "Epoch 4401/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.5757 - val_loss: 94.9822\n",
      "Epoch 4402/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.4976 - val_loss: 94.9467\n",
      "Epoch 4403/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.9289 - val_loss: 94.5767\n",
      "Epoch 4404/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.0665 - val_loss: 94.5038\n",
      "Epoch 4405/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.6443 - val_loss: 94.5810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4406/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 94.6155 - val_loss: 94.4616\n",
      "Epoch 4407/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.9294 - val_loss: 94.9030\n",
      "Epoch 4408/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.3214 - val_loss: 94.8134\n",
      "Epoch 4409/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.2758 - val_loss: 94.8250\n",
      "Epoch 4410/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.9818 - val_loss: 95.1952\n",
      "Epoch 4411/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.2587 - val_loss: 94.6235\n",
      "Epoch 4412/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.7149 - val_loss: 94.4173\n",
      "Epoch 4413/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.1871 - val_loss: 94.5770\n",
      "Epoch 4414/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.4366 - val_loss: 94.4018\n",
      "Epoch 4415/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.3038 - val_loss: 94.4026\n",
      "Epoch 4416/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.4795 - val_loss: 95.2333\n",
      "Epoch 4417/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.7104 - val_loss: 95.3119\n",
      "Epoch 4418/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.7392 - val_loss: 94.7462\n",
      "Epoch 4419/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.6110 - val_loss: 94.3441\n",
      "Epoch 4420/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 92.3428 - val_loss: 94.5143\n",
      "Epoch 4421/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.5112 - val_loss: 94.3824\n",
      "Epoch 4422/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.2022 - val_loss: 94.9852\n",
      "Epoch 4423/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2862 - val_loss: 94.7941\n",
      "Epoch 4424/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.7049 - val_loss: 94.8201\n",
      "Epoch 4425/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.5917 - val_loss: 95.0567\n",
      "Epoch 4426/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.3670 - val_loss: 95.3585\n",
      "Epoch 4427/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.5801 - val_loss: 94.8795\n",
      "Epoch 4428/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.8999 - val_loss: 94.7428\n",
      "Epoch 4429/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.9259 - val_loss: 94.6411\n",
      "Epoch 4430/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 95.7724 - val_loss: 94.6623\n",
      "Epoch 4431/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.6718 - val_loss: 94.5397\n",
      "Epoch 4432/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.3063 - val_loss: 94.0909\n",
      "Epoch 4433/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.5431 - val_loss: 94.7445\n",
      "Epoch 4434/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.1809 - val_loss: 94.3631\n",
      "Epoch 4435/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.2986 - val_loss: 94.6222\n",
      "Epoch 4436/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 95.1558 - val_loss: 94.6597\n",
      "Epoch 4437/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.3643 - val_loss: 94.9714\n",
      "Epoch 4438/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.7203 - val_loss: 95.0098\n",
      "Epoch 4439/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.0177 - val_loss: 94.9043\n",
      "Epoch 4440/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.0630 - val_loss: 94.6169\n",
      "Epoch 4441/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.7056 - val_loss: 94.2740\n",
      "Epoch 4442/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.0614 - val_loss: 94.6798\n",
      "Epoch 4443/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.1537 - val_loss: 93.6370\n",
      "Epoch 4444/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.7250 - val_loss: 94.7131\n",
      "Epoch 4445/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.9738 - val_loss: 95.0779\n",
      "Epoch 4446/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.8522 - val_loss: 94.3384\n",
      "Epoch 4447/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 94.0398 - val_loss: 94.4746\n",
      "Epoch 4448/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 95.1491 - val_loss: 93.8664\n",
      "Epoch 4449/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.1445 - val_loss: 94.0163\n",
      "Epoch 4450/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9036 - val_loss: 94.7104\n",
      "Epoch 4451/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2731 - val_loss: 95.1910\n",
      "Epoch 4452/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.9702 - val_loss: 94.2955\n",
      "Epoch 4453/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.7330 - val_loss: 94.0298\n",
      "Epoch 4454/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.6409 - val_loss: 94.9481\n",
      "Epoch 4455/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.2282 - val_loss: 94.2712\n",
      "Epoch 4456/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.5566 - val_loss: 94.4416\n",
      "Epoch 4457/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1689 - val_loss: 94.5876\n",
      "Epoch 4458/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.1399 - val_loss: 94.9823\n",
      "Epoch 4459/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.2933 - val_loss: 94.6493\n",
      "Epoch 4460/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1594 - val_loss: 94.6820\n",
      "Epoch 4461/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.2380 - val_loss: 94.5177\n",
      "Epoch 4462/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.2967 - val_loss: 94.3756\n",
      "Epoch 4463/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.4945 - val_loss: 94.8021\n",
      "Epoch 4464/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6070 - val_loss: 95.2654\n",
      "Epoch 4465/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.1778 - val_loss: 94.1452\n",
      "Epoch 4466/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.3776 - val_loss: 94.7956\n",
      "Epoch 4467/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.9207 - val_loss: 95.6814\n",
      "Epoch 4468/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.7852 - val_loss: 94.6014\n",
      "Epoch 4469/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 92.7181 - val_loss: 93.7266\n",
      "Epoch 4470/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.8581 - val_loss: 94.5430\n",
      "Epoch 4471/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.5132 - val_loss: 94.0126\n",
      "Epoch 4472/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.4299 - val_loss: 94.5102\n",
      "Epoch 4473/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.0929 - val_loss: 94.2415\n",
      "Epoch 4474/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.1248 - val_loss: 94.8543\n",
      "Epoch 4475/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.3519 - val_loss: 93.9234\n",
      "Epoch 4476/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.0443 - val_loss: 94.7529\n",
      "Epoch 4477/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.4006 - val_loss: 94.2138\n",
      "Epoch 4478/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 94.5166 - val_loss: 94.8375\n",
      "Epoch 4479/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.6272 - val_loss: 94.9192\n",
      "Epoch 4480/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7502 - val_loss: 94.9546\n",
      "Epoch 4481/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 95.1493 - val_loss: 95.1892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4482/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 94.3368 - val_loss: 95.4026\n",
      "Epoch 4483/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 94.2685 - val_loss: 95.2444\n",
      "Epoch 4484/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.2815 - val_loss: 95.0417\n",
      "Epoch 4485/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.4271 - val_loss: 94.9746\n",
      "Epoch 4486/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.8918 - val_loss: 94.3497\n",
      "Epoch 4487/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.7843 - val_loss: 94.6602\n",
      "Epoch 4488/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.6941 - val_loss: 95.1624\n",
      "Epoch 4489/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.6335 - val_loss: 95.0307\n",
      "Epoch 4490/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 93.1005 - val_loss: 94.2513\n",
      "Epoch 4491/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.0539 - val_loss: 95.3530\n",
      "Epoch 4492/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.5621 - val_loss: 94.1745\n",
      "Epoch 4493/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.1281 - val_loss: 94.0213\n",
      "Epoch 4494/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.8953 - val_loss: 93.7550\n",
      "Epoch 4495/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.7248 - val_loss: 94.5436\n",
      "Epoch 4496/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.3236 - val_loss: 94.7215\n",
      "Epoch 4497/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.4363 - val_loss: 94.7380\n",
      "Epoch 4498/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.7486 - val_loss: 94.5582\n",
      "Epoch 4499/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.0439 - val_loss: 94.1750\n",
      "Epoch 4500/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.4367 - val_loss: 94.6473\n",
      "Epoch 4501/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.2834 - val_loss: 93.9674\n",
      "Epoch 4502/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.9476 - val_loss: 94.3864\n",
      "Epoch 4503/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2538 - val_loss: 94.8908\n",
      "Epoch 4504/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 92.5280 - val_loss: 94.1387\n",
      "Epoch 4505/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.1568 - val_loss: 94.2161\n",
      "Epoch 4506/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.8813 - val_loss: 94.2124\n",
      "Epoch 4507/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.4202 - val_loss: 95.2318\n",
      "Epoch 4508/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.8838 - val_loss: 94.7460\n",
      "Epoch 4509/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.8567 - val_loss: 93.9851\n",
      "Epoch 4510/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.1116 - val_loss: 95.0393\n",
      "Epoch 4511/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 92.9232 - val_loss: 94.5954\n",
      "Epoch 4512/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 91.9585 - val_loss: 94.1616\n",
      "Epoch 4513/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.9924 - val_loss: 94.6633\n",
      "Epoch 4514/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.2044 - val_loss: 93.9344\n",
      "Epoch 4515/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.3762 - val_loss: 94.4822\n",
      "Epoch 4516/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.1400 - val_loss: 94.6060\n",
      "Epoch 4517/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.9789 - val_loss: 94.5106\n",
      "Epoch 4518/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.8434 - val_loss: 94.4883\n",
      "Epoch 4519/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.4509 - val_loss: 94.7797\n",
      "Epoch 4520/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.2006 - val_loss: 94.4245\n",
      "Epoch 4521/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.4921 - val_loss: 95.2273\n",
      "Epoch 4522/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.5873 - val_loss: 95.1453\n",
      "Epoch 4523/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.8249 - val_loss: 94.8815\n",
      "Epoch 4524/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.8596 - val_loss: 94.3093\n",
      "Epoch 4525/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 92.6596 - val_loss: 94.1655\n",
      "Epoch 4526/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.5734 - val_loss: 95.0326\n",
      "Epoch 4527/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.4562 - val_loss: 94.3922\n",
      "Epoch 4528/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.3012 - val_loss: 94.2315\n",
      "Epoch 4529/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 93.4709 - val_loss: 94.6240\n",
      "Epoch 4530/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1483 - val_loss: 94.6833\n",
      "Epoch 4531/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.1826 - val_loss: 94.2850\n",
      "Epoch 4532/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5425 - val_loss: 94.1634\n",
      "Epoch 4533/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.7814 - val_loss: 94.7752\n",
      "Epoch 4534/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.8260 - val_loss: 95.0118\n",
      "Epoch 4535/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.1284 - val_loss: 95.1005\n",
      "Epoch 4536/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.2183 - val_loss: 95.7168\n",
      "Epoch 4537/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.2409 - val_loss: 95.0874\n",
      "Epoch 4538/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.8640 - val_loss: 94.0998\n",
      "Epoch 4539/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.1671 - val_loss: 94.7357\n",
      "Epoch 4540/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.9374 - val_loss: 94.7302\n",
      "Epoch 4541/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.0509 - val_loss: 93.9863\n",
      "Epoch 4542/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.7464 - val_loss: 94.8010\n",
      "Epoch 4543/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.2824 - val_loss: 94.5729\n",
      "Epoch 4544/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.6831 - val_loss: 93.3734\n",
      "Epoch 4545/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8698 - val_loss: 93.9276\n",
      "Epoch 4546/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.2616 - val_loss: 94.1051\n",
      "Epoch 4547/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.9422 - val_loss: 94.1356\n",
      "Epoch 4548/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.1674 - val_loss: 94.7929\n",
      "Epoch 4549/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.9467 - val_loss: 94.5485\n",
      "Epoch 4550/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.7608 - val_loss: 94.4891\n",
      "Epoch 4551/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.2450 - val_loss: 94.2369\n",
      "Epoch 4552/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.1904 - val_loss: 95.4589\n",
      "Epoch 4553/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.9468 - val_loss: 94.6811\n",
      "Epoch 4554/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.9471 - val_loss: 94.9866\n",
      "Epoch 4555/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0698 - val_loss: 94.3043\n",
      "Epoch 4556/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.0515 - val_loss: 94.7586\n",
      "Epoch 4557/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.9591 - val_loss: 94.2560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4558/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.5498 - val_loss: 94.6107\n",
      "Epoch 4559/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.4498 - val_loss: 94.7342\n",
      "Epoch 4560/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.9301 - val_loss: 94.6911\n",
      "Epoch 4561/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.0709 - val_loss: 94.7887\n",
      "Epoch 4562/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.7840 - val_loss: 94.3543\n",
      "Epoch 4563/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.8178 - val_loss: 93.8631\n",
      "Epoch 4564/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9170 - val_loss: 94.2125\n",
      "Epoch 4565/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5325 - val_loss: 94.1918\n",
      "Epoch 4566/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.6925 - val_loss: 94.2093\n",
      "Epoch 4567/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.9470 - val_loss: 94.3759\n",
      "Epoch 4568/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.1601 - val_loss: 94.5643\n",
      "Epoch 4569/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.5962 - val_loss: 95.2922\n",
      "Epoch 4570/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.8110 - val_loss: 94.6465\n",
      "Epoch 4571/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.3932 - val_loss: 94.6422\n",
      "Epoch 4572/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.6952 - val_loss: 94.3481\n",
      "Epoch 4573/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1806 - val_loss: 94.2833\n",
      "Epoch 4574/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.8628 - val_loss: 94.5451\n",
      "Epoch 4575/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.3317 - val_loss: 94.3672\n",
      "Epoch 4576/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.0675 - val_loss: 94.1595\n",
      "Epoch 4577/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6560 - val_loss: 94.5477\n",
      "Epoch 4578/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9323 - val_loss: 94.3238\n",
      "Epoch 4579/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2695 - val_loss: 94.8437\n",
      "Epoch 4580/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.0544 - val_loss: 94.4697\n",
      "Epoch 4581/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.6013 - val_loss: 94.9293\n",
      "Epoch 4582/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.0387 - val_loss: 94.2106\n",
      "Epoch 4583/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.0620 - val_loss: 94.8933\n",
      "Epoch 4584/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.1395 - val_loss: 94.1507\n",
      "Epoch 4585/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.1987 - val_loss: 93.6138\n",
      "Epoch 4586/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.7407 - val_loss: 94.6533\n",
      "Epoch 4587/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 93.0815 - val_loss: 94.7585\n",
      "Epoch 4588/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.7891 - val_loss: 95.5430\n",
      "Epoch 4589/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.6255 - val_loss: 94.7141\n",
      "Epoch 4590/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.1583 - val_loss: 94.5634\n",
      "Epoch 4591/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.8756 - val_loss: 94.4854\n",
      "Epoch 4592/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.8296 - val_loss: 94.3405\n",
      "Epoch 4593/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.0659 - val_loss: 94.2998\n",
      "Epoch 4594/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.9045 - val_loss: 94.2330\n",
      "Epoch 4595/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.5087 - val_loss: 94.4873\n",
      "Epoch 4596/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9888 - val_loss: 94.1401\n",
      "Epoch 4597/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2432 - val_loss: 94.2697\n",
      "Epoch 4598/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.3235 - val_loss: 95.2365\n",
      "Epoch 4599/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.5276 - val_loss: 94.9282\n",
      "Epoch 4600/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.5444 - val_loss: 95.0045\n",
      "Epoch 4601/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.8481 - val_loss: 93.9539\n",
      "Epoch 4602/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.8363 - val_loss: 94.2878\n",
      "Epoch 4603/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2531 - val_loss: 94.5079\n",
      "Epoch 4604/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 91.6301 - val_loss: 94.4587\n",
      "Epoch 4605/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.7719 - val_loss: 94.2287\n",
      "Epoch 4606/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.4175 - val_loss: 94.4468\n",
      "Epoch 4607/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.9790 - val_loss: 94.4022\n",
      "Epoch 4608/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6540 - val_loss: 94.4474\n",
      "Epoch 4609/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.7285 - val_loss: 94.5194\n",
      "Epoch 4610/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.8543 - val_loss: 94.4722\n",
      "Epoch 4611/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.5082 - val_loss: 94.6513\n",
      "Epoch 4612/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.6472 - val_loss: 94.4618\n",
      "Epoch 4613/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.5014 - val_loss: 94.2148\n",
      "Epoch 4614/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.4781 - val_loss: 94.3244\n",
      "Epoch 4615/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.5529 - val_loss: 93.7271\n",
      "Epoch 4616/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.1653 - val_loss: 93.9205\n",
      "Epoch 4617/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.3947 - val_loss: 94.2468\n",
      "Epoch 4618/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.7451 - val_loss: 94.7971\n",
      "Epoch 4619/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.8177 - val_loss: 94.0958\n",
      "Epoch 4620/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6436 - val_loss: 94.5340\n",
      "Epoch 4621/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.9626 - val_loss: 94.3021\n",
      "Epoch 4622/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.6186 - val_loss: 94.9631\n",
      "Epoch 4623/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 92.5856 - val_loss: 94.4247\n",
      "Epoch 4624/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.5887 - val_loss: 94.2232\n",
      "Epoch 4625/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.0596 - val_loss: 93.8419\n",
      "Epoch 4626/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 93.3092 - val_loss: 94.2945\n",
      "Epoch 4627/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.7312 - val_loss: 93.8596\n",
      "Epoch 4628/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.6128 - val_loss: 94.3831\n",
      "Epoch 4629/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.7042 - val_loss: 94.4189\n",
      "Epoch 4630/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 91.3053 - val_loss: 93.8181\n",
      "Epoch 4631/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.3457 - val_loss: 94.9356\n",
      "Epoch 4632/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.1245 - val_loss: 94.0876\n",
      "Epoch 4633/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.9252 - val_loss: 94.1662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4634/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.6804 - val_loss: 95.1286\n",
      "Epoch 4635/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.9354 - val_loss: 94.0265\n",
      "Epoch 4636/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.6696 - val_loss: 94.2292\n",
      "Epoch 4637/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.7277 - val_loss: 94.2203\n",
      "Epoch 4638/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.3950 - val_loss: 94.7647\n",
      "Epoch 4639/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.0678 - val_loss: 94.6823\n",
      "Epoch 4640/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.9038 - val_loss: 94.2368\n",
      "Epoch 4641/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.8008 - val_loss: 94.8413\n",
      "Epoch 4642/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.2674 - val_loss: 94.3642\n",
      "Epoch 4643/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.7657 - val_loss: 93.9238\n",
      "Epoch 4644/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.4785 - val_loss: 95.4937\n",
      "Epoch 4645/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.5041 - val_loss: 94.5369\n",
      "Epoch 4646/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2415 - val_loss: 94.1867\n",
      "Epoch 4647/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.6071 - val_loss: 94.3770\n",
      "Epoch 4648/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1739 - val_loss: 94.8225\n",
      "Epoch 4649/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 91.5671 - val_loss: 94.0564\n",
      "Epoch 4650/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.9022 - val_loss: 94.4566\n",
      "Epoch 4651/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.7541 - val_loss: 93.9788\n",
      "Epoch 4652/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.4742 - val_loss: 94.4521\n",
      "Epoch 4653/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.7683 - val_loss: 93.7858\n",
      "Epoch 4654/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.4533 - val_loss: 93.7117\n",
      "Epoch 4655/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.1549 - val_loss: 94.2803\n",
      "Epoch 4656/10000\n",
      "96/96 [==============================] - 0s 151us/step - loss: 93.3046 - val_loss: 94.3368\n",
      "Epoch 4657/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 91.6417 - val_loss: 93.9945\n",
      "Epoch 4658/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.1026 - val_loss: 94.5443\n",
      "Epoch 4659/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.2737 - val_loss: 93.8156\n",
      "Epoch 4660/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.0348 - val_loss: 94.7968\n",
      "Epoch 4661/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 91.9502 - val_loss: 94.0249\n",
      "Epoch 4662/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2223 - val_loss: 94.3125\n",
      "Epoch 4663/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.1645 - val_loss: 94.4640\n",
      "Epoch 4664/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.0489 - val_loss: 94.4246\n",
      "Epoch 4665/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.5783 - val_loss: 94.6269\n",
      "Epoch 4666/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.0406 - val_loss: 93.9874\n",
      "Epoch 4667/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.2297 - val_loss: 94.6016\n",
      "Epoch 4668/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.8494 - val_loss: 94.3540\n",
      "Epoch 4669/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.4302 - val_loss: 94.3400\n",
      "Epoch 4670/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.6612 - val_loss: 95.3430\n",
      "Epoch 4671/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 91.4640 - val_loss: 94.4015\n",
      "Epoch 4672/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.2920 - val_loss: 94.1065\n",
      "Epoch 4673/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3529 - val_loss: 94.3157\n",
      "Epoch 4674/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.7133 - val_loss: 94.3879\n",
      "Epoch 4675/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.7139 - val_loss: 95.0869\n",
      "Epoch 4676/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.7769 - val_loss: 94.3111\n",
      "Epoch 4677/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6372 - val_loss: 93.9284\n",
      "Epoch 4678/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.0404 - val_loss: 94.8449\n",
      "Epoch 4679/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 93.0782 - val_loss: 94.6697\n",
      "Epoch 4680/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.3083 - val_loss: 94.2485\n",
      "Epoch 4681/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.7273 - val_loss: 94.4349\n",
      "Epoch 4682/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.7304 - val_loss: 94.3420\n",
      "Epoch 4683/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5697 - val_loss: 93.8951\n",
      "Epoch 4684/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.5103 - val_loss: 94.3929\n",
      "Epoch 4685/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.0499 - val_loss: 94.4878\n",
      "Epoch 4686/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.4613 - val_loss: 94.4710\n",
      "Epoch 4687/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.0070 - val_loss: 94.0385\n",
      "Epoch 4688/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.0628 - val_loss: 94.1992\n",
      "Epoch 4689/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 91.6686 - val_loss: 94.6953\n",
      "Epoch 4690/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.3766 - val_loss: 93.8624\n",
      "Epoch 4691/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.0671 - val_loss: 94.9504\n",
      "Epoch 4692/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.7824 - val_loss: 93.9480\n",
      "Epoch 4693/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.6813 - val_loss: 94.2072\n",
      "Epoch 4694/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.2371 - val_loss: 94.6371\n",
      "Epoch 4695/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.9349 - val_loss: 94.7809\n",
      "Epoch 4696/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2743 - val_loss: 93.8380\n",
      "Epoch 4697/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.6443 - val_loss: 94.3776\n",
      "Epoch 4698/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.7816 - val_loss: 93.7789\n",
      "Epoch 4699/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.8274 - val_loss: 94.1120\n",
      "Epoch 4700/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.6709 - val_loss: 94.2410\n",
      "Epoch 4701/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.0074 - val_loss: 94.8792\n",
      "Epoch 4702/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.0420 - val_loss: 94.2056\n",
      "Epoch 4703/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.0903 - val_loss: 94.3750\n",
      "Epoch 4704/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.3852 - val_loss: 93.9095\n",
      "Epoch 4705/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 94.5612 - val_loss: 94.5444\n",
      "Epoch 4706/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.2992 - val_loss: 93.9851\n",
      "Epoch 4707/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.5074 - val_loss: 94.6269\n",
      "Epoch 4708/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.6966 - val_loss: 94.0158\n",
      "Epoch 4709/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.3942 - val_loss: 93.9449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4710/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.6241 - val_loss: 93.9828\n",
      "Epoch 4711/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.4652 - val_loss: 94.7019\n",
      "Epoch 4712/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.8607 - val_loss: 94.3447\n",
      "Epoch 4713/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.6390 - val_loss: 94.5158\n",
      "Epoch 4714/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.7134 - val_loss: 94.4047\n",
      "Epoch 4715/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.8880 - val_loss: 94.2633\n",
      "Epoch 4716/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 92.6350 - val_loss: 94.5456\n",
      "Epoch 4717/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.0194 - val_loss: 93.5563\n",
      "Epoch 4718/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.2570 - val_loss: 94.6567\n",
      "Epoch 4719/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.5411 - val_loss: 94.1858\n",
      "Epoch 4720/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.9014 - val_loss: 94.5377\n",
      "Epoch 4721/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.7438 - val_loss: 94.3644\n",
      "Epoch 4722/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.8834 - val_loss: 94.6422\n",
      "Epoch 4723/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.9984 - val_loss: 94.6077\n",
      "Epoch 4724/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.1412 - val_loss: 94.5136\n",
      "Epoch 4725/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0050 - val_loss: 94.2613\n",
      "Epoch 4726/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.8883 - val_loss: 93.9577\n",
      "Epoch 4727/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.6337 - val_loss: 94.5012\n",
      "Epoch 4728/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.9259 - val_loss: 94.2609\n",
      "Epoch 4729/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 92.1934 - val_loss: 94.3193\n",
      "Epoch 4730/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.0121 - val_loss: 93.7219\n",
      "Epoch 4731/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.0417 - val_loss: 94.2550\n",
      "Epoch 4732/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.2272 - val_loss: 94.1402\n",
      "Epoch 4733/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.3268 - val_loss: 93.5779\n",
      "Epoch 4734/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.4486 - val_loss: 94.2037\n",
      "Epoch 4735/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 91.7533 - val_loss: 94.1642\n",
      "Epoch 4736/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1013 - val_loss: 93.8906\n",
      "Epoch 4737/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.1136 - val_loss: 94.1818\n",
      "Epoch 4738/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.7426 - val_loss: 94.1958\n",
      "Epoch 4739/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.7896 - val_loss: 94.0393\n",
      "Epoch 4740/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.4759 - val_loss: 94.0088\n",
      "Epoch 4741/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1354 - val_loss: 94.7449\n",
      "Epoch 4742/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.5500 - val_loss: 93.9054\n",
      "Epoch 4743/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.1989 - val_loss: 94.5029\n",
      "Epoch 4744/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.1189 - val_loss: 94.6354\n",
      "Epoch 4745/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.5597 - val_loss: 93.9698\n",
      "Epoch 4746/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.9936 - val_loss: 93.9176\n",
      "Epoch 4747/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.1369 - val_loss: 93.9788\n",
      "Epoch 4748/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0640 - val_loss: 93.8021\n",
      "Epoch 4749/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.4689 - val_loss: 94.5249\n",
      "Epoch 4750/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 92.4494 - val_loss: 94.1068\n",
      "Epoch 4751/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 91.3976 - val_loss: 94.4081\n",
      "Epoch 4752/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.4124 - val_loss: 94.8483\n",
      "Epoch 4753/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1131 - val_loss: 93.8605\n",
      "Epoch 4754/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.2644 - val_loss: 94.6389\n",
      "Epoch 4755/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1680 - val_loss: 94.4725\n",
      "Epoch 4756/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 92.2660 - val_loss: 93.7433\n",
      "Epoch 4757/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.7896 - val_loss: 93.9298\n",
      "Epoch 4758/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.7505 - val_loss: 93.8613\n",
      "Epoch 4759/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.0588 - val_loss: 94.1342\n",
      "Epoch 4760/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6323 - val_loss: 93.8361\n",
      "Epoch 4761/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.7128 - val_loss: 94.3520\n",
      "Epoch 4762/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.1366 - val_loss: 94.6369\n",
      "Epoch 4763/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8597 - val_loss: 93.8807\n",
      "Epoch 4764/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.8463 - val_loss: 94.2780\n",
      "Epoch 4765/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.7439 - val_loss: 93.8937\n",
      "Epoch 4766/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 93.5910 - val_loss: 94.1442\n",
      "Epoch 4767/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 92.5935 - val_loss: 95.2790\n",
      "Epoch 4768/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.1391 - val_loss: 94.5410\n",
      "Epoch 4769/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.5974 - val_loss: 94.7046\n",
      "Epoch 4770/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.1765 - val_loss: 94.0469\n",
      "Epoch 4771/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.3015 - val_loss: 94.3673\n",
      "Epoch 4772/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.3794 - val_loss: 94.3099\n",
      "Epoch 4773/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.8769 - val_loss: 94.2793\n",
      "Epoch 4774/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2790 - val_loss: 94.2571\n",
      "Epoch 4775/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.0121 - val_loss: 93.9890\n",
      "Epoch 4776/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.3338 - val_loss: 93.9822\n",
      "Epoch 4777/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.9080 - val_loss: 93.7636\n",
      "Epoch 4778/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.7029 - val_loss: 94.4842\n",
      "Epoch 4779/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.1328 - val_loss: 94.0206\n",
      "Epoch 4780/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.5150 - val_loss: 93.8962\n",
      "Epoch 4781/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6552 - val_loss: 94.1727\n",
      "Epoch 4782/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 92.7497 - val_loss: 94.9900\n",
      "Epoch 4783/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.8356 - val_loss: 94.4137\n",
      "Epoch 4784/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 92.3181 - val_loss: 93.9624\n",
      "Epoch 4785/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 94.2454 - val_loss: 94.8264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4786/10000\n",
      "96/96 [==============================] - 0s 147us/step - loss: 94.3112 - val_loss: 94.0901\n",
      "Epoch 4787/10000\n",
      "96/96 [==============================] - 0s 148us/step - loss: 91.8344 - val_loss: 94.5492\n",
      "Epoch 4788/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 92.6465 - val_loss: 94.1058\n",
      "Epoch 4789/10000\n",
      "96/96 [==============================] - 0s 149us/step - loss: 93.2634 - val_loss: 94.0055\n",
      "Epoch 4790/10000\n",
      "96/96 [==============================] - 0s 153us/step - loss: 94.1957 - val_loss: 94.5380\n",
      "Epoch 4791/10000\n",
      "96/96 [==============================] - 0s 150us/step - loss: 92.1847 - val_loss: 95.1008\n",
      "Epoch 4792/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 92.4741 - val_loss: 93.5502\n",
      "Epoch 4793/10000\n",
      "96/96 [==============================] - 0s 155us/step - loss: 91.9542 - val_loss: 94.2470\n",
      "Epoch 4794/10000\n",
      "96/96 [==============================] - 0s 159us/step - loss: 90.9993 - val_loss: 94.1760\n",
      "Epoch 4795/10000\n",
      "96/96 [==============================] - 0s 155us/step - loss: 93.4953 - val_loss: 94.0505\n",
      "Epoch 4796/10000\n",
      "96/96 [==============================] - 0s 156us/step - loss: 92.6583 - val_loss: 94.3805\n",
      "Epoch 4797/10000\n",
      "96/96 [==============================] - 0s 160us/step - loss: 93.8695 - val_loss: 94.5031\n",
      "Epoch 4798/10000\n",
      "96/96 [==============================] - 0s 166us/step - loss: 92.8250 - val_loss: 94.8313\n",
      "Epoch 4799/10000\n",
      "96/96 [==============================] - 0s 174us/step - loss: 94.1689 - val_loss: 95.2771\n",
      "Epoch 4800/10000\n",
      "96/96 [==============================] - 0s 171us/step - loss: 93.0973 - val_loss: 94.7616\n",
      "Epoch 4801/10000\n",
      "96/96 [==============================] - 0s 167us/step - loss: 92.7472 - val_loss: 94.1982\n",
      "Epoch 4802/10000\n",
      "96/96 [==============================] - 0s 157us/step - loss: 91.9025 - val_loss: 94.1077\n",
      "Epoch 4803/10000\n",
      "96/96 [==============================] - 0s 148us/step - loss: 91.3887 - val_loss: 94.5424\n",
      "Epoch 4804/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.3005 - val_loss: 94.7712\n",
      "Epoch 4805/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.6132 - val_loss: 94.4739\n",
      "Epoch 4806/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.4839 - val_loss: 93.8668\n",
      "Epoch 4807/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.9761 - val_loss: 94.4246\n",
      "Epoch 4808/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6553 - val_loss: 94.0118\n",
      "Epoch 4809/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 91.5964 - val_loss: 94.3846\n",
      "Epoch 4810/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.6534 - val_loss: 94.0424\n",
      "Epoch 4811/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.0478 - val_loss: 94.0570\n",
      "Epoch 4812/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.9581 - val_loss: 94.1026\n",
      "Epoch 4813/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.9326 - val_loss: 94.0739\n",
      "Epoch 4814/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.1347 - val_loss: 93.6971\n",
      "Epoch 4815/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.0840 - val_loss: 93.5844\n",
      "Epoch 4816/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.4755 - val_loss: 94.3824\n",
      "Epoch 4817/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1827 - val_loss: 94.2434\n",
      "Epoch 4818/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9919 - val_loss: 94.2946\n",
      "Epoch 4819/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.0581 - val_loss: 93.8887\n",
      "Epoch 4820/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.3634 - val_loss: 93.9296\n",
      "Epoch 4821/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.1713 - val_loss: 94.3604\n",
      "Epoch 4822/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.8191 - val_loss: 94.0449\n",
      "Epoch 4823/10000\n",
      "96/96 [==============================] - 0s 160us/step - loss: 93.0557 - val_loss: 95.3679\n",
      "Epoch 4824/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.6674 - val_loss: 94.3605\n",
      "Epoch 4825/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7290 - val_loss: 94.4374\n",
      "Epoch 4826/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.8379 - val_loss: 94.4050\n",
      "Epoch 4827/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.2543 - val_loss: 94.0630\n",
      "Epoch 4828/10000\n",
      "96/96 [==============================] - 0s 202us/step - loss: 91.9860 - val_loss: 94.1068\n",
      "Epoch 4829/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 92.4748 - val_loss: 94.1535\n",
      "Epoch 4830/10000\n",
      "96/96 [==============================] - 0s 240us/step - loss: 93.2030 - val_loss: 94.3646\n",
      "Epoch 4831/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.5774 - val_loss: 94.3582\n",
      "Epoch 4832/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.5770 - val_loss: 94.2745\n",
      "Epoch 4833/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.9559 - val_loss: 94.4199\n",
      "Epoch 4834/10000\n",
      "96/96 [==============================] - 0s 158us/step - loss: 91.9310 - val_loss: 94.1770\n",
      "Epoch 4835/10000\n",
      "96/96 [==============================] - 0s 157us/step - loss: 91.7895 - val_loss: 94.8305\n",
      "Epoch 4836/10000\n",
      "96/96 [==============================] - 0s 151us/step - loss: 91.5437 - val_loss: 94.1447\n",
      "Epoch 4837/10000\n",
      "96/96 [==============================] - 0s 152us/step - loss: 94.8252 - val_loss: 94.4979\n",
      "Epoch 4838/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 92.2373 - val_loss: 95.1806\n",
      "Epoch 4839/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 91.9515 - val_loss: 94.3978\n",
      "Epoch 4840/10000\n",
      "96/96 [==============================] - 0s 153us/step - loss: 93.0911 - val_loss: 94.2478\n",
      "Epoch 4841/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 92.3254 - val_loss: 94.3941\n",
      "Epoch 4842/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 92.1695 - val_loss: 93.9498\n",
      "Epoch 4843/10000\n",
      "96/96 [==============================] - 0s 151us/step - loss: 92.0850 - val_loss: 94.2530\n",
      "Epoch 4844/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 92.4414 - val_loss: 94.5492\n",
      "Epoch 4845/10000\n",
      "96/96 [==============================] - 0s 147us/step - loss: 93.4970 - val_loss: 94.1495\n",
      "Epoch 4846/10000\n",
      "96/96 [==============================] - 0s 149us/step - loss: 91.9984 - val_loss: 93.7736\n",
      "Epoch 4847/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 93.2715 - val_loss: 94.4297\n",
      "Epoch 4848/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.7989 - val_loss: 94.5341\n",
      "Epoch 4849/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 92.9284 - val_loss: 94.4621\n",
      "Epoch 4850/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 93.1461 - val_loss: 94.0207\n",
      "Epoch 4851/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 93.4855 - val_loss: 94.3724\n",
      "Epoch 4852/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.4452 - val_loss: 93.9941\n",
      "Epoch 4853/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.0165 - val_loss: 94.7546\n",
      "Epoch 4854/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1559 - val_loss: 94.0356\n",
      "Epoch 4855/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.3707 - val_loss: 94.0441\n",
      "Epoch 4856/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.0183 - val_loss: 93.6689\n",
      "Epoch 4857/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 94.5168 - val_loss: 94.2020\n",
      "Epoch 4858/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 92.6728 - val_loss: 94.0226\n",
      "Epoch 4859/10000\n",
      "96/96 [==============================] - 0s 161us/step - loss: 92.4660 - val_loss: 93.9204\n",
      "Epoch 4860/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 92.2638 - val_loss: 94.1335\n",
      "Epoch 4861/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 91.7411 - val_loss: 95.1551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4862/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.0471 - val_loss: 94.4313\n",
      "Epoch 4863/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 92.2561 - val_loss: 94.5239\n",
      "Epoch 4864/10000\n",
      "96/96 [==============================] - 0s 155us/step - loss: 91.5924 - val_loss: 94.2918\n",
      "Epoch 4865/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.5809 - val_loss: 94.6133\n",
      "Epoch 4866/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.3927 - val_loss: 94.4415\n",
      "Epoch 4867/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.3969 - val_loss: 94.1734\n",
      "Epoch 4868/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3708 - val_loss: 93.6935\n",
      "Epoch 4869/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 92.8977 - val_loss: 93.6884\n",
      "Epoch 4870/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.5930 - val_loss: 94.1227\n",
      "Epoch 4871/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5961 - val_loss: 94.7266\n",
      "Epoch 4872/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.0420 - val_loss: 94.7477\n",
      "Epoch 4873/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.9981 - val_loss: 94.4186\n",
      "Epoch 4874/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.1977 - val_loss: 94.2572\n",
      "Epoch 4875/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6731 - val_loss: 93.9025\n",
      "Epoch 4876/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.6180 - val_loss: 94.4076\n",
      "Epoch 4877/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.0129 - val_loss: 93.3444\n",
      "Epoch 4878/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.2477 - val_loss: 94.5124\n",
      "Epoch 4879/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2990 - val_loss: 94.3922\n",
      "Epoch 4880/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.4097 - val_loss: 93.8946\n",
      "Epoch 4881/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3318 - val_loss: 94.0832\n",
      "Epoch 4882/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.4455 - val_loss: 94.5066\n",
      "Epoch 4883/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.8658 - val_loss: 94.0075\n",
      "Epoch 4884/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.2744 - val_loss: 94.2511\n",
      "Epoch 4885/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.8334 - val_loss: 94.0771\n",
      "Epoch 4886/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.6743 - val_loss: 94.4082\n",
      "Epoch 4887/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.4337 - val_loss: 94.4255\n",
      "Epoch 4888/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.9233 - val_loss: 93.9348\n",
      "Epoch 4889/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2227 - val_loss: 93.9739\n",
      "Epoch 4890/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.6259 - val_loss: 94.0231\n",
      "Epoch 4891/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8618 - val_loss: 94.0048\n",
      "Epoch 4892/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.0785 - val_loss: 94.4008\n",
      "Epoch 4893/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 92.2074 - val_loss: 94.3618\n",
      "Epoch 4894/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.0174 - val_loss: 93.9676\n",
      "Epoch 4895/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.5933 - val_loss: 94.1833\n",
      "Epoch 4896/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 92.8451 - val_loss: 94.1651\n",
      "Epoch 4897/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2854 - val_loss: 94.2383\n",
      "Epoch 4898/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.1238 - val_loss: 94.0726\n",
      "Epoch 4899/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.6822 - val_loss: 94.3555\n",
      "Epoch 4900/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5149 - val_loss: 94.2469\n",
      "Epoch 4901/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.0965 - val_loss: 94.2668\n",
      "Epoch 4902/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.0160 - val_loss: 93.9345\n",
      "Epoch 4903/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 92.4004 - val_loss: 94.0059\n",
      "Epoch 4904/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.1459 - val_loss: 94.3318\n",
      "Epoch 4905/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.8715 - val_loss: 94.3712\n",
      "Epoch 4906/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.5378 - val_loss: 94.2177\n",
      "Epoch 4907/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.2562 - val_loss: 93.9829\n",
      "Epoch 4908/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.5663 - val_loss: 94.5369\n",
      "Epoch 4909/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.4664 - val_loss: 93.7976\n",
      "Epoch 4910/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.7663 - val_loss: 94.1962\n",
      "Epoch 4911/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7461 - val_loss: 94.3878\n",
      "Epoch 4912/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.5998 - val_loss: 93.9492\n",
      "Epoch 4913/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2399 - val_loss: 95.0665\n",
      "Epoch 4914/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8504 - val_loss: 94.4966\n",
      "Epoch 4915/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2367 - val_loss: 94.2456\n",
      "Epoch 4916/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.1746 - val_loss: 94.0199\n",
      "Epoch 4917/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3141 - val_loss: 94.4229\n",
      "Epoch 4918/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.3459 - val_loss: 94.0140\n",
      "Epoch 4919/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.9789 - val_loss: 93.9286\n",
      "Epoch 4920/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.8171 - val_loss: 93.9896\n",
      "Epoch 4921/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5323 - val_loss: 94.3809\n",
      "Epoch 4922/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.5640 - val_loss: 93.7125\n",
      "Epoch 4923/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.3892 - val_loss: 95.1588\n",
      "Epoch 4924/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6720 - val_loss: 93.6524\n",
      "Epoch 4925/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.5838 - val_loss: 94.0574\n",
      "Epoch 4926/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.0713 - val_loss: 93.7341\n",
      "Epoch 4927/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.2773 - val_loss: 94.1437\n",
      "Epoch 4928/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5768 - val_loss: 94.2588\n",
      "Epoch 4929/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.8864 - val_loss: 94.2781\n",
      "Epoch 4930/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.2135 - val_loss: 94.2762\n",
      "Epoch 4931/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9822 - val_loss: 94.3499\n",
      "Epoch 4932/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5930 - val_loss: 94.1461\n",
      "Epoch 4933/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.6489 - val_loss: 93.3495\n",
      "Epoch 4934/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.6617 - val_loss: 94.0165\n",
      "Epoch 4935/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.9208 - val_loss: 94.2868\n",
      "Epoch 4936/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.6092 - val_loss: 94.6239\n",
      "Epoch 4937/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.8978 - val_loss: 94.4315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4938/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.5491 - val_loss: 94.3933\n",
      "Epoch 4939/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9807 - val_loss: 94.1402\n",
      "Epoch 4940/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 92.5630 - val_loss: 93.7603\n",
      "Epoch 4941/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.7277 - val_loss: 94.4325\n",
      "Epoch 4942/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6240 - val_loss: 94.0912\n",
      "Epoch 4943/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.9326 - val_loss: 94.1208\n",
      "Epoch 4944/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.0465 - val_loss: 93.8640\n",
      "Epoch 4945/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.7256 - val_loss: 93.7700\n",
      "Epoch 4946/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.2716 - val_loss: 94.2156\n",
      "Epoch 4947/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.3607 - val_loss: 93.8376\n",
      "Epoch 4948/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 94.9221 - val_loss: 94.1584\n",
      "Epoch 4949/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5836 - val_loss: 94.3289\n",
      "Epoch 4950/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.9546 - val_loss: 94.0005\n",
      "Epoch 4951/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5323 - val_loss: 94.0842\n",
      "Epoch 4952/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.7974 - val_loss: 94.2807\n",
      "Epoch 4953/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.3163 - val_loss: 93.8951\n",
      "Epoch 4954/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.3031 - val_loss: 93.7847\n",
      "Epoch 4955/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2714 - val_loss: 94.0425\n",
      "Epoch 4956/10000\n",
      "96/96 [==============================] - 0s 150us/step - loss: 92.4010 - val_loss: 94.0275\n",
      "Epoch 4957/10000\n",
      "96/96 [==============================] - 0s 164us/step - loss: 92.4906 - val_loss: 94.0242\n",
      "Epoch 4958/10000\n",
      "96/96 [==============================] - 0s 183us/step - loss: 91.8551 - val_loss: 94.3047\n",
      "Epoch 4959/10000\n",
      "96/96 [==============================] - 0s 190us/step - loss: 92.5966 - val_loss: 94.0912\n",
      "Epoch 4960/10000\n",
      "96/96 [==============================] - 0s 181us/step - loss: 91.7861 - val_loss: 94.3533\n",
      "Epoch 4961/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 92.3281 - val_loss: 94.4013\n",
      "Epoch 4962/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 92.1259 - val_loss: 93.6501\n",
      "Epoch 4963/10000\n",
      "96/96 [==============================] - 0s 150us/step - loss: 92.5456 - val_loss: 93.8627\n",
      "Epoch 4964/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 91.8030 - val_loss: 94.1293\n",
      "Epoch 4965/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 91.6944 - val_loss: 94.2011\n",
      "Epoch 4966/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 92.5991 - val_loss: 93.9508\n",
      "Epoch 4967/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 92.1727 - val_loss: 94.1121\n",
      "Epoch 4968/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 92.3444 - val_loss: 94.2141\n",
      "Epoch 4969/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.3137 - val_loss: 93.8326\n",
      "Epoch 4970/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 93.4132 - val_loss: 93.7430\n",
      "Epoch 4971/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.9956 - val_loss: 94.0534\n",
      "Epoch 4972/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.2393 - val_loss: 94.3294\n",
      "Epoch 4973/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.6861 - val_loss: 94.4700\n",
      "Epoch 4974/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.9411 - val_loss: 93.7472\n",
      "Epoch 4975/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.9778 - val_loss: 93.8251\n",
      "Epoch 4976/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 92.5525 - val_loss: 94.7854\n",
      "Epoch 4977/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 92.2608 - val_loss: 94.0618\n",
      "Epoch 4978/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 92.1005 - val_loss: 94.3305\n",
      "Epoch 4979/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 93.7160 - val_loss: 94.3242\n",
      "Epoch 4980/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 92.8199 - val_loss: 93.7814\n",
      "Epoch 4981/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 92.1922 - val_loss: 93.9324\n",
      "Epoch 4982/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 92.9593 - val_loss: 94.5449\n",
      "Epoch 4983/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.6541 - val_loss: 94.3495\n",
      "Epoch 4984/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.8598 - val_loss: 93.9913\n",
      "Epoch 4985/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.9607 - val_loss: 94.1903\n",
      "Epoch 4986/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 91.7956 - val_loss: 94.5801\n",
      "Epoch 4987/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 91.7197 - val_loss: 94.1951\n",
      "Epoch 4988/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 91.8920 - val_loss: 95.2769\n",
      "Epoch 4989/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.9066 - val_loss: 93.8922\n",
      "Epoch 4990/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.8824 - val_loss: 94.2737\n",
      "Epoch 4991/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.6989 - val_loss: 94.0814\n",
      "Epoch 4992/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.6724 - val_loss: 93.4201\n",
      "Epoch 4993/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 92.4901 - val_loss: 94.9598\n",
      "Epoch 4994/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.4575 - val_loss: 94.2622\n",
      "Epoch 4995/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 92.4847 - val_loss: 93.8044\n",
      "Epoch 4996/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.8732 - val_loss: 93.7850\n",
      "Epoch 4997/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 91.7442 - val_loss: 93.9102\n",
      "Epoch 4998/10000\n",
      "96/96 [==============================] - 0s 154us/step - loss: 92.1091 - val_loss: 94.2668\n",
      "Epoch 4999/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 93.2609 - val_loss: 94.0739\n",
      "Epoch 5000/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 92.6300 - val_loss: 93.9741\n",
      "Epoch 5001/10000\n",
      "96/96 [==============================] - 0s 146us/step - loss: 92.1160 - val_loss: 94.2718\n",
      "Epoch 5002/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 91.6186 - val_loss: 94.2604\n",
      "Epoch 5003/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.7546 - val_loss: 94.5860\n",
      "Epoch 5004/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.8105 - val_loss: 93.9293\n",
      "Epoch 5005/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.4442 - val_loss: 94.3662\n",
      "Epoch 5006/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 91.9584 - val_loss: 94.2152\n",
      "Epoch 5007/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 92.1338 - val_loss: 93.8317\n",
      "Epoch 5008/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 91.4704 - val_loss: 94.5291\n",
      "Epoch 5009/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 92.8572 - val_loss: 93.5037\n",
      "Epoch 5010/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 92.8928 - val_loss: 93.9576\n",
      "Epoch 5011/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 93.0283 - val_loss: 94.1862\n",
      "Epoch 5012/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 92.1814 - val_loss: 95.0431\n",
      "Epoch 5013/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 92.2244 - val_loss: 93.3264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5014/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 92.5638 - val_loss: 94.1258\n",
      "Epoch 5015/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 92.0424 - val_loss: 93.7771\n",
      "Epoch 5016/10000\n",
      "96/96 [==============================] - 0s 163us/step - loss: 92.9121 - val_loss: 93.9918\n",
      "Epoch 5017/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 93.0279 - val_loss: 93.9570\n",
      "Epoch 5018/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 92.6172 - val_loss: 94.2140\n",
      "Epoch 5019/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 92.1044 - val_loss: 95.4073\n",
      "Epoch 5020/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 92.0359 - val_loss: 93.7086\n",
      "Epoch 5021/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 92.4350 - val_loss: 93.9488\n",
      "Epoch 5022/10000\n",
      "96/96 [==============================] - 0s 158us/step - loss: 93.3494 - val_loss: 94.1696\n",
      "Epoch 5023/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 92.6336 - val_loss: 93.7622\n",
      "Epoch 5024/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 92.7642 - val_loss: 93.8972\n",
      "Epoch 5025/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 91.9326 - val_loss: 94.1441\n",
      "Epoch 5026/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 92.9471 - val_loss: 93.9027\n",
      "Epoch 5027/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 92.3587 - val_loss: 94.0608\n",
      "Epoch 5028/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 92.3572 - val_loss: 94.4095\n",
      "Epoch 5029/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 93.5560 - val_loss: 94.0963\n",
      "Epoch 5030/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.3592 - val_loss: 94.1330\n",
      "Epoch 5031/10000\n",
      "96/96 [==============================] - 0s 155us/step - loss: 93.1070 - val_loss: 94.2781\n",
      "Epoch 5032/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 91.5447 - val_loss: 94.8394\n",
      "Epoch 5033/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 94.0141 - val_loss: 94.5926\n",
      "Epoch 5034/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.1564 - val_loss: 94.3289\n",
      "Epoch 5035/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 92.8774 - val_loss: 94.0156\n",
      "Epoch 5036/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 92.0997 - val_loss: 94.1403\n",
      "Epoch 5037/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.7087 - val_loss: 93.9763\n",
      "Epoch 5038/10000\n",
      "96/96 [==============================] - 0s 154us/step - loss: 91.5770 - val_loss: 94.0456\n",
      "Epoch 5039/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.4594 - val_loss: 94.2084\n",
      "Epoch 5040/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.5414 - val_loss: 93.9019\n",
      "Epoch 5041/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 92.7196 - val_loss: 93.9863\n",
      "Epoch 5042/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 92.1933 - val_loss: 93.8544\n",
      "Epoch 5043/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.6428 - val_loss: 94.4068\n",
      "Epoch 5044/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.3357 - val_loss: 94.0441\n",
      "Epoch 5045/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 92.7452 - val_loss: 94.0468\n",
      "Epoch 5046/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.1709 - val_loss: 93.7113\n",
      "Epoch 5047/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 92.1401 - val_loss: 93.6519\n",
      "Epoch 5048/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 91.7208 - val_loss: 93.4506\n",
      "Epoch 5049/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 92.6138 - val_loss: 93.8999\n",
      "Epoch 5050/10000\n",
      "96/96 [==============================] - 0s 148us/step - loss: 92.5834 - val_loss: 94.3423\n",
      "Epoch 5051/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.6172 - val_loss: 94.2008\n",
      "Epoch 5052/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5300 - val_loss: 94.1629\n",
      "Epoch 5053/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.8225 - val_loss: 94.1353\n",
      "Epoch 5054/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.4840 - val_loss: 93.9394\n",
      "Epoch 5055/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.6527 - val_loss: 94.2839\n",
      "Epoch 5056/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.5775 - val_loss: 94.4111\n",
      "Epoch 5057/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.3879 - val_loss: 94.0648\n",
      "Epoch 5058/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 92.3550 - val_loss: 94.1052\n",
      "Epoch 5059/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.5304 - val_loss: 94.3845\n",
      "Epoch 5060/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 91.8992 - val_loss: 94.1058\n",
      "Epoch 5061/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.7171 - val_loss: 93.9537\n",
      "Epoch 5062/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 91.9225 - val_loss: 94.0882\n",
      "Epoch 5063/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 91.4066 - val_loss: 94.2102\n",
      "Epoch 5064/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 93.6775 - val_loss: 93.9876\n",
      "Epoch 5065/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.2900 - val_loss: 93.6407\n",
      "Epoch 5066/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 91.7144 - val_loss: 94.0915\n",
      "Epoch 5067/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.4068 - val_loss: 94.2587\n",
      "Epoch 5068/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.8225 - val_loss: 94.0569\n",
      "Epoch 5069/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.9005 - val_loss: 93.6585\n",
      "Epoch 5070/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 92.0859 - val_loss: 94.3567\n",
      "Epoch 5071/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 92.9832 - val_loss: 94.7756\n",
      "Epoch 5072/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.4260 - val_loss: 93.8300\n",
      "Epoch 5073/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 91.4279 - val_loss: 94.3149\n",
      "Epoch 5074/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 93.0169 - val_loss: 93.7758\n",
      "Epoch 5075/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 93.2272 - val_loss: 94.4902\n",
      "Epoch 5076/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 92.1793 - val_loss: 94.0541\n",
      "Epoch 5077/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.7264 - val_loss: 94.1200\n",
      "Epoch 5078/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 91.2258 - val_loss: 94.0915\n",
      "Epoch 5079/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 91.6025 - val_loss: 93.9251\n",
      "Epoch 5080/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 92.2963 - val_loss: 94.0264\n",
      "Epoch 5081/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 91.8187 - val_loss: 94.4209\n",
      "Epoch 5082/10000\n",
      "96/96 [==============================] - 0s 148us/step - loss: 91.7814 - val_loss: 93.9078\n",
      "Epoch 5083/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 94.0652 - val_loss: 94.3185\n",
      "Epoch 5084/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.3801 - val_loss: 93.9346\n",
      "Epoch 5085/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.2282 - val_loss: 93.6697\n",
      "Epoch 5086/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 91.5596 - val_loss: 93.9402\n",
      "Epoch 5087/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.9175 - val_loss: 94.3444\n",
      "Epoch 5088/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 91.8464 - val_loss: 94.0358\n",
      "Epoch 5089/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 91.5356 - val_loss: 94.2961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5090/10000\n",
      "96/96 [==============================] - 0s 147us/step - loss: 92.7834 - val_loss: 94.8130\n",
      "Epoch 5091/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 92.8948 - val_loss: 93.9740\n",
      "Epoch 5092/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.4938 - val_loss: 93.9885\n",
      "Epoch 5093/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 92.5540 - val_loss: 93.7087\n",
      "Epoch 5094/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.4417 - val_loss: 93.6983\n",
      "Epoch 5095/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.1524 - val_loss: 93.5897\n",
      "Epoch 5096/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.6515 - val_loss: 94.3745\n",
      "Epoch 5097/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.1848 - val_loss: 93.5706\n",
      "Epoch 5098/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.3137 - val_loss: 94.2193\n",
      "Epoch 5099/10000\n",
      "96/96 [==============================] - 0s 147us/step - loss: 92.5241 - val_loss: 94.0393\n",
      "Epoch 5100/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 92.2741 - val_loss: 93.8848\n",
      "Epoch 5101/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.9743 - val_loss: 93.7936\n",
      "Epoch 5102/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.1659 - val_loss: 93.8113\n",
      "Epoch 5103/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.8463 - val_loss: 93.6294\n",
      "Epoch 5104/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2177 - val_loss: 94.0136\n",
      "Epoch 5105/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.0265 - val_loss: 94.1278\n",
      "Epoch 5106/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.0188 - val_loss: 93.8392\n",
      "Epoch 5107/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6991 - val_loss: 93.8761\n",
      "Epoch 5108/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 93.2536 - val_loss: 93.9652\n",
      "Epoch 5109/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.4904 - val_loss: 94.0576\n",
      "Epoch 5110/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.8494 - val_loss: 93.8530\n",
      "Epoch 5111/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.7287 - val_loss: 93.8533\n",
      "Epoch 5112/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.9674 - val_loss: 94.0741\n",
      "Epoch 5113/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.8816 - val_loss: 93.9508\n",
      "Epoch 5114/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 92.4465 - val_loss: 94.0845\n",
      "Epoch 5115/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.1807 - val_loss: 93.3279\n",
      "Epoch 5116/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.8037 - val_loss: 94.0815\n",
      "Epoch 5117/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2237 - val_loss: 93.6811\n",
      "Epoch 5118/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.3926 - val_loss: 93.9681\n",
      "Epoch 5119/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.4883 - val_loss: 93.9983\n",
      "Epoch 5120/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.1455 - val_loss: 94.1891\n",
      "Epoch 5121/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.7683 - val_loss: 94.2205\n",
      "Epoch 5122/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.2975 - val_loss: 94.0211\n",
      "Epoch 5123/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.2813 - val_loss: 94.0576\n",
      "Epoch 5124/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.2325 - val_loss: 94.2401\n",
      "Epoch 5125/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.8438 - val_loss: 94.2208\n",
      "Epoch 5126/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.9069 - val_loss: 93.7558\n",
      "Epoch 5127/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.4961 - val_loss: 93.8566\n",
      "Epoch 5128/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.7447 - val_loss: 93.5588\n",
      "Epoch 5129/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.4832 - val_loss: 94.1183\n",
      "Epoch 5130/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.3447 - val_loss: 94.2110\n",
      "Epoch 5131/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.2239 - val_loss: 94.0215\n",
      "Epoch 5132/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.5224 - val_loss: 93.9992\n",
      "Epoch 5133/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3587 - val_loss: 93.7779\n",
      "Epoch 5134/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3072 - val_loss: 94.1370\n",
      "Epoch 5135/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.6064 - val_loss: 93.8433\n",
      "Epoch 5136/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1686 - val_loss: 94.2664\n",
      "Epoch 5137/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0260 - val_loss: 94.1361\n",
      "Epoch 5138/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.1462 - val_loss: 94.0476\n",
      "Epoch 5139/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.2012 - val_loss: 94.0199\n",
      "Epoch 5140/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1848 - val_loss: 93.7826\n",
      "Epoch 5141/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.6204 - val_loss: 93.7801\n",
      "Epoch 5142/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.3715 - val_loss: 93.9018\n",
      "Epoch 5143/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8513 - val_loss: 94.2192\n",
      "Epoch 5144/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.5807 - val_loss: 93.9495\n",
      "Epoch 5145/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.0175 - val_loss: 93.8523\n",
      "Epoch 5146/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6899 - val_loss: 94.0117\n",
      "Epoch 5147/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.6845 - val_loss: 93.9667\n",
      "Epoch 5148/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.4430 - val_loss: 93.1404\n",
      "Epoch 5149/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.5357 - val_loss: 93.7510\n",
      "Epoch 5150/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.4195 - val_loss: 94.0352\n",
      "Epoch 5151/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.9312 - val_loss: 93.6016\n",
      "Epoch 5152/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5421 - val_loss: 93.5777\n",
      "Epoch 5153/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7817 - val_loss: 94.3147\n",
      "Epoch 5154/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 91.6160 - val_loss: 94.1706\n",
      "Epoch 5155/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 92.2109 - val_loss: 94.1316\n",
      "Epoch 5156/10000\n",
      "96/96 [==============================] - 0s 168us/step - loss: 92.0832 - val_loss: 93.6641\n",
      "Epoch 5157/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 92.0761 - val_loss: 93.6486\n",
      "Epoch 5158/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 94.1898 - val_loss: 93.8139\n",
      "Epoch 5159/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 91.9946 - val_loss: 93.7204\n",
      "Epoch 5160/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 92.7405 - val_loss: 93.8256\n",
      "Epoch 5161/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 92.2854 - val_loss: 94.0420\n",
      "Epoch 5162/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 93.3201 - val_loss: 93.9562\n",
      "Epoch 5163/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 92.9793 - val_loss: 94.0060\n",
      "Epoch 5164/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 92.0613 - val_loss: 94.3399\n",
      "Epoch 5165/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 91.7516 - val_loss: 94.1840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5166/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 92.2997 - val_loss: 94.1760\n",
      "Epoch 5167/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 92.5840 - val_loss: 93.9846\n",
      "Epoch 5168/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 91.9218 - val_loss: 94.1156\n",
      "Epoch 5169/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 92.0528 - val_loss: 93.6403\n",
      "Epoch 5170/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 92.1427 - val_loss: 93.9706\n",
      "Epoch 5171/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.7863 - val_loss: 94.3476\n",
      "Epoch 5172/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.6461 - val_loss: 93.6812\n",
      "Epoch 5173/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 92.4237 - val_loss: 94.1928\n",
      "Epoch 5174/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 93.9832 - val_loss: 93.9072\n",
      "Epoch 5175/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.9490 - val_loss: 93.8826\n",
      "Epoch 5176/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.1335 - val_loss: 93.8882\n",
      "Epoch 5177/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 91.4527 - val_loss: 93.9462\n",
      "Epoch 5178/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.3239 - val_loss: 94.0551\n",
      "Epoch 5179/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.9539 - val_loss: 93.7779\n",
      "Epoch 5180/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.4388 - val_loss: 93.6734\n",
      "Epoch 5181/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 92.2337 - val_loss: 93.9963\n",
      "Epoch 5182/10000\n",
      "96/96 [==============================] - 0s 146us/step - loss: 92.2022 - val_loss: 93.9856\n",
      "Epoch 5183/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 92.6016 - val_loss: 94.0756\n",
      "Epoch 5184/10000\n",
      "96/96 [==============================] - 0s 150us/step - loss: 92.8304 - val_loss: 94.0075\n",
      "Epoch 5185/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 91.4003 - val_loss: 93.8425\n",
      "Epoch 5186/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 92.2734 - val_loss: 93.9039\n",
      "Epoch 5187/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 92.4209 - val_loss: 93.9185\n",
      "Epoch 5188/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 93.6498 - val_loss: 94.1611\n",
      "Epoch 5189/10000\n",
      "96/96 [==============================] - 0s 148us/step - loss: 92.0891 - val_loss: 93.5191\n",
      "Epoch 5190/10000\n",
      "96/96 [==============================] - 0s 149us/step - loss: 92.8278 - val_loss: 94.2773\n",
      "Epoch 5191/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 92.5876 - val_loss: 93.4655\n",
      "Epoch 5192/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 94.5289 - val_loss: 94.1887\n",
      "Epoch 5193/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 92.4704 - val_loss: 94.8085\n",
      "Epoch 5194/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 93.2431 - val_loss: 93.6639\n",
      "Epoch 5195/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 91.9749 - val_loss: 94.3369\n",
      "Epoch 5196/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 91.9185 - val_loss: 93.3192\n",
      "Epoch 5197/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 93.4773 - val_loss: 94.1577\n",
      "Epoch 5198/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 94.4354 - val_loss: 94.3730\n",
      "Epoch 5199/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.6279 - val_loss: 94.1334\n",
      "Epoch 5200/10000\n",
      "96/96 [==============================] - 0s 149us/step - loss: 92.2485 - val_loss: 94.1483\n",
      "Epoch 5201/10000\n",
      "96/96 [==============================] - 0s 175us/step - loss: 92.5890 - val_loss: 94.2154\n",
      "Epoch 5202/10000\n",
      "96/96 [==============================] - 0s 147us/step - loss: 92.1342 - val_loss: 93.5017\n",
      "Epoch 5203/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.3024 - val_loss: 94.6835\n",
      "Epoch 5204/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.8027 - val_loss: 94.0866\n",
      "Epoch 5205/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 94.3094 - val_loss: 93.9099\n",
      "Epoch 5206/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 92.2895 - val_loss: 94.1663\n",
      "Epoch 5207/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.5053 - val_loss: 94.4289\n",
      "Epoch 5208/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.3015 - val_loss: 93.8930\n",
      "Epoch 5209/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 91.9687 - val_loss: 93.8875\n",
      "Epoch 5210/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 91.9775 - val_loss: 94.2631\n",
      "Epoch 5211/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.8864 - val_loss: 94.1113\n",
      "Epoch 5212/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 93.6009 - val_loss: 93.8668\n",
      "Epoch 5213/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 92.7268 - val_loss: 94.2731\n",
      "Epoch 5214/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.0058 - val_loss: 94.0970\n",
      "Epoch 5215/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 92.6459 - val_loss: 93.9163\n",
      "Epoch 5216/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 93.2921 - val_loss: 94.2357\n",
      "Epoch 5217/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.8674 - val_loss: 93.8826\n",
      "Epoch 5218/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 92.1987 - val_loss: 93.7466\n",
      "Epoch 5219/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.6855 - val_loss: 93.7698\n",
      "Epoch 5220/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.8082 - val_loss: 94.8190\n",
      "Epoch 5221/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.9350 - val_loss: 94.3865\n",
      "Epoch 5222/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 92.4341 - val_loss: 94.3457\n",
      "Epoch 5223/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.6777 - val_loss: 93.8398\n",
      "Epoch 5224/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 93.0512 - val_loss: 93.6543\n",
      "Epoch 5225/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.6242 - val_loss: 93.8397\n",
      "Epoch 5226/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 92.9469 - val_loss: 93.8381\n",
      "Epoch 5227/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 93.6448 - val_loss: 93.8503\n",
      "Epoch 5228/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.7338 - val_loss: 94.1291\n",
      "Epoch 5229/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.7516 - val_loss: 94.2450\n",
      "Epoch 5230/10000\n",
      "96/96 [==============================] - 0s 226us/step - loss: 91.5240 - val_loss: 93.9626\n",
      "Epoch 5231/10000\n",
      "96/96 [==============================] - 0s 282us/step - loss: 91.6295 - val_loss: 94.3341\n",
      "Epoch 5232/10000\n",
      "96/96 [==============================] - 0s 257us/step - loss: 92.0343 - val_loss: 94.0844\n",
      "Epoch 5233/10000\n",
      "96/96 [==============================] - 0s 169us/step - loss: 91.3709 - val_loss: 93.8410\n",
      "Epoch 5234/10000\n",
      "96/96 [==============================] - 0s 237us/step - loss: 94.6865 - val_loss: 93.7850\n",
      "Epoch 5235/10000\n",
      "96/96 [==============================] - 0s 244us/step - loss: 92.5722 - val_loss: 93.8773\n",
      "Epoch 5236/10000\n",
      "96/96 [==============================] - 0s 218us/step - loss: 93.1937 - val_loss: 93.7676\n",
      "Epoch 5237/10000\n",
      "96/96 [==============================] - 0s 147us/step - loss: 92.5918 - val_loss: 94.1158\n",
      "Epoch 5238/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 92.6303 - val_loss: 93.7542\n",
      "Epoch 5239/10000\n",
      "96/96 [==============================] - 0s 147us/step - loss: 92.7463 - val_loss: 93.8110\n",
      "Epoch 5240/10000\n",
      "96/96 [==============================] - 0s 157us/step - loss: 91.1192 - val_loss: 94.1368\n",
      "Epoch 5241/10000\n",
      "96/96 [==============================] - 0s 151us/step - loss: 91.6500 - val_loss: 94.3371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5242/10000\n",
      "96/96 [==============================] - 0s 147us/step - loss: 93.4052 - val_loss: 94.1064\n",
      "Epoch 5243/10000\n",
      "96/96 [==============================] - 0s 154us/step - loss: 93.0659 - val_loss: 93.9921\n",
      "Epoch 5244/10000\n",
      "96/96 [==============================] - 0s 154us/step - loss: 92.5800 - val_loss: 93.5120\n",
      "Epoch 5245/10000\n",
      "96/96 [==============================] - 0s 154us/step - loss: 92.2305 - val_loss: 94.2034\n",
      "Epoch 5246/10000\n",
      "96/96 [==============================] - 0s 152us/step - loss: 94.0423 - val_loss: 94.6070\n",
      "Epoch 5247/10000\n",
      "96/96 [==============================] - 0s 153us/step - loss: 93.7645 - val_loss: 94.0298\n",
      "Epoch 5248/10000\n",
      "96/96 [==============================] - 0s 151us/step - loss: 92.5219 - val_loss: 94.2467\n",
      "Epoch 5249/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 92.7441 - val_loss: 93.7355\n",
      "Epoch 5250/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 94.5105 - val_loss: 93.4570\n",
      "Epoch 5251/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 92.3711 - val_loss: 93.7814\n",
      "Epoch 5252/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 91.9122 - val_loss: 94.1124\n",
      "Epoch 5253/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 92.4963 - val_loss: 94.6006\n",
      "Epoch 5254/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 93.7890 - val_loss: 93.9000\n",
      "Epoch 5255/10000\n",
      "96/96 [==============================] - 0s 148us/step - loss: 92.2328 - val_loss: 94.0833\n",
      "Epoch 5256/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 91.5688 - val_loss: 94.2855\n",
      "Epoch 5257/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 92.2831 - val_loss: 94.1562\n",
      "Epoch 5258/10000\n",
      "96/96 [==============================] - 0s 154us/step - loss: 91.5354 - val_loss: 93.9963\n",
      "Epoch 5259/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 91.7684 - val_loss: 93.8294\n",
      "Epoch 5260/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 92.6974 - val_loss: 94.0150\n",
      "Epoch 5261/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 91.3905 - val_loss: 94.0753\n",
      "Epoch 5262/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 92.0808 - val_loss: 93.5971\n",
      "Epoch 5263/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 91.8466 - val_loss: 93.6372\n",
      "Epoch 5264/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 92.6932 - val_loss: 94.0815\n",
      "Epoch 5265/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 92.5371 - val_loss: 93.8912\n",
      "Epoch 5266/10000\n",
      "96/96 [==============================] - 0s 148us/step - loss: 91.1072 - val_loss: 94.1856\n",
      "Epoch 5267/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 94.3783 - val_loss: 93.8542\n",
      "Epoch 5268/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 92.1520 - val_loss: 93.6422\n",
      "Epoch 5269/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 92.1101 - val_loss: 93.5616\n",
      "Epoch 5270/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 92.2697 - val_loss: 95.1169\n",
      "Epoch 5271/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 91.6935 - val_loss: 93.5924\n",
      "Epoch 5272/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 92.3440 - val_loss: 93.5358\n",
      "Epoch 5273/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 92.8347 - val_loss: 93.5939\n",
      "Epoch 5274/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 93.2091 - val_loss: 93.3483\n",
      "Epoch 5275/10000\n",
      "96/96 [==============================] - 0s 160us/step - loss: 91.1163 - val_loss: 94.4459\n",
      "Epoch 5276/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.8132 - val_loss: 94.1635\n",
      "Epoch 5277/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.0821 - val_loss: 93.7793\n",
      "Epoch 5278/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.2306 - val_loss: 93.9715\n",
      "Epoch 5279/10000\n",
      "96/96 [==============================] - 0s 147us/step - loss: 91.5275 - val_loss: 94.1059\n",
      "Epoch 5280/10000\n",
      "96/96 [==============================] - 0s 146us/step - loss: 91.3406 - val_loss: 94.1673\n",
      "Epoch 5281/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 92.9224 - val_loss: 93.9376\n",
      "Epoch 5282/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 92.6259 - val_loss: 93.7521\n",
      "Epoch 5283/10000\n",
      "96/96 [==============================] - 0s 158us/step - loss: 91.5526 - val_loss: 93.9384\n",
      "Epoch 5284/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 93.4622 - val_loss: 94.0242\n",
      "Epoch 5285/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 92.3771 - val_loss: 94.0502\n",
      "Epoch 5286/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 91.5497 - val_loss: 94.0581\n",
      "Epoch 5287/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.4736 - val_loss: 93.7623\n",
      "Epoch 5288/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 94.9521 - val_loss: 93.6142\n",
      "Epoch 5289/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 93.8195 - val_loss: 94.3212\n",
      "Epoch 5290/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.2343 - val_loss: 93.8836\n",
      "Epoch 5291/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 92.1082 - val_loss: 94.0931\n",
      "Epoch 5292/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.5804 - val_loss: 93.9814\n",
      "Epoch 5293/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.9851 - val_loss: 93.7174\n",
      "Epoch 5294/10000\n",
      "96/96 [==============================] - 0s 157us/step - loss: 92.7916 - val_loss: 94.1705\n",
      "Epoch 5295/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.3654 - val_loss: 94.0609\n",
      "Epoch 5296/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.2586 - val_loss: 93.5784\n",
      "Epoch 5297/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2789 - val_loss: 93.8025\n",
      "Epoch 5298/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.6152 - val_loss: 94.1482\n",
      "Epoch 5299/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.5610 - val_loss: 94.5515\n",
      "Epoch 5300/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.4402 - val_loss: 94.0134\n",
      "Epoch 5301/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5574 - val_loss: 94.0926\n",
      "Epoch 5302/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.8699 - val_loss: 93.7781\n",
      "Epoch 5303/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.5099 - val_loss: 94.9194\n",
      "Epoch 5304/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.1592 - val_loss: 94.0156\n",
      "Epoch 5305/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.3254 - val_loss: 93.7597\n",
      "Epoch 5306/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 91.8448 - val_loss: 94.1355\n",
      "Epoch 5307/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.4295 - val_loss: 93.9520\n",
      "Epoch 5308/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.6935 - val_loss: 93.8929\n",
      "Epoch 5309/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.4402 - val_loss: 93.7246\n",
      "Epoch 5310/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2468 - val_loss: 93.9787\n",
      "Epoch 5311/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.0482 - val_loss: 93.8125\n",
      "Epoch 5312/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.8454 - val_loss: 94.0419\n",
      "Epoch 5313/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.6068 - val_loss: 94.0657\n",
      "Epoch 5314/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.8141 - val_loss: 94.1085\n",
      "Epoch 5315/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.1211 - val_loss: 93.7607\n",
      "Epoch 5316/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6464 - val_loss: 93.7974\n",
      "Epoch 5317/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2247 - val_loss: 93.3896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5318/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.8846 - val_loss: 93.7403\n",
      "Epoch 5319/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.8255 - val_loss: 93.3640\n",
      "Epoch 5320/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.1917 - val_loss: 93.2713\n",
      "Epoch 5321/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6554 - val_loss: 93.7977\n",
      "Epoch 5322/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.0029 - val_loss: 94.2043\n",
      "Epoch 5323/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.3891 - val_loss: 94.0512\n",
      "Epoch 5324/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.2272 - val_loss: 93.9555\n",
      "Epoch 5325/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.6024 - val_loss: 94.1865\n",
      "Epoch 5326/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.7955 - val_loss: 94.2718\n",
      "Epoch 5327/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.3198 - val_loss: 93.8204\n",
      "Epoch 5328/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8473 - val_loss: 93.8787\n",
      "Epoch 5329/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.1504 - val_loss: 94.0650\n",
      "Epoch 5330/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.3763 - val_loss: 93.5457\n",
      "Epoch 5331/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.6317 - val_loss: 94.3668\n",
      "Epoch 5332/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7091 - val_loss: 94.0422\n",
      "Epoch 5333/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.6791 - val_loss: 93.4909\n",
      "Epoch 5334/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2705 - val_loss: 94.1462\n",
      "Epoch 5335/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.7730 - val_loss: 93.5189\n",
      "Epoch 5336/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.8662 - val_loss: 93.6902\n",
      "Epoch 5337/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3054 - val_loss: 94.0165\n",
      "Epoch 5338/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.3607 - val_loss: 93.8524\n",
      "Epoch 5339/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6786 - val_loss: 93.5534\n",
      "Epoch 5340/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.4689 - val_loss: 94.3982\n",
      "Epoch 5341/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.6282 - val_loss: 94.2549\n",
      "Epoch 5342/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.4938 - val_loss: 93.4513\n",
      "Epoch 5343/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.7208 - val_loss: 94.0097\n",
      "Epoch 5344/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9976 - val_loss: 93.9013\n",
      "Epoch 5345/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6498 - val_loss: 94.0875\n",
      "Epoch 5346/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.5194 - val_loss: 93.8403\n",
      "Epoch 5347/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.6809 - val_loss: 93.5995\n",
      "Epoch 5348/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3304 - val_loss: 94.2873\n",
      "Epoch 5349/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.0273 - val_loss: 94.0040\n",
      "Epoch 5350/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.9500 - val_loss: 94.5577\n",
      "Epoch 5351/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.8922 - val_loss: 93.7370\n",
      "Epoch 5352/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1918 - val_loss: 94.0340\n",
      "Epoch 5353/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.4177 - val_loss: 93.9308\n",
      "Epoch 5354/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.0084 - val_loss: 93.7148\n",
      "Epoch 5355/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.6526 - val_loss: 93.8733\n",
      "Epoch 5356/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.0761 - val_loss: 93.8345\n",
      "Epoch 5357/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3877 - val_loss: 93.8702\n",
      "Epoch 5358/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4376 - val_loss: 94.2072\n",
      "Epoch 5359/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.7576 - val_loss: 93.9787\n",
      "Epoch 5360/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0751 - val_loss: 93.9104\n",
      "Epoch 5361/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0836 - val_loss: 93.9955\n",
      "Epoch 5362/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6964 - val_loss: 94.6290\n",
      "Epoch 5363/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.5672 - val_loss: 93.7008\n",
      "Epoch 5364/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5458 - val_loss: 93.9818\n",
      "Epoch 5365/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.4487 - val_loss: 93.7912\n",
      "Epoch 5366/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.8680 - val_loss: 93.9544\n",
      "Epoch 5367/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2475 - val_loss: 93.8865\n",
      "Epoch 5368/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.6126 - val_loss: 93.9067\n",
      "Epoch 5369/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.6681 - val_loss: 93.8273\n",
      "Epoch 5370/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2067 - val_loss: 93.9855\n",
      "Epoch 5371/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.6025 - val_loss: 93.8540\n",
      "Epoch 5372/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.3539 - val_loss: 93.9874\n",
      "Epoch 5373/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6634 - val_loss: 93.7804\n",
      "Epoch 5374/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2667 - val_loss: 93.3991\n",
      "Epoch 5375/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.7709 - val_loss: 93.6353\n",
      "Epoch 5376/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2744 - val_loss: 94.2789\n",
      "Epoch 5377/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.7515 - val_loss: 93.8099\n",
      "Epoch 5378/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.4678 - val_loss: 94.0368\n",
      "Epoch 5379/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6381 - val_loss: 93.9168\n",
      "Epoch 5380/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.5785 - val_loss: 94.1782\n",
      "Epoch 5381/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 90.7807 - val_loss: 93.8918\n",
      "Epoch 5382/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.7461 - val_loss: 94.1764\n",
      "Epoch 5383/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.4179 - val_loss: 93.8114\n",
      "Epoch 5384/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.3277 - val_loss: 93.6963\n",
      "Epoch 5385/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2110 - val_loss: 94.1353\n",
      "Epoch 5386/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3617 - val_loss: 93.6436\n",
      "Epoch 5387/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.8758 - val_loss: 93.3195\n",
      "Epoch 5388/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.9719 - val_loss: 93.8421\n",
      "Epoch 5389/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.6920 - val_loss: 93.5399\n",
      "Epoch 5390/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.9795 - val_loss: 93.4992\n",
      "Epoch 5391/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.0982 - val_loss: 93.9355\n",
      "Epoch 5392/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.1517 - val_loss: 93.7980\n",
      "Epoch 5393/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.4594 - val_loss: 93.2402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5394/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.7278 - val_loss: 93.9838\n",
      "Epoch 5395/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1994 - val_loss: 93.5471\n",
      "Epoch 5396/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.2865 - val_loss: 93.6079\n",
      "Epoch 5397/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.1507 - val_loss: 93.7589\n",
      "Epoch 5398/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8544 - val_loss: 93.5601\n",
      "Epoch 5399/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.8713 - val_loss: 93.6655\n",
      "Epoch 5400/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.5536 - val_loss: 94.0714\n",
      "Epoch 5401/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2180 - val_loss: 94.5787\n",
      "Epoch 5402/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.7022 - val_loss: 94.0966\n",
      "Epoch 5403/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.4950 - val_loss: 94.1198\n",
      "Epoch 5404/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7586 - val_loss: 93.9986\n",
      "Epoch 5405/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.0470 - val_loss: 93.6466\n",
      "Epoch 5406/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4821 - val_loss: 94.6018\n",
      "Epoch 5407/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.2124 - val_loss: 94.2788\n",
      "Epoch 5408/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.9670 - val_loss: 93.9330\n",
      "Epoch 5409/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7076 - val_loss: 94.4025\n",
      "Epoch 5410/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.9253 - val_loss: 93.8315\n",
      "Epoch 5411/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.5981 - val_loss: 93.9324\n",
      "Epoch 5412/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3369 - val_loss: 94.0972\n",
      "Epoch 5413/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.4700 - val_loss: 93.4550\n",
      "Epoch 5414/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.6068 - val_loss: 94.1436\n",
      "Epoch 5415/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8982 - val_loss: 94.1262\n",
      "Epoch 5416/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.7908 - val_loss: 93.3426\n",
      "Epoch 5417/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.8607 - val_loss: 93.5049\n",
      "Epoch 5418/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.0262 - val_loss: 93.9679\n",
      "Epoch 5419/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.1995 - val_loss: 93.8591\n",
      "Epoch 5420/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 93.4828 - val_loss: 93.8231\n",
      "Epoch 5421/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.7818 - val_loss: 93.8492\n",
      "Epoch 5422/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.9806 - val_loss: 93.9329\n",
      "Epoch 5423/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.9169 - val_loss: 93.4397\n",
      "Epoch 5424/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.3067 - val_loss: 93.6787\n",
      "Epoch 5425/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.6286 - val_loss: 93.8822\n",
      "Epoch 5426/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.0875 - val_loss: 93.9520\n",
      "Epoch 5427/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0882 - val_loss: 93.4949\n",
      "Epoch 5428/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.9655 - val_loss: 93.6814\n",
      "Epoch 5429/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5946 - val_loss: 93.6735\n",
      "Epoch 5430/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.7683 - val_loss: 93.8953\n",
      "Epoch 5431/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.0623 - val_loss: 93.9124\n",
      "Epoch 5432/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3140 - val_loss: 94.0656\n",
      "Epoch 5433/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.4615 - val_loss: 93.9222\n",
      "Epoch 5434/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.2433 - val_loss: 93.4673\n",
      "Epoch 5435/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.6200 - val_loss: 94.1081\n",
      "Epoch 5436/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.5532 - val_loss: 94.1143\n",
      "Epoch 5437/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.0068 - val_loss: 93.4858\n",
      "Epoch 5438/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.0619 - val_loss: 93.4490\n",
      "Epoch 5439/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6317 - val_loss: 94.1849\n",
      "Epoch 5440/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.6606 - val_loss: 93.5705\n",
      "Epoch 5441/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.7265 - val_loss: 93.8892\n",
      "Epoch 5442/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.9256 - val_loss: 93.3944\n",
      "Epoch 5443/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1544 - val_loss: 93.8227\n",
      "Epoch 5444/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.9023 - val_loss: 93.3610\n",
      "Epoch 5445/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.9095 - val_loss: 93.6033\n",
      "Epoch 5446/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.9725 - val_loss: 94.1401\n",
      "Epoch 5447/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.0734 - val_loss: 94.0259\n",
      "Epoch 5448/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.6490 - val_loss: 94.2262\n",
      "Epoch 5449/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.8127 - val_loss: 94.0316\n",
      "Epoch 5450/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 94.5852 - val_loss: 93.9844\n",
      "Epoch 5451/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.2163 - val_loss: 93.7712\n",
      "Epoch 5452/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.8506 - val_loss: 94.1091\n",
      "Epoch 5453/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.8475 - val_loss: 93.6393\n",
      "Epoch 5454/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.0032 - val_loss: 93.5006\n",
      "Epoch 5455/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.0954 - val_loss: 93.9776\n",
      "Epoch 5456/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7804 - val_loss: 94.2305\n",
      "Epoch 5457/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2997 - val_loss: 93.9777\n",
      "Epoch 5458/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7894 - val_loss: 94.0548\n",
      "Epoch 5459/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.4829 - val_loss: 93.4696\n",
      "Epoch 5460/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6543 - val_loss: 93.7519\n",
      "Epoch 5461/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.1986 - val_loss: 93.9355\n",
      "Epoch 5462/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.3208 - val_loss: 93.4444\n",
      "Epoch 5463/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.5297 - val_loss: 94.1691\n",
      "Epoch 5464/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.2397 - val_loss: 93.7091\n",
      "Epoch 5465/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1485 - val_loss: 94.0138\n",
      "Epoch 5466/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.0015 - val_loss: 94.0814\n",
      "Epoch 5467/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6199 - val_loss: 93.8032\n",
      "Epoch 5468/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.1700 - val_loss: 93.8361\n",
      "Epoch 5469/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5976 - val_loss: 93.8094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5470/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.9175 - val_loss: 93.6163\n",
      "Epoch 5471/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.0743 - val_loss: 93.4501\n",
      "Epoch 5472/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6463 - val_loss: 93.9753\n",
      "Epoch 5473/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2710 - val_loss: 93.7789\n",
      "Epoch 5474/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9455 - val_loss: 93.5843\n",
      "Epoch 5475/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4828 - val_loss: 94.2664\n",
      "Epoch 5476/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.4688 - val_loss: 94.0996\n",
      "Epoch 5477/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.3411 - val_loss: 93.8725\n",
      "Epoch 5478/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.3144 - val_loss: 94.1265\n",
      "Epoch 5479/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.6688 - val_loss: 93.8313\n",
      "Epoch 5480/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.8591 - val_loss: 93.6845\n",
      "Epoch 5481/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.9569 - val_loss: 93.8252\n",
      "Epoch 5482/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6952 - val_loss: 94.0834\n",
      "Epoch 5483/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7672 - val_loss: 93.8278\n",
      "Epoch 5484/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.2228 - val_loss: 94.5931\n",
      "Epoch 5485/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 94.6053 - val_loss: 93.3554\n",
      "Epoch 5486/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.6734 - val_loss: 93.8787\n",
      "Epoch 5487/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4382 - val_loss: 94.0329\n",
      "Epoch 5488/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6620 - val_loss: 93.8600\n",
      "Epoch 5489/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.1128 - val_loss: 93.6572\n",
      "Epoch 5490/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1612 - val_loss: 93.4545\n",
      "Epoch 5491/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6729 - val_loss: 93.8757\n",
      "Epoch 5492/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5069 - val_loss: 93.8180\n",
      "Epoch 5493/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.7894 - val_loss: 94.1842\n",
      "Epoch 5494/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2588 - val_loss: 93.8183\n",
      "Epoch 5495/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.3259 - val_loss: 93.7496\n",
      "Epoch 5496/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.1354 - val_loss: 93.9761\n",
      "Epoch 5497/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.1577 - val_loss: 93.9097\n",
      "Epoch 5498/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.4284 - val_loss: 93.9017\n",
      "Epoch 5499/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6952 - val_loss: 93.7438\n",
      "Epoch 5500/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.9847 - val_loss: 94.0630\n",
      "Epoch 5501/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4278 - val_loss: 93.7640\n",
      "Epoch 5502/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.1369 - val_loss: 93.8508\n",
      "Epoch 5503/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9422 - val_loss: 93.7561\n",
      "Epoch 5504/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.1020 - val_loss: 93.9888\n",
      "Epoch 5505/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.7552 - val_loss: 93.5639\n",
      "Epoch 5506/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.3703 - val_loss: 93.7973\n",
      "Epoch 5507/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.7013 - val_loss: 93.6910\n",
      "Epoch 5508/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3799 - val_loss: 93.7496\n",
      "Epoch 5509/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4683 - val_loss: 93.9474\n",
      "Epoch 5510/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.1719 - val_loss: 93.6576\n",
      "Epoch 5511/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3135 - val_loss: 93.7481\n",
      "Epoch 5512/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2511 - val_loss: 93.7545\n",
      "Epoch 5513/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.4291 - val_loss: 93.9074\n",
      "Epoch 5514/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.9055 - val_loss: 93.7055\n",
      "Epoch 5515/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.6762 - val_loss: 93.8386\n",
      "Epoch 5516/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7882 - val_loss: 93.8947\n",
      "Epoch 5517/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1227 - val_loss: 94.0492\n",
      "Epoch 5518/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.0216 - val_loss: 93.9380\n",
      "Epoch 5519/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3869 - val_loss: 94.2328\n",
      "Epoch 5520/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.7951 - val_loss: 93.8445\n",
      "Epoch 5521/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.9737 - val_loss: 93.5245\n",
      "Epoch 5522/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6384 - val_loss: 94.0530\n",
      "Epoch 5523/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.7134 - val_loss: 93.7761\n",
      "Epoch 5524/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.7306 - val_loss: 94.0534\n",
      "Epoch 5525/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6618 - val_loss: 93.4612\n",
      "Epoch 5526/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.3860 - val_loss: 93.8665\n",
      "Epoch 5527/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.2314 - val_loss: 93.9707\n",
      "Epoch 5528/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.7523 - val_loss: 93.8554\n",
      "Epoch 5529/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.0398 - val_loss: 93.8988\n",
      "Epoch 5530/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.7408 - val_loss: 94.2647\n",
      "Epoch 5531/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.1513 - val_loss: 94.5582\n",
      "Epoch 5532/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.9279 - val_loss: 94.0652\n",
      "Epoch 5533/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.9511 - val_loss: 94.0478\n",
      "Epoch 5534/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8984 - val_loss: 94.0154\n",
      "Epoch 5535/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4178 - val_loss: 93.8185\n",
      "Epoch 5536/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.1046 - val_loss: 93.8702\n",
      "Epoch 5537/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.7352 - val_loss: 94.0204\n",
      "Epoch 5538/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.0536 - val_loss: 93.9700\n",
      "Epoch 5539/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.0196 - val_loss: 94.0640\n",
      "Epoch 5540/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2656 - val_loss: 93.9190\n",
      "Epoch 5541/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.2091 - val_loss: 93.9731\n",
      "Epoch 5542/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1566 - val_loss: 93.7603\n",
      "Epoch 5543/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.9473 - val_loss: 93.7727\n",
      "Epoch 5544/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.1793 - val_loss: 93.9642\n",
      "Epoch 5545/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9999 - val_loss: 93.8585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5546/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.3271 - val_loss: 93.5795\n",
      "Epoch 5547/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5682 - val_loss: 93.2453\n",
      "Epoch 5548/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1332 - val_loss: 94.2110\n",
      "Epoch 5549/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.8864 - val_loss: 94.2047\n",
      "Epoch 5550/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7090 - val_loss: 94.0082\n",
      "Epoch 5551/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.8374 - val_loss: 94.3188\n",
      "Epoch 5552/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2061 - val_loss: 93.9886\n",
      "Epoch 5553/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.9726 - val_loss: 93.7486\n",
      "Epoch 5554/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.1262 - val_loss: 94.0531\n",
      "Epoch 5555/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8865 - val_loss: 94.0911\n",
      "Epoch 5556/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.8064 - val_loss: 93.8528\n",
      "Epoch 5557/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.1348 - val_loss: 94.1902\n",
      "Epoch 5558/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0795 - val_loss: 93.6320\n",
      "Epoch 5559/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.7714 - val_loss: 93.9513\n",
      "Epoch 5560/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.5875 - val_loss: 93.8752\n",
      "Epoch 5561/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8508 - val_loss: 93.9920\n",
      "Epoch 5562/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6309 - val_loss: 93.9421\n",
      "Epoch 5563/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.0270 - val_loss: 94.0226\n",
      "Epoch 5564/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.5116 - val_loss: 93.4753\n",
      "Epoch 5565/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.7691 - val_loss: 94.0509\n",
      "Epoch 5566/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.0726 - val_loss: 93.6366\n",
      "Epoch 5567/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.9845 - val_loss: 94.0152\n",
      "Epoch 5568/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.5375 - val_loss: 93.6845\n",
      "Epoch 5569/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.1543 - val_loss: 93.1279\n",
      "Epoch 5570/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.6316 - val_loss: 93.8306\n",
      "Epoch 5571/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.9149 - val_loss: 93.9984\n",
      "Epoch 5572/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2436 - val_loss: 93.8421\n",
      "Epoch 5573/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.1129 - val_loss: 93.8463\n",
      "Epoch 5574/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.2705 - val_loss: 93.7442\n",
      "Epoch 5575/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.9209 - val_loss: 94.1167\n",
      "Epoch 5576/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5390 - val_loss: 94.1379\n",
      "Epoch 5577/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2548 - val_loss: 94.1446\n",
      "Epoch 5578/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.6849 - val_loss: 93.7314\n",
      "Epoch 5579/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.3187 - val_loss: 93.8285\n",
      "Epoch 5580/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 94.6202 - val_loss: 94.9293\n",
      "Epoch 5581/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.3807 - val_loss: 93.8317\n",
      "Epoch 5582/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.0477 - val_loss: 94.4323\n",
      "Epoch 5583/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3693 - val_loss: 93.7379\n",
      "Epoch 5584/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.1622 - val_loss: 94.2007\n",
      "Epoch 5585/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5531 - val_loss: 93.7166\n",
      "Epoch 5586/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.5214 - val_loss: 94.3166\n",
      "Epoch 5587/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.1296 - val_loss: 93.7522\n",
      "Epoch 5588/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.2501 - val_loss: 93.6118\n",
      "Epoch 5589/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3320 - val_loss: 93.6720\n",
      "Epoch 5590/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.4851 - val_loss: 94.0678\n",
      "Epoch 5591/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6433 - val_loss: 93.5095\n",
      "Epoch 5592/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3149 - val_loss: 93.7997\n",
      "Epoch 5593/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1889 - val_loss: 93.8256\n",
      "Epoch 5594/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5385 - val_loss: 93.7100\n",
      "Epoch 5595/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.7652 - val_loss: 93.4430\n",
      "Epoch 5596/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.6427 - val_loss: 94.6595\n",
      "Epoch 5597/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.3438 - val_loss: 93.9703\n",
      "Epoch 5598/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.2165 - val_loss: 93.7181\n",
      "Epoch 5599/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.7476 - val_loss: 93.6298\n",
      "Epoch 5600/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.9566 - val_loss: 94.0569\n",
      "Epoch 5601/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.9361 - val_loss: 93.8449\n",
      "Epoch 5602/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.4795 - val_loss: 93.9549\n",
      "Epoch 5603/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6645 - val_loss: 93.7476\n",
      "Epoch 5604/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.9322 - val_loss: 94.7411\n",
      "Epoch 5605/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8647 - val_loss: 94.1475\n",
      "Epoch 5606/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.0377 - val_loss: 94.0325\n",
      "Epoch 5607/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8345 - val_loss: 93.5560\n",
      "Epoch 5608/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4551 - val_loss: 93.8008\n",
      "Epoch 5609/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.5379 - val_loss: 93.6489\n",
      "Epoch 5610/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6291 - val_loss: 93.8425\n",
      "Epoch 5611/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.1406 - val_loss: 94.0277\n",
      "Epoch 5612/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.3550 - val_loss: 93.8270\n",
      "Epoch 5613/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4575 - val_loss: 94.0920\n",
      "Epoch 5614/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.6690 - val_loss: 93.6367\n",
      "Epoch 5615/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.0679 - val_loss: 93.7023\n",
      "Epoch 5616/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.6652 - val_loss: 93.7630\n",
      "Epoch 5617/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.4408 - val_loss: 93.3619\n",
      "Epoch 5618/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.4300 - val_loss: 93.7344\n",
      "Epoch 5619/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.5534 - val_loss: 93.9913\n",
      "Epoch 5620/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.9960 - val_loss: 93.4058\n",
      "Epoch 5621/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8467 - val_loss: 93.6098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5622/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.2777 - val_loss: 94.0658\n",
      "Epoch 5623/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9756 - val_loss: 93.6221\n",
      "Epoch 5624/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.2228 - val_loss: 94.1124\n",
      "Epoch 5625/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.8610 - val_loss: 94.0316\n",
      "Epoch 5626/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.2887 - val_loss: 93.2811\n",
      "Epoch 5627/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.2945 - val_loss: 93.9354\n",
      "Epoch 5628/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.8158 - val_loss: 94.0298\n",
      "Epoch 5629/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9953 - val_loss: 93.4170\n",
      "Epoch 5630/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.2492 - val_loss: 93.3452\n",
      "Epoch 5631/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5587 - val_loss: 94.6093\n",
      "Epoch 5632/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.0284 - val_loss: 93.7649\n",
      "Epoch 5633/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.2617 - val_loss: 93.9056\n",
      "Epoch 5634/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.5473 - val_loss: 93.4725\n",
      "Epoch 5635/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.9619 - val_loss: 93.9024\n",
      "Epoch 5636/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4663 - val_loss: 94.0439\n",
      "Epoch 5637/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.7623 - val_loss: 93.9702\n",
      "Epoch 5638/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.9726 - val_loss: 93.5985\n",
      "Epoch 5639/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.2783 - val_loss: 93.3899\n",
      "Epoch 5640/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7018 - val_loss: 93.9076\n",
      "Epoch 5641/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.9428 - val_loss: 93.8416\n",
      "Epoch 5642/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7776 - val_loss: 93.9043\n",
      "Epoch 5643/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.2981 - val_loss: 93.6267\n",
      "Epoch 5644/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.5261 - val_loss: 94.1925\n",
      "Epoch 5645/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.6368 - val_loss: 93.2569\n",
      "Epoch 5646/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1842 - val_loss: 93.5797\n",
      "Epoch 5647/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2456 - val_loss: 94.5842\n",
      "Epoch 5648/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2978 - val_loss: 93.4698\n",
      "Epoch 5649/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6009 - val_loss: 94.0945\n",
      "Epoch 5650/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.0614 - val_loss: 93.7794\n",
      "Epoch 5651/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.9348 - val_loss: 93.6450\n",
      "Epoch 5652/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6940 - val_loss: 94.3683\n",
      "Epoch 5653/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0399 - val_loss: 93.9151\n",
      "Epoch 5654/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.5618 - val_loss: 93.9351\n",
      "Epoch 5655/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1325 - val_loss: 93.5991\n",
      "Epoch 5656/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.6790 - val_loss: 93.8697\n",
      "Epoch 5657/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6674 - val_loss: 93.0610\n",
      "Epoch 5658/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.0419 - val_loss: 94.1107\n",
      "Epoch 5659/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.1928 - val_loss: 94.0269\n",
      "Epoch 5660/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6229 - val_loss: 93.4175\n",
      "Epoch 5661/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.9363 - val_loss: 93.8764\n",
      "Epoch 5662/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.6233 - val_loss: 93.7802\n",
      "Epoch 5663/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4806 - val_loss: 93.8119\n",
      "Epoch 5664/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0933 - val_loss: 93.8157\n",
      "Epoch 5665/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.1981 - val_loss: 93.7582\n",
      "Epoch 5666/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.4110 - val_loss: 94.3336\n",
      "Epoch 5667/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6160 - val_loss: 93.6837\n",
      "Epoch 5668/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.9382 - val_loss: 94.1863\n",
      "Epoch 5669/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.1075 - val_loss: 93.5296\n",
      "Epoch 5670/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.3647 - val_loss: 93.5837\n",
      "Epoch 5671/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6163 - val_loss: 93.8889\n",
      "Epoch 5672/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 93.9398 - val_loss: 93.8906\n",
      "Epoch 5673/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.4722 - val_loss: 93.5215\n",
      "Epoch 5674/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.7387 - val_loss: 93.7661\n",
      "Epoch 5675/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.5542 - val_loss: 93.9904\n",
      "Epoch 5676/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8166 - val_loss: 93.1697\n",
      "Epoch 5677/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.3668 - val_loss: 93.4410\n",
      "Epoch 5678/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.7908 - val_loss: 93.2758\n",
      "Epoch 5679/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9735 - val_loss: 93.7642\n",
      "Epoch 5680/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.9099 - val_loss: 93.7257\n",
      "Epoch 5681/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 92.0736 - val_loss: 93.8987\n",
      "Epoch 5682/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9139 - val_loss: 93.7250\n",
      "Epoch 5683/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.9450 - val_loss: 93.1605\n",
      "Epoch 5684/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.4836 - val_loss: 93.9492\n",
      "Epoch 5685/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5033 - val_loss: 93.8124\n",
      "Epoch 5686/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.4149 - val_loss: 93.8918\n",
      "Epoch 5687/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.7352 - val_loss: 93.4806\n",
      "Epoch 5688/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7161 - val_loss: 93.9181\n",
      "Epoch 5689/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.4794 - val_loss: 94.4447\n",
      "Epoch 5690/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.7346 - val_loss: 93.8826\n",
      "Epoch 5691/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6699 - val_loss: 93.6645\n",
      "Epoch 5692/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 91.5915 - val_loss: 94.1452\n",
      "Epoch 5693/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.3947 - val_loss: 93.7210\n",
      "Epoch 5694/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5598 - val_loss: 93.4846\n",
      "Epoch 5695/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.8533 - val_loss: 93.7333\n",
      "Epoch 5696/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.9891 - val_loss: 93.8724\n",
      "Epoch 5697/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.0464 - val_loss: 93.0501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5698/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3330 - val_loss: 93.5884\n",
      "Epoch 5699/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.8678 - val_loss: 93.6426\n",
      "Epoch 5700/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.9577 - val_loss: 93.2655\n",
      "Epoch 5701/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.2782 - val_loss: 93.6492\n",
      "Epoch 5702/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.1359 - val_loss: 93.8101\n",
      "Epoch 5703/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.9224 - val_loss: 93.8593\n",
      "Epoch 5704/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.3404 - val_loss: 94.3114\n",
      "Epoch 5705/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.2024 - val_loss: 93.7603\n",
      "Epoch 5706/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3201 - val_loss: 93.5038\n",
      "Epoch 5707/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.4152 - val_loss: 93.8253\n",
      "Epoch 5708/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 91.9372 - val_loss: 93.4520\n",
      "Epoch 5709/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7438 - val_loss: 93.9018\n",
      "Epoch 5710/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5657 - val_loss: 93.8018\n",
      "Epoch 5711/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.7933 - val_loss: 93.4934\n",
      "Epoch 5712/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8168 - val_loss: 93.4152\n",
      "Epoch 5713/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.6766 - val_loss: 93.7619\n",
      "Epoch 5714/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.4257 - val_loss: 93.6604\n",
      "Epoch 5715/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.8325 - val_loss: 93.6317\n",
      "Epoch 5716/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3148 - val_loss: 94.0052\n",
      "Epoch 5717/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.2872 - val_loss: 93.2174\n",
      "Epoch 5718/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.8616 - val_loss: 93.8472\n",
      "Epoch 5719/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.8181 - val_loss: 93.8963\n",
      "Epoch 5720/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.5314 - val_loss: 93.6103\n",
      "Epoch 5721/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.3295 - val_loss: 93.9779\n",
      "Epoch 5722/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.5744 - val_loss: 93.0855\n",
      "Epoch 5723/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 92.9647 - val_loss: 93.9818\n",
      "Epoch 5724/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.7922 - val_loss: 93.8906\n",
      "Epoch 5725/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0574 - val_loss: 94.0875\n",
      "Epoch 5726/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.8790 - val_loss: 93.7175\n",
      "Epoch 5727/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5841 - val_loss: 93.6915\n",
      "Epoch 5728/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.6635 - val_loss: 94.0560\n",
      "Epoch 5729/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.4653 - val_loss: 93.3485\n",
      "Epoch 5730/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.9982 - val_loss: 94.0122\n",
      "Epoch 5731/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8579 - val_loss: 94.1336\n",
      "Epoch 5732/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.1627 - val_loss: 93.8127\n",
      "Epoch 5733/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.7736 - val_loss: 94.1957\n",
      "Epoch 5734/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.3090 - val_loss: 93.9315\n",
      "Epoch 5735/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3083 - val_loss: 93.5759\n",
      "Epoch 5736/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 92.7272 - val_loss: 93.2242\n",
      "Epoch 5737/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 93.6036 - val_loss: 93.7944\n",
      "Epoch 5738/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 92.8362 - val_loss: 93.9111\n",
      "Epoch 5739/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 91.5896 - val_loss: 93.8160\n",
      "Epoch 5740/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 91.0610 - val_loss: 93.2799\n",
      "Epoch 5741/10000\n",
      "96/96 [==============================] - 0s 147us/step - loss: 91.4854 - val_loss: 93.9265\n",
      "Epoch 5742/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 91.8556 - val_loss: 93.3612\n",
      "Epoch 5743/10000\n",
      "96/96 [==============================] - 0s 154us/step - loss: 92.0973 - val_loss: 93.5153\n",
      "Epoch 5744/10000\n",
      "96/96 [==============================] - 0s 157us/step - loss: 91.6172 - val_loss: 92.9270\n",
      "Epoch 5745/10000\n",
      "96/96 [==============================] - 0s 157us/step - loss: 92.1136 - val_loss: 93.9469\n",
      "Epoch 5746/10000\n",
      "96/96 [==============================] - 0s 162us/step - loss: 91.7236 - val_loss: 93.9692\n",
      "Epoch 5747/10000\n",
      "96/96 [==============================] - 0s 147us/step - loss: 91.3299 - val_loss: 93.5882\n",
      "Epoch 5748/10000\n",
      "96/96 [==============================] - 0s 162us/step - loss: 92.1448 - val_loss: 93.5578\n",
      "Epoch 5749/10000\n",
      "96/96 [==============================] - 0s 160us/step - loss: 91.7839 - val_loss: 93.8292\n",
      "Epoch 5750/10000\n",
      "96/96 [==============================] - 0s 162us/step - loss: 91.8349 - val_loss: 93.6842\n",
      "Epoch 5751/10000\n",
      "96/96 [==============================] - 0s 159us/step - loss: 91.4390 - val_loss: 93.8911\n",
      "Epoch 5752/10000\n",
      "96/96 [==============================] - 0s 156us/step - loss: 92.4458 - val_loss: 93.4347\n",
      "Epoch 5753/10000\n",
      "96/96 [==============================] - 0s 153us/step - loss: 92.4902 - val_loss: 93.9369\n",
      "Epoch 5754/10000\n",
      "96/96 [==============================] - 0s 146us/step - loss: 91.8486 - val_loss: 93.6003\n",
      "Epoch 5755/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 91.7927 - val_loss: 93.9295\n",
      "Epoch 5756/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.8186 - val_loss: 93.3518\n",
      "Epoch 5757/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5999 - val_loss: 93.7036\n",
      "Epoch 5758/10000\n",
      "96/96 [==============================] - 0s 256us/step - loss: 91.6136 - val_loss: 93.6702\n",
      "Epoch 5759/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 91.2816 - val_loss: 93.7182\n",
      "Epoch 5760/10000\n",
      "96/96 [==============================] - 0s 190us/step - loss: 91.5826 - val_loss: 93.9047\n",
      "Epoch 5761/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5922 - val_loss: 93.0979\n",
      "Epoch 5762/10000\n",
      "96/96 [==============================] - 0s 163us/step - loss: 94.5889 - val_loss: 93.8724\n",
      "Epoch 5763/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 91.5021 - val_loss: 93.7865\n",
      "Epoch 5764/10000\n",
      "96/96 [==============================] - 0s 153us/step - loss: 93.7011 - val_loss: 93.6173\n",
      "Epoch 5765/10000\n",
      "96/96 [==============================] - 0s 146us/step - loss: 91.9827 - val_loss: 93.3641\n",
      "Epoch 5766/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 91.1980 - val_loss: 93.4764\n",
      "Epoch 5767/10000\n",
      "96/96 [==============================] - 0s 164us/step - loss: 92.0599 - val_loss: 94.0617\n",
      "Epoch 5768/10000\n",
      "96/96 [==============================] - 0s 164us/step - loss: 92.3717 - val_loss: 93.7640\n",
      "Epoch 5769/10000\n",
      "96/96 [==============================] - 0s 146us/step - loss: 92.9577 - val_loss: 93.5163\n",
      "Epoch 5770/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 92.3548 - val_loss: 93.2932\n",
      "Epoch 5771/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 91.9528 - val_loss: 93.3905\n",
      "Epoch 5772/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.2027 - val_loss: 93.4214\n",
      "Epoch 5773/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 92.3471 - val_loss: 93.8966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5774/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 92.4421 - val_loss: 94.2849\n",
      "Epoch 5775/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.1910 - val_loss: 93.7187\n",
      "Epoch 5776/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.1486 - val_loss: 93.5989\n",
      "Epoch 5777/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 92.0626 - val_loss: 93.4616\n",
      "Epoch 5778/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 91.7755 - val_loss: 93.7445\n",
      "Epoch 5779/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 92.3647 - val_loss: 93.9315\n",
      "Epoch 5780/10000\n",
      "96/96 [==============================] - 0s 154us/step - loss: 92.8567 - val_loss: 93.7915\n",
      "Epoch 5781/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 92.2034 - val_loss: 94.0373\n",
      "Epoch 5782/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5921 - val_loss: 93.5804\n",
      "Epoch 5783/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.7037 - val_loss: 93.9708\n",
      "Epoch 5784/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1376 - val_loss: 93.9480\n",
      "Epoch 5785/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8921 - val_loss: 93.6985\n",
      "Epoch 5786/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.3567 - val_loss: 93.9742\n",
      "Epoch 5787/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.8160 - val_loss: 94.1202\n",
      "Epoch 5788/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3405 - val_loss: 94.0007\n",
      "Epoch 5789/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 91.7332 - val_loss: 93.9902\n",
      "Epoch 5790/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.9329 - val_loss: 93.2439\n",
      "Epoch 5791/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 91.6988 - val_loss: 93.8831\n",
      "Epoch 5792/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 91.0217 - val_loss: 93.5719\n",
      "Epoch 5793/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.8990 - val_loss: 93.4740\n",
      "Epoch 5794/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 93.0518 - val_loss: 93.8184\n",
      "Epoch 5795/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 92.1511 - val_loss: 93.9613\n",
      "Epoch 5796/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.4588 - val_loss: 93.6174\n",
      "Epoch 5797/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1977 - val_loss: 93.4849\n",
      "Epoch 5798/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0812 - val_loss: 93.5166\n",
      "Epoch 5799/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3321 - val_loss: 93.6714\n",
      "Epoch 5800/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7563 - val_loss: 93.9049\n",
      "Epoch 5801/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.9045 - val_loss: 94.2122\n",
      "Epoch 5802/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.1599 - val_loss: 93.5888\n",
      "Epoch 5803/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.6320 - val_loss: 93.7356\n",
      "Epoch 5804/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.4508 - val_loss: 93.5333\n",
      "Epoch 5805/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.2897 - val_loss: 93.6432\n",
      "Epoch 5806/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.5148 - val_loss: 93.5171\n",
      "Epoch 5807/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7796 - val_loss: 93.2802\n",
      "Epoch 5808/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.6102 - val_loss: 93.8919\n",
      "Epoch 5809/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.5789 - val_loss: 93.3230\n",
      "Epoch 5810/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.8626 - val_loss: 93.7499\n",
      "Epoch 5811/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5820 - val_loss: 94.1053\n",
      "Epoch 5812/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.3316 - val_loss: 93.7250\n",
      "Epoch 5813/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.2054 - val_loss: 93.5764\n",
      "Epoch 5814/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.5012 - val_loss: 93.7122\n",
      "Epoch 5815/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.4061 - val_loss: 93.9161\n",
      "Epoch 5816/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.5187 - val_loss: 93.6184\n",
      "Epoch 5817/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0843 - val_loss: 93.9819\n",
      "Epoch 5818/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.4834 - val_loss: 94.0299\n",
      "Epoch 5819/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.6447 - val_loss: 93.2128\n",
      "Epoch 5820/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.4710 - val_loss: 94.2573\n",
      "Epoch 5821/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1944 - val_loss: 93.4792\n",
      "Epoch 5822/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3212 - val_loss: 94.1385\n",
      "Epoch 5823/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7716 - val_loss: 93.6569\n",
      "Epoch 5824/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.9933 - val_loss: 93.9227\n",
      "Epoch 5825/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6852 - val_loss: 94.1462\n",
      "Epoch 5826/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1513 - val_loss: 93.4265\n",
      "Epoch 5827/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.4043 - val_loss: 93.5688\n",
      "Epoch 5828/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8293 - val_loss: 93.7013\n",
      "Epoch 5829/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8004 - val_loss: 93.7663\n",
      "Epoch 5830/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.4141 - val_loss: 93.7784\n",
      "Epoch 5831/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.5612 - val_loss: 94.0407\n",
      "Epoch 5832/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.6887 - val_loss: 93.8011\n",
      "Epoch 5833/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6741 - val_loss: 94.1056\n",
      "Epoch 5834/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.8050 - val_loss: 93.8468\n",
      "Epoch 5835/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.3504 - val_loss: 93.5870\n",
      "Epoch 5836/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6571 - val_loss: 94.3751\n",
      "Epoch 5837/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1057 - val_loss: 93.8488\n",
      "Epoch 5838/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3550 - val_loss: 93.8782\n",
      "Epoch 5839/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.7955 - val_loss: 93.3562\n",
      "Epoch 5840/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.6227 - val_loss: 93.6936\n",
      "Epoch 5841/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.0685 - val_loss: 93.8743\n",
      "Epoch 5842/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.9999 - val_loss: 93.8071\n",
      "Epoch 5843/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.1054 - val_loss: 93.6398\n",
      "Epoch 5844/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.5286 - val_loss: 93.9014\n",
      "Epoch 5845/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5013 - val_loss: 94.0711\n",
      "Epoch 5846/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.7688 - val_loss: 93.8276\n",
      "Epoch 5847/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.7648 - val_loss: 93.7812\n",
      "Epoch 5848/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.0136 - val_loss: 94.1057\n",
      "Epoch 5849/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1967 - val_loss: 93.9643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5850/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.9274 - val_loss: 93.9779\n",
      "Epoch 5851/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.0858 - val_loss: 93.5857\n",
      "Epoch 5852/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.6306 - val_loss: 93.7652\n",
      "Epoch 5853/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9236 - val_loss: 93.7493\n",
      "Epoch 5854/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3237 - val_loss: 92.9590\n",
      "Epoch 5855/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 92.3016 - val_loss: 93.9698\n",
      "Epoch 5856/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8230 - val_loss: 94.1846\n",
      "Epoch 5857/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8475 - val_loss: 93.6736\n",
      "Epoch 5858/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.6739 - val_loss: 93.2682\n",
      "Epoch 5859/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.0653 - val_loss: 93.8708\n",
      "Epoch 5860/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 91.7831 - val_loss: 93.7924\n",
      "Epoch 5861/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.7311 - val_loss: 93.7546\n",
      "Epoch 5862/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.7851 - val_loss: 93.5293\n",
      "Epoch 5863/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 91.4293 - val_loss: 93.6890\n",
      "Epoch 5864/10000\n",
      "96/96 [==============================] - 0s 112us/step - loss: 91.7755 - val_loss: 93.3982\n",
      "Epoch 5865/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.5531 - val_loss: 93.4878\n",
      "Epoch 5866/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8960 - val_loss: 93.7051\n",
      "Epoch 5867/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.0065 - val_loss: 93.6565\n",
      "Epoch 5868/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 91.4433 - val_loss: 93.8947\n",
      "Epoch 5869/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 91.2982 - val_loss: 93.8237\n",
      "Epoch 5870/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8512 - val_loss: 93.8284\n",
      "Epoch 5871/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.9938 - val_loss: 94.1339\n",
      "Epoch 5872/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.4251 - val_loss: 93.7527\n",
      "Epoch 5873/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6450 - val_loss: 93.9970\n",
      "Epoch 5874/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.7650 - val_loss: 93.6684\n",
      "Epoch 5875/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.3142 - val_loss: 93.8832\n",
      "Epoch 5876/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9926 - val_loss: 93.9229\n",
      "Epoch 5877/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.3344 - val_loss: 93.5672\n",
      "Epoch 5878/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.6736 - val_loss: 93.5766\n",
      "Epoch 5879/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6740 - val_loss: 94.1021\n",
      "Epoch 5880/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3024 - val_loss: 93.6042\n",
      "Epoch 5881/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2474 - val_loss: 93.9033\n",
      "Epoch 5882/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1199 - val_loss: 93.8342\n",
      "Epoch 5883/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.2389 - val_loss: 93.8996\n",
      "Epoch 5884/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.2096 - val_loss: 93.7764\n",
      "Epoch 5885/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.9978 - val_loss: 93.6495\n",
      "Epoch 5886/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.7014 - val_loss: 94.1436\n",
      "Epoch 5887/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 91.9690 - val_loss: 93.8748\n",
      "Epoch 5888/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.8107 - val_loss: 93.8017\n",
      "Epoch 5889/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.7599 - val_loss: 93.9583\n",
      "Epoch 5890/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.8921 - val_loss: 93.4475\n",
      "Epoch 5891/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6812 - val_loss: 93.3282\n",
      "Epoch 5892/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.9305 - val_loss: 93.2361\n",
      "Epoch 5893/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4426 - val_loss: 93.7162\n",
      "Epoch 5894/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8526 - val_loss: 93.9794\n",
      "Epoch 5895/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8063 - val_loss: 93.5910\n",
      "Epoch 5896/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.1425 - val_loss: 93.6035\n",
      "Epoch 5897/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6423 - val_loss: 93.7399\n",
      "Epoch 5898/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.5926 - val_loss: 93.8498\n",
      "Epoch 5899/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.5180 - val_loss: 93.8855\n",
      "Epoch 5900/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8463 - val_loss: 94.1037\n",
      "Epoch 5901/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4363 - val_loss: 93.7832\n",
      "Epoch 5902/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.2550 - val_loss: 93.8135\n",
      "Epoch 5903/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.6671 - val_loss: 93.5573\n",
      "Epoch 5904/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3633 - val_loss: 93.8369\n",
      "Epoch 5905/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5306 - val_loss: 93.9372\n",
      "Epoch 5906/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.0396 - val_loss: 93.2618\n",
      "Epoch 5907/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.5055 - val_loss: 94.9199\n",
      "Epoch 5908/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9882 - val_loss: 93.8935\n",
      "Epoch 5909/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.1155 - val_loss: 93.8458\n",
      "Epoch 5910/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.0838 - val_loss: 93.5353\n",
      "Epoch 5911/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2793 - val_loss: 93.9194\n",
      "Epoch 5912/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.7866 - val_loss: 93.9903\n",
      "Epoch 5913/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.1517 - val_loss: 93.7368\n",
      "Epoch 5914/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.5296 - val_loss: 93.6231\n",
      "Epoch 5915/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.8731 - val_loss: 94.2059\n",
      "Epoch 5916/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.7539 - val_loss: 93.7214\n",
      "Epoch 5917/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.4048 - val_loss: 93.5511\n",
      "Epoch 5918/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.0253 - val_loss: 93.4550\n",
      "Epoch 5919/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.3871 - val_loss: 93.8219\n",
      "Epoch 5920/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.3422 - val_loss: 93.7277\n",
      "Epoch 5921/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.5479 - val_loss: 93.7826\n",
      "Epoch 5922/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.7168 - val_loss: 93.8608\n",
      "Epoch 5923/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.9621 - val_loss: 94.0317\n",
      "Epoch 5924/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.0836 - val_loss: 93.5814\n",
      "Epoch 5925/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.8209 - val_loss: 93.5424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5926/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6333 - val_loss: 93.8172\n",
      "Epoch 5927/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3431 - val_loss: 93.4676\n",
      "Epoch 5928/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.8340 - val_loss: 93.4693\n",
      "Epoch 5929/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.3187 - val_loss: 93.9337\n",
      "Epoch 5930/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2967 - val_loss: 93.6680\n",
      "Epoch 5931/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6606 - val_loss: 93.4459\n",
      "Epoch 5932/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.2678 - val_loss: 93.0321\n",
      "Epoch 5933/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0852 - val_loss: 93.3478\n",
      "Epoch 5934/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.0553 - val_loss: 93.7351\n",
      "Epoch 5935/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.0340 - val_loss: 93.8256\n",
      "Epoch 5936/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.3108 - val_loss: 93.3115\n",
      "Epoch 5937/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.5716 - val_loss: 93.5193\n",
      "Epoch 5938/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6036 - val_loss: 94.1093\n",
      "Epoch 5939/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8205 - val_loss: 93.1779\n",
      "Epoch 5940/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.3182 - val_loss: 92.9607\n",
      "Epoch 5941/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.4521 - val_loss: 93.5232\n",
      "Epoch 5942/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.8726 - val_loss: 93.6440\n",
      "Epoch 5943/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.9627 - val_loss: 93.5157\n",
      "Epoch 5944/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.0171 - val_loss: 93.7528\n",
      "Epoch 5945/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.0707 - val_loss: 93.0359\n",
      "Epoch 5946/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.9401 - val_loss: 93.1703\n",
      "Epoch 5947/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.8467 - val_loss: 94.1578\n",
      "Epoch 5948/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7191 - val_loss: 93.5644\n",
      "Epoch 5949/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7565 - val_loss: 93.7643\n",
      "Epoch 5950/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.6904 - val_loss: 93.8419\n",
      "Epoch 5951/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5750 - val_loss: 93.5826\n",
      "Epoch 5952/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5434 - val_loss: 93.4259\n",
      "Epoch 5953/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0184 - val_loss: 93.7297\n",
      "Epoch 5954/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.8904 - val_loss: 93.9635\n",
      "Epoch 5955/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.0468 - val_loss: 93.9718\n",
      "Epoch 5956/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4106 - val_loss: 93.4656\n",
      "Epoch 5957/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8765 - val_loss: 93.9409\n",
      "Epoch 5958/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.4675 - val_loss: 93.7250\n",
      "Epoch 5959/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8307 - val_loss: 94.0514\n",
      "Epoch 5960/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2403 - val_loss: 93.3080\n",
      "Epoch 5961/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.0899 - val_loss: 93.6214\n",
      "Epoch 5962/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.2074 - val_loss: 94.3442\n",
      "Epoch 5963/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.6034 - val_loss: 93.5830\n",
      "Epoch 5964/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.2629 - val_loss: 93.8560\n",
      "Epoch 5965/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.0731 - val_loss: 93.7491\n",
      "Epoch 5966/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.1903 - val_loss: 93.9206\n",
      "Epoch 5967/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.8882 - val_loss: 93.9156\n",
      "Epoch 5968/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.4848 - val_loss: 93.2162\n",
      "Epoch 5969/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.7856 - val_loss: 93.3546\n",
      "Epoch 5970/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8145 - val_loss: 93.9122\n",
      "Epoch 5971/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1451 - val_loss: 93.4142\n",
      "Epoch 5972/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.6122 - val_loss: 93.8368\n",
      "Epoch 5973/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.8736 - val_loss: 93.4756\n",
      "Epoch 5974/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8801 - val_loss: 94.0387\n",
      "Epoch 5975/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.0017 - val_loss: 93.5810\n",
      "Epoch 5976/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.3992 - val_loss: 93.6333\n",
      "Epoch 5977/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6452 - val_loss: 94.0196\n",
      "Epoch 5978/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6168 - val_loss: 93.7006\n",
      "Epoch 5979/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.1422 - val_loss: 93.4743\n",
      "Epoch 5980/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9294 - val_loss: 93.5909\n",
      "Epoch 5981/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8866 - val_loss: 93.7181\n",
      "Epoch 5982/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.2875 - val_loss: 94.1609\n",
      "Epoch 5983/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.9917 - val_loss: 93.5946\n",
      "Epoch 5984/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.0683 - val_loss: 93.8101\n",
      "Epoch 5985/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6703 - val_loss: 93.9583\n",
      "Epoch 5986/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.9861 - val_loss: 93.8714\n",
      "Epoch 5987/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.4963 - val_loss: 93.0166\n",
      "Epoch 5988/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.6307 - val_loss: 93.1138\n",
      "Epoch 5989/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.5699 - val_loss: 93.5878\n",
      "Epoch 5990/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5501 - val_loss: 93.7153\n",
      "Epoch 5991/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.3444 - val_loss: 94.0196\n",
      "Epoch 5992/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6042 - val_loss: 93.8438\n",
      "Epoch 5993/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4745 - val_loss: 93.6589\n",
      "Epoch 5994/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.7374 - val_loss: 93.8507\n",
      "Epoch 5995/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.2093 - val_loss: 93.5108\n",
      "Epoch 5996/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2486 - val_loss: 93.5716\n",
      "Epoch 5997/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.9987 - val_loss: 93.8239\n",
      "Epoch 5998/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 90.9043 - val_loss: 93.7763\n",
      "Epoch 5999/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2576 - val_loss: 93.6403\n",
      "Epoch 6000/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.4956 - val_loss: 93.4397\n",
      "Epoch 6001/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2114 - val_loss: 93.6855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6002/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.7884 - val_loss: 93.3567\n",
      "Epoch 6003/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.2978 - val_loss: 93.5150\n",
      "Epoch 6004/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3586 - val_loss: 93.8527\n",
      "Epoch 6005/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.5382 - val_loss: 93.8095\n",
      "Epoch 6006/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.0410 - val_loss: 93.7525\n",
      "Epoch 6007/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.4041 - val_loss: 93.5349\n",
      "Epoch 6008/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.8641 - val_loss: 93.7955\n",
      "Epoch 6009/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.0872 - val_loss: 93.7173\n",
      "Epoch 6010/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 96.5749 - val_loss: 94.0432\n",
      "Epoch 6011/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.1573 - val_loss: 94.0411\n",
      "Epoch 6012/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.3242 - val_loss: 93.7407\n",
      "Epoch 6013/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4929 - val_loss: 93.3735\n",
      "Epoch 6014/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8276 - val_loss: 93.7590\n",
      "Epoch 6015/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.8535 - val_loss: 93.7005\n",
      "Epoch 6016/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.0561 - val_loss: 93.4286\n",
      "Epoch 6017/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4763 - val_loss: 93.7414\n",
      "Epoch 6018/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3494 - val_loss: 93.4382\n",
      "Epoch 6019/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.8046 - val_loss: 93.4971\n",
      "Epoch 6020/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1264 - val_loss: 93.4896\n",
      "Epoch 6021/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.6288 - val_loss: 93.8682\n",
      "Epoch 6022/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4809 - val_loss: 93.4441\n",
      "Epoch 6023/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.4744 - val_loss: 93.8266\n",
      "Epoch 6024/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3649 - val_loss: 93.8867\n",
      "Epoch 6025/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7960 - val_loss: 93.6707\n",
      "Epoch 6026/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6385 - val_loss: 93.5679\n",
      "Epoch 6027/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.0391 - val_loss: 93.3015\n",
      "Epoch 6028/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.8255 - val_loss: 93.6052\n",
      "Epoch 6029/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8221 - val_loss: 93.4004\n",
      "Epoch 6030/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.4230 - val_loss: 93.5777\n",
      "Epoch 6031/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4305 - val_loss: 93.8521\n",
      "Epoch 6032/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.3450 - val_loss: 93.0572\n",
      "Epoch 6033/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.5421 - val_loss: 93.7157\n",
      "Epoch 6034/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.6466 - val_loss: 93.6016\n",
      "Epoch 6035/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2092 - val_loss: 93.6598\n",
      "Epoch 6036/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.9114 - val_loss: 93.7139\n",
      "Epoch 6037/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.3773 - val_loss: 93.8814\n",
      "Epoch 6038/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8154 - val_loss: 93.8808\n",
      "Epoch 6039/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8190 - val_loss: 92.9107\n",
      "Epoch 6040/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3203 - val_loss: 93.6176\n",
      "Epoch 6041/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6056 - val_loss: 93.5600\n",
      "Epoch 6042/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4498 - val_loss: 93.3188\n",
      "Epoch 6043/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.8390 - val_loss: 93.5071\n",
      "Epoch 6044/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.5999 - val_loss: 93.5675\n",
      "Epoch 6045/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.7119 - val_loss: 93.6267\n",
      "Epoch 6046/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.4084 - val_loss: 93.6745\n",
      "Epoch 6047/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.8769 - val_loss: 93.6703\n",
      "Epoch 6048/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.6443 - val_loss: 94.0333\n",
      "Epoch 6049/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.8074 - val_loss: 93.5424\n",
      "Epoch 6050/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.2156 - val_loss: 93.7665\n",
      "Epoch 6051/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.7964 - val_loss: 93.6162\n",
      "Epoch 6052/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8391 - val_loss: 93.3153\n",
      "Epoch 6053/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2145 - val_loss: 93.5592\n",
      "Epoch 6054/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.2541 - val_loss: 93.0626\n",
      "Epoch 6055/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6056 - val_loss: 93.6759\n",
      "Epoch 6056/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3187 - val_loss: 93.7653\n",
      "Epoch 6057/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.1504 - val_loss: 93.7338\n",
      "Epoch 6058/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.0758 - val_loss: 93.8235\n",
      "Epoch 6059/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9804 - val_loss: 93.4823\n",
      "Epoch 6060/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.8854 - val_loss: 93.6744\n",
      "Epoch 6061/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4885 - val_loss: 93.5582\n",
      "Epoch 6062/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0573 - val_loss: 94.5404\n",
      "Epoch 6063/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3788 - val_loss: 93.0421\n",
      "Epoch 6064/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.9148 - val_loss: 93.7234\n",
      "Epoch 6065/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.4217 - val_loss: 93.6597\n",
      "Epoch 6066/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.4721 - val_loss: 93.8255\n",
      "Epoch 6067/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.3230 - val_loss: 93.4301\n",
      "Epoch 6068/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6729 - val_loss: 93.3748\n",
      "Epoch 6069/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6962 - val_loss: 93.8829\n",
      "Epoch 6070/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.1892 - val_loss: 93.5852\n",
      "Epoch 6071/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.9936 - val_loss: 93.7694\n",
      "Epoch 6072/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.7733 - val_loss: 93.5225\n",
      "Epoch 6073/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.5181 - val_loss: 93.7115\n",
      "Epoch 6074/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.7081 - val_loss: 93.5088\n",
      "Epoch 6075/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 92.6388 - val_loss: 93.7236\n",
      "Epoch 6076/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5893 - val_loss: 93.6837\n",
      "Epoch 6077/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3181 - val_loss: 93.2974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6078/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.9679 - val_loss: 93.4659\n",
      "Epoch 6079/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.1841 - val_loss: 94.1194\n",
      "Epoch 6080/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.9786 - val_loss: 93.2910\n",
      "Epoch 6081/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.7395 - val_loss: 93.4271\n",
      "Epoch 6082/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.5898 - val_loss: 93.4197\n",
      "Epoch 6083/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.8065 - val_loss: 93.5546\n",
      "Epoch 6084/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.4997 - val_loss: 93.4572\n",
      "Epoch 6085/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.6848 - val_loss: 93.7073\n",
      "Epoch 6086/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.0687 - val_loss: 93.6729\n",
      "Epoch 6087/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7496 - val_loss: 93.7963\n",
      "Epoch 6088/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.9504 - val_loss: 93.5523\n",
      "Epoch 6089/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.1169 - val_loss: 93.8739\n",
      "Epoch 6090/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.7963 - val_loss: 93.8921\n",
      "Epoch 6091/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.4471 - val_loss: 93.6131\n",
      "Epoch 6092/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.3994 - val_loss: 94.0699\n",
      "Epoch 6093/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.1353 - val_loss: 93.7728\n",
      "Epoch 6094/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.2575 - val_loss: 93.8151\n",
      "Epoch 6095/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.4937 - val_loss: 94.1320\n",
      "Epoch 6096/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.9850 - val_loss: 93.8767\n",
      "Epoch 6097/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.5657 - val_loss: 93.5174\n",
      "Epoch 6098/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3765 - val_loss: 93.5896\n",
      "Epoch 6099/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.1827 - val_loss: 93.7451\n",
      "Epoch 6100/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.7817 - val_loss: 93.2200\n",
      "Epoch 6101/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.1925 - val_loss: 93.1995\n",
      "Epoch 6102/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.4059 - val_loss: 93.1431\n",
      "Epoch 6103/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.2206 - val_loss: 93.6014\n",
      "Epoch 6104/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.0303 - val_loss: 93.4233\n",
      "Epoch 6105/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.6604 - val_loss: 93.3800\n",
      "Epoch 6106/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.1583 - val_loss: 92.8712\n",
      "Epoch 6107/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5966 - val_loss: 93.9447\n",
      "Epoch 6108/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 94.0804 - val_loss: 94.1082\n",
      "Epoch 6109/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 92.6625 - val_loss: 93.4127\n",
      "Epoch 6110/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.0674 - val_loss: 93.3526\n",
      "Epoch 6111/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.7353 - val_loss: 93.8141\n",
      "Epoch 6112/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.4413 - val_loss: 93.6740\n",
      "Epoch 6113/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9221 - val_loss: 93.4465\n",
      "Epoch 6114/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.8020 - val_loss: 93.7884\n",
      "Epoch 6115/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 93.7729 - val_loss: 93.5763\n",
      "Epoch 6116/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.6030 - val_loss: 93.5016\n",
      "Epoch 6117/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.1415 - val_loss: 93.7011\n",
      "Epoch 6118/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6930 - val_loss: 93.8337\n",
      "Epoch 6119/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4434 - val_loss: 93.5798\n",
      "Epoch 6120/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.6365 - val_loss: 93.5242\n",
      "Epoch 6121/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.9094 - val_loss: 93.0248\n",
      "Epoch 6122/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3601 - val_loss: 93.5182\n",
      "Epoch 6123/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2493 - val_loss: 93.6549\n",
      "Epoch 6124/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3455 - val_loss: 93.7651\n",
      "Epoch 6125/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.5190 - val_loss: 93.6822\n",
      "Epoch 6126/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.0468 - val_loss: 93.6900\n",
      "Epoch 6127/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7073 - val_loss: 93.3909\n",
      "Epoch 6128/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.7684 - val_loss: 93.6799\n",
      "Epoch 6129/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.5449 - val_loss: 93.7361\n",
      "Epoch 6130/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.3425 - val_loss: 93.9523\n",
      "Epoch 6131/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.6148 - val_loss: 93.7192\n",
      "Epoch 6132/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.1496 - val_loss: 93.0648\n",
      "Epoch 6133/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.7364 - val_loss: 94.1683\n",
      "Epoch 6134/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.3076 - val_loss: 93.7975\n",
      "Epoch 6135/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6842 - val_loss: 93.8484\n",
      "Epoch 6136/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8943 - val_loss: 93.7653\n",
      "Epoch 6137/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5895 - val_loss: 93.5124\n",
      "Epoch 6138/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.1916 - val_loss: 93.8191\n",
      "Epoch 6139/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.8999 - val_loss: 93.3456\n",
      "Epoch 6140/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6917 - val_loss: 93.2290\n",
      "Epoch 6141/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2679 - val_loss: 93.6407\n",
      "Epoch 6142/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5227 - val_loss: 93.8211\n",
      "Epoch 6143/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1074 - val_loss: 93.7847\n",
      "Epoch 6144/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3811 - val_loss: 93.7614\n",
      "Epoch 6145/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.4721 - val_loss: 93.7552\n",
      "Epoch 6146/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2211 - val_loss: 93.4235\n",
      "Epoch 6147/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.7456 - val_loss: 93.3377\n",
      "Epoch 6148/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.1160 - val_loss: 93.5941\n",
      "Epoch 6149/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.0654 - val_loss: 93.1296\n",
      "Epoch 6150/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8155 - val_loss: 93.9794\n",
      "Epoch 6151/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.4364 - val_loss: 93.8583\n",
      "Epoch 6152/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.7192 - val_loss: 93.2721\n",
      "Epoch 6153/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.3791 - val_loss: 93.5731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6154/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6216 - val_loss: 93.5290\n",
      "Epoch 6155/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.4383 - val_loss: 93.5081\n",
      "Epoch 6156/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.3952 - val_loss: 93.8977\n",
      "Epoch 6157/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.9563 - val_loss: 93.6657\n",
      "Epoch 6158/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6261 - val_loss: 93.7328\n",
      "Epoch 6159/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3133 - val_loss: 93.8325\n",
      "Epoch 6160/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.5213 - val_loss: 93.6743\n",
      "Epoch 6161/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.7909 - val_loss: 93.4614\n",
      "Epoch 6162/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.6043 - val_loss: 93.7006\n",
      "Epoch 6163/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.0086 - val_loss: 93.6979\n",
      "Epoch 6164/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5433 - val_loss: 93.8894\n",
      "Epoch 6165/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.3214 - val_loss: 93.8505\n",
      "Epoch 6166/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.6751 - val_loss: 93.7348\n",
      "Epoch 6167/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.3200 - val_loss: 93.3442\n",
      "Epoch 6168/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.8212 - val_loss: 93.9835\n",
      "Epoch 6169/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.6871 - val_loss: 93.6934\n",
      "Epoch 6170/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.4689 - val_loss: 93.1221\n",
      "Epoch 6171/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.6842 - val_loss: 93.6669\n",
      "Epoch 6172/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6385 - val_loss: 93.7313\n",
      "Epoch 6173/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6819 - val_loss: 94.0734\n",
      "Epoch 6174/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.8545 - val_loss: 93.4438\n",
      "Epoch 6175/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.3591 - val_loss: 93.5302\n",
      "Epoch 6176/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.3021 - val_loss: 94.0413\n",
      "Epoch 6177/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5383 - val_loss: 93.5200\n",
      "Epoch 6178/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.5011 - val_loss: 93.8013\n",
      "Epoch 6179/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2924 - val_loss: 93.8048\n",
      "Epoch 6180/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3576 - val_loss: 93.5802\n",
      "Epoch 6181/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3088 - val_loss: 93.2087\n",
      "Epoch 6182/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.0215 - val_loss: 93.7673\n",
      "Epoch 6183/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3895 - val_loss: 93.5050\n",
      "Epoch 6184/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.8345 - val_loss: 93.7616\n",
      "Epoch 6185/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.7869 - val_loss: 93.9926\n",
      "Epoch 6186/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.0780 - val_loss: 93.3428\n",
      "Epoch 6187/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.0476 - val_loss: 93.3124\n",
      "Epoch 6188/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.1627 - val_loss: 93.5605\n",
      "Epoch 6189/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6009 - val_loss: 93.9289\n",
      "Epoch 6190/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.6267 - val_loss: 93.4552\n",
      "Epoch 6191/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4875 - val_loss: 93.3826\n",
      "Epoch 6192/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.7641 - val_loss: 93.6350\n",
      "Epoch 6193/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.4720 - val_loss: 93.8876\n",
      "Epoch 6194/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.9539 - val_loss: 93.6984\n",
      "Epoch 6195/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.4420 - val_loss: 93.7039\n",
      "Epoch 6196/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.1529 - val_loss: 93.6528\n",
      "Epoch 6197/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.6740 - val_loss: 93.7310\n",
      "Epoch 6198/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.1643 - val_loss: 93.4268\n",
      "Epoch 6199/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9529 - val_loss: 94.1565\n",
      "Epoch 6200/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.5070 - val_loss: 93.6561\n",
      "Epoch 6201/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.6913 - val_loss: 93.7770\n",
      "Epoch 6202/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.8130 - val_loss: 93.7342\n",
      "Epoch 6203/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1376 - val_loss: 93.5451\n",
      "Epoch 6204/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.9994 - val_loss: 92.9220\n",
      "Epoch 6205/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2957 - val_loss: 93.4941\n",
      "Epoch 6206/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.0913 - val_loss: 93.2661\n",
      "Epoch 6207/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.6228 - val_loss: 93.7325\n",
      "Epoch 6208/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.8928 - val_loss: 93.5846\n",
      "Epoch 6209/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.0616 - val_loss: 94.0903\n",
      "Epoch 6210/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0478 - val_loss: 93.7629\n",
      "Epoch 6211/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.7645 - val_loss: 93.7894\n",
      "Epoch 6212/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1319 - val_loss: 93.3122\n",
      "Epoch 6213/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.4550 - val_loss: 93.4188\n",
      "Epoch 6214/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3964 - val_loss: 93.1286\n",
      "Epoch 6215/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.7388 - val_loss: 93.6241\n",
      "Epoch 6216/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.7006 - val_loss: 93.4984\n",
      "Epoch 6217/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.0927 - val_loss: 93.8282\n",
      "Epoch 6218/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.2617 - val_loss: 93.6323\n",
      "Epoch 6219/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6919 - val_loss: 93.6420\n",
      "Epoch 6220/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.8819 - val_loss: 93.1524\n",
      "Epoch 6221/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2826 - val_loss: 94.0695\n",
      "Epoch 6222/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4874 - val_loss: 93.5328\n",
      "Epoch 6223/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1927 - val_loss: 93.7748\n",
      "Epoch 6224/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 92.1535 - val_loss: 93.7422\n",
      "Epoch 6225/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.9636 - val_loss: 93.6222\n",
      "Epoch 6226/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.2421 - val_loss: 93.5505\n",
      "Epoch 6227/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5212 - val_loss: 93.2705\n",
      "Epoch 6228/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4901 - val_loss: 93.2466\n",
      "Epoch 6229/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8078 - val_loss: 93.7303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6230/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.4923 - val_loss: 93.8136\n",
      "Epoch 6231/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.4066 - val_loss: 93.8114\n",
      "Epoch 6232/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6141 - val_loss: 93.6776\n",
      "Epoch 6233/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8859 - val_loss: 94.1485\n",
      "Epoch 6234/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 91.3928 - val_loss: 93.2130\n",
      "Epoch 6235/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1027 - val_loss: 93.5728\n",
      "Epoch 6236/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.4483 - val_loss: 93.9017\n",
      "Epoch 6237/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.9951 - val_loss: 93.6955\n",
      "Epoch 6238/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.5624 - val_loss: 93.6285\n",
      "Epoch 6239/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.7645 - val_loss: 93.4354\n",
      "Epoch 6240/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.7404 - val_loss: 93.2794\n",
      "Epoch 6241/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.8972 - val_loss: 93.6421\n",
      "Epoch 6242/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.0209 - val_loss: 93.9018\n",
      "Epoch 6243/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.5343 - val_loss: 93.6964\n",
      "Epoch 6244/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.3456 - val_loss: 93.9089\n",
      "Epoch 6245/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.7415 - val_loss: 93.4248\n",
      "Epoch 6246/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.3800 - val_loss: 93.9594\n",
      "Epoch 6247/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.9575 - val_loss: 93.5935\n",
      "Epoch 6248/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.4913 - val_loss: 93.7625\n",
      "Epoch 6249/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.4721 - val_loss: 93.7159\n",
      "Epoch 6250/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.8595 - val_loss: 93.9111\n",
      "Epoch 6251/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.8038 - val_loss: 93.7641\n",
      "Epoch 6252/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3786 - val_loss: 93.7218\n",
      "Epoch 6253/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.4978 - val_loss: 93.4402\n",
      "Epoch 6254/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6355 - val_loss: 93.4986\n",
      "Epoch 6255/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.8626 - val_loss: 93.6766\n",
      "Epoch 6256/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.2278 - val_loss: 93.4330\n",
      "Epoch 6257/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1356 - val_loss: 93.3649\n",
      "Epoch 6258/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.7088 - val_loss: 93.6364\n",
      "Epoch 6259/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.4016 - val_loss: 93.7169\n",
      "Epoch 6260/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.8362 - val_loss: 93.4526\n",
      "Epoch 6261/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6855 - val_loss: 93.4923\n",
      "Epoch 6262/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9444 - val_loss: 93.8559\n",
      "Epoch 6263/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.0208 - val_loss: 93.8955\n",
      "Epoch 6264/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2331 - val_loss: 93.3891\n",
      "Epoch 6265/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6668 - val_loss: 94.1046\n",
      "Epoch 6266/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9086 - val_loss: 93.6752\n",
      "Epoch 6267/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2643 - val_loss: 93.4893\n",
      "Epoch 6268/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.4778 - val_loss: 93.0596\n",
      "Epoch 6269/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.3189 - val_loss: 93.9609\n",
      "Epoch 6270/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3729 - val_loss: 93.4598\n",
      "Epoch 6271/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.1778 - val_loss: 93.1033\n",
      "Epoch 6272/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.0668 - val_loss: 93.5724\n",
      "Epoch 6273/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 92.4021 - val_loss: 93.9330\n",
      "Epoch 6274/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 92.4246 - val_loss: 93.9003\n",
      "Epoch 6275/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.9513 - val_loss: 93.6200\n",
      "Epoch 6276/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.3106 - val_loss: 93.6481\n",
      "Epoch 6277/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.2796 - val_loss: 93.9289\n",
      "Epoch 6278/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.2404 - val_loss: 94.0222\n",
      "Epoch 6279/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4116 - val_loss: 93.4872\n",
      "Epoch 6280/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.8429 - val_loss: 93.6416\n",
      "Epoch 6281/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.7241 - val_loss: 93.5605\n",
      "Epoch 6282/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.1045 - val_loss: 93.5079\n",
      "Epoch 6283/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.5778 - val_loss: 93.7362\n",
      "Epoch 6284/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9217 - val_loss: 93.3463\n",
      "Epoch 6285/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.9990 - val_loss: 93.2797\n",
      "Epoch 6286/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.0833 - val_loss: 93.7578\n",
      "Epoch 6287/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0276 - val_loss: 94.1134\n",
      "Epoch 6288/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9204 - val_loss: 93.8176\n",
      "Epoch 6289/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6584 - val_loss: 93.6389\n",
      "Epoch 6290/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8643 - val_loss: 93.8685\n",
      "Epoch 6291/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.9823 - val_loss: 93.3157\n",
      "Epoch 6292/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.9932 - val_loss: 93.1215\n",
      "Epoch 6293/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.7882 - val_loss: 93.8032\n",
      "Epoch 6294/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.7395 - val_loss: 93.2687\n",
      "Epoch 6295/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.5885 - val_loss: 94.1879\n",
      "Epoch 6296/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.9381 - val_loss: 93.5340\n",
      "Epoch 6297/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9423 - val_loss: 93.5269\n",
      "Epoch 6298/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.0617 - val_loss: 93.3353\n",
      "Epoch 6299/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.6161 - val_loss: 93.1799\n",
      "Epoch 6300/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.9743 - val_loss: 93.7093\n",
      "Epoch 6301/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.4188 - val_loss: 93.4777\n",
      "Epoch 6302/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8423 - val_loss: 93.4821\n",
      "Epoch 6303/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.5888 - val_loss: 93.5741\n",
      "Epoch 6304/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.0154 - val_loss: 93.6383\n",
      "Epoch 6305/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.2061 - val_loss: 94.0858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6306/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.7181 - val_loss: 92.9867\n",
      "Epoch 6307/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4115 - val_loss: 93.7473\n",
      "Epoch 6308/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2546 - val_loss: 93.1905\n",
      "Epoch 6309/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.3329 - val_loss: 93.5315\n",
      "Epoch 6310/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1465 - val_loss: 93.5275\n",
      "Epoch 6311/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.6372 - val_loss: 93.8059\n",
      "Epoch 6312/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2239 - val_loss: 93.5134\n",
      "Epoch 6313/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.2359 - val_loss: 93.6735\n",
      "Epoch 6314/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8139 - val_loss: 93.5862\n",
      "Epoch 6315/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.4594 - val_loss: 93.7304\n",
      "Epoch 6316/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.5983 - val_loss: 93.5189\n",
      "Epoch 6317/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3246 - val_loss: 94.1749\n",
      "Epoch 6318/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8824 - val_loss: 93.7804\n",
      "Epoch 6319/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.5773 - val_loss: 93.3666\n",
      "Epoch 6320/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.0701 - val_loss: 93.8830\n",
      "Epoch 6321/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.9447 - val_loss: 93.6704\n",
      "Epoch 6322/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.6126 - val_loss: 93.8394\n",
      "Epoch 6323/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3637 - val_loss: 93.7129\n",
      "Epoch 6324/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.8036 - val_loss: 93.8834\n",
      "Epoch 6325/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.6493 - val_loss: 93.5319\n",
      "Epoch 6326/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5243 - val_loss: 93.4617\n",
      "Epoch 6327/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9811 - val_loss: 93.3235\n",
      "Epoch 6328/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.4534 - val_loss: 93.6889\n",
      "Epoch 6329/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.7824 - val_loss: 93.6160\n",
      "Epoch 6330/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.0868 - val_loss: 93.4174\n",
      "Epoch 6331/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8647 - val_loss: 93.3541\n",
      "Epoch 6332/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.4202 - val_loss: 93.2439\n",
      "Epoch 6333/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6371 - val_loss: 93.4955\n",
      "Epoch 6334/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2426 - val_loss: 93.4541\n",
      "Epoch 6335/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8191 - val_loss: 93.6512\n",
      "Epoch 6336/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.5257 - val_loss: 93.8914\n",
      "Epoch 6337/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6334 - val_loss: 93.5808\n",
      "Epoch 6338/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.4145 - val_loss: 93.8424\n",
      "Epoch 6339/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4633 - val_loss: 93.6290\n",
      "Epoch 6340/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2316 - val_loss: 93.0894\n",
      "Epoch 6341/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2913 - val_loss: 93.1535\n",
      "Epoch 6342/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 91.3330 - val_loss: 93.2674\n",
      "Epoch 6343/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4176 - val_loss: 93.2956\n",
      "Epoch 6344/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.3224 - val_loss: 93.5736\n",
      "Epoch 6345/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.9114 - val_loss: 93.6499\n",
      "Epoch 6346/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1232 - val_loss: 93.6043\n",
      "Epoch 6347/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.8158 - val_loss: 93.9177\n",
      "Epoch 6348/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.1000 - val_loss: 93.2414\n",
      "Epoch 6349/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.1745 - val_loss: 93.3198\n",
      "Epoch 6350/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0638 - val_loss: 93.9378\n",
      "Epoch 6351/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.4922 - val_loss: 93.2568\n",
      "Epoch 6352/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9869 - val_loss: 93.6700\n",
      "Epoch 6353/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.0562 - val_loss: 94.0090\n",
      "Epoch 6354/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.3994 - val_loss: 93.5538\n",
      "Epoch 6355/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6932 - val_loss: 93.7358\n",
      "Epoch 6356/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.3443 - val_loss: 93.0105\n",
      "Epoch 6357/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6767 - val_loss: 93.5360\n",
      "Epoch 6358/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.4685 - val_loss: 93.2646\n",
      "Epoch 6359/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.7812 - val_loss: 93.4596\n",
      "Epoch 6360/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5523 - val_loss: 93.3895\n",
      "Epoch 6361/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.3929 - val_loss: 92.9777\n",
      "Epoch 6362/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.4315 - val_loss: 93.4077\n",
      "Epoch 6363/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8712 - val_loss: 93.3446\n",
      "Epoch 6364/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.8811 - val_loss: 93.6829\n",
      "Epoch 6365/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3362 - val_loss: 93.1558\n",
      "Epoch 6366/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6411 - val_loss: 93.5133\n",
      "Epoch 6367/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.6917 - val_loss: 93.5626\n",
      "Epoch 6368/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.0510 - val_loss: 93.1792\n",
      "Epoch 6369/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.6074 - val_loss: 93.7610\n",
      "Epoch 6370/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1958 - val_loss: 93.6420\n",
      "Epoch 6371/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9369 - val_loss: 93.3508\n",
      "Epoch 6372/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.7255 - val_loss: 93.4261\n",
      "Epoch 6373/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.7111 - val_loss: 93.5749\n",
      "Epoch 6374/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7827 - val_loss: 93.5444\n",
      "Epoch 6375/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.0935 - val_loss: 93.3900\n",
      "Epoch 6376/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.4081 - val_loss: 93.7399\n",
      "Epoch 6377/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7603 - val_loss: 93.4333\n",
      "Epoch 6378/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.0725 - val_loss: 93.6265\n",
      "Epoch 6379/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.8641 - val_loss: 93.6570\n",
      "Epoch 6380/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.1028 - val_loss: 93.7065\n",
      "Epoch 6381/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.0769 - val_loss: 93.6186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6382/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5489 - val_loss: 93.3556\n",
      "Epoch 6383/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5665 - val_loss: 93.6936\n",
      "Epoch 6384/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.6664 - val_loss: 93.6868\n",
      "Epoch 6385/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.6125 - val_loss: 93.3361\n",
      "Epoch 6386/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4637 - val_loss: 93.0227\n",
      "Epoch 6387/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.6438 - val_loss: 93.3173\n",
      "Epoch 6388/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.9755 - val_loss: 93.6062\n",
      "Epoch 6389/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1227 - val_loss: 93.7370\n",
      "Epoch 6390/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.0365 - val_loss: 94.2268\n",
      "Epoch 6391/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5512 - val_loss: 93.5778\n",
      "Epoch 6392/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.6945 - val_loss: 93.4677\n",
      "Epoch 6393/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2120 - val_loss: 93.8125\n",
      "Epoch 6394/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4738 - val_loss: 93.4540\n",
      "Epoch 6395/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2380 - val_loss: 93.4262\n",
      "Epoch 6396/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1629 - val_loss: 93.3577\n",
      "Epoch 6397/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9980 - val_loss: 93.6286\n",
      "Epoch 6398/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.8313 - val_loss: 93.1655\n",
      "Epoch 6399/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.0889 - val_loss: 93.4673\n",
      "Epoch 6400/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1185 - val_loss: 93.5099\n",
      "Epoch 6401/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.4381 - val_loss: 93.4750\n",
      "Epoch 6402/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.0011 - val_loss: 93.8646\n",
      "Epoch 6403/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5897 - val_loss: 93.2462\n",
      "Epoch 6404/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5311 - val_loss: 93.9236\n",
      "Epoch 6405/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1926 - val_loss: 94.0404\n",
      "Epoch 6406/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.3105 - val_loss: 93.5110\n",
      "Epoch 6407/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2665 - val_loss: 93.5892\n",
      "Epoch 6408/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.7911 - val_loss: 93.3324\n",
      "Epoch 6409/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3417 - val_loss: 93.4629\n",
      "Epoch 6410/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3422 - val_loss: 93.5288\n",
      "Epoch 6411/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.8146 - val_loss: 93.1755\n",
      "Epoch 6412/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6946 - val_loss: 93.9212\n",
      "Epoch 6413/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.3918 - val_loss: 93.6528\n",
      "Epoch 6414/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2346 - val_loss: 94.0424\n",
      "Epoch 6415/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6608 - val_loss: 93.9452\n",
      "Epoch 6416/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2842 - val_loss: 93.2132\n",
      "Epoch 6417/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.1670 - val_loss: 93.1271\n",
      "Epoch 6418/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.9827 - val_loss: 93.5332\n",
      "Epoch 6419/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5014 - val_loss: 93.6667\n",
      "Epoch 6420/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.0279 - val_loss: 93.6949\n",
      "Epoch 6421/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.0777 - val_loss: 93.3925\n",
      "Epoch 6422/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.4015 - val_loss: 93.4742\n",
      "Epoch 6423/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.5915 - val_loss: 93.2298\n",
      "Epoch 6424/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.6229 - val_loss: 93.2322\n",
      "Epoch 6425/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.4757 - val_loss: 94.0330\n",
      "Epoch 6426/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2902 - val_loss: 93.0114\n",
      "Epoch 6427/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.5141 - val_loss: 93.3879\n",
      "Epoch 6428/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8948 - val_loss: 93.7080\n",
      "Epoch 6429/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.5874 - val_loss: 93.0080\n",
      "Epoch 6430/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3392 - val_loss: 93.4978\n",
      "Epoch 6431/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2400 - val_loss: 93.4919\n",
      "Epoch 6432/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 94.4056 - val_loss: 93.6342\n",
      "Epoch 6433/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 93.8209 - val_loss: 93.9983\n",
      "Epoch 6434/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.0955 - val_loss: 93.4449\n",
      "Epoch 6435/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1175 - val_loss: 92.9060\n",
      "Epoch 6436/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.1771 - val_loss: 93.7954\n",
      "Epoch 6437/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.0868 - val_loss: 93.3265\n",
      "Epoch 6438/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.6510 - val_loss: 93.8289\n",
      "Epoch 6439/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.4299 - val_loss: 93.8877\n",
      "Epoch 6440/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 90.8871 - val_loss: 93.6032\n",
      "Epoch 6441/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.3510 - val_loss: 93.5700\n",
      "Epoch 6442/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.7449 - val_loss: 93.1235\n",
      "Epoch 6443/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6766 - val_loss: 93.4815\n",
      "Epoch 6444/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2212 - val_loss: 93.5797\n",
      "Epoch 6445/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 91.6079 - val_loss: 93.5955\n",
      "Epoch 6446/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 92.1096 - val_loss: 93.6165\n",
      "Epoch 6447/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.8728 - val_loss: 93.5095\n",
      "Epoch 6448/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 91.4291 - val_loss: 93.9772\n",
      "Epoch 6449/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.0514 - val_loss: 93.5618\n",
      "Epoch 6450/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.0215 - val_loss: 93.8268\n",
      "Epoch 6451/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.4439 - val_loss: 93.3532\n",
      "Epoch 6452/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 93.7815 - val_loss: 93.7469\n",
      "Epoch 6453/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 91.4872 - val_loss: 93.5479\n",
      "Epoch 6454/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.4620 - val_loss: 93.6269\n",
      "Epoch 6455/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8731 - val_loss: 93.5023\n",
      "Epoch 6456/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.4203 - val_loss: 93.3819\n",
      "Epoch 6457/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0476 - val_loss: 93.5225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6458/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9523 - val_loss: 93.9155\n",
      "Epoch 6459/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.2643 - val_loss: 93.3981\n",
      "Epoch 6460/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.1650 - val_loss: 93.2644\n",
      "Epoch 6461/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.7932 - val_loss: 93.0299\n",
      "Epoch 6462/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.0905 - val_loss: 93.9381\n",
      "Epoch 6463/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.6940 - val_loss: 93.3399\n",
      "Epoch 6464/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.2173 - val_loss: 93.5089\n",
      "Epoch 6465/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 90.9665 - val_loss: 93.3076\n",
      "Epoch 6466/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.4041 - val_loss: 93.5596\n",
      "Epoch 6467/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.6843 - val_loss: 93.3517\n",
      "Epoch 6468/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.3301 - val_loss: 93.1846\n",
      "Epoch 6469/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3548 - val_loss: 93.6026\n",
      "Epoch 6470/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.4384 - val_loss: 93.5652\n",
      "Epoch 6471/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 91.0346 - val_loss: 93.2885\n",
      "Epoch 6472/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.5616 - val_loss: 93.6992\n",
      "Epoch 6473/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5627 - val_loss: 93.6760\n",
      "Epoch 6474/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.7886 - val_loss: 93.5563\n",
      "Epoch 6475/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.9479 - val_loss: 93.0576\n",
      "Epoch 6476/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.8192 - val_loss: 93.2511\n",
      "Epoch 6477/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.4935 - val_loss: 93.5165\n",
      "Epoch 6478/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4436 - val_loss: 93.6329\n",
      "Epoch 6479/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2024 - val_loss: 93.6902\n",
      "Epoch 6480/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.5624 - val_loss: 93.6229\n",
      "Epoch 6481/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.6228 - val_loss: 93.8019\n",
      "Epoch 6482/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.6391 - val_loss: 93.1043\n",
      "Epoch 6483/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 92.3861 - val_loss: 93.3562\n",
      "Epoch 6484/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4313 - val_loss: 94.0478\n",
      "Epoch 6485/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.9393 - val_loss: 93.2075\n",
      "Epoch 6486/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.8382 - val_loss: 93.7541\n",
      "Epoch 6487/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.2059 - val_loss: 92.9682\n",
      "Epoch 6488/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.9301 - val_loss: 93.5639\n",
      "Epoch 6489/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.4442 - val_loss: 93.7161\n",
      "Epoch 6490/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.4276 - val_loss: 93.5751\n",
      "Epoch 6491/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9661 - val_loss: 93.7145\n",
      "Epoch 6492/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1248 - val_loss: 93.2076\n",
      "Epoch 6493/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2167 - val_loss: 93.2883\n",
      "Epoch 6494/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.9925 - val_loss: 93.4451\n",
      "Epoch 6495/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.2771 - val_loss: 93.4527\n",
      "Epoch 6496/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2416 - val_loss: 93.5867\n",
      "Epoch 6497/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.7313 - val_loss: 93.6057\n",
      "Epoch 6498/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.8721 - val_loss: 93.8930\n",
      "Epoch 6499/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.0524 - val_loss: 93.6196\n",
      "Epoch 6500/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4884 - val_loss: 93.8019\n",
      "Epoch 6501/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6260 - val_loss: 93.2181\n",
      "Epoch 6502/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.7788 - val_loss: 93.4781\n",
      "Epoch 6503/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9433 - val_loss: 94.0841\n",
      "Epoch 6504/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.3925 - val_loss: 93.6419\n",
      "Epoch 6505/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8668 - val_loss: 93.3943\n",
      "Epoch 6506/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1579 - val_loss: 93.2301\n",
      "Epoch 6507/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.3105 - val_loss: 93.5236\n",
      "Epoch 6508/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.3875 - val_loss: 93.5666\n",
      "Epoch 6509/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.1492 - val_loss: 93.5197\n",
      "Epoch 6510/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.1257 - val_loss: 93.9539\n",
      "Epoch 6511/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.4443 - val_loss: 92.9022\n",
      "Epoch 6512/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.4591 - val_loss: 93.3131\n",
      "Epoch 6513/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3460 - val_loss: 93.5849\n",
      "Epoch 6514/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6019 - val_loss: 93.8117\n",
      "Epoch 6515/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.7805 - val_loss: 93.5005\n",
      "Epoch 6516/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.1133 - val_loss: 93.4643\n",
      "Epoch 6517/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.4469 - val_loss: 93.2103\n",
      "Epoch 6518/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.2871 - val_loss: 93.5869\n",
      "Epoch 6519/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5205 - val_loss: 93.6497\n",
      "Epoch 6520/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5443 - val_loss: 93.3651\n",
      "Epoch 6521/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0741 - val_loss: 93.7033\n",
      "Epoch 6522/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7150 - val_loss: 93.5280\n",
      "Epoch 6523/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.4328 - val_loss: 93.7509\n",
      "Epoch 6524/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2283 - val_loss: 93.4062\n",
      "Epoch 6525/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5448 - val_loss: 93.6498\n",
      "Epoch 6526/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.2151 - val_loss: 93.2618\n",
      "Epoch 6527/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.0417 - val_loss: 93.3501\n",
      "Epoch 6528/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.1389 - val_loss: 93.1819\n",
      "Epoch 6529/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.0859 - val_loss: 94.0125\n",
      "Epoch 6530/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2664 - val_loss: 93.3284\n",
      "Epoch 6531/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.9738 - val_loss: 93.5087\n",
      "Epoch 6532/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.7128 - val_loss: 93.4029\n",
      "Epoch 6533/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.4853 - val_loss: 93.1815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6534/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 91.6856 - val_loss: 93.6323\n",
      "Epoch 6535/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.0875 - val_loss: 93.6221\n",
      "Epoch 6536/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2933 - val_loss: 93.4877\n",
      "Epoch 6537/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1763 - val_loss: 93.1255\n",
      "Epoch 6538/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2316 - val_loss: 93.5917\n",
      "Epoch 6539/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.8715 - val_loss: 93.6690\n",
      "Epoch 6540/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.7077 - val_loss: 93.3243\n",
      "Epoch 6541/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9143 - val_loss: 93.4290\n",
      "Epoch 6542/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.2967 - val_loss: 93.1294\n",
      "Epoch 6543/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.1614 - val_loss: 93.9200\n",
      "Epoch 6544/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.2069 - val_loss: 93.6697\n",
      "Epoch 6545/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.0886 - val_loss: 93.7015\n",
      "Epoch 6546/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.7165 - val_loss: 93.8372\n",
      "Epoch 6547/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.3192 - val_loss: 93.1029\n",
      "Epoch 6548/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.9338 - val_loss: 93.5220\n",
      "Epoch 6549/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6911 - val_loss: 93.8057\n",
      "Epoch 6550/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.1231 - val_loss: 93.6622\n",
      "Epoch 6551/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4404 - val_loss: 93.5781\n",
      "Epoch 6552/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.3418 - val_loss: 93.8266\n",
      "Epoch 6553/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.4236 - val_loss: 93.6972\n",
      "Epoch 6554/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.1263 - val_loss: 93.6154\n",
      "Epoch 6555/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.6306 - val_loss: 93.6656\n",
      "Epoch 6556/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.9411 - val_loss: 93.7471\n",
      "Epoch 6557/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.0001 - val_loss: 93.7831\n",
      "Epoch 6558/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.6852 - val_loss: 93.4884\n",
      "Epoch 6559/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.7697 - val_loss: 93.5553\n",
      "Epoch 6560/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.9205 - val_loss: 93.4843\n",
      "Epoch 6561/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.6196 - val_loss: 93.4538\n",
      "Epoch 6562/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 91.2678 - val_loss: 92.9123\n",
      "Epoch 6563/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.7890 - val_loss: 93.4143\n",
      "Epoch 6564/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.8324 - val_loss: 93.5542\n",
      "Epoch 6565/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2617 - val_loss: 93.5475\n",
      "Epoch 6566/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9540 - val_loss: 93.5728\n",
      "Epoch 6567/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0391 - val_loss: 93.7972\n",
      "Epoch 6568/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.4029 - val_loss: 93.6683\n",
      "Epoch 6569/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6830 - val_loss: 93.0473\n",
      "Epoch 6570/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.7078 - val_loss: 93.6626\n",
      "Epoch 6571/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 94.1721 - val_loss: 93.6087\n",
      "Epoch 6572/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.4687 - val_loss: 93.3536\n",
      "Epoch 6573/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3580 - val_loss: 93.3030\n",
      "Epoch 6574/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.2459 - val_loss: 93.9016\n",
      "Epoch 6575/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.5569 - val_loss: 93.4538\n",
      "Epoch 6576/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.8249 - val_loss: 93.5615\n",
      "Epoch 6577/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.1858 - val_loss: 93.4163\n",
      "Epoch 6578/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8048 - val_loss: 93.3229\n",
      "Epoch 6579/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.2123 - val_loss: 93.3780\n",
      "Epoch 6580/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6991 - val_loss: 93.2073\n",
      "Epoch 6581/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7543 - val_loss: 93.6373\n",
      "Epoch 6582/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8110 - val_loss: 93.5907\n",
      "Epoch 6583/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4793 - val_loss: 93.8187\n",
      "Epoch 6584/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.0748 - val_loss: 93.7610\n",
      "Epoch 6585/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.0320 - val_loss: 93.6305\n",
      "Epoch 6586/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8783 - val_loss: 93.1096\n",
      "Epoch 6587/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.1432 - val_loss: 93.4482\n",
      "Epoch 6588/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.7247 - val_loss: 93.7676\n",
      "Epoch 6589/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.0515 - val_loss: 93.6127\n",
      "Epoch 6590/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.1536 - val_loss: 93.6782\n",
      "Epoch 6591/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.9637 - val_loss: 93.7024\n",
      "Epoch 6592/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6281 - val_loss: 93.6964\n",
      "Epoch 6593/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5549 - val_loss: 93.7635\n",
      "Epoch 6594/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.5977 - val_loss: 92.9285\n",
      "Epoch 6595/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6295 - val_loss: 93.8032\n",
      "Epoch 6596/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2658 - val_loss: 93.9384\n",
      "Epoch 6597/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6072 - val_loss: 93.1132\n",
      "Epoch 6598/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 91.7370 - val_loss: 93.8886\n",
      "Epoch 6599/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5039 - val_loss: 93.2885\n",
      "Epoch 6600/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.0140 - val_loss: 93.5697\n",
      "Epoch 6601/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.9261 - val_loss: 93.6735\n",
      "Epoch 6602/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.1199 - val_loss: 93.4719\n",
      "Epoch 6603/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1661 - val_loss: 93.6598\n",
      "Epoch 6604/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1539 - val_loss: 93.5138\n",
      "Epoch 6605/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 93.1049 - val_loss: 93.8982\n",
      "Epoch 6606/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.5402 - val_loss: 93.4160\n",
      "Epoch 6607/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.7391 - val_loss: 93.4507\n",
      "Epoch 6608/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.3988 - val_loss: 93.6312\n",
      "Epoch 6609/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.6310 - val_loss: 93.4972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6610/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2615 - val_loss: 93.1688\n",
      "Epoch 6611/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 91.3152 - val_loss: 93.4580\n",
      "Epoch 6612/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3570 - val_loss: 93.4891\n",
      "Epoch 6613/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0133 - val_loss: 93.2462\n",
      "Epoch 6614/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.2808 - val_loss: 93.3978\n",
      "Epoch 6615/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.0978 - val_loss: 93.6641\n",
      "Epoch 6616/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.7250 - val_loss: 93.3139\n",
      "Epoch 6617/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2486 - val_loss: 93.4815\n",
      "Epoch 6618/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4543 - val_loss: 93.5256\n",
      "Epoch 6619/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.1900 - val_loss: 93.8888\n",
      "Epoch 6620/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.9660 - val_loss: 93.2672\n",
      "Epoch 6621/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.6956 - val_loss: 93.7221\n",
      "Epoch 6622/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.4064 - val_loss: 93.2919\n",
      "Epoch 6623/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.5813 - val_loss: 93.7855\n",
      "Epoch 6624/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.9833 - val_loss: 93.6473\n",
      "Epoch 6625/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.8081 - val_loss: 93.5145\n",
      "Epoch 6626/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5833 - val_loss: 93.9012\n",
      "Epoch 6627/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.4855 - val_loss: 93.6444\n",
      "Epoch 6628/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.3195 - val_loss: 93.1833\n",
      "Epoch 6629/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6225 - val_loss: 93.6586\n",
      "Epoch 6630/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.5049 - val_loss: 92.7924\n",
      "Epoch 6631/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.8918 - val_loss: 93.8389\n",
      "Epoch 6632/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5073 - val_loss: 93.7611\n",
      "Epoch 6633/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.0381 - val_loss: 93.4920\n",
      "Epoch 6634/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.3386 - val_loss: 93.6211\n",
      "Epoch 6635/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.1529 - val_loss: 93.8425\n",
      "Epoch 6636/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2928 - val_loss: 94.0814\n",
      "Epoch 6637/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5641 - val_loss: 93.4125\n",
      "Epoch 6638/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.4514 - val_loss: 92.9645\n",
      "Epoch 6639/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.7558 - val_loss: 93.4096\n",
      "Epoch 6640/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.0477 - val_loss: 93.4132\n",
      "Epoch 6641/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.3483 - val_loss: 94.0227\n",
      "Epoch 6642/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.8569 - val_loss: 93.4592\n",
      "Epoch 6643/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1028 - val_loss: 93.3433\n",
      "Epoch 6644/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.1431 - val_loss: 93.6744\n",
      "Epoch 6645/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0790 - val_loss: 93.6539\n",
      "Epoch 6646/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.6324 - val_loss: 93.4006\n",
      "Epoch 6647/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1171 - val_loss: 93.2811\n",
      "Epoch 6648/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6738 - val_loss: 93.6455\n",
      "Epoch 6649/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4233 - val_loss: 93.2763\n",
      "Epoch 6650/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.0465 - val_loss: 93.0904\n",
      "Epoch 6651/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.2274 - val_loss: 93.3126\n",
      "Epoch 6652/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.2091 - val_loss: 93.4890\n",
      "Epoch 6653/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5543 - val_loss: 93.6622\n",
      "Epoch 6654/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3994 - val_loss: 93.4027\n",
      "Epoch 6655/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.9702 - val_loss: 93.2202\n",
      "Epoch 6656/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5236 - val_loss: 93.9537\n",
      "Epoch 6657/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.3620 - val_loss: 93.7638\n",
      "Epoch 6658/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.7025 - val_loss: 93.1220\n",
      "Epoch 6659/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.9378 - val_loss: 92.9798\n",
      "Epoch 6660/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3253 - val_loss: 93.9211\n",
      "Epoch 6661/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.1588 - val_loss: 93.4349\n",
      "Epoch 6662/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9726 - val_loss: 93.9701\n",
      "Epoch 6663/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6518 - val_loss: 93.4767\n",
      "Epoch 6664/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.6485 - val_loss: 92.9168\n",
      "Epoch 6665/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.6461 - val_loss: 93.1273\n",
      "Epoch 6666/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.1469 - val_loss: 93.5040\n",
      "Epoch 6667/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6832 - val_loss: 93.4100\n",
      "Epoch 6668/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.4855 - val_loss: 93.3275\n",
      "Epoch 6669/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2468 - val_loss: 93.5164\n",
      "Epoch 6670/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.4803 - val_loss: 93.3264\n",
      "Epoch 6671/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.6456 - val_loss: 93.7498\n",
      "Epoch 6672/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.9466 - val_loss: 93.5367\n",
      "Epoch 6673/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6698 - val_loss: 92.7884\n",
      "Epoch 6674/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.6461 - val_loss: 93.3560\n",
      "Epoch 6675/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.2326 - val_loss: 93.5156\n",
      "Epoch 6676/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.3104 - val_loss: 93.6595\n",
      "Epoch 6677/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.2816 - val_loss: 93.0515\n",
      "Epoch 6678/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.6885 - val_loss: 93.4661\n",
      "Epoch 6679/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.1298 - val_loss: 93.4100\n",
      "Epoch 6680/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2191 - val_loss: 93.2766\n",
      "Epoch 6681/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.3242 - val_loss: 93.0188\n",
      "Epoch 6682/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1139 - val_loss: 93.1267\n",
      "Epoch 6683/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6911 - val_loss: 93.4223\n",
      "Epoch 6684/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.9428 - val_loss: 93.7104\n",
      "Epoch 6685/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.0027 - val_loss: 93.1052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6686/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3034 - val_loss: 93.8842\n",
      "Epoch 6687/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3319 - val_loss: 93.2623\n",
      "Epoch 6688/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.5794 - val_loss: 93.6396\n",
      "Epoch 6689/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.2282 - val_loss: 93.4539\n",
      "Epoch 6690/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.5500 - val_loss: 93.3986\n",
      "Epoch 6691/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4372 - val_loss: 93.7670\n",
      "Epoch 6692/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.1942 - val_loss: 93.6521\n",
      "Epoch 6693/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.2994 - val_loss: 93.2867\n",
      "Epoch 6694/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.9104 - val_loss: 93.2928\n",
      "Epoch 6695/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.1231 - val_loss: 93.6949\n",
      "Epoch 6696/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.0152 - val_loss: 92.9758\n",
      "Epoch 6697/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3129 - val_loss: 93.5966\n",
      "Epoch 6698/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.5340 - val_loss: 93.5220\n",
      "Epoch 6699/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.8902 - val_loss: 92.9872\n",
      "Epoch 6700/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5597 - val_loss: 93.2697\n",
      "Epoch 6701/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.0414 - val_loss: 93.5949\n",
      "Epoch 6702/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.7672 - val_loss: 93.7273\n",
      "Epoch 6703/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.1955 - val_loss: 93.5912\n",
      "Epoch 6704/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7911 - val_loss: 94.0457\n",
      "Epoch 6705/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6300 - val_loss: 93.3136\n",
      "Epoch 6706/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.0057 - val_loss: 93.7580\n",
      "Epoch 6707/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.6383 - val_loss: 93.0974\n",
      "Epoch 6708/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2518 - val_loss: 93.4002\n",
      "Epoch 6709/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5358 - val_loss: 93.4500\n",
      "Epoch 6710/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9906 - val_loss: 93.4778\n",
      "Epoch 6711/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6414 - val_loss: 93.1852\n",
      "Epoch 6712/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1781 - val_loss: 93.1256\n",
      "Epoch 6713/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6370 - val_loss: 93.7238\n",
      "Epoch 6714/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.7317 - val_loss: 93.3616\n",
      "Epoch 6715/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3857 - val_loss: 93.6319\n",
      "Epoch 6716/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2924 - val_loss: 93.2631\n",
      "Epoch 6717/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.8668 - val_loss: 93.1053\n",
      "Epoch 6718/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.7412 - val_loss: 93.0645\n",
      "Epoch 6719/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.3297 - val_loss: 93.2236\n",
      "Epoch 6720/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.2988 - val_loss: 93.0723\n",
      "Epoch 6721/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0838 - val_loss: 93.5257\n",
      "Epoch 6722/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.3771 - val_loss: 93.6564\n",
      "Epoch 6723/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.9361 - val_loss: 93.4686\n",
      "Epoch 6724/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5548 - val_loss: 94.0352\n",
      "Epoch 6725/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.9511 - val_loss: 93.7943\n",
      "Epoch 6726/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.4540 - val_loss: 93.7096\n",
      "Epoch 6727/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.9014 - val_loss: 93.4539\n",
      "Epoch 6728/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.6943 - val_loss: 93.7789\n",
      "Epoch 6729/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.5366 - val_loss: 93.6023\n",
      "Epoch 6730/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.0208 - val_loss: 93.2820\n",
      "Epoch 6731/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.1828 - val_loss: 93.7495\n",
      "Epoch 6732/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.8733 - val_loss: 93.3060\n",
      "Epoch 6733/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.6192 - val_loss: 92.6843\n",
      "Epoch 6734/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.5387 - val_loss: 93.2386\n",
      "Epoch 6735/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.1323 - val_loss: 93.9184\n",
      "Epoch 6736/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.6784 - val_loss: 93.6793\n",
      "Epoch 6737/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.4030 - val_loss: 93.2104\n",
      "Epoch 6738/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.9713 - val_loss: 93.6839\n",
      "Epoch 6739/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.7443 - val_loss: 93.6360\n",
      "Epoch 6740/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5800 - val_loss: 93.7144\n",
      "Epoch 6741/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.7545 - val_loss: 93.7535\n",
      "Epoch 6742/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.1353 - val_loss: 93.0383\n",
      "Epoch 6743/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3522 - val_loss: 93.4269\n",
      "Epoch 6744/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.5352 - val_loss: 93.2548\n",
      "Epoch 6745/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.5686 - val_loss: 93.2607\n",
      "Epoch 6746/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4050 - val_loss: 93.8312\n",
      "Epoch 6747/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.3739 - val_loss: 92.9434\n",
      "Epoch 6748/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1293 - val_loss: 93.6772\n",
      "Epoch 6749/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.9705 - val_loss: 93.0849\n",
      "Epoch 6750/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3558 - val_loss: 93.8556\n",
      "Epoch 6751/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8431 - val_loss: 93.4525\n",
      "Epoch 6752/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3157 - val_loss: 93.6199\n",
      "Epoch 6753/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4284 - val_loss: 93.5293\n",
      "Epoch 6754/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.0316 - val_loss: 93.5873\n",
      "Epoch 6755/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5840 - val_loss: 93.6893\n",
      "Epoch 6756/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.1575 - val_loss: 93.5210\n",
      "Epoch 6757/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.1296 - val_loss: 92.7776\n",
      "Epoch 6758/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.7382 - val_loss: 93.1857\n",
      "Epoch 6759/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3007 - val_loss: 93.8320\n",
      "Epoch 6760/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1473 - val_loss: 93.2799\n",
      "Epoch 6761/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.4002 - val_loss: 93.3932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6762/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0496 - val_loss: 93.3612\n",
      "Epoch 6763/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0455 - val_loss: 93.4912\n",
      "Epoch 6764/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.0690 - val_loss: 93.5634\n",
      "Epoch 6765/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5437 - val_loss: 93.8865\n",
      "Epoch 6766/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.9319 - val_loss: 93.3703\n",
      "Epoch 6767/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.9316 - val_loss: 93.5521\n",
      "Epoch 6768/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.0847 - val_loss: 93.8310\n",
      "Epoch 6769/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 90.4021 - val_loss: 93.6342\n",
      "Epoch 6770/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6263 - val_loss: 93.9324\n",
      "Epoch 6771/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.4263 - val_loss: 93.6694\n",
      "Epoch 6772/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5593 - val_loss: 93.3152\n",
      "Epoch 6773/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3864 - val_loss: 93.3320\n",
      "Epoch 6774/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.2060 - val_loss: 93.0483\n",
      "Epoch 6775/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.3806 - val_loss: 93.1941\n",
      "Epoch 6776/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6346 - val_loss: 93.7882\n",
      "Epoch 6777/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.7924 - val_loss: 93.7048\n",
      "Epoch 6778/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0630 - val_loss: 93.3033\n",
      "Epoch 6779/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.9442 - val_loss: 93.7006\n",
      "Epoch 6780/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.9288 - val_loss: 93.4794\n",
      "Epoch 6781/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6503 - val_loss: 93.3384\n",
      "Epoch 6782/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.9512 - val_loss: 93.5011\n",
      "Epoch 6783/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.6320 - val_loss: 93.2017\n",
      "Epoch 6784/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3185 - val_loss: 94.0301\n",
      "Epoch 6785/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3101 - val_loss: 92.8705\n",
      "Epoch 6786/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.0444 - val_loss: 93.4125\n",
      "Epoch 6787/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.7801 - val_loss: 93.4842\n",
      "Epoch 6788/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.5091 - val_loss: 93.4722\n",
      "Epoch 6789/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2936 - val_loss: 93.4755\n",
      "Epoch 6790/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.3897 - val_loss: 93.1895\n",
      "Epoch 6791/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.0459 - val_loss: 93.4737\n",
      "Epoch 6792/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.0508 - val_loss: 92.7176\n",
      "Epoch 6793/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8641 - val_loss: 93.6523\n",
      "Epoch 6794/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.7443 - val_loss: 93.7987\n",
      "Epoch 6795/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6581 - val_loss: 93.1606\n",
      "Epoch 6796/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1415 - val_loss: 92.9612\n",
      "Epoch 6797/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4510 - val_loss: 93.0932\n",
      "Epoch 6798/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.3409 - val_loss: 93.0558\n",
      "Epoch 6799/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2349 - val_loss: 93.6623\n",
      "Epoch 6800/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.8450 - val_loss: 93.1946\n",
      "Epoch 6801/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.8633 - val_loss: 93.8662\n",
      "Epoch 6802/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.6769 - val_loss: 93.4512\n",
      "Epoch 6803/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.8158 - val_loss: 92.9562\n",
      "Epoch 6804/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 91.4641 - val_loss: 93.0452\n",
      "Epoch 6805/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5782 - val_loss: 93.4567\n",
      "Epoch 6806/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.4808 - val_loss: 93.5371\n",
      "Epoch 6807/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.3754 - val_loss: 92.9596\n",
      "Epoch 6808/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.0673 - val_loss: 93.3547\n",
      "Epoch 6809/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1522 - val_loss: 93.1994\n",
      "Epoch 6810/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.8928 - val_loss: 93.3194\n",
      "Epoch 6811/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.6944 - val_loss: 93.7992\n",
      "Epoch 6812/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3833 - val_loss: 93.5686\n",
      "Epoch 6813/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.0582 - val_loss: 92.9959\n",
      "Epoch 6814/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.6762 - val_loss: 93.6739\n",
      "Epoch 6815/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.7024 - val_loss: 93.2562\n",
      "Epoch 6816/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.9420 - val_loss: 93.6681\n",
      "Epoch 6817/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.1956 - val_loss: 93.4508\n",
      "Epoch 6818/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6106 - val_loss: 93.5990\n",
      "Epoch 6819/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.9312 - val_loss: 92.9196\n",
      "Epoch 6820/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.0018 - val_loss: 93.7694\n",
      "Epoch 6821/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.0105 - val_loss: 93.3206\n",
      "Epoch 6822/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3416 - val_loss: 92.5362\n",
      "Epoch 6823/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7599 - val_loss: 92.9055\n",
      "Epoch 6824/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.9750 - val_loss: 93.4524\n",
      "Epoch 6825/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.6938 - val_loss: 93.0315\n",
      "Epoch 6826/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2592 - val_loss: 93.1994\n",
      "Epoch 6827/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0219 - val_loss: 93.8584\n",
      "Epoch 6828/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.6720 - val_loss: 93.3967\n",
      "Epoch 6829/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.3605 - val_loss: 93.1025\n",
      "Epoch 6830/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2474 - val_loss: 93.3583\n",
      "Epoch 6831/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.9068 - val_loss: 92.9023\n",
      "Epoch 6832/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5449 - val_loss: 93.5254\n",
      "Epoch 6833/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5192 - val_loss: 93.5027\n",
      "Epoch 6834/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8332 - val_loss: 93.1388\n",
      "Epoch 6835/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.1123 - val_loss: 93.4905\n",
      "Epoch 6836/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5446 - val_loss: 92.8206\n",
      "Epoch 6837/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2163 - val_loss: 93.3385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6838/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.9447 - val_loss: 93.5604\n",
      "Epoch 6839/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.8245 - val_loss: 93.2109\n",
      "Epoch 6840/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2081 - val_loss: 93.5734\n",
      "Epoch 6841/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.4814 - val_loss: 93.5444\n",
      "Epoch 6842/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.8410 - val_loss: 93.6675\n",
      "Epoch 6843/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.4535 - val_loss: 93.4116\n",
      "Epoch 6844/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.7852 - val_loss: 93.5628\n",
      "Epoch 6845/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.0048 - val_loss: 93.3195\n",
      "Epoch 6846/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.9655 - val_loss: 92.6295\n",
      "Epoch 6847/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1721 - val_loss: 93.2672\n",
      "Epoch 6848/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.6422 - val_loss: 93.9958\n",
      "Epoch 6849/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.1897 - val_loss: 93.0506\n",
      "Epoch 6850/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.6305 - val_loss: 93.4615\n",
      "Epoch 6851/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.9359 - val_loss: 93.3035\n",
      "Epoch 6852/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.7988 - val_loss: 93.8177\n",
      "Epoch 6853/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.1509 - val_loss: 93.4288\n",
      "Epoch 6854/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.7490 - val_loss: 93.6122\n",
      "Epoch 6855/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.0551 - val_loss: 93.2973\n",
      "Epoch 6856/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.3724 - val_loss: 92.9810\n",
      "Epoch 6857/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2159 - val_loss: 93.4090\n",
      "Epoch 6858/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.1786 - val_loss: 93.4112\n",
      "Epoch 6859/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.3119 - val_loss: 93.8499\n",
      "Epoch 6860/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.9250 - val_loss: 93.2827\n",
      "Epoch 6861/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.5244 - val_loss: 93.9338\n",
      "Epoch 6862/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.0574 - val_loss: 93.3884\n",
      "Epoch 6863/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.3436 - val_loss: 93.4942\n",
      "Epoch 6864/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.7965 - val_loss: 93.0104\n",
      "Epoch 6865/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.0654 - val_loss: 93.1282\n",
      "Epoch 6866/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.6591 - val_loss: 93.1320\n",
      "Epoch 6867/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.1894 - val_loss: 93.5477\n",
      "Epoch 6868/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 90.9445 - val_loss: 93.4195\n",
      "Epoch 6869/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6872 - val_loss: 93.2438\n",
      "Epoch 6870/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8964 - val_loss: 92.6081\n",
      "Epoch 6871/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.5836 - val_loss: 92.4632\n",
      "Epoch 6872/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.0476 - val_loss: 93.3358\n",
      "Epoch 6873/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.5424 - val_loss: 93.5340\n",
      "Epoch 6874/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.9952 - val_loss: 93.3806\n",
      "Epoch 6875/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.8664 - val_loss: 93.3620\n",
      "Epoch 6876/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.0164 - val_loss: 93.4198\n",
      "Epoch 6877/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.2741 - val_loss: 93.0365\n",
      "Epoch 6878/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 94.5000 - val_loss: 93.5462\n",
      "Epoch 6879/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5850 - val_loss: 93.8110\n",
      "Epoch 6880/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2006 - val_loss: 93.2041\n",
      "Epoch 6881/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7919 - val_loss: 93.1522\n",
      "Epoch 6882/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5695 - val_loss: 93.0370\n",
      "Epoch 6883/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.9847 - val_loss: 93.5828\n",
      "Epoch 6884/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8225 - val_loss: 93.5161\n",
      "Epoch 6885/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.9650 - val_loss: 94.1853\n",
      "Epoch 6886/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5198 - val_loss: 92.6178\n",
      "Epoch 6887/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.0684 - val_loss: 93.3739\n",
      "Epoch 6888/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.7667 - val_loss: 93.1398\n",
      "Epoch 6889/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5408 - val_loss: 93.5563\n",
      "Epoch 6890/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.2460 - val_loss: 93.3644\n",
      "Epoch 6891/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 90.9805 - val_loss: 93.5073\n",
      "Epoch 6892/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.8170 - val_loss: 93.3687\n",
      "Epoch 6893/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.1038 - val_loss: 93.6245\n",
      "Epoch 6894/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4943 - val_loss: 93.2581\n",
      "Epoch 6895/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3747 - val_loss: 93.0330\n",
      "Epoch 6896/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.9989 - val_loss: 93.5478\n",
      "Epoch 6897/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2282 - val_loss: 93.5853\n",
      "Epoch 6898/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.5942 - val_loss: 93.2190\n",
      "Epoch 6899/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.9206 - val_loss: 93.3928\n",
      "Epoch 6900/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.5676 - val_loss: 92.5914\n",
      "Epoch 6901/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6119 - val_loss: 93.6760\n",
      "Epoch 6902/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.8810 - val_loss: 93.3138\n",
      "Epoch 6903/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7113 - val_loss: 93.3020\n",
      "Epoch 6904/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 92.5204 - val_loss: 93.1610\n",
      "Epoch 6905/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8415 - val_loss: 93.2870\n",
      "Epoch 6906/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.0425 - val_loss: 93.9016\n",
      "Epoch 6907/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.2986 - val_loss: 93.2176\n",
      "Epoch 6908/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.4957 - val_loss: 93.4343\n",
      "Epoch 6909/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.3814 - val_loss: 93.0461\n",
      "Epoch 6910/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.0455 - val_loss: 93.2868\n",
      "Epoch 6911/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3557 - val_loss: 93.5364\n",
      "Epoch 6912/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.2023 - val_loss: 93.5808\n",
      "Epoch 6913/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4947 - val_loss: 93.2082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6914/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.9449 - val_loss: 93.5070\n",
      "Epoch 6915/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.4030 - val_loss: 93.2661\n",
      "Epoch 6916/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.7448 - val_loss: 93.0788\n",
      "Epoch 6917/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.3838 - val_loss: 93.6674\n",
      "Epoch 6918/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3862 - val_loss: 93.1008\n",
      "Epoch 6919/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.0941 - val_loss: 93.0537\n",
      "Epoch 6920/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.0176 - val_loss: 93.3188\n",
      "Epoch 6921/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.8703 - val_loss: 93.3857\n",
      "Epoch 6922/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.2211 - val_loss: 93.1940\n",
      "Epoch 6923/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.7904 - val_loss: 93.1423\n",
      "Epoch 6924/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5345 - val_loss: 93.5979\n",
      "Epoch 6925/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4622 - val_loss: 93.8739\n",
      "Epoch 6926/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.1049 - val_loss: 93.6589\n",
      "Epoch 6927/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.8060 - val_loss: 93.7925\n",
      "Epoch 6928/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.5619 - val_loss: 93.1938\n",
      "Epoch 6929/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.4550 - val_loss: 93.2555\n",
      "Epoch 6930/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.0335 - val_loss: 93.0593\n",
      "Epoch 6931/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3651 - val_loss: 93.6331\n",
      "Epoch 6932/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2795 - val_loss: 93.2101\n",
      "Epoch 6933/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.4463 - val_loss: 92.8301\n",
      "Epoch 6934/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.9148 - val_loss: 92.8633\n",
      "Epoch 6935/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.3531 - val_loss: 93.7287\n",
      "Epoch 6936/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.9628 - val_loss: 93.0092\n",
      "Epoch 6937/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.3534 - val_loss: 93.0503\n",
      "Epoch 6938/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.2039 - val_loss: 93.0563\n",
      "Epoch 6939/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.6114 - val_loss: 92.8030\n",
      "Epoch 6940/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.6498 - val_loss: 93.4532\n",
      "Epoch 6941/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.3089 - val_loss: 93.6428\n",
      "Epoch 6942/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.2761 - val_loss: 92.9297\n",
      "Epoch 6943/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.7968 - val_loss: 93.3687\n",
      "Epoch 6944/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5175 - val_loss: 93.6583\n",
      "Epoch 6945/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.7459 - val_loss: 92.8164\n",
      "Epoch 6946/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0567 - val_loss: 93.1747\n",
      "Epoch 6947/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.3781 - val_loss: 93.3206\n",
      "Epoch 6948/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.8536 - val_loss: 93.3930\n",
      "Epoch 6949/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9579 - val_loss: 93.7784\n",
      "Epoch 6950/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.5682 - val_loss: 93.2235\n",
      "Epoch 6951/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.2361 - val_loss: 93.3717\n",
      "Epoch 6952/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.0055 - val_loss: 93.4363\n",
      "Epoch 6953/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.4040 - val_loss: 92.9732\n",
      "Epoch 6954/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5781 - val_loss: 93.4310\n",
      "Epoch 6955/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.3335 - val_loss: 93.1595\n",
      "Epoch 6956/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.1895 - val_loss: 93.3601\n",
      "Epoch 6957/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.3101 - val_loss: 92.9024\n",
      "Epoch 6958/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.4640 - val_loss: 93.6346\n",
      "Epoch 6959/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2382 - val_loss: 93.2397\n",
      "Epoch 6960/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.6600 - val_loss: 93.0608\n",
      "Epoch 6961/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.4192 - val_loss: 93.1638\n",
      "Epoch 6962/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.1889 - val_loss: 93.3269\n",
      "Epoch 6963/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.0402 - val_loss: 93.8139\n",
      "Epoch 6964/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.3064 - val_loss: 93.3689\n",
      "Epoch 6965/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6239 - val_loss: 93.0646\n",
      "Epoch 6966/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.5667 - val_loss: 93.9102\n",
      "Epoch 6967/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.4255 - val_loss: 93.3412\n",
      "Epoch 6968/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.3531 - val_loss: 93.1877\n",
      "Epoch 6969/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.9250 - val_loss: 93.2489\n",
      "Epoch 6970/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.9086 - val_loss: 93.3180\n",
      "Epoch 6971/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.1719 - val_loss: 93.4346\n",
      "Epoch 6972/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.5623 - val_loss: 93.2010\n",
      "Epoch 6973/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.2234 - val_loss: 93.1263\n",
      "Epoch 6974/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.3369 - val_loss: 92.8059\n",
      "Epoch 6975/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.8022 - val_loss: 93.6170\n",
      "Epoch 6976/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.3945 - val_loss: 93.5397\n",
      "Epoch 6977/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1977 - val_loss: 93.8052\n",
      "Epoch 6978/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.1474 - val_loss: 93.2890\n",
      "Epoch 6979/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.0853 - val_loss: 93.3413\n",
      "Epoch 6980/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.2346 - val_loss: 93.5017\n",
      "Epoch 6981/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1127 - val_loss: 93.2483\n",
      "Epoch 6982/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.8664 - val_loss: 93.6907\n",
      "Epoch 6983/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.8023 - val_loss: 93.5633\n",
      "Epoch 6984/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.4441 - val_loss: 92.7488\n",
      "Epoch 6985/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.2394 - val_loss: 93.1455\n",
      "Epoch 6986/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.4518 - val_loss: 93.5714\n",
      "Epoch 6987/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.8899 - val_loss: 93.5568\n",
      "Epoch 6988/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3719 - val_loss: 93.1323\n",
      "Epoch 6989/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5287 - val_loss: 93.4903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6990/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.6062 - val_loss: 93.5815\n",
      "Epoch 6991/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.4247 - val_loss: 92.7391\n",
      "Epoch 6992/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.4344 - val_loss: 94.2269\n",
      "Epoch 6993/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.3184 - val_loss: 92.7902\n",
      "Epoch 6994/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 91.2071 - val_loss: 93.6749\n",
      "Epoch 6995/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.4342 - val_loss: 93.1728\n",
      "Epoch 6996/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2494 - val_loss: 93.4379\n",
      "Epoch 6997/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5418 - val_loss: 93.1542\n",
      "Epoch 6998/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.0377 - val_loss: 93.5780\n",
      "Epoch 6999/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.4227 - val_loss: 93.1745\n",
      "Epoch 7000/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1491 - val_loss: 93.3060\n",
      "Epoch 7001/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4909 - val_loss: 93.7284\n",
      "Epoch 7002/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3736 - val_loss: 93.2489\n",
      "Epoch 7003/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 91.1655 - val_loss: 93.1342\n",
      "Epoch 7004/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.0659 - val_loss: 93.6562\n",
      "Epoch 7005/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.3733 - val_loss: 93.2171\n",
      "Epoch 7006/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.7468 - val_loss: 92.3464\n",
      "Epoch 7007/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1139 - val_loss: 93.6288\n",
      "Epoch 7008/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6017 - val_loss: 92.9894\n",
      "Epoch 7009/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.0668 - val_loss: 93.6743\n",
      "Epoch 7010/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4686 - val_loss: 93.4862\n",
      "Epoch 7011/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2913 - val_loss: 93.2494\n",
      "Epoch 7012/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.4382 - val_loss: 93.2815\n",
      "Epoch 7013/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4366 - val_loss: 92.9020\n",
      "Epoch 7014/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.0153 - val_loss: 93.2592\n",
      "Epoch 7015/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3073 - val_loss: 93.8921\n",
      "Epoch 7016/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8378 - val_loss: 93.6404\n",
      "Epoch 7017/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.5062 - val_loss: 93.5292\n",
      "Epoch 7018/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3940 - val_loss: 93.0448\n",
      "Epoch 7019/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.3341 - val_loss: 93.4676\n",
      "Epoch 7020/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.3621 - val_loss: 93.2151\n",
      "Epoch 7021/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4782 - val_loss: 93.3453\n",
      "Epoch 7022/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.5520 - val_loss: 93.4847\n",
      "Epoch 7023/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.1004 - val_loss: 93.2449\n",
      "Epoch 7024/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7217 - val_loss: 93.2449\n",
      "Epoch 7025/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.3833 - val_loss: 92.9663\n",
      "Epoch 7026/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.0976 - val_loss: 93.6640\n",
      "Epoch 7027/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7739 - val_loss: 93.4322\n",
      "Epoch 7028/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4366 - val_loss: 93.2076\n",
      "Epoch 7029/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.2847 - val_loss: 93.1807\n",
      "Epoch 7030/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.8191 - val_loss: 93.5251\n",
      "Epoch 7031/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.9680 - val_loss: 93.1502\n",
      "Epoch 7032/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3217 - val_loss: 93.7543\n",
      "Epoch 7033/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.3670 - val_loss: 93.2620\n",
      "Epoch 7034/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.7483 - val_loss: 93.7304\n",
      "Epoch 7035/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.7229 - val_loss: 93.6196\n",
      "Epoch 7036/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.6207 - val_loss: 93.3299\n",
      "Epoch 7037/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1872 - val_loss: 93.2308\n",
      "Epoch 7038/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7033 - val_loss: 93.4614\n",
      "Epoch 7039/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.4793 - val_loss: 93.3913\n",
      "Epoch 7040/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.7913 - val_loss: 93.3237\n",
      "Epoch 7041/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.1892 - val_loss: 93.6377\n",
      "Epoch 7042/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.2540 - val_loss: 93.3961\n",
      "Epoch 7043/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.0295 - val_loss: 94.0091\n",
      "Epoch 7044/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9493 - val_loss: 93.5206\n",
      "Epoch 7045/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.4447 - val_loss: 93.3332\n",
      "Epoch 7046/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.4161 - val_loss: 93.3210\n",
      "Epoch 7047/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8774 - val_loss: 93.1788\n",
      "Epoch 7048/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.6063 - val_loss: 93.4578\n",
      "Epoch 7049/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.5601 - val_loss: 93.3483\n",
      "Epoch 7050/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.4311 - val_loss: 93.2091\n",
      "Epoch 7051/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5068 - val_loss: 93.1149\n",
      "Epoch 7052/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1629 - val_loss: 93.4041\n",
      "Epoch 7053/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.5880 - val_loss: 93.1652\n",
      "Epoch 7054/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1061 - val_loss: 93.2757\n",
      "Epoch 7055/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.8597 - val_loss: 93.1252\n",
      "Epoch 7056/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.4557 - val_loss: 92.6094\n",
      "Epoch 7057/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.3727 - val_loss: 93.3241\n",
      "Epoch 7058/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.5272 - val_loss: 93.1303\n",
      "Epoch 7059/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.4556 - val_loss: 93.7351\n",
      "Epoch 7060/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.8713 - val_loss: 93.1674\n",
      "Epoch 7061/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2371 - val_loss: 93.6593\n",
      "Epoch 7062/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.0655 - val_loss: 93.1933\n",
      "Epoch 7063/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.2019 - val_loss: 93.4180\n",
      "Epoch 7064/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.4094 - val_loss: 92.8112\n",
      "Epoch 7065/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4892 - val_loss: 93.3453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7066/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.0829 - val_loss: 92.9111\n",
      "Epoch 7067/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2725 - val_loss: 93.2118\n",
      "Epoch 7068/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.6918 - val_loss: 93.2182\n",
      "Epoch 7069/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.1964 - val_loss: 93.3497\n",
      "Epoch 7070/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.2733 - val_loss: 93.4478\n",
      "Epoch 7071/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.9837 - val_loss: 93.3789\n",
      "Epoch 7072/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3388 - val_loss: 93.2498\n",
      "Epoch 7073/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.1843 - val_loss: 93.0951\n",
      "Epoch 7074/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.1438 - val_loss: 93.7574\n",
      "Epoch 7075/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.4705 - val_loss: 93.2103\n",
      "Epoch 7076/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.2006 - val_loss: 93.4949\n",
      "Epoch 7077/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.2139 - val_loss: 93.4989\n",
      "Epoch 7078/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.9961 - val_loss: 93.2895\n",
      "Epoch 7079/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 93.2988 - val_loss: 93.6094\n",
      "Epoch 7080/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0147 - val_loss: 93.5961\n",
      "Epoch 7081/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.0934 - val_loss: 93.4250\n",
      "Epoch 7082/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.0298 - val_loss: 93.1991\n",
      "Epoch 7083/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 92.5462 - val_loss: 93.3901\n",
      "Epoch 7084/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 91.1444 - val_loss: 93.6265\n",
      "Epoch 7085/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3845 - val_loss: 92.7951\n",
      "Epoch 7086/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 91.6049 - val_loss: 93.7770\n",
      "Epoch 7087/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.6301 - val_loss: 92.8371\n",
      "Epoch 7088/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.1485 - val_loss: 93.3671\n",
      "Epoch 7089/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.2606 - val_loss: 92.4646\n",
      "Epoch 7090/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7692 - val_loss: 94.0160\n",
      "Epoch 7091/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 92.5015 - val_loss: 92.7109\n",
      "Epoch 7092/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 93.1665 - val_loss: 93.4661\n",
      "Epoch 7093/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 90.7909 - val_loss: 93.3643\n",
      "Epoch 7094/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.8063 - val_loss: 93.3027\n",
      "Epoch 7095/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.7369 - val_loss: 93.6006\n",
      "Epoch 7096/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.5453 - val_loss: 92.7710\n",
      "Epoch 7097/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.5390 - val_loss: 92.7015\n",
      "Epoch 7098/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5481 - val_loss: 93.3454\n",
      "Epoch 7099/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.9441 - val_loss: 93.1679\n",
      "Epoch 7100/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.3852 - val_loss: 93.3124\n",
      "Epoch 7101/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 91.4085 - val_loss: 93.0286\n",
      "Epoch 7102/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.0948 - val_loss: 92.5786\n",
      "Epoch 7103/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.2616 - val_loss: 93.4678\n",
      "Epoch 7104/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.8732 - val_loss: 93.6642\n",
      "Epoch 7105/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.4369 - val_loss: 93.1871\n",
      "Epoch 7106/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.5974 - val_loss: 93.6366\n",
      "Epoch 7107/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.8374 - val_loss: 93.6008\n",
      "Epoch 7108/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.0880 - val_loss: 93.8486\n",
      "Epoch 7109/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.2320 - val_loss: 93.4368\n",
      "Epoch 7110/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2486 - val_loss: 93.9305\n",
      "Epoch 7111/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.9048 - val_loss: 93.1973\n",
      "Epoch 7112/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9392 - val_loss: 93.2838\n",
      "Epoch 7113/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2215 - val_loss: 93.2457\n",
      "Epoch 7114/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.8707 - val_loss: 92.8067\n",
      "Epoch 7115/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4292 - val_loss: 93.0489\n",
      "Epoch 7116/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.2678 - val_loss: 93.2449\n",
      "Epoch 7117/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.5712 - val_loss: 92.8631\n",
      "Epoch 7118/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.1156 - val_loss: 92.7897\n",
      "Epoch 7119/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5666 - val_loss: 93.5825\n",
      "Epoch 7120/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 90.9067 - val_loss: 93.0391\n",
      "Epoch 7121/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.7059 - val_loss: 93.6624\n",
      "Epoch 7122/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.8668 - val_loss: 93.0707\n",
      "Epoch 7123/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4767 - val_loss: 93.6718\n",
      "Epoch 7124/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.3956 - val_loss: 93.6072\n",
      "Epoch 7125/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.7901 - val_loss: 93.7757\n",
      "Epoch 7126/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.7198 - val_loss: 93.2990\n",
      "Epoch 7127/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.4968 - val_loss: 93.3114\n",
      "Epoch 7128/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6535 - val_loss: 93.3408\n",
      "Epoch 7129/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.3156 - val_loss: 93.6321\n",
      "Epoch 7130/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1864 - val_loss: 92.9443\n",
      "Epoch 7131/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.6989 - val_loss: 93.4491\n",
      "Epoch 7132/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.8860 - val_loss: 93.4476\n",
      "Epoch 7133/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.1994 - val_loss: 94.2725\n",
      "Epoch 7134/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.7274 - val_loss: 93.0412\n",
      "Epoch 7135/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.0418 - val_loss: 93.1040\n",
      "Epoch 7136/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5613 - val_loss: 93.1637\n",
      "Epoch 7137/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.7128 - val_loss: 92.8236\n",
      "Epoch 7138/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5486 - val_loss: 93.3743\n",
      "Epoch 7139/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.4775 - val_loss: 93.3703\n",
      "Epoch 7140/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.4764 - val_loss: 93.5341\n",
      "Epoch 7141/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 90.4410 - val_loss: 93.2346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7142/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.8720 - val_loss: 93.5654\n",
      "Epoch 7143/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.4314 - val_loss: 93.8424\n",
      "Epoch 7144/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.5586 - val_loss: 93.4045\n",
      "Epoch 7145/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.5105 - val_loss: 93.2246\n",
      "Epoch 7146/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8384 - val_loss: 93.1409\n",
      "Epoch 7147/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.7058 - val_loss: 93.3586\n",
      "Epoch 7148/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1082 - val_loss: 93.2062\n",
      "Epoch 7149/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.5279 - val_loss: 92.9789\n",
      "Epoch 7150/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3635 - val_loss: 92.9854\n",
      "Epoch 7151/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.7273 - val_loss: 93.4643\n",
      "Epoch 7152/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.5583 - val_loss: 93.4317\n",
      "Epoch 7153/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.7132 - val_loss: 93.3986\n",
      "Epoch 7154/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.5763 - val_loss: 94.0658\n",
      "Epoch 7155/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1919 - val_loss: 93.0119\n",
      "Epoch 7156/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1835 - val_loss: 92.8646\n",
      "Epoch 7157/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.2004 - val_loss: 92.7042\n",
      "Epoch 7158/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.9568 - val_loss: 93.5118\n",
      "Epoch 7159/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.7452 - val_loss: 93.5390\n",
      "Epoch 7160/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.7676 - val_loss: 93.5238\n",
      "Epoch 7161/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8612 - val_loss: 93.1957\n",
      "Epoch 7162/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.8018 - val_loss: 93.6023\n",
      "Epoch 7163/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.8659 - val_loss: 92.4336\n",
      "Epoch 7164/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.4659 - val_loss: 93.4836\n",
      "Epoch 7165/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.0449 - val_loss: 93.1370\n",
      "Epoch 7166/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.5816 - val_loss: 92.7918\n",
      "Epoch 7167/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2936 - val_loss: 93.3981\n",
      "Epoch 7168/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.1595 - val_loss: 93.3286\n",
      "Epoch 7169/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.8233 - val_loss: 93.4910\n",
      "Epoch 7170/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5181 - val_loss: 93.1490\n",
      "Epoch 7171/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2644 - val_loss: 92.8730\n",
      "Epoch 7172/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.2271 - val_loss: 93.3382\n",
      "Epoch 7173/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4708 - val_loss: 93.2802\n",
      "Epoch 7174/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.7212 - val_loss: 93.4808\n",
      "Epoch 7175/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 90.8930 - val_loss: 93.4968\n",
      "Epoch 7176/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8762 - val_loss: 92.5195\n",
      "Epoch 7177/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2409 - val_loss: 93.4276\n",
      "Epoch 7178/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2722 - val_loss: 93.1524\n",
      "Epoch 7179/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.6952 - val_loss: 93.0048\n",
      "Epoch 7180/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.4892 - val_loss: 92.9037\n",
      "Epoch 7181/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.2373 - val_loss: 93.9220\n",
      "Epoch 7182/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2541 - val_loss: 93.1133\n",
      "Epoch 7183/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.2198 - val_loss: 93.1514\n",
      "Epoch 7184/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2985 - val_loss: 93.4461\n",
      "Epoch 7185/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.5855 - val_loss: 93.0565\n",
      "Epoch 7186/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0453 - val_loss: 93.1253\n",
      "Epoch 7187/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.3156 - val_loss: 93.1872\n",
      "Epoch 7188/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6491 - val_loss: 92.7273\n",
      "Epoch 7189/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.0048 - val_loss: 93.3277\n",
      "Epoch 7190/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.8168 - val_loss: 93.5092\n",
      "Epoch 7191/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.6816 - val_loss: 92.7749\n",
      "Epoch 7192/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.6869 - val_loss: 93.2115\n",
      "Epoch 7193/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.9265 - val_loss: 93.0537\n",
      "Epoch 7194/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3409 - val_loss: 93.6272\n",
      "Epoch 7195/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.9993 - val_loss: 93.9351\n",
      "Epoch 7196/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.5288 - val_loss: 94.0183\n",
      "Epoch 7197/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.8202 - val_loss: 93.3667\n",
      "Epoch 7198/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.4872 - val_loss: 93.7191\n",
      "Epoch 7199/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.1440 - val_loss: 92.8495\n",
      "Epoch 7200/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7488 - val_loss: 93.5484\n",
      "Epoch 7201/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.9329 - val_loss: 93.0650\n",
      "Epoch 7202/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.4118 - val_loss: 93.3877\n",
      "Epoch 7203/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.5262 - val_loss: 92.9773\n",
      "Epoch 7204/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.9509 - val_loss: 93.7175\n",
      "Epoch 7205/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.3419 - val_loss: 93.3232\n",
      "Epoch 7206/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.7208 - val_loss: 93.2239\n",
      "Epoch 7207/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2086 - val_loss: 92.9545\n",
      "Epoch 7208/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2030 - val_loss: 93.5675\n",
      "Epoch 7209/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.6774 - val_loss: 93.6477\n",
      "Epoch 7210/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8560 - val_loss: 93.5014\n",
      "Epoch 7211/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.0650 - val_loss: 92.7948\n",
      "Epoch 7212/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.4616 - val_loss: 92.7372\n",
      "Epoch 7213/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.1373 - val_loss: 93.0609\n",
      "Epoch 7214/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5426 - val_loss: 92.4245\n",
      "Epoch 7215/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.5145 - val_loss: 93.5826\n",
      "Epoch 7216/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.5425 - val_loss: 93.1860\n",
      "Epoch 7217/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 90.4515 - val_loss: 93.7560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7218/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.5932 - val_loss: 92.9446\n",
      "Epoch 7219/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3522 - val_loss: 93.2599\n",
      "Epoch 7220/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.2573 - val_loss: 93.0674\n",
      "Epoch 7221/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.0086 - val_loss: 93.6009\n",
      "Epoch 7222/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.2456 - val_loss: 93.2310\n",
      "Epoch 7223/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.7217 - val_loss: 93.7330\n",
      "Epoch 7224/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0407 - val_loss: 93.1868\n",
      "Epoch 7225/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.9699 - val_loss: 93.2057\n",
      "Epoch 7226/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3957 - val_loss: 93.3557\n",
      "Epoch 7227/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.1621 - val_loss: 93.3333\n",
      "Epoch 7228/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7916 - val_loss: 93.5371\n",
      "Epoch 7229/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.2110 - val_loss: 92.4830\n",
      "Epoch 7230/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.3014 - val_loss: 93.0987\n",
      "Epoch 7231/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.4016 - val_loss: 92.7425\n",
      "Epoch 7232/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.7947 - val_loss: 93.2816\n",
      "Epoch 7233/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.0758 - val_loss: 93.6593\n",
      "Epoch 7234/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.5625 - val_loss: 93.1145\n",
      "Epoch 7235/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.2877 - val_loss: 92.7301\n",
      "Epoch 7236/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2873 - val_loss: 93.5669\n",
      "Epoch 7237/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3930 - val_loss: 93.3577\n",
      "Epoch 7238/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.6619 - val_loss: 92.8680\n",
      "Epoch 7239/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.3574 - val_loss: 93.1441\n",
      "Epoch 7240/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8924 - val_loss: 93.6126\n",
      "Epoch 7241/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.9043 - val_loss: 93.3511\n",
      "Epoch 7242/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2044 - val_loss: 93.3431\n",
      "Epoch 7243/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.4610 - val_loss: 93.1634\n",
      "Epoch 7244/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.7355 - val_loss: 93.4432\n",
      "Epoch 7245/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.2148 - val_loss: 92.7356\n",
      "Epoch 7246/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.7841 - val_loss: 92.9431\n",
      "Epoch 7247/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.6886 - val_loss: 93.0802\n",
      "Epoch 7248/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.1955 - val_loss: 93.0288\n",
      "Epoch 7249/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.7671 - val_loss: 93.1730\n",
      "Epoch 7250/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.3730 - val_loss: 93.1720\n",
      "Epoch 7251/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.0988 - val_loss: 93.6025\n",
      "Epoch 7252/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6066 - val_loss: 93.4505\n",
      "Epoch 7253/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3954 - val_loss: 93.5793\n",
      "Epoch 7254/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.2405 - val_loss: 93.5714\n",
      "Epoch 7255/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1691 - val_loss: 93.4776\n",
      "Epoch 7256/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.1368 - val_loss: 93.6587\n",
      "Epoch 7257/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2860 - val_loss: 93.1888\n",
      "Epoch 7258/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.7843 - val_loss: 92.8414\n",
      "Epoch 7259/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.3734 - val_loss: 93.5693\n",
      "Epoch 7260/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.7262 - val_loss: 93.9733\n",
      "Epoch 7261/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.8157 - val_loss: 93.2654\n",
      "Epoch 7262/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.7524 - val_loss: 93.6683\n",
      "Epoch 7263/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5581 - val_loss: 93.1822\n",
      "Epoch 7264/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.2979 - val_loss: 93.0646\n",
      "Epoch 7265/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6024 - val_loss: 93.4444\n",
      "Epoch 7266/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.5575 - val_loss: 92.8657\n",
      "Epoch 7267/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.5841 - val_loss: 93.3779\n",
      "Epoch 7268/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1114 - val_loss: 93.2132\n",
      "Epoch 7269/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.1141 - val_loss: 93.5583\n",
      "Epoch 7270/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.6826 - val_loss: 93.2169\n",
      "Epoch 7271/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.7153 - val_loss: 92.7281\n",
      "Epoch 7272/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.2212 - val_loss: 93.5721\n",
      "Epoch 7273/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.0897 - val_loss: 93.5504\n",
      "Epoch 7274/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.5068 - val_loss: 93.1886\n",
      "Epoch 7275/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.5678 - val_loss: 93.2047\n",
      "Epoch 7276/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 91.4116 - val_loss: 93.2526\n",
      "Epoch 7277/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.9279 - val_loss: 93.0617\n",
      "Epoch 7278/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.1725 - val_loss: 93.1841\n",
      "Epoch 7279/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 91.3468 - val_loss: 93.1125\n",
      "Epoch 7280/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.2209 - val_loss: 92.9811\n",
      "Epoch 7281/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.0022 - val_loss: 93.6372\n",
      "Epoch 7282/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.4007 - val_loss: 93.6323\n",
      "Epoch 7283/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.6852 - val_loss: 93.0949\n",
      "Epoch 7284/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.1573 - val_loss: 93.0866\n",
      "Epoch 7285/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.8881 - val_loss: 93.1813\n",
      "Epoch 7286/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.2989 - val_loss: 92.3342\n",
      "Epoch 7287/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.9719 - val_loss: 93.5979\n",
      "Epoch 7288/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.3705 - val_loss: 93.3951\n",
      "Epoch 7289/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.6400 - val_loss: 93.4296\n",
      "Epoch 7290/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.7666 - val_loss: 93.3670\n",
      "Epoch 7291/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3030 - val_loss: 93.4886\n",
      "Epoch 7292/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6133 - val_loss: 92.9857\n",
      "Epoch 7293/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.4201 - val_loss: 93.2166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7294/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.8129 - val_loss: 93.1089\n",
      "Epoch 7295/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5349 - val_loss: 93.3505\n",
      "Epoch 7296/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6333 - val_loss: 93.5145\n",
      "Epoch 7297/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.5914 - val_loss: 92.9242\n",
      "Epoch 7298/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.4316 - val_loss: 93.0487\n",
      "Epoch 7299/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.0342 - val_loss: 93.0397\n",
      "Epoch 7300/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.9616 - val_loss: 93.9277\n",
      "Epoch 7301/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.8746 - val_loss: 92.7186\n",
      "Epoch 7302/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1592 - val_loss: 93.4014\n",
      "Epoch 7303/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.7716 - val_loss: 93.5822\n",
      "Epoch 7304/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1515 - val_loss: 93.2553\n",
      "Epoch 7305/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.5946 - val_loss: 93.2501\n",
      "Epoch 7306/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.5591 - val_loss: 93.6689\n",
      "Epoch 7307/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5472 - val_loss: 93.3729\n",
      "Epoch 7308/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.8839 - val_loss: 93.4750\n",
      "Epoch 7309/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6089 - val_loss: 93.0078\n",
      "Epoch 7310/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.7004 - val_loss: 93.7365\n",
      "Epoch 7311/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 89.9978 - val_loss: 93.4694\n",
      "Epoch 7312/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.0178 - val_loss: 92.9693\n",
      "Epoch 7313/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 90.6588 - val_loss: 93.4463\n",
      "Epoch 7314/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6843 - val_loss: 93.4842\n",
      "Epoch 7315/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.2815 - val_loss: 93.3251\n",
      "Epoch 7316/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.7585 - val_loss: 93.1913\n",
      "Epoch 7317/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.6180 - val_loss: 93.1977\n",
      "Epoch 7318/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.7725 - val_loss: 93.3189\n",
      "Epoch 7319/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.7587 - val_loss: 93.6866\n",
      "Epoch 7320/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.7384 - val_loss: 93.5505\n",
      "Epoch 7321/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.4676 - val_loss: 93.2188\n",
      "Epoch 7322/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 90.6118 - val_loss: 93.7588\n",
      "Epoch 7323/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8307 - val_loss: 93.2926\n",
      "Epoch 7324/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.9598 - val_loss: 93.3798\n",
      "Epoch 7325/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.6454 - val_loss: 92.7824\n",
      "Epoch 7326/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1835 - val_loss: 93.2597\n",
      "Epoch 7327/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.7146 - val_loss: 93.3972\n",
      "Epoch 7328/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.5737 - val_loss: 92.9029\n",
      "Epoch 7329/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.8662 - val_loss: 93.5063\n",
      "Epoch 7330/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 90.6581 - val_loss: 93.4271\n",
      "Epoch 7331/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.7821 - val_loss: 93.5555\n",
      "Epoch 7332/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.0296 - val_loss: 93.0914\n",
      "Epoch 7333/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.1634 - val_loss: 93.4830\n",
      "Epoch 7334/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.2034 - val_loss: 93.0929\n",
      "Epoch 7335/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7850 - val_loss: 92.8894\n",
      "Epoch 7336/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3180 - val_loss: 93.2673\n",
      "Epoch 7337/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.7651 - val_loss: 92.8576\n",
      "Epoch 7338/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.3839 - val_loss: 92.7393\n",
      "Epoch 7339/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.6785 - val_loss: 92.9726\n",
      "Epoch 7340/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9737 - val_loss: 93.2353\n",
      "Epoch 7341/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.4768 - val_loss: 93.1297\n",
      "Epoch 7342/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6411 - val_loss: 93.3393\n",
      "Epoch 7343/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8072 - val_loss: 93.2228\n",
      "Epoch 7344/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.4480 - val_loss: 92.4497\n",
      "Epoch 7345/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.4218 - val_loss: 93.5057\n",
      "Epoch 7346/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.4269 - val_loss: 93.6600\n",
      "Epoch 7347/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 93.9751 - val_loss: 93.2064\n",
      "Epoch 7348/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.0708 - val_loss: 93.5196\n",
      "Epoch 7349/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1169 - val_loss: 93.0643\n",
      "Epoch 7350/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3913 - val_loss: 93.2728\n",
      "Epoch 7351/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.4575 - val_loss: 93.4319\n",
      "Epoch 7352/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 90.9645 - val_loss: 92.4699\n",
      "Epoch 7353/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2679 - val_loss: 93.3700\n",
      "Epoch 7354/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.6448 - val_loss: 93.3538\n",
      "Epoch 7355/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.1964 - val_loss: 92.8510\n",
      "Epoch 7356/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.7698 - val_loss: 93.4641\n",
      "Epoch 7357/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.7996 - val_loss: 93.6526\n",
      "Epoch 7358/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.0876 - val_loss: 93.0173\n",
      "Epoch 7359/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2414 - val_loss: 93.1139\n",
      "Epoch 7360/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.2422 - val_loss: 93.2571\n",
      "Epoch 7361/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5298 - val_loss: 93.3653\n",
      "Epoch 7362/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.7362 - val_loss: 93.5933\n",
      "Epoch 7363/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.5130 - val_loss: 92.6822\n",
      "Epoch 7364/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.7565 - val_loss: 92.7233\n",
      "Epoch 7365/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5716 - val_loss: 93.1084\n",
      "Epoch 7366/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.1595 - val_loss: 92.9932\n",
      "Epoch 7367/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.9399 - val_loss: 93.0012\n",
      "Epoch 7368/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2525 - val_loss: 92.7316\n",
      "Epoch 7369/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.0918 - val_loss: 92.4941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7370/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.7209 - val_loss: 92.5955\n",
      "Epoch 7371/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8010 - val_loss: 93.0172\n",
      "Epoch 7372/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4357 - val_loss: 93.4661\n",
      "Epoch 7373/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.8607 - val_loss: 93.4725\n",
      "Epoch 7374/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.9715 - val_loss: 93.0488\n",
      "Epoch 7375/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.4803 - val_loss: 93.0157\n",
      "Epoch 7376/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6858 - val_loss: 93.1658\n",
      "Epoch 7377/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7315 - val_loss: 93.3615\n",
      "Epoch 7378/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8426 - val_loss: 92.6768\n",
      "Epoch 7379/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0139 - val_loss: 93.1951\n",
      "Epoch 7380/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8223 - val_loss: 92.9592\n",
      "Epoch 7381/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8979 - val_loss: 93.0389\n",
      "Epoch 7382/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.5845 - val_loss: 93.4473\n",
      "Epoch 7383/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.1434 - val_loss: 92.9963\n",
      "Epoch 7384/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.2343 - val_loss: 92.6610\n",
      "Epoch 7385/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.2915 - val_loss: 92.8868\n",
      "Epoch 7386/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.8289 - val_loss: 93.0295\n",
      "Epoch 7387/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.1528 - val_loss: 93.1554\n",
      "Epoch 7388/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.0476 - val_loss: 92.8148\n",
      "Epoch 7389/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.2405 - val_loss: 93.2678\n",
      "Epoch 7390/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.3082 - val_loss: 93.8762\n",
      "Epoch 7391/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.6390 - val_loss: 93.2208\n",
      "Epoch 7392/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8110 - val_loss: 93.0048\n",
      "Epoch 7393/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.1636 - val_loss: 92.8378\n",
      "Epoch 7394/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.8647 - val_loss: 93.4066\n",
      "Epoch 7395/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.2166 - val_loss: 92.8156\n",
      "Epoch 7396/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.7519 - val_loss: 92.8705\n",
      "Epoch 7397/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.3046 - val_loss: 92.8085\n",
      "Epoch 7398/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.6854 - val_loss: 93.1962\n",
      "Epoch 7399/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5575 - val_loss: 93.2918\n",
      "Epoch 7400/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.2290 - val_loss: 92.8559\n",
      "Epoch 7401/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2738 - val_loss: 93.0835\n",
      "Epoch 7402/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2494 - val_loss: 92.7194\n",
      "Epoch 7403/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.6264 - val_loss: 93.8543\n",
      "Epoch 7404/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.2564 - val_loss: 94.0766\n",
      "Epoch 7405/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.6854 - val_loss: 92.6774\n",
      "Epoch 7406/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9262 - val_loss: 92.8214\n",
      "Epoch 7407/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.9304 - val_loss: 93.0435\n",
      "Epoch 7408/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.0031 - val_loss: 92.9173\n",
      "Epoch 7409/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.5757 - val_loss: 93.1582\n",
      "Epoch 7410/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.9559 - val_loss: 92.5262\n",
      "Epoch 7411/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8022 - val_loss: 93.2072\n",
      "Epoch 7412/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8026 - val_loss: 93.1762\n",
      "Epoch 7413/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8460 - val_loss: 93.1466\n",
      "Epoch 7414/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.8242 - val_loss: 93.4070\n",
      "Epoch 7415/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.8005 - val_loss: 92.7455\n",
      "Epoch 7416/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.0132 - val_loss: 93.4625\n",
      "Epoch 7417/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.4217 - val_loss: 93.5303\n",
      "Epoch 7418/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8589 - val_loss: 93.1015\n",
      "Epoch 7419/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.7622 - val_loss: 92.8619\n",
      "Epoch 7420/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.6910 - val_loss: 93.1655\n",
      "Epoch 7421/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4010 - val_loss: 93.1146\n",
      "Epoch 7422/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.8426 - val_loss: 93.2390\n",
      "Epoch 7423/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.3409 - val_loss: 92.7668\n",
      "Epoch 7424/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.7945 - val_loss: 93.5546\n",
      "Epoch 7425/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.7502 - val_loss: 93.5103\n",
      "Epoch 7426/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.4148 - val_loss: 93.1060\n",
      "Epoch 7427/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.6000 - val_loss: 92.7002\n",
      "Epoch 7428/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.8152 - val_loss: 93.0923\n",
      "Epoch 7429/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.5464 - val_loss: 93.2712\n",
      "Epoch 7430/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.2600 - val_loss: 93.2999\n",
      "Epoch 7431/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.6631 - val_loss: 93.0474\n",
      "Epoch 7432/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.7193 - val_loss: 92.9731\n",
      "Epoch 7433/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.9309 - val_loss: 93.2198\n",
      "Epoch 7434/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1620 - val_loss: 93.3042\n",
      "Epoch 7435/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.7952 - val_loss: 92.9691\n",
      "Epoch 7436/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.5019 - val_loss: 92.9720\n",
      "Epoch 7437/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.6254 - val_loss: 92.9123\n",
      "Epoch 7438/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7591 - val_loss: 93.2313\n",
      "Epoch 7439/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4437 - val_loss: 92.6231\n",
      "Epoch 7440/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.7884 - val_loss: 92.6240\n",
      "Epoch 7441/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8340 - val_loss: 93.5778\n",
      "Epoch 7442/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1572 - val_loss: 93.0001\n",
      "Epoch 7443/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.6781 - val_loss: 93.2383\n",
      "Epoch 7444/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0643 - val_loss: 92.2830\n",
      "Epoch 7445/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.8383 - val_loss: 92.3512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7446/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.0653 - val_loss: 93.2685\n",
      "Epoch 7447/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.4503 - val_loss: 93.3490\n",
      "Epoch 7448/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8162 - val_loss: 93.1664\n",
      "Epoch 7449/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8696 - val_loss: 93.3098\n",
      "Epoch 7450/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.1130 - val_loss: 93.3733\n",
      "Epoch 7451/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.5188 - val_loss: 93.1475\n",
      "Epoch 7452/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3838 - val_loss: 93.2098\n",
      "Epoch 7453/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5753 - val_loss: 93.1685\n",
      "Epoch 7454/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.8293 - val_loss: 93.4998\n",
      "Epoch 7455/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 91.5050 - val_loss: 93.1671\n",
      "Epoch 7456/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6101 - val_loss: 93.2515\n",
      "Epoch 7457/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5927 - val_loss: 93.4489\n",
      "Epoch 7458/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.7371 - val_loss: 93.0619\n",
      "Epoch 7459/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.4521 - val_loss: 93.3722\n",
      "Epoch 7460/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2857 - val_loss: 92.4106\n",
      "Epoch 7461/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.3454 - val_loss: 93.0199\n",
      "Epoch 7462/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.4079 - val_loss: 93.4689\n",
      "Epoch 7463/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.8975 - val_loss: 93.6674\n",
      "Epoch 7464/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.9400 - val_loss: 92.7511\n",
      "Epoch 7465/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.0291 - val_loss: 92.5487\n",
      "Epoch 7466/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.7330 - val_loss: 93.3700\n",
      "Epoch 7467/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.1753 - val_loss: 93.1447\n",
      "Epoch 7468/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.3559 - val_loss: 92.8923\n",
      "Epoch 7469/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.6408 - val_loss: 92.8403\n",
      "Epoch 7470/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.4211 - val_loss: 92.9485\n",
      "Epoch 7471/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.4196 - val_loss: 93.0969\n",
      "Epoch 7472/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1699 - val_loss: 93.0723\n",
      "Epoch 7473/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3110 - val_loss: 93.3592\n",
      "Epoch 7474/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.3729 - val_loss: 92.3381\n",
      "Epoch 7475/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.8406 - val_loss: 93.2095\n",
      "Epoch 7476/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.0134 - val_loss: 92.7554\n",
      "Epoch 7477/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.7680 - val_loss: 93.4049\n",
      "Epoch 7478/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.7467 - val_loss: 92.7214\n",
      "Epoch 7479/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6239 - val_loss: 93.6520\n",
      "Epoch 7480/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.6533 - val_loss: 92.5748\n",
      "Epoch 7481/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.6583 - val_loss: 93.0264\n",
      "Epoch 7482/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.6255 - val_loss: 92.9993\n",
      "Epoch 7483/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.2830 - val_loss: 93.7912\n",
      "Epoch 7484/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.6833 - val_loss: 92.5051\n",
      "Epoch 7485/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.9669 - val_loss: 93.3045\n",
      "Epoch 7486/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.4585 - val_loss: 92.9640\n",
      "Epoch 7487/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.8720 - val_loss: 93.0313\n",
      "Epoch 7488/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.6808 - val_loss: 93.4771\n",
      "Epoch 7489/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8162 - val_loss: 93.0639\n",
      "Epoch 7490/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.0831 - val_loss: 92.6520\n",
      "Epoch 7491/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.2721 - val_loss: 93.3848\n",
      "Epoch 7492/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 90.9846 - val_loss: 92.8866\n",
      "Epoch 7493/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.7555 - val_loss: 93.1851\n",
      "Epoch 7494/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 90.4475 - val_loss: 92.6025\n",
      "Epoch 7495/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.5075 - val_loss: 93.1647\n",
      "Epoch 7496/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.6388 - val_loss: 93.1155\n",
      "Epoch 7497/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.0541 - val_loss: 93.1287\n",
      "Epoch 7498/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.7741 - val_loss: 93.0979\n",
      "Epoch 7499/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.8630 - val_loss: 94.0404\n",
      "Epoch 7500/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2322 - val_loss: 92.8141\n",
      "Epoch 7501/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.8657 - val_loss: 93.2746\n",
      "Epoch 7502/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.8192 - val_loss: 93.3172\n",
      "Epoch 7503/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5822 - val_loss: 92.6343\n",
      "Epoch 7504/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.1972 - val_loss: 92.6412\n",
      "Epoch 7505/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.7955 - val_loss: 92.2289\n",
      "Epoch 7506/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.5886 - val_loss: 92.7427\n",
      "Epoch 7507/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.4787 - val_loss: 93.1094\n",
      "Epoch 7508/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.5388 - val_loss: 93.1009\n",
      "Epoch 7509/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.3602 - val_loss: 93.2273\n",
      "Epoch 7510/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.4561 - val_loss: 93.1097\n",
      "Epoch 7511/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8046 - val_loss: 92.9357\n",
      "Epoch 7512/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5427 - val_loss: 92.7820\n",
      "Epoch 7513/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.6292 - val_loss: 93.6593\n",
      "Epoch 7514/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.4343 - val_loss: 93.1037\n",
      "Epoch 7515/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.9788 - val_loss: 93.4236\n",
      "Epoch 7516/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6086 - val_loss: 93.1738\n",
      "Epoch 7517/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.2133 - val_loss: 93.1733\n",
      "Epoch 7518/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.8469 - val_loss: 92.9328\n",
      "Epoch 7519/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.9517 - val_loss: 92.5387\n",
      "Epoch 7520/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.7082 - val_loss: 93.0039\n",
      "Epoch 7521/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.0496 - val_loss: 92.8669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7522/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5351 - val_loss: 92.8976\n",
      "Epoch 7523/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.4749 - val_loss: 93.1908\n",
      "Epoch 7524/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.5705 - val_loss: 93.2098\n",
      "Epoch 7525/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1869 - val_loss: 92.8253\n",
      "Epoch 7526/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7723 - val_loss: 93.1616\n",
      "Epoch 7527/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.0608 - val_loss: 92.8889\n",
      "Epoch 7528/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.0628 - val_loss: 92.3734\n",
      "Epoch 7529/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1354 - val_loss: 92.4043\n",
      "Epoch 7530/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4765 - val_loss: 92.8442\n",
      "Epoch 7531/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.7269 - val_loss: 93.1310\n",
      "Epoch 7532/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.9479 - val_loss: 92.8892\n",
      "Epoch 7533/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.9079 - val_loss: 93.5338\n",
      "Epoch 7534/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.0195 - val_loss: 93.5570\n",
      "Epoch 7535/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9214 - val_loss: 93.5778\n",
      "Epoch 7536/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5054 - val_loss: 92.9606\n",
      "Epoch 7537/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6041 - val_loss: 93.0931\n",
      "Epoch 7538/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.7895 - val_loss: 93.2135\n",
      "Epoch 7539/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 91.1409 - val_loss: 92.8630\n",
      "Epoch 7540/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.5406 - val_loss: 93.0556\n",
      "Epoch 7541/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.5690 - val_loss: 92.5826\n",
      "Epoch 7542/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.4019 - val_loss: 92.9966\n",
      "Epoch 7543/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.3006 - val_loss: 92.9773\n",
      "Epoch 7544/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6670 - val_loss: 93.8296\n",
      "Epoch 7545/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.5516 - val_loss: 93.7585\n",
      "Epoch 7546/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.5274 - val_loss: 93.5259\n",
      "Epoch 7547/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1941 - val_loss: 92.6621\n",
      "Epoch 7548/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.2698 - val_loss: 92.9127\n",
      "Epoch 7549/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.0663 - val_loss: 93.4406\n",
      "Epoch 7550/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.9439 - val_loss: 92.7421\n",
      "Epoch 7551/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4819 - val_loss: 93.0145\n",
      "Epoch 7552/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.3289 - val_loss: 93.0129\n",
      "Epoch 7553/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1620 - val_loss: 92.9985\n",
      "Epoch 7554/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.4253 - val_loss: 92.8415\n",
      "Epoch 7555/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.1317 - val_loss: 93.3683\n",
      "Epoch 7556/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.0151 - val_loss: 93.5007\n",
      "Epoch 7557/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.9599 - val_loss: 93.5630\n",
      "Epoch 7558/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3195 - val_loss: 93.3635\n",
      "Epoch 7559/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5908 - val_loss: 93.0289\n",
      "Epoch 7560/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.7479 - val_loss: 93.0335\n",
      "Epoch 7561/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.5308 - val_loss: 93.5034\n",
      "Epoch 7562/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.0293 - val_loss: 93.7278\n",
      "Epoch 7563/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.9493 - val_loss: 93.1551\n",
      "Epoch 7564/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.1509 - val_loss: 93.6417\n",
      "Epoch 7565/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.9171 - val_loss: 92.9873\n",
      "Epoch 7566/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.5164 - val_loss: 93.1673\n",
      "Epoch 7567/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7127 - val_loss: 92.3587\n",
      "Epoch 7568/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.2881 - val_loss: 93.8931\n",
      "Epoch 7569/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.4671 - val_loss: 93.0349\n",
      "Epoch 7570/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.0101 - val_loss: 93.2283\n",
      "Epoch 7571/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.1550 - val_loss: 93.1824\n",
      "Epoch 7572/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.0536 - val_loss: 92.9556\n",
      "Epoch 7573/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.0266 - val_loss: 93.3022\n",
      "Epoch 7574/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.3241 - val_loss: 93.2490\n",
      "Epoch 7575/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.2102 - val_loss: 93.0730\n",
      "Epoch 7576/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.4601 - val_loss: 93.4094\n",
      "Epoch 7577/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.1863 - val_loss: 93.0622\n",
      "Epoch 7578/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.4647 - val_loss: 92.4987\n",
      "Epoch 7579/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.8043 - val_loss: 92.9426\n",
      "Epoch 7580/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.0459 - val_loss: 93.2675\n",
      "Epoch 7581/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3257 - val_loss: 92.8205\n",
      "Epoch 7582/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5830 - val_loss: 92.8540\n",
      "Epoch 7583/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.3447 - val_loss: 93.0514\n",
      "Epoch 7584/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8494 - val_loss: 93.4731\n",
      "Epoch 7585/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3024 - val_loss: 93.7210\n",
      "Epoch 7586/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6949 - val_loss: 93.2678\n",
      "Epoch 7587/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.5522 - val_loss: 92.7810\n",
      "Epoch 7588/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2468 - val_loss: 93.6165\n",
      "Epoch 7589/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.6012 - val_loss: 93.2300\n",
      "Epoch 7590/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.9191 - val_loss: 92.7945\n",
      "Epoch 7591/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3259 - val_loss: 92.6989\n",
      "Epoch 7592/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.2628 - val_loss: 92.9533\n",
      "Epoch 7593/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.9769 - val_loss: 93.1055\n",
      "Epoch 7594/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.8853 - val_loss: 92.9654\n",
      "Epoch 7595/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.8229 - val_loss: 93.1355\n",
      "Epoch 7596/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.7045 - val_loss: 92.9073\n",
      "Epoch 7597/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2359 - val_loss: 92.3418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7598/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.0921 - val_loss: 93.0814\n",
      "Epoch 7599/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 91.2363 - val_loss: 94.2052\n",
      "Epoch 7600/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6282 - val_loss: 93.0755\n",
      "Epoch 7601/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.8462 - val_loss: 92.7132\n",
      "Epoch 7602/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.9892 - val_loss: 94.0367\n",
      "Epoch 7603/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.7523 - val_loss: 93.9343\n",
      "Epoch 7604/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.0738 - val_loss: 93.1971\n",
      "Epoch 7605/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.2173 - val_loss: 93.0889\n",
      "Epoch 7606/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6960 - val_loss: 92.8452\n",
      "Epoch 7607/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1238 - val_loss: 93.0503\n",
      "Epoch 7608/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.0909 - val_loss: 93.6488\n",
      "Epoch 7609/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.8088 - val_loss: 92.5609\n",
      "Epoch 7610/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 93.6208 - val_loss: 93.1552\n",
      "Epoch 7611/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.5270 - val_loss: 93.0068\n",
      "Epoch 7612/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7558 - val_loss: 93.2646\n",
      "Epoch 7613/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.7483 - val_loss: 92.7932\n",
      "Epoch 7614/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.8889 - val_loss: 92.7603\n",
      "Epoch 7615/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7796 - val_loss: 92.6568\n",
      "Epoch 7616/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.7899 - val_loss: 93.2726\n",
      "Epoch 7617/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.7788 - val_loss: 93.3613\n",
      "Epoch 7618/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.0852 - val_loss: 93.1033\n",
      "Epoch 7619/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.9131 - val_loss: 93.2374\n",
      "Epoch 7620/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6001 - val_loss: 92.9678\n",
      "Epoch 7621/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2590 - val_loss: 93.3381\n",
      "Epoch 7622/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.6571 - val_loss: 93.1753\n",
      "Epoch 7623/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.4952 - val_loss: 94.1678\n",
      "Epoch 7624/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7704 - val_loss: 93.0984\n",
      "Epoch 7625/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.0287 - val_loss: 92.3729\n",
      "Epoch 7626/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.5351 - val_loss: 92.7469\n",
      "Epoch 7627/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.5410 - val_loss: 93.0261\n",
      "Epoch 7628/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3119 - val_loss: 93.1213\n",
      "Epoch 7629/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.1248 - val_loss: 93.9221\n",
      "Epoch 7630/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.5006 - val_loss: 93.0384\n",
      "Epoch 7631/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1667 - val_loss: 93.2349\n",
      "Epoch 7632/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.2550 - val_loss: 92.4824\n",
      "Epoch 7633/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4489 - val_loss: 93.3099\n",
      "Epoch 7634/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9918 - val_loss: 93.2421\n",
      "Epoch 7635/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.0768 - val_loss: 93.7274\n",
      "Epoch 7636/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2716 - val_loss: 93.3102\n",
      "Epoch 7637/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.3156 - val_loss: 93.6751\n",
      "Epoch 7638/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.0549 - val_loss: 92.6632\n",
      "Epoch 7639/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8442 - val_loss: 92.5999\n",
      "Epoch 7640/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.5669 - val_loss: 93.6571\n",
      "Epoch 7641/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.8979 - val_loss: 92.9618\n",
      "Epoch 7642/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.1182 - val_loss: 93.6571\n",
      "Epoch 7643/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.8698 - val_loss: 93.1389\n",
      "Epoch 7644/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.9694 - val_loss: 92.5603\n",
      "Epoch 7645/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.0849 - val_loss: 92.6913\n",
      "Epoch 7646/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.4068 - val_loss: 93.1681\n",
      "Epoch 7647/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.7989 - val_loss: 92.6356\n",
      "Epoch 7648/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.5446 - val_loss: 93.3646\n",
      "Epoch 7649/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.4165 - val_loss: 92.8471\n",
      "Epoch 7650/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.8916 - val_loss: 93.0984\n",
      "Epoch 7651/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8531 - val_loss: 93.2468\n",
      "Epoch 7652/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.3130 - val_loss: 93.0211\n",
      "Epoch 7653/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.9344 - val_loss: 93.4567\n",
      "Epoch 7654/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4285 - val_loss: 93.5187\n",
      "Epoch 7655/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.7502 - val_loss: 93.6963\n",
      "Epoch 7656/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.9070 - val_loss: 93.2149\n",
      "Epoch 7657/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.7207 - val_loss: 93.3147\n",
      "Epoch 7658/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.0222 - val_loss: 93.0765\n",
      "Epoch 7659/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.5866 - val_loss: 93.2439\n",
      "Epoch 7660/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.7114 - val_loss: 92.9707\n",
      "Epoch 7661/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.7134 - val_loss: 93.4076\n",
      "Epoch 7662/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.1878 - val_loss: 92.6358\n",
      "Epoch 7663/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.4706 - val_loss: 92.8941\n",
      "Epoch 7664/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.9946 - val_loss: 92.8577\n",
      "Epoch 7665/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.0758 - val_loss: 92.9789\n",
      "Epoch 7666/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2194 - val_loss: 92.9856\n",
      "Epoch 7667/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.0179 - val_loss: 93.8865\n",
      "Epoch 7668/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3223 - val_loss: 93.7692\n",
      "Epoch 7669/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.4665 - val_loss: 92.5350\n",
      "Epoch 7670/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.4374 - val_loss: 92.9459\n",
      "Epoch 7671/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.6829 - val_loss: 93.1755\n",
      "Epoch 7672/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.0446 - val_loss: 93.3257\n",
      "Epoch 7673/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8971 - val_loss: 93.0618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7674/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.6781 - val_loss: 93.1274\n",
      "Epoch 7675/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.2036 - val_loss: 93.4868\n",
      "Epoch 7676/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.1594 - val_loss: 93.0925\n",
      "Epoch 7677/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.5598 - val_loss: 92.8259\n",
      "Epoch 7678/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.8248 - val_loss: 93.4283\n",
      "Epoch 7679/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.5450 - val_loss: 93.2650\n",
      "Epoch 7680/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.3839 - val_loss: 93.0809\n",
      "Epoch 7681/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.3643 - val_loss: 92.7774\n",
      "Epoch 7682/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.8761 - val_loss: 93.2448\n",
      "Epoch 7683/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9444 - val_loss: 93.0400\n",
      "Epoch 7684/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.8918 - val_loss: 93.3380\n",
      "Epoch 7685/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 90.8578 - val_loss: 92.5300\n",
      "Epoch 7686/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6299 - val_loss: 93.0127\n",
      "Epoch 7687/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.4543 - val_loss: 93.0487\n",
      "Epoch 7688/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.5924 - val_loss: 92.5656\n",
      "Epoch 7689/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8913 - val_loss: 93.1552\n",
      "Epoch 7690/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.0544 - val_loss: 92.8149\n",
      "Epoch 7691/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.2344 - val_loss: 92.8166\n",
      "Epoch 7692/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.4208 - val_loss: 92.7357\n",
      "Epoch 7693/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.1543 - val_loss: 92.8528\n",
      "Epoch 7694/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.7758 - val_loss: 92.9676\n",
      "Epoch 7695/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8820 - val_loss: 92.9568\n",
      "Epoch 7696/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.3890 - val_loss: 93.0367\n",
      "Epoch 7697/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7104 - val_loss: 92.9996\n",
      "Epoch 7698/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 93.0642 - val_loss: 93.4286\n",
      "Epoch 7699/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.9187 - val_loss: 93.3375\n",
      "Epoch 7700/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6426 - val_loss: 92.6736\n",
      "Epoch 7701/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.9753 - val_loss: 92.9273\n",
      "Epoch 7702/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.4128 - val_loss: 92.5535\n",
      "Epoch 7703/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.0200 - val_loss: 93.0544\n",
      "Epoch 7704/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.0507 - val_loss: 92.7581\n",
      "Epoch 7705/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.3698 - val_loss: 93.7764\n",
      "Epoch 7706/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.8703 - val_loss: 93.2368\n",
      "Epoch 7707/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.9094 - val_loss: 92.7410\n",
      "Epoch 7708/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6313 - val_loss: 92.5777\n",
      "Epoch 7709/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.6341 - val_loss: 92.7973\n",
      "Epoch 7710/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.9695 - val_loss: 93.0240\n",
      "Epoch 7711/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2809 - val_loss: 93.1090\n",
      "Epoch 7712/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.6502 - val_loss: 92.8819\n",
      "Epoch 7713/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.0426 - val_loss: 93.2205\n",
      "Epoch 7714/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.7224 - val_loss: 93.4939\n",
      "Epoch 7715/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9846 - val_loss: 93.5592\n",
      "Epoch 7716/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.0340 - val_loss: 93.5630\n",
      "Epoch 7717/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.1856 - val_loss: 92.5004\n",
      "Epoch 7718/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.7681 - val_loss: 93.2480\n",
      "Epoch 7719/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.6624 - val_loss: 93.9309\n",
      "Epoch 7720/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.9991 - val_loss: 92.7148\n",
      "Epoch 7721/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4711 - val_loss: 93.0705\n",
      "Epoch 7722/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.8757 - val_loss: 92.9047\n",
      "Epoch 7723/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.1015 - val_loss: 93.2585\n",
      "Epoch 7724/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.0656 - val_loss: 93.1281\n",
      "Epoch 7725/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.3599 - val_loss: 93.1970\n",
      "Epoch 7726/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.0564 - val_loss: 92.3408\n",
      "Epoch 7727/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2332 - val_loss: 92.6808\n",
      "Epoch 7728/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8655 - val_loss: 92.9222\n",
      "Epoch 7729/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.5682 - val_loss: 93.5155\n",
      "Epoch 7730/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.2606 - val_loss: 93.2973\n",
      "Epoch 7731/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.3065 - val_loss: 92.2536\n",
      "Epoch 7732/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.8517 - val_loss: 93.1314\n",
      "Epoch 7733/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.2730 - val_loss: 93.3300\n",
      "Epoch 7734/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1461 - val_loss: 93.2029\n",
      "Epoch 7735/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5990 - val_loss: 93.4441\n",
      "Epoch 7736/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 95.0254 - val_loss: 93.1635\n",
      "Epoch 7737/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.7008 - val_loss: 92.7822\n",
      "Epoch 7738/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 90.9333 - val_loss: 92.8683\n",
      "Epoch 7739/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3504 - val_loss: 92.7555\n",
      "Epoch 7740/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 95.2465 - val_loss: 93.1396\n",
      "Epoch 7741/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.7678 - val_loss: 93.1756\n",
      "Epoch 7742/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8693 - val_loss: 93.1788\n",
      "Epoch 7743/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.7081 - val_loss: 93.4574\n",
      "Epoch 7744/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.2267 - val_loss: 92.8183\n",
      "Epoch 7745/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6440 - val_loss: 93.3655\n",
      "Epoch 7746/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.0114 - val_loss: 93.1502\n",
      "Epoch 7747/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7232 - val_loss: 92.7716\n",
      "Epoch 7748/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 90.9287 - val_loss: 92.4153\n",
      "Epoch 7749/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.1897 - val_loss: 92.9169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7750/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.0870 - val_loss: 93.4326\n",
      "Epoch 7751/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5166 - val_loss: 92.6831\n",
      "Epoch 7752/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.0408 - val_loss: 92.8113\n",
      "Epoch 7753/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.6199 - val_loss: 93.0901\n",
      "Epoch 7754/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9030 - val_loss: 93.4883\n",
      "Epoch 7755/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.0324 - val_loss: 93.0815\n",
      "Epoch 7756/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.6147 - val_loss: 92.8651\n",
      "Epoch 7757/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7549 - val_loss: 93.1189\n",
      "Epoch 7758/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.1204 - val_loss: 92.7352\n",
      "Epoch 7759/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.3762 - val_loss: 92.8892\n",
      "Epoch 7760/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.2394 - val_loss: 93.7806\n",
      "Epoch 7761/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.6389 - val_loss: 92.5597\n",
      "Epoch 7762/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.8124 - val_loss: 93.0627\n",
      "Epoch 7763/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.6591 - val_loss: 93.1085\n",
      "Epoch 7764/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.4630 - val_loss: 92.8285\n",
      "Epoch 7765/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.4265 - val_loss: 93.5128\n",
      "Epoch 7766/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.5648 - val_loss: 93.5228\n",
      "Epoch 7767/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.0914 - val_loss: 92.8807\n",
      "Epoch 7768/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6126 - val_loss: 92.8967\n",
      "Epoch 7769/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.2772 - val_loss: 93.4600\n",
      "Epoch 7770/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.2220 - val_loss: 93.8530\n",
      "Epoch 7771/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.3970 - val_loss: 93.0663\n",
      "Epoch 7772/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.4449 - val_loss: 93.3038\n",
      "Epoch 7773/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 92.3052 - val_loss: 92.8448\n",
      "Epoch 7774/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8037 - val_loss: 92.8763\n",
      "Epoch 7775/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.7613 - val_loss: 92.9537\n",
      "Epoch 7776/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.5205 - val_loss: 93.2117\n",
      "Epoch 7777/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.4849 - val_loss: 93.1476\n",
      "Epoch 7778/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.2129 - val_loss: 93.4768\n",
      "Epoch 7779/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7904 - val_loss: 92.5894\n",
      "Epoch 7780/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1079 - val_loss: 93.1769\n",
      "Epoch 7781/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.8391 - val_loss: 92.4791\n",
      "Epoch 7782/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.2868 - val_loss: 93.3447\n",
      "Epoch 7783/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9856 - val_loss: 93.7381\n",
      "Epoch 7784/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.1076 - val_loss: 92.8947\n",
      "Epoch 7785/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.0267 - val_loss: 92.6229\n",
      "Epoch 7786/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4791 - val_loss: 92.4897\n",
      "Epoch 7787/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.2976 - val_loss: 93.0743\n",
      "Epoch 7788/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.7042 - val_loss: 93.0875\n",
      "Epoch 7789/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6115 - val_loss: 93.2633\n",
      "Epoch 7790/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.6208 - val_loss: 92.5105\n",
      "Epoch 7791/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.1160 - val_loss: 92.9926\n",
      "Epoch 7792/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.8964 - val_loss: 93.0413\n",
      "Epoch 7793/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.6050 - val_loss: 92.7243\n",
      "Epoch 7794/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.8955 - val_loss: 93.2742\n",
      "Epoch 7795/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.4327 - val_loss: 93.1812\n",
      "Epoch 7796/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.7093 - val_loss: 93.1816\n",
      "Epoch 7797/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.6704 - val_loss: 92.8622\n",
      "Epoch 7798/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.0057 - val_loss: 93.6875\n",
      "Epoch 7799/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.7364 - val_loss: 92.9817\n",
      "Epoch 7800/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.0033 - val_loss: 93.4619\n",
      "Epoch 7801/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.1836 - val_loss: 93.6441\n",
      "Epoch 7802/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.3171 - val_loss: 93.4326\n",
      "Epoch 7803/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2260 - val_loss: 93.1648\n",
      "Epoch 7804/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.5050 - val_loss: 92.6990\n",
      "Epoch 7805/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 90.8543 - val_loss: 93.1405\n",
      "Epoch 7806/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.1861 - val_loss: 92.9876\n",
      "Epoch 7807/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9579 - val_loss: 93.3981\n",
      "Epoch 7808/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3909 - val_loss: 92.8275\n",
      "Epoch 7809/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.3983 - val_loss: 92.7380\n",
      "Epoch 7810/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.7717 - val_loss: 93.3350\n",
      "Epoch 7811/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4313 - val_loss: 93.1289\n",
      "Epoch 7812/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.0732 - val_loss: 93.4305\n",
      "Epoch 7813/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.6116 - val_loss: 92.3630\n",
      "Epoch 7814/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.8407 - val_loss: 93.5399\n",
      "Epoch 7815/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.4424 - val_loss: 92.6725\n",
      "Epoch 7816/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.3063 - val_loss: 93.3475\n",
      "Epoch 7817/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.9586 - val_loss: 92.7128\n",
      "Epoch 7818/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7112 - val_loss: 92.8875\n",
      "Epoch 7819/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.6109 - val_loss: 93.0751\n",
      "Epoch 7820/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.4020 - val_loss: 93.0882\n",
      "Epoch 7821/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9757 - val_loss: 92.6754\n",
      "Epoch 7822/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.2042 - val_loss: 93.5810\n",
      "Epoch 7823/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.0408 - val_loss: 93.3654\n",
      "Epoch 7824/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.6796 - val_loss: 92.9563\n",
      "Epoch 7825/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2841 - val_loss: 92.8058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7826/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6374 - val_loss: 93.4211\n",
      "Epoch 7827/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.1007 - val_loss: 93.2385\n",
      "Epoch 7828/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.9116 - val_loss: 93.0801\n",
      "Epoch 7829/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.7542 - val_loss: 92.7728\n",
      "Epoch 7830/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.7335 - val_loss: 92.8111\n",
      "Epoch 7831/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5914 - val_loss: 93.3812\n",
      "Epoch 7832/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.5385 - val_loss: 92.9028\n",
      "Epoch 7833/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9215 - val_loss: 93.0205\n",
      "Epoch 7834/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.4830 - val_loss: 92.8600\n",
      "Epoch 7835/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.5735 - val_loss: 93.3192\n",
      "Epoch 7836/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.5386 - val_loss: 93.4945\n",
      "Epoch 7837/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.8636 - val_loss: 93.2461\n",
      "Epoch 7838/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4050 - val_loss: 93.3655\n",
      "Epoch 7839/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.7292 - val_loss: 93.0710\n",
      "Epoch 7840/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1440 - val_loss: 92.8303\n",
      "Epoch 7841/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.7789 - val_loss: 92.8286\n",
      "Epoch 7842/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.3430 - val_loss: 92.4251\n",
      "Epoch 7843/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.5782 - val_loss: 93.3431\n",
      "Epoch 7844/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.2201 - val_loss: 93.1121\n",
      "Epoch 7845/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4633 - val_loss: 93.2548\n",
      "Epoch 7846/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9786 - val_loss: 92.8016\n",
      "Epoch 7847/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.0537 - val_loss: 93.1704\n",
      "Epoch 7848/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.0118 - val_loss: 92.8864\n",
      "Epoch 7849/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 91.9177 - val_loss: 92.7203\n",
      "Epoch 7850/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.7917 - val_loss: 92.4416\n",
      "Epoch 7851/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5905 - val_loss: 93.3153\n",
      "Epoch 7852/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2984 - val_loss: 92.9827\n",
      "Epoch 7853/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.6946 - val_loss: 92.5118\n",
      "Epoch 7854/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3546 - val_loss: 93.1230\n",
      "Epoch 7855/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9537 - val_loss: 92.2095\n",
      "Epoch 7856/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4614 - val_loss: 92.6859\n",
      "Epoch 7857/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.9016 - val_loss: 92.9603\n",
      "Epoch 7858/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6021 - val_loss: 92.4049\n",
      "Epoch 7859/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 90.7298 - val_loss: 91.9706\n",
      "Epoch 7860/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3735 - val_loss: 92.6317\n",
      "Epoch 7861/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4615 - val_loss: 93.3076\n",
      "Epoch 7862/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.6232 - val_loss: 92.4452\n",
      "Epoch 7863/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8662 - val_loss: 92.7209\n",
      "Epoch 7864/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.2834 - val_loss: 93.4834\n",
      "Epoch 7865/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.7367 - val_loss: 92.1570\n",
      "Epoch 7866/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.0370 - val_loss: 93.4252\n",
      "Epoch 7867/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7624 - val_loss: 92.9761\n",
      "Epoch 7868/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4641 - val_loss: 92.6374\n",
      "Epoch 7869/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.0464 - val_loss: 92.8092\n",
      "Epoch 7870/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0370 - val_loss: 93.0448\n",
      "Epoch 7871/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.1250 - val_loss: 93.1562\n",
      "Epoch 7872/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1916 - val_loss: 92.3888\n",
      "Epoch 7873/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3922 - val_loss: 93.1537\n",
      "Epoch 7874/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.1353 - val_loss: 93.0508\n",
      "Epoch 7875/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.9763 - val_loss: 93.4077\n",
      "Epoch 7876/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.0221 - val_loss: 92.5765\n",
      "Epoch 7877/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2806 - val_loss: 93.1269\n",
      "Epoch 7878/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.4587 - val_loss: 92.6805\n",
      "Epoch 7879/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.9622 - val_loss: 92.9917\n",
      "Epoch 7880/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.9102 - val_loss: 93.1958\n",
      "Epoch 7881/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2876 - val_loss: 93.1967\n",
      "Epoch 7882/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.7626 - val_loss: 93.3298\n",
      "Epoch 7883/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.4798 - val_loss: 92.7132\n",
      "Epoch 7884/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.3438 - val_loss: 92.9749\n",
      "Epoch 7885/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.1101 - val_loss: 92.9703\n",
      "Epoch 7886/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.5182 - val_loss: 93.1382\n",
      "Epoch 7887/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.3904 - val_loss: 93.1266\n",
      "Epoch 7888/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.4545 - val_loss: 92.9624\n",
      "Epoch 7889/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.4757 - val_loss: 93.3663\n",
      "Epoch 7890/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.5380 - val_loss: 93.0823\n",
      "Epoch 7891/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.0734 - val_loss: 93.0388\n",
      "Epoch 7892/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.1668 - val_loss: 92.7891\n",
      "Epoch 7893/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.9671 - val_loss: 93.3609\n",
      "Epoch 7894/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.5614 - val_loss: 93.4686\n",
      "Epoch 7895/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5708 - val_loss: 94.0622\n",
      "Epoch 7896/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 90.9627 - val_loss: 92.7824\n",
      "Epoch 7897/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.2776 - val_loss: 93.4637\n",
      "Epoch 7898/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.1054 - val_loss: 92.3496\n",
      "Epoch 7899/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.1065 - val_loss: 93.0891\n",
      "Epoch 7900/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.0423 - val_loss: 93.6670\n",
      "Epoch 7901/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 89.6648 - val_loss: 92.7947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7902/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.4053 - val_loss: 92.5797\n",
      "Epoch 7903/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.7030 - val_loss: 93.8928\n",
      "Epoch 7904/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.9417 - val_loss: 92.8421\n",
      "Epoch 7905/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.7785 - val_loss: 92.7142\n",
      "Epoch 7906/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.8796 - val_loss: 93.5125\n",
      "Epoch 7907/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.2018 - val_loss: 93.5788\n",
      "Epoch 7908/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.9852 - val_loss: 93.4982\n",
      "Epoch 7909/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.1035 - val_loss: 93.2748\n",
      "Epoch 7910/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1043 - val_loss: 91.7479\n",
      "Epoch 7911/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.6364 - val_loss: 93.4281\n",
      "Epoch 7912/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6720 - val_loss: 92.6219\n",
      "Epoch 7913/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.1915 - val_loss: 92.9491\n",
      "Epoch 7914/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5641 - val_loss: 93.0109\n",
      "Epoch 7915/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.9932 - val_loss: 93.1451\n",
      "Epoch 7916/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.0227 - val_loss: 91.9238\n",
      "Epoch 7917/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.8087 - val_loss: 92.9075\n",
      "Epoch 7918/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 90.1165 - val_loss: 93.0528\n",
      "Epoch 7919/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.2658 - val_loss: 93.1732\n",
      "Epoch 7920/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.2224 - val_loss: 92.8313\n",
      "Epoch 7921/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.8274 - val_loss: 91.7943\n",
      "Epoch 7922/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2050 - val_loss: 92.7270\n",
      "Epoch 7923/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.4949 - val_loss: 93.1102\n",
      "Epoch 7924/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2594 - val_loss: 93.1803\n",
      "Epoch 7925/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.0391 - val_loss: 92.8416\n",
      "Epoch 7926/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2267 - val_loss: 93.4392\n",
      "Epoch 7927/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8756 - val_loss: 93.1429\n",
      "Epoch 7928/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.6404 - val_loss: 92.8600\n",
      "Epoch 7929/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4704 - val_loss: 93.4033\n",
      "Epoch 7930/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.3450 - val_loss: 92.3266\n",
      "Epoch 7931/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.5203 - val_loss: 93.4743\n",
      "Epoch 7932/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.0467 - val_loss: 92.9276\n",
      "Epoch 7933/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.1468 - val_loss: 93.1427\n",
      "Epoch 7934/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.4526 - val_loss: 93.3418\n",
      "Epoch 7935/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2591 - val_loss: 93.2390\n",
      "Epoch 7936/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.6847 - val_loss: 93.1594\n",
      "Epoch 7937/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.4203 - val_loss: 93.0986\n",
      "Epoch 7938/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3493 - val_loss: 93.2246\n",
      "Epoch 7939/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 90.5270 - val_loss: 92.5930\n",
      "Epoch 7940/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.2977 - val_loss: 92.7393\n",
      "Epoch 7941/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1079 - val_loss: 93.0361\n",
      "Epoch 7942/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.4163 - val_loss: 92.7176\n",
      "Epoch 7943/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.4790 - val_loss: 92.9617\n",
      "Epoch 7944/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 91.0604 - val_loss: 93.3834\n",
      "Epoch 7945/10000\n",
      "96/96 [==============================] - 0s 113us/step - loss: 90.3784 - val_loss: 93.4850\n",
      "Epoch 7946/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.5613 - val_loss: 93.1263\n",
      "Epoch 7947/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9198 - val_loss: 93.1260\n",
      "Epoch 7948/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9667 - val_loss: 93.1376\n",
      "Epoch 7949/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.3884 - val_loss: 93.2322\n",
      "Epoch 7950/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.4267 - val_loss: 93.1652\n",
      "Epoch 7951/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.7036 - val_loss: 92.4001\n",
      "Epoch 7952/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.7563 - val_loss: 92.7326\n",
      "Epoch 7953/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.1844 - val_loss: 92.4450\n",
      "Epoch 7954/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3342 - val_loss: 92.0244\n",
      "Epoch 7955/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 94.7279 - val_loss: 93.5088\n",
      "Epoch 7956/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8316 - val_loss: 92.3290\n",
      "Epoch 7957/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5028 - val_loss: 93.2230\n",
      "Epoch 7958/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.2712 - val_loss: 93.0111\n",
      "Epoch 7959/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.8562 - val_loss: 93.4727\n",
      "Epoch 7960/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.3698 - val_loss: 93.3512\n",
      "Epoch 7961/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.1501 - val_loss: 92.5158\n",
      "Epoch 7962/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.7601 - val_loss: 93.3419\n",
      "Epoch 7963/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.0613 - val_loss: 92.4106\n",
      "Epoch 7964/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5758 - val_loss: 92.9887\n",
      "Epoch 7965/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.8063 - val_loss: 93.6517\n",
      "Epoch 7966/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.6431 - val_loss: 93.1565\n",
      "Epoch 7967/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.1661 - val_loss: 92.8821\n",
      "Epoch 7968/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3489 - val_loss: 93.4790\n",
      "Epoch 7969/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.7723 - val_loss: 92.6010\n",
      "Epoch 7970/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6980 - val_loss: 93.3155\n",
      "Epoch 7971/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8294 - val_loss: 91.9428\n",
      "Epoch 7972/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.0176 - val_loss: 93.0974\n",
      "Epoch 7973/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.6519 - val_loss: 93.8848\n",
      "Epoch 7974/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.5657 - val_loss: 93.5172\n",
      "Epoch 7975/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3841 - val_loss: 93.5036\n",
      "Epoch 7976/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.7923 - val_loss: 93.1156\n",
      "Epoch 7977/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2034 - val_loss: 93.1536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7978/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6512 - val_loss: 93.3655\n",
      "Epoch 7979/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.6004 - val_loss: 93.0643\n",
      "Epoch 7980/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.3602 - val_loss: 93.0693\n",
      "Epoch 7981/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.4046 - val_loss: 93.0595\n",
      "Epoch 7982/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.1452 - val_loss: 92.7421\n",
      "Epoch 7983/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6365 - val_loss: 91.7551\n",
      "Epoch 7984/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.6923 - val_loss: 93.2052\n",
      "Epoch 7985/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.9070 - val_loss: 93.4037\n",
      "Epoch 7986/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4024 - val_loss: 93.0876\n",
      "Epoch 7987/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1031 - val_loss: 93.1589\n",
      "Epoch 7988/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.9876 - val_loss: 92.3320\n",
      "Epoch 7989/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.5526 - val_loss: 92.5162\n",
      "Epoch 7990/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.7359 - val_loss: 93.2182\n",
      "Epoch 7991/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.6901 - val_loss: 92.6411\n",
      "Epoch 7992/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.1415 - val_loss: 93.0160\n",
      "Epoch 7993/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 90.7476 - val_loss: 93.1718\n",
      "Epoch 7994/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.4935 - val_loss: 93.0340\n",
      "Epoch 7995/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3723 - val_loss: 93.1753\n",
      "Epoch 7996/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 90.9182 - val_loss: 93.5070\n",
      "Epoch 7997/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.7197 - val_loss: 92.6250\n",
      "Epoch 7998/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7447 - val_loss: 93.0093\n",
      "Epoch 7999/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.1011 - val_loss: 92.7469\n",
      "Epoch 8000/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.1230 - val_loss: 92.0549\n",
      "Epoch 8001/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2595 - val_loss: 93.0062\n",
      "Epoch 8002/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.0967 - val_loss: 93.7030\n",
      "Epoch 8003/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.5910 - val_loss: 93.4715\n",
      "Epoch 8004/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.9627 - val_loss: 93.4621\n",
      "Epoch 8005/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.1865 - val_loss: 93.7527\n",
      "Epoch 8006/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9103 - val_loss: 93.1599\n",
      "Epoch 8007/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9378 - val_loss: 92.8963\n",
      "Epoch 8008/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 89.5081 - val_loss: 93.6508\n",
      "Epoch 8009/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.8963 - val_loss: 93.1397\n",
      "Epoch 8010/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0340 - val_loss: 93.2078\n",
      "Epoch 8011/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.1073 - val_loss: 93.5757\n",
      "Epoch 8012/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.7630 - val_loss: 92.7388\n",
      "Epoch 8013/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.7942 - val_loss: 93.1693\n",
      "Epoch 8014/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.9121 - val_loss: 93.2537\n",
      "Epoch 8015/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.6643 - val_loss: 92.9404\n",
      "Epoch 8016/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.6941 - val_loss: 92.7987\n",
      "Epoch 8017/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2023 - val_loss: 92.9851\n",
      "Epoch 8018/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.8593 - val_loss: 93.0273\n",
      "Epoch 8019/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3195 - val_loss: 93.3446\n",
      "Epoch 8020/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.7123 - val_loss: 93.3832\n",
      "Epoch 8021/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.0102 - val_loss: 92.9038\n",
      "Epoch 8022/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6797 - val_loss: 92.7689\n",
      "Epoch 8023/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.9301 - val_loss: 93.2170\n",
      "Epoch 8024/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.8078 - val_loss: 93.8071\n",
      "Epoch 8025/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2333 - val_loss: 93.4455\n",
      "Epoch 8026/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.0876 - val_loss: 92.5505\n",
      "Epoch 8027/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.2520 - val_loss: 93.2669\n",
      "Epoch 8028/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 90.6024 - val_loss: 92.7025\n",
      "Epoch 8029/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.1905 - val_loss: 93.4411\n",
      "Epoch 8030/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.7476 - val_loss: 93.2223\n",
      "Epoch 8031/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.5330 - val_loss: 93.2314\n",
      "Epoch 8032/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 93.8239 - val_loss: 93.3572\n",
      "Epoch 8033/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.8603 - val_loss: 93.3961\n",
      "Epoch 8034/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.5109 - val_loss: 93.7046\n",
      "Epoch 8035/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.6635 - val_loss: 93.6877\n",
      "Epoch 8036/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.8921 - val_loss: 93.0605\n",
      "Epoch 8037/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 93.4874 - val_loss: 93.3071\n",
      "Epoch 8038/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.3875 - val_loss: 92.9728\n",
      "Epoch 8039/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.7343 - val_loss: 93.2315\n",
      "Epoch 8040/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3485 - val_loss: 93.4571\n",
      "Epoch 8041/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 89.7240 - val_loss: 92.9044\n",
      "Epoch 8042/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.3251 - val_loss: 92.4058\n",
      "Epoch 8043/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.5957 - val_loss: 93.1564\n",
      "Epoch 8044/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.6366 - val_loss: 93.5203\n",
      "Epoch 8045/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.8987 - val_loss: 93.2110\n",
      "Epoch 8046/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.2415 - val_loss: 92.7609\n",
      "Epoch 8047/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0857 - val_loss: 92.4092\n",
      "Epoch 8048/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.6172 - val_loss: 93.1687\n",
      "Epoch 8049/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.5810 - val_loss: 92.8382\n",
      "Epoch 8050/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 89.7539 - val_loss: 93.5389\n",
      "Epoch 8051/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2408 - val_loss: 93.2235\n",
      "Epoch 8052/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.8564 - val_loss: 93.0591\n",
      "Epoch 8053/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.7633 - val_loss: 93.1279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8054/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 91.6678 - val_loss: 93.1545\n",
      "Epoch 8055/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.3976 - val_loss: 93.4314\n",
      "Epoch 8056/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.5609 - val_loss: 92.5536\n",
      "Epoch 8057/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.5462 - val_loss: 93.0356\n",
      "Epoch 8058/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 89.8764 - val_loss: 93.6005\n",
      "Epoch 8059/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.1188 - val_loss: 92.3812\n",
      "Epoch 8060/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.7328 - val_loss: 92.4701\n",
      "Epoch 8061/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 89.9625 - val_loss: 92.9395\n",
      "Epoch 8062/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 90.5019 - val_loss: 92.6498\n",
      "Epoch 8063/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9594 - val_loss: 92.9486\n",
      "Epoch 8064/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.7284 - val_loss: 93.3434\n",
      "Epoch 8065/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.8659 - val_loss: 92.9313\n",
      "Epoch 8066/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.0343 - val_loss: 92.8920\n",
      "Epoch 8067/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.1696 - val_loss: 92.6089\n",
      "Epoch 8068/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8101 - val_loss: 92.8613\n",
      "Epoch 8069/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.3571 - val_loss: 92.8121\n",
      "Epoch 8070/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.8608 - val_loss: 93.1816\n",
      "Epoch 8071/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.3842 - val_loss: 93.0773\n",
      "Epoch 8072/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.6731 - val_loss: 92.5040\n",
      "Epoch 8073/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.5160 - val_loss: 92.6676\n",
      "Epoch 8074/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.0360 - val_loss: 92.8363\n",
      "Epoch 8075/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.1207 - val_loss: 92.9313\n",
      "Epoch 8076/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.2818 - val_loss: 93.1997\n",
      "Epoch 8077/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.5223 - val_loss: 92.7202\n",
      "Epoch 8078/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.1146 - val_loss: 93.2415\n",
      "Epoch 8079/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.9091 - val_loss: 92.9368\n",
      "Epoch 8080/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.2884 - val_loss: 93.8526\n",
      "Epoch 8081/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6229 - val_loss: 94.0415\n",
      "Epoch 8082/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.2234 - val_loss: 92.8370\n",
      "Epoch 8083/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.8010 - val_loss: 92.2274\n",
      "Epoch 8084/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.7577 - val_loss: 92.8715\n",
      "Epoch 8085/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4295 - val_loss: 92.7907\n",
      "Epoch 8086/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.9636 - val_loss: 92.5064\n",
      "Epoch 8087/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2645 - val_loss: 92.8550\n",
      "Epoch 8088/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3758 - val_loss: 93.2947\n",
      "Epoch 8089/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.6539 - val_loss: 93.3140\n",
      "Epoch 8090/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.3738 - val_loss: 93.4574\n",
      "Epoch 8091/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.2697 - val_loss: 92.4608\n",
      "Epoch 8092/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7621 - val_loss: 93.4697\n",
      "Epoch 8093/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.0363 - val_loss: 93.7979\n",
      "Epoch 8094/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.7748 - val_loss: 92.8713\n",
      "Epoch 8095/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.6500 - val_loss: 91.5789\n",
      "Epoch 8096/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.4761 - val_loss: 92.4452\n",
      "Epoch 8097/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 89.9268 - val_loss: 93.2690\n",
      "Epoch 8098/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4587 - val_loss: 93.3352\n",
      "Epoch 8099/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8427 - val_loss: 93.3564\n",
      "Epoch 8100/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.5426 - val_loss: 92.7393\n",
      "Epoch 8101/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4220 - val_loss: 92.8461\n",
      "Epoch 8102/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.5463 - val_loss: 93.4355\n",
      "Epoch 8103/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.1726 - val_loss: 93.2362\n",
      "Epoch 8104/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.5287 - val_loss: 92.8073\n",
      "Epoch 8105/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.7423 - val_loss: 93.8270\n",
      "Epoch 8106/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.2349 - val_loss: 93.1155\n",
      "Epoch 8107/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 90.4946 - val_loss: 93.5635\n",
      "Epoch 8108/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 89.9087 - val_loss: 93.0122\n",
      "Epoch 8109/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.3055 - val_loss: 93.1418\n",
      "Epoch 8110/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4430 - val_loss: 92.2698\n",
      "Epoch 8111/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.6409 - val_loss: 92.7446\n",
      "Epoch 8112/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.0430 - val_loss: 93.2124\n",
      "Epoch 8113/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.9475 - val_loss: 93.0351\n",
      "Epoch 8114/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4141 - val_loss: 93.1320\n",
      "Epoch 8115/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.0327 - val_loss: 92.7283\n",
      "Epoch 8116/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8545 - val_loss: 92.8325\n",
      "Epoch 8117/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.9624 - val_loss: 92.5795\n",
      "Epoch 8118/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4444 - val_loss: 93.0925\n",
      "Epoch 8119/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.1937 - val_loss: 93.2558\n",
      "Epoch 8120/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.1897 - val_loss: 92.7214\n",
      "Epoch 8121/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.1650 - val_loss: 93.1785\n",
      "Epoch 8122/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3803 - val_loss: 93.0109\n",
      "Epoch 8123/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.4458 - val_loss: 92.9271\n",
      "Epoch 8124/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.9725 - val_loss: 92.7373\n",
      "Epoch 8125/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.9829 - val_loss: 92.4457\n",
      "Epoch 8126/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.8273 - val_loss: 92.5490\n",
      "Epoch 8127/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.4492 - val_loss: 92.8404\n",
      "Epoch 8128/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.4178 - val_loss: 92.6787\n",
      "Epoch 8129/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8633 - val_loss: 93.3001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8130/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.6578 - val_loss: 93.8146\n",
      "Epoch 8131/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.1719 - val_loss: 92.9496\n",
      "Epoch 8132/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.6795 - val_loss: 92.8486\n",
      "Epoch 8133/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.0537 - val_loss: 93.7212\n",
      "Epoch 8134/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.5323 - val_loss: 92.6090\n",
      "Epoch 8135/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.2812 - val_loss: 92.9539\n",
      "Epoch 8136/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.3670 - val_loss: 93.0037\n",
      "Epoch 8137/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.0653 - val_loss: 92.4766\n",
      "Epoch 8138/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2134 - val_loss: 92.5629\n",
      "Epoch 8139/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.6706 - val_loss: 92.7022\n",
      "Epoch 8140/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1559 - val_loss: 94.1107\n",
      "Epoch 8141/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.2256 - val_loss: 92.6749\n",
      "Epoch 8142/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.9767 - val_loss: 93.2841\n",
      "Epoch 8143/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.2200 - val_loss: 92.8226\n",
      "Epoch 8144/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.5558 - val_loss: 93.1277\n",
      "Epoch 8145/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.8428 - val_loss: 93.6118\n",
      "Epoch 8146/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.1312 - val_loss: 92.4428\n",
      "Epoch 8147/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.3051 - val_loss: 92.8059\n",
      "Epoch 8148/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.4586 - val_loss: 92.5910\n",
      "Epoch 8149/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.5809 - val_loss: 92.3763\n",
      "Epoch 8150/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.9005 - val_loss: 92.1805\n",
      "Epoch 8151/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.6899 - val_loss: 93.3010\n",
      "Epoch 8152/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.4086 - val_loss: 93.1141\n",
      "Epoch 8153/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.9907 - val_loss: 92.2938\n",
      "Epoch 8154/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.5801 - val_loss: 93.4098\n",
      "Epoch 8155/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.2499 - val_loss: 92.8939\n",
      "Epoch 8156/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 89.6338 - val_loss: 93.2350\n",
      "Epoch 8157/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6852 - val_loss: 93.0044\n",
      "Epoch 8158/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.7684 - val_loss: 93.3620\n",
      "Epoch 8159/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.6662 - val_loss: 93.0757\n",
      "Epoch 8160/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9800 - val_loss: 92.7975\n",
      "Epoch 8161/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.2140 - val_loss: 92.9808\n",
      "Epoch 8162/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.0237 - val_loss: 93.1226\n",
      "Epoch 8163/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.9301 - val_loss: 93.6988\n",
      "Epoch 8164/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1839 - val_loss: 93.4589\n",
      "Epoch 8165/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.2211 - val_loss: 93.2709\n",
      "Epoch 8166/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.8343 - val_loss: 93.1517\n",
      "Epoch 8167/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.6278 - val_loss: 92.7896\n",
      "Epoch 8168/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.5463 - val_loss: 92.7839\n",
      "Epoch 8169/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.7438 - val_loss: 91.8461\n",
      "Epoch 8170/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.5237 - val_loss: 92.5102\n",
      "Epoch 8171/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.7881 - val_loss: 94.1678\n",
      "Epoch 8172/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1590 - val_loss: 92.3183\n",
      "Epoch 8173/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2678 - val_loss: 93.5594\n",
      "Epoch 8174/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.2061 - val_loss: 92.7092\n",
      "Epoch 8175/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.8008 - val_loss: 92.7981\n",
      "Epoch 8176/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.8084 - val_loss: 93.3945\n",
      "Epoch 8177/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.1047 - val_loss: 93.6138\n",
      "Epoch 8178/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.8818 - val_loss: 92.9665\n",
      "Epoch 8179/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.4115 - val_loss: 92.9206\n",
      "Epoch 8180/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2966 - val_loss: 92.9704\n",
      "Epoch 8181/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 90.8513 - val_loss: 93.7917\n",
      "Epoch 8182/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.4337 - val_loss: 92.7993\n",
      "Epoch 8183/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.2550 - val_loss: 92.8886\n",
      "Epoch 8184/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.0764 - val_loss: 92.8435\n",
      "Epoch 8185/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.4300 - val_loss: 92.6398\n",
      "Epoch 8186/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9966 - val_loss: 92.7691\n",
      "Epoch 8187/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6962 - val_loss: 92.9946\n",
      "Epoch 8188/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.3238 - val_loss: 92.6867\n",
      "Epoch 8189/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.5299 - val_loss: 92.9536\n",
      "Epoch 8190/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 93.9383 - val_loss: 92.6634\n",
      "Epoch 8191/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.2912 - val_loss: 93.1770\n",
      "Epoch 8192/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.5500 - val_loss: 92.7027\n",
      "Epoch 8193/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 91.1849 - val_loss: 92.5141\n",
      "Epoch 8194/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.6942 - val_loss: 92.9614\n",
      "Epoch 8195/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 90.9177 - val_loss: 92.8611\n",
      "Epoch 8196/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2470 - val_loss: 92.5078\n",
      "Epoch 8197/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.1534 - val_loss: 92.8689\n",
      "Epoch 8198/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.1578 - val_loss: 93.5817\n",
      "Epoch 8199/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.9010 - val_loss: 92.9399\n",
      "Epoch 8200/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.9967 - val_loss: 92.8098\n",
      "Epoch 8201/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.9986 - val_loss: 93.6412\n",
      "Epoch 8202/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.5272 - val_loss: 93.5919\n",
      "Epoch 8203/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.4264 - val_loss: 92.9386\n",
      "Epoch 8204/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.8728 - val_loss: 93.2633\n",
      "Epoch 8205/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.2450 - val_loss: 92.6981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8206/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6136 - val_loss: 93.2034\n",
      "Epoch 8207/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.1693 - val_loss: 92.9837\n",
      "Epoch 8208/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.2123 - val_loss: 93.0107\n",
      "Epoch 8209/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 90.6594 - val_loss: 93.4525\n",
      "Epoch 8210/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.6796 - val_loss: 92.7864\n",
      "Epoch 8211/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.0054 - val_loss: 92.8858\n",
      "Epoch 8212/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 92.1414 - val_loss: 92.3601\n",
      "Epoch 8213/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8285 - val_loss: 92.9464\n",
      "Epoch 8214/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.7725 - val_loss: 93.2002\n",
      "Epoch 8215/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.2213 - val_loss: 92.7198\n",
      "Epoch 8216/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.9096 - val_loss: 92.4846\n",
      "Epoch 8217/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.6348 - val_loss: 93.0112\n",
      "Epoch 8218/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1694 - val_loss: 92.8260\n",
      "Epoch 8219/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.7715 - val_loss: 92.7431\n",
      "Epoch 8220/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.1310 - val_loss: 92.8300\n",
      "Epoch 8221/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.5579 - val_loss: 92.6520\n",
      "Epoch 8222/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9954 - val_loss: 92.9469\n",
      "Epoch 8223/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.1940 - val_loss: 92.8219\n",
      "Epoch 8224/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.9713 - val_loss: 92.3353\n",
      "Epoch 8225/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.1833 - val_loss: 93.2210\n",
      "Epoch 8226/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.0426 - val_loss: 93.0253\n",
      "Epoch 8227/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.2727 - val_loss: 93.2981\n",
      "Epoch 8228/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 94.1648 - val_loss: 92.7893\n",
      "Epoch 8229/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.2943 - val_loss: 93.0347\n",
      "Epoch 8230/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 90.6283 - val_loss: 93.0362\n",
      "Epoch 8231/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.1580 - val_loss: 93.1277\n",
      "Epoch 8232/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6642 - val_loss: 92.6065\n",
      "Epoch 8233/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.7547 - val_loss: 93.5101\n",
      "Epoch 8234/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.6026 - val_loss: 92.8668\n",
      "Epoch 8235/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.5507 - val_loss: 92.8416\n",
      "Epoch 8236/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.8422 - val_loss: 93.7143\n",
      "Epoch 8237/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.1783 - val_loss: 92.2585\n",
      "Epoch 8238/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 94.6967 - val_loss: 93.4771\n",
      "Epoch 8239/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.5154 - val_loss: 93.3972\n",
      "Epoch 8240/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.3993 - val_loss: 92.2336\n",
      "Epoch 8241/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.2074 - val_loss: 93.4886\n",
      "Epoch 8242/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3944 - val_loss: 92.5893\n",
      "Epoch 8243/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.9903 - val_loss: 93.2566\n",
      "Epoch 8244/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.8818 - val_loss: 92.8034\n",
      "Epoch 8245/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9740 - val_loss: 91.6669\n",
      "Epoch 8246/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.9400 - val_loss: 92.7745\n",
      "Epoch 8247/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.2397 - val_loss: 92.4955\n",
      "Epoch 8248/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 91.5612 - val_loss: 92.4794\n",
      "Epoch 8249/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.4734 - val_loss: 92.8368\n",
      "Epoch 8250/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8081 - val_loss: 91.7317\n",
      "Epoch 8251/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.8657 - val_loss: 93.1008\n",
      "Epoch 8252/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.4241 - val_loss: 93.5579\n",
      "Epoch 8253/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.6492 - val_loss: 92.6563\n",
      "Epoch 8254/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.8000 - val_loss: 92.8212\n",
      "Epoch 8255/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 89.9571 - val_loss: 92.6653\n",
      "Epoch 8256/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8833 - val_loss: 93.1799\n",
      "Epoch 8257/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.7197 - val_loss: 92.5605\n",
      "Epoch 8258/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.5904 - val_loss: 92.4460\n",
      "Epoch 8259/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.0906 - val_loss: 93.3797\n",
      "Epoch 8260/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.6501 - val_loss: 93.1142\n",
      "Epoch 8261/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.3943 - val_loss: 92.5265\n",
      "Epoch 8262/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.2993 - val_loss: 92.8189\n",
      "Epoch 8263/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.5621 - val_loss: 93.3066\n",
      "Epoch 8264/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.7906 - val_loss: 92.3133\n",
      "Epoch 8265/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.2773 - val_loss: 92.8562\n",
      "Epoch 8266/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.0114 - val_loss: 93.1271\n",
      "Epoch 8267/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.1509 - val_loss: 92.8562\n",
      "Epoch 8268/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3454 - val_loss: 93.0751\n",
      "Epoch 8269/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.8752 - val_loss: 93.4277\n",
      "Epoch 8270/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8990 - val_loss: 93.2288\n",
      "Epoch 8271/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.2467 - val_loss: 93.1817\n",
      "Epoch 8272/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4718 - val_loss: 92.1844\n",
      "Epoch 8273/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.7514 - val_loss: 93.4453\n",
      "Epoch 8274/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2252 - val_loss: 93.1799\n",
      "Epoch 8275/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8794 - val_loss: 93.2512\n",
      "Epoch 8276/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4112 - val_loss: 92.5022\n",
      "Epoch 8277/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.4130 - val_loss: 93.1184\n",
      "Epoch 8278/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.7879 - val_loss: 93.0703\n",
      "Epoch 8279/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.3175 - val_loss: 93.2552\n",
      "Epoch 8280/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.5652 - val_loss: 92.5125\n",
      "Epoch 8281/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.2599 - val_loss: 93.2471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8282/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.0718 - val_loss: 92.3459\n",
      "Epoch 8283/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.1359 - val_loss: 93.0821\n",
      "Epoch 8284/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5589 - val_loss: 92.7971\n",
      "Epoch 8285/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.5409 - val_loss: 93.3282\n",
      "Epoch 8286/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.0569 - val_loss: 92.3204\n",
      "Epoch 8287/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.1332 - val_loss: 93.1415\n",
      "Epoch 8288/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.4163 - val_loss: 93.1607\n",
      "Epoch 8289/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.5522 - val_loss: 93.2551\n",
      "Epoch 8290/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.7896 - val_loss: 93.1962\n",
      "Epoch 8291/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.9957 - val_loss: 93.2836\n",
      "Epoch 8292/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.2965 - val_loss: 92.7106\n",
      "Epoch 8293/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.6648 - val_loss: 93.2025\n",
      "Epoch 8294/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.6677 - val_loss: 93.3476\n",
      "Epoch 8295/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.3852 - val_loss: 92.9810\n",
      "Epoch 8296/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.8010 - val_loss: 93.2007\n",
      "Epoch 8297/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.6614 - val_loss: 92.6344\n",
      "Epoch 8298/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.6200 - val_loss: 92.6517\n",
      "Epoch 8299/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6859 - val_loss: 93.0572\n",
      "Epoch 8300/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 90.2838 - val_loss: 92.7622\n",
      "Epoch 8301/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 89.6921 - val_loss: 92.4679\n",
      "Epoch 8302/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.2758 - val_loss: 92.7304\n",
      "Epoch 8303/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5086 - val_loss: 94.0512\n",
      "Epoch 8304/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9906 - val_loss: 92.6937\n",
      "Epoch 8305/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.8091 - val_loss: 93.2562\n",
      "Epoch 8306/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.0442 - val_loss: 92.5134\n",
      "Epoch 8307/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.1302 - val_loss: 92.8468\n",
      "Epoch 8308/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.1526 - val_loss: 92.6255\n",
      "Epoch 8309/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.0147 - val_loss: 92.9825\n",
      "Epoch 8310/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.4176 - val_loss: 92.8810\n",
      "Epoch 8311/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 94.8122 - val_loss: 93.5685\n",
      "Epoch 8312/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.0742 - val_loss: 93.0597\n",
      "Epoch 8313/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.4817 - val_loss: 92.5882\n",
      "Epoch 8314/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.0550 - val_loss: 92.8429\n",
      "Epoch 8315/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.8503 - val_loss: 93.7121\n",
      "Epoch 8316/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3632 - val_loss: 92.6866\n",
      "Epoch 8317/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.8249 - val_loss: 93.0140\n",
      "Epoch 8318/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.7046 - val_loss: 93.0217\n",
      "Epoch 8319/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 90.7942 - val_loss: 93.2728\n",
      "Epoch 8320/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.8381 - val_loss: 93.1856\n",
      "Epoch 8321/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.8151 - val_loss: 93.5638\n",
      "Epoch 8322/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.5760 - val_loss: 93.1518\n",
      "Epoch 8323/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.0194 - val_loss: 93.0523\n",
      "Epoch 8324/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.8937 - val_loss: 92.4882\n",
      "Epoch 8325/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.2349 - val_loss: 92.8373\n",
      "Epoch 8326/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.5858 - val_loss: 92.8033\n",
      "Epoch 8327/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0991 - val_loss: 92.5037\n",
      "Epoch 8328/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2799 - val_loss: 93.3107\n",
      "Epoch 8329/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 90.3761 - val_loss: 93.0995\n",
      "Epoch 8330/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.7792 - val_loss: 92.8909\n",
      "Epoch 8331/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.2806 - val_loss: 92.8395\n",
      "Epoch 8332/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.7860 - val_loss: 93.2572\n",
      "Epoch 8333/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.5465 - val_loss: 93.1702\n",
      "Epoch 8334/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.8172 - val_loss: 93.2892\n",
      "Epoch 8335/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.2661 - val_loss: 93.2043\n",
      "Epoch 8336/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.6847 - val_loss: 92.5331\n",
      "Epoch 8337/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6051 - val_loss: 93.4046\n",
      "Epoch 8338/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.1183 - val_loss: 92.9950\n",
      "Epoch 8339/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.4858 - val_loss: 92.1108\n",
      "Epoch 8340/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.1334 - val_loss: 92.9160\n",
      "Epoch 8341/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.7697 - val_loss: 92.6316\n",
      "Epoch 8342/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 90.9022 - val_loss: 92.1666\n",
      "Epoch 8343/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.2852 - val_loss: 93.1672\n",
      "Epoch 8344/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5429 - val_loss: 92.4958\n",
      "Epoch 8345/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.9673 - val_loss: 93.1215\n",
      "Epoch 8346/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.0968 - val_loss: 93.0612\n",
      "Epoch 8347/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1958 - val_loss: 92.1772\n",
      "Epoch 8348/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.7519 - val_loss: 93.6068\n",
      "Epoch 8349/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8533 - val_loss: 92.7562\n",
      "Epoch 8350/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 89.4352 - val_loss: 92.7959\n",
      "Epoch 8351/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.4427 - val_loss: 92.5308\n",
      "Epoch 8352/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.0129 - val_loss: 93.3929\n",
      "Epoch 8353/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.4588 - val_loss: 92.0393\n",
      "Epoch 8354/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.8244 - val_loss: 92.7802\n",
      "Epoch 8355/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.4638 - val_loss: 94.0237\n",
      "Epoch 8356/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9826 - val_loss: 92.4867\n",
      "Epoch 8357/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3981 - val_loss: 92.9888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8358/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.3011 - val_loss: 92.8812\n",
      "Epoch 8359/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 89.7557 - val_loss: 93.1671\n",
      "Epoch 8360/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.5077 - val_loss: 93.1477\n",
      "Epoch 8361/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.1646 - val_loss: 91.8723\n",
      "Epoch 8362/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.1510 - val_loss: 92.5319\n",
      "Epoch 8363/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.1024 - val_loss: 92.6628\n",
      "Epoch 8364/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.9449 - val_loss: 92.8416\n",
      "Epoch 8365/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.9316 - val_loss: 93.3746\n",
      "Epoch 8366/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8991 - val_loss: 93.2283\n",
      "Epoch 8367/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.0032 - val_loss: 92.7079\n",
      "Epoch 8368/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 91.4844 - val_loss: 92.7954\n",
      "Epoch 8369/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.9092 - val_loss: 93.0894\n",
      "Epoch 8370/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.2369 - val_loss: 92.9305\n",
      "Epoch 8371/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.1652 - val_loss: 93.1129\n",
      "Epoch 8372/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9323 - val_loss: 92.7808\n",
      "Epoch 8373/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.5975 - val_loss: 92.4848\n",
      "Epoch 8374/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 89.9754 - val_loss: 92.9945\n",
      "Epoch 8375/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.1306 - val_loss: 92.9382\n",
      "Epoch 8376/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.6627 - val_loss: 92.6247\n",
      "Epoch 8377/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.6398 - val_loss: 92.9324\n",
      "Epoch 8378/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.4829 - val_loss: 92.0311\n",
      "Epoch 8379/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.1622 - val_loss: 92.8591\n",
      "Epoch 8380/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.6184 - val_loss: 92.7490\n",
      "Epoch 8381/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.3105 - val_loss: 93.8829\n",
      "Epoch 8382/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.5140 - val_loss: 92.9022\n",
      "Epoch 8383/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.0330 - val_loss: 92.6640\n",
      "Epoch 8384/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.9957 - val_loss: 92.8122\n",
      "Epoch 8385/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.4068 - val_loss: 93.0298\n",
      "Epoch 8386/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.2259 - val_loss: 91.9829\n",
      "Epoch 8387/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5795 - val_loss: 92.4102\n",
      "Epoch 8388/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.4465 - val_loss: 91.9740\n",
      "Epoch 8389/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.7850 - val_loss: 92.7184\n",
      "Epoch 8390/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5431 - val_loss: 92.3177\n",
      "Epoch 8391/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2015 - val_loss: 92.8122\n",
      "Epoch 8392/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.4096 - val_loss: 93.0504\n",
      "Epoch 8393/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.7736 - val_loss: 93.0089\n",
      "Epoch 8394/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9124 - val_loss: 92.5578\n",
      "Epoch 8395/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3122 - val_loss: 93.0738\n",
      "Epoch 8396/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5219 - val_loss: 93.0976\n",
      "Epoch 8397/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.3749 - val_loss: 93.5575\n",
      "Epoch 8398/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.5171 - val_loss: 92.9221\n",
      "Epoch 8399/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.7695 - val_loss: 92.7775\n",
      "Epoch 8400/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 89.8915 - val_loss: 93.4733\n",
      "Epoch 8401/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.7349 - val_loss: 92.6747\n",
      "Epoch 8402/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.7238 - val_loss: 93.2771\n",
      "Epoch 8403/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.2567 - val_loss: 92.2289\n",
      "Epoch 8404/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.8765 - val_loss: 93.0345\n",
      "Epoch 8405/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.2160 - val_loss: 92.9103\n",
      "Epoch 8406/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.7319 - val_loss: 92.9509\n",
      "Epoch 8407/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.0351 - val_loss: 92.4098\n",
      "Epoch 8408/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.2197 - val_loss: 93.3953\n",
      "Epoch 8409/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.7434 - val_loss: 92.9980\n",
      "Epoch 8410/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.4086 - val_loss: 93.6570\n",
      "Epoch 8411/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.8893 - val_loss: 93.0797\n",
      "Epoch 8412/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.4250 - val_loss: 92.9499\n",
      "Epoch 8413/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.4497 - val_loss: 92.8288\n",
      "Epoch 8414/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.4898 - val_loss: 92.6391\n",
      "Epoch 8415/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.0213 - val_loss: 92.9514\n",
      "Epoch 8416/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.4432 - val_loss: 93.6946\n",
      "Epoch 8417/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.6399 - val_loss: 93.1251\n",
      "Epoch 8418/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.1852 - val_loss: 93.4952\n",
      "Epoch 8419/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.8666 - val_loss: 93.0034\n",
      "Epoch 8420/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.1708 - val_loss: 93.5696\n",
      "Epoch 8421/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.1148 - val_loss: 92.5397\n",
      "Epoch 8422/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.4632 - val_loss: 93.3479\n",
      "Epoch 8423/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.9515 - val_loss: 92.1749\n",
      "Epoch 8424/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.8908 - val_loss: 93.1767\n",
      "Epoch 8425/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.2136 - val_loss: 93.3989\n",
      "Epoch 8426/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2068 - val_loss: 91.9306\n",
      "Epoch 8427/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.8630 - val_loss: 93.5775\n",
      "Epoch 8428/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.6344 - val_loss: 92.9637\n",
      "Epoch 8429/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.4609 - val_loss: 93.4035\n",
      "Epoch 8430/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.9671 - val_loss: 92.8144\n",
      "Epoch 8431/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.5944 - val_loss: 92.9601\n",
      "Epoch 8432/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.4851 - val_loss: 92.5912\n",
      "Epoch 8433/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 89.6543 - val_loss: 93.2276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8434/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.5461 - val_loss: 93.3550\n",
      "Epoch 8435/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3617 - val_loss: 93.2615\n",
      "Epoch 8436/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.6160 - val_loss: 92.2715\n",
      "Epoch 8437/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.2213 - val_loss: 93.0160\n",
      "Epoch 8438/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4208 - val_loss: 92.7923\n",
      "Epoch 8439/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.3191 - val_loss: 93.2567\n",
      "Epoch 8440/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.8069 - val_loss: 93.1349\n",
      "Epoch 8441/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.9606 - val_loss: 92.7890\n",
      "Epoch 8442/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.3699 - val_loss: 92.9656\n",
      "Epoch 8443/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5442 - val_loss: 93.4772\n",
      "Epoch 8444/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.2048 - val_loss: 92.6767\n",
      "Epoch 8445/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3921 - val_loss: 92.9460\n",
      "Epoch 8446/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5413 - val_loss: 92.3971\n",
      "Epoch 8447/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 89.4261 - val_loss: 92.2387\n",
      "Epoch 8448/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.4543 - val_loss: 92.5683\n",
      "Epoch 8449/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.7987 - val_loss: 93.1248\n",
      "Epoch 8450/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.8529 - val_loss: 93.0007\n",
      "Epoch 8451/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.2857 - val_loss: 93.3619\n",
      "Epoch 8452/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.2400 - val_loss: 92.4405\n",
      "Epoch 8453/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.0014 - val_loss: 93.1755\n",
      "Epoch 8454/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.7730 - val_loss: 93.3090\n",
      "Epoch 8455/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.8477 - val_loss: 93.2196\n",
      "Epoch 8456/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9883 - val_loss: 92.6746\n",
      "Epoch 8457/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7340 - val_loss: 93.0271\n",
      "Epoch 8458/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.1385 - val_loss: 93.2549\n",
      "Epoch 8459/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9137 - val_loss: 92.5886\n",
      "Epoch 8460/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.5513 - val_loss: 92.4483\n",
      "Epoch 8461/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.8493 - val_loss: 93.5389\n",
      "Epoch 8462/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.7377 - val_loss: 92.3475\n",
      "Epoch 8463/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.3007 - val_loss: 93.7294\n",
      "Epoch 8464/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.1908 - val_loss: 92.6331\n",
      "Epoch 8465/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 90.0206 - val_loss: 94.1040\n",
      "Epoch 8466/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.5787 - val_loss: 91.8732\n",
      "Epoch 8467/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.3602 - val_loss: 92.7493\n",
      "Epoch 8468/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.2657 - val_loss: 93.1028\n",
      "Epoch 8469/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.5524 - val_loss: 94.0209\n",
      "Epoch 8470/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.3978 - val_loss: 93.4766\n",
      "Epoch 8471/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8010 - val_loss: 92.4715\n",
      "Epoch 8472/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.2099 - val_loss: 92.6841\n",
      "Epoch 8473/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.7134 - val_loss: 92.3361\n",
      "Epoch 8474/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.7660 - val_loss: 93.3263\n",
      "Epoch 8475/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.5417 - val_loss: 92.9876\n",
      "Epoch 8476/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2691 - val_loss: 93.0075\n",
      "Epoch 8477/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.2586 - val_loss: 93.1702\n",
      "Epoch 8478/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.2855 - val_loss: 92.6172\n",
      "Epoch 8479/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6739 - val_loss: 93.2315\n",
      "Epoch 8480/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.0710 - val_loss: 93.2244\n",
      "Epoch 8481/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.4305 - val_loss: 93.3939\n",
      "Epoch 8482/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.0291 - val_loss: 94.1591\n",
      "Epoch 8483/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.5605 - val_loss: 92.5782\n",
      "Epoch 8484/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 89.5714 - val_loss: 93.2136\n",
      "Epoch 8485/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.7991 - val_loss: 93.0643\n",
      "Epoch 8486/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.0579 - val_loss: 93.0241\n",
      "Epoch 8487/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.4678 - val_loss: 93.0517\n",
      "Epoch 8488/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.8249 - val_loss: 93.2922\n",
      "Epoch 8489/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.6625 - val_loss: 93.1195\n",
      "Epoch 8490/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.6782 - val_loss: 92.4274\n",
      "Epoch 8491/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5664 - val_loss: 93.3102\n",
      "Epoch 8492/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 89.5803 - val_loss: 93.0985\n",
      "Epoch 8493/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.0524 - val_loss: 93.1843\n",
      "Epoch 8494/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3907 - val_loss: 92.7254\n",
      "Epoch 8495/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.6797 - val_loss: 93.5363\n",
      "Epoch 8496/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.5709 - val_loss: 92.8591\n",
      "Epoch 8497/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.8742 - val_loss: 93.7514\n",
      "Epoch 8498/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.0812 - val_loss: 93.0173\n",
      "Epoch 8499/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1547 - val_loss: 93.1554\n",
      "Epoch 8500/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.1099 - val_loss: 93.0332\n",
      "Epoch 8501/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.8389 - val_loss: 92.4253\n",
      "Epoch 8502/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.6684 - val_loss: 93.0839\n",
      "Epoch 8503/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.0495 - val_loss: 93.0146\n",
      "Epoch 8504/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.3890 - val_loss: 92.3735\n",
      "Epoch 8505/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.3744 - val_loss: 92.3926\n",
      "Epoch 8506/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.4212 - val_loss: 93.0676\n",
      "Epoch 8507/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.4572 - val_loss: 93.3432\n",
      "Epoch 8508/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9443 - val_loss: 92.6566\n",
      "Epoch 8509/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7655 - val_loss: 92.8597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8510/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.0717 - val_loss: 93.4903\n",
      "Epoch 8511/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2683 - val_loss: 92.5807\n",
      "Epoch 8512/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 89.8788 - val_loss: 93.2964\n",
      "Epoch 8513/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.7573 - val_loss: 93.5245\n",
      "Epoch 8514/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.9484 - val_loss: 92.9807\n",
      "Epoch 8515/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.1884 - val_loss: 92.7737\n",
      "Epoch 8516/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.2426 - val_loss: 92.5851\n",
      "Epoch 8517/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.4308 - val_loss: 93.4670\n",
      "Epoch 8518/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2701 - val_loss: 91.9985\n",
      "Epoch 8519/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.0188 - val_loss: 93.5127\n",
      "Epoch 8520/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 89.9598 - val_loss: 93.0915\n",
      "Epoch 8521/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.6693 - val_loss: 92.8187\n",
      "Epoch 8522/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.9018 - val_loss: 92.8586\n",
      "Epoch 8523/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.1143 - val_loss: 92.9854\n",
      "Epoch 8524/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.7244 - val_loss: 92.6888\n",
      "Epoch 8525/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3714 - val_loss: 92.8854\n",
      "Epoch 8526/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.3049 - val_loss: 93.2313\n",
      "Epoch 8527/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7540 - val_loss: 92.9596\n",
      "Epoch 8528/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.8014 - val_loss: 93.0160\n",
      "Epoch 8529/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.8492 - val_loss: 92.7529\n",
      "Epoch 8530/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.4461 - val_loss: 93.3573\n",
      "Epoch 8531/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.3279 - val_loss: 93.2751\n",
      "Epoch 8532/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.3427 - val_loss: 92.5879\n",
      "Epoch 8533/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.6835 - val_loss: 93.1638\n",
      "Epoch 8534/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.5261 - val_loss: 93.1592\n",
      "Epoch 8535/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.1309 - val_loss: 92.0167\n",
      "Epoch 8536/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.9992 - val_loss: 92.9971\n",
      "Epoch 8537/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.9076 - val_loss: 92.8318\n",
      "Epoch 8538/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.8425 - val_loss: 92.8827\n",
      "Epoch 8539/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.1411 - val_loss: 92.1433\n",
      "Epoch 8540/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.1249 - val_loss: 92.9611\n",
      "Epoch 8541/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.1779 - val_loss: 92.4653\n",
      "Epoch 8542/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.6296 - val_loss: 92.9059\n",
      "Epoch 8543/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.3732 - val_loss: 93.1213\n",
      "Epoch 8544/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3585 - val_loss: 92.9190\n",
      "Epoch 8545/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.1938 - val_loss: 92.9012\n",
      "Epoch 8546/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.5529 - val_loss: 93.4698\n",
      "Epoch 8547/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 89.9321 - val_loss: 93.2350\n",
      "Epoch 8548/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.4883 - val_loss: 92.3420\n",
      "Epoch 8549/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.0373 - val_loss: 93.1339\n",
      "Epoch 8550/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.4177 - val_loss: 92.3818\n",
      "Epoch 8551/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.9257 - val_loss: 92.9693\n",
      "Epoch 8552/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9041 - val_loss: 92.8327\n",
      "Epoch 8553/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.4539 - val_loss: 92.7525\n",
      "Epoch 8554/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 90.5151 - val_loss: 92.7207\n",
      "Epoch 8555/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.6708 - val_loss: 92.8245\n",
      "Epoch 8556/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.1563 - val_loss: 92.8945\n",
      "Epoch 8557/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.6266 - val_loss: 93.0866\n",
      "Epoch 8558/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.7952 - val_loss: 93.2678\n",
      "Epoch 8559/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 89.8479 - val_loss: 92.6846\n",
      "Epoch 8560/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.5275 - val_loss: 92.7352\n",
      "Epoch 8561/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.7256 - val_loss: 93.2377\n",
      "Epoch 8562/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.1011 - val_loss: 93.3273\n",
      "Epoch 8563/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.0892 - val_loss: 92.9409\n",
      "Epoch 8564/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.9715 - val_loss: 92.0613\n",
      "Epoch 8565/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.2908 - val_loss: 93.1826\n",
      "Epoch 8566/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.5065 - val_loss: 93.0100\n",
      "Epoch 8567/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.9915 - val_loss: 92.9469\n",
      "Epoch 8568/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4509 - val_loss: 92.7362\n",
      "Epoch 8569/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1397 - val_loss: 92.6498\n",
      "Epoch 8570/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0961 - val_loss: 92.8850\n",
      "Epoch 8571/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 90.2617 - val_loss: 93.0227\n",
      "Epoch 8572/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.5267 - val_loss: 92.7929\n",
      "Epoch 8573/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.8843 - val_loss: 92.3757\n",
      "Epoch 8574/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2513 - val_loss: 92.7830\n",
      "Epoch 8575/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 89.8053 - val_loss: 93.2667\n",
      "Epoch 8576/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.1889 - val_loss: 93.0056\n",
      "Epoch 8577/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.9917 - val_loss: 92.1966\n",
      "Epoch 8578/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.7886 - val_loss: 92.4961\n",
      "Epoch 8579/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0003 - val_loss: 92.9641\n",
      "Epoch 8580/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.1617 - val_loss: 92.8588\n",
      "Epoch 8581/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.5405 - val_loss: 92.4189\n",
      "Epoch 8582/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0214 - val_loss: 93.1129\n",
      "Epoch 8583/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 94.3252 - val_loss: 92.9135\n",
      "Epoch 8584/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5547 - val_loss: 93.1152\n",
      "Epoch 8585/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.1569 - val_loss: 93.3352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8586/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.6463 - val_loss: 92.7425\n",
      "Epoch 8587/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.0607 - val_loss: 92.7273\n",
      "Epoch 8588/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.1652 - val_loss: 92.3005\n",
      "Epoch 8589/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.8340 - val_loss: 92.9794\n",
      "Epoch 8590/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.8617 - val_loss: 92.8263\n",
      "Epoch 8591/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.0302 - val_loss: 92.9733\n",
      "Epoch 8592/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.5036 - val_loss: 92.0122\n",
      "Epoch 8593/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.4656 - val_loss: 93.3441\n",
      "Epoch 8594/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7074 - val_loss: 92.3830\n",
      "Epoch 8595/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.2845 - val_loss: 93.4383\n",
      "Epoch 8596/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.1029 - val_loss: 93.2005\n",
      "Epoch 8597/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8349 - val_loss: 92.9304\n",
      "Epoch 8598/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.1242 - val_loss: 92.5564\n",
      "Epoch 8599/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.5238 - val_loss: 92.7889\n",
      "Epoch 8600/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.2914 - val_loss: 92.7005\n",
      "Epoch 8601/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.7191 - val_loss: 92.8473\n",
      "Epoch 8602/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.5346 - val_loss: 92.6989\n",
      "Epoch 8603/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8166 - val_loss: 92.9622\n",
      "Epoch 8604/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.0466 - val_loss: 92.4911\n",
      "Epoch 8605/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6445 - val_loss: 93.4807\n",
      "Epoch 8606/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.5312 - val_loss: 92.8871\n",
      "Epoch 8607/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 89.7657 - val_loss: 93.0820\n",
      "Epoch 8608/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 89.8811 - val_loss: 92.7827\n",
      "Epoch 8609/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8916 - val_loss: 92.7419\n",
      "Epoch 8610/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.2829 - val_loss: 93.2986\n",
      "Epoch 8611/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.3024 - val_loss: 92.3835\n",
      "Epoch 8612/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.9128 - val_loss: 92.6724\n",
      "Epoch 8613/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 89.9387 - val_loss: 92.5268\n",
      "Epoch 8614/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.6926 - val_loss: 92.8836\n",
      "Epoch 8615/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.4233 - val_loss: 92.9785\n",
      "Epoch 8616/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.5043 - val_loss: 92.0522\n",
      "Epoch 8617/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 94.4633 - val_loss: 93.8102\n",
      "Epoch 8618/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.7504 - val_loss: 93.1007\n",
      "Epoch 8619/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3140 - val_loss: 92.9927\n",
      "Epoch 8620/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.6854 - val_loss: 93.3939\n",
      "Epoch 8621/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6974 - val_loss: 92.3033\n",
      "Epoch 8622/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.5635 - val_loss: 92.3160\n",
      "Epoch 8623/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.3946 - val_loss: 92.0309\n",
      "Epoch 8624/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 91.2809 - val_loss: 93.8696\n",
      "Epoch 8625/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.0957 - val_loss: 92.1931\n",
      "Epoch 8626/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6303 - val_loss: 93.7630\n",
      "Epoch 8627/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.8740 - val_loss: 91.7029\n",
      "Epoch 8628/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.0852 - val_loss: 92.4173\n",
      "Epoch 8629/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.1796 - val_loss: 92.9577\n",
      "Epoch 8630/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.7079 - val_loss: 92.7418\n",
      "Epoch 8631/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.4401 - val_loss: 93.0074\n",
      "Epoch 8632/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9739 - val_loss: 93.1039\n",
      "Epoch 8633/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.7144 - val_loss: 92.8192\n",
      "Epoch 8634/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.4810 - val_loss: 92.7879\n",
      "Epoch 8635/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.2655 - val_loss: 93.1272\n",
      "Epoch 8636/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.8106 - val_loss: 92.1514\n",
      "Epoch 8637/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.9780 - val_loss: 92.6788\n",
      "Epoch 8638/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.3393 - val_loss: 92.9614\n",
      "Epoch 8639/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9222 - val_loss: 92.2850\n",
      "Epoch 8640/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.8587 - val_loss: 92.3855\n",
      "Epoch 8641/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.0711 - val_loss: 92.5932\n",
      "Epoch 8642/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3188 - val_loss: 92.8033\n",
      "Epoch 8643/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.0964 - val_loss: 92.8253\n",
      "Epoch 8644/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1461 - val_loss: 92.5424\n",
      "Epoch 8645/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.7847 - val_loss: 93.2521\n",
      "Epoch 8646/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.3483 - val_loss: 92.6613\n",
      "Epoch 8647/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.8058 - val_loss: 92.7958\n",
      "Epoch 8648/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.4588 - val_loss: 92.7416\n",
      "Epoch 8649/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.9339 - val_loss: 93.1334\n",
      "Epoch 8650/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.5737 - val_loss: 93.0866\n",
      "Epoch 8651/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.8309 - val_loss: 92.7945\n",
      "Epoch 8652/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.4738 - val_loss: 92.6069\n",
      "Epoch 8653/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.1951 - val_loss: 92.7266\n",
      "Epoch 8654/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.2838 - val_loss: 93.3831\n",
      "Epoch 8655/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.2275 - val_loss: 91.5898\n",
      "Epoch 8656/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.0323 - val_loss: 93.2024\n",
      "Epoch 8657/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1121 - val_loss: 92.7206\n",
      "Epoch 8658/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 90.3461 - val_loss: 93.4321\n",
      "Epoch 8659/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.5688 - val_loss: 93.1128\n",
      "Epoch 8660/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.4827 - val_loss: 92.6504\n",
      "Epoch 8661/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.7646 - val_loss: 93.1684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8662/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 89.4599 - val_loss: 93.1481\n",
      "Epoch 8663/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8507 - val_loss: 92.6794\n",
      "Epoch 8664/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.8409 - val_loss: 93.0466\n",
      "Epoch 8665/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7475 - val_loss: 92.7975\n",
      "Epoch 8666/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.4605 - val_loss: 92.9103\n",
      "Epoch 8667/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.6359 - val_loss: 92.7435\n",
      "Epoch 8668/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.5138 - val_loss: 93.0494\n",
      "Epoch 8669/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.4185 - val_loss: 92.2650\n",
      "Epoch 8670/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.2251 - val_loss: 93.3615\n",
      "Epoch 8671/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.6323 - val_loss: 93.5295\n",
      "Epoch 8672/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.9560 - val_loss: 92.9844\n",
      "Epoch 8673/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.7688 - val_loss: 92.7278\n",
      "Epoch 8674/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3427 - val_loss: 92.7320\n",
      "Epoch 8675/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.5041 - val_loss: 92.7116\n",
      "Epoch 8676/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.0975 - val_loss: 92.1822\n",
      "Epoch 8677/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 92.2883 - val_loss: 92.1312\n",
      "Epoch 8678/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 89.8492 - val_loss: 91.9402\n",
      "Epoch 8679/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 90.9241 - val_loss: 93.2603\n",
      "Epoch 8680/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.4541 - val_loss: 92.7914\n",
      "Epoch 8681/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 91.5840 - val_loss: 94.1566\n",
      "Epoch 8682/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 89.9631 - val_loss: 92.7391\n",
      "Epoch 8683/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 90.1639 - val_loss: 91.5540\n",
      "Epoch 8684/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9392 - val_loss: 92.4414\n",
      "Epoch 8685/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.5473 - val_loss: 92.6031\n",
      "Epoch 8686/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.1614 - val_loss: 92.6308\n",
      "Epoch 8687/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1431 - val_loss: 92.9051\n",
      "Epoch 8688/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.6419 - val_loss: 92.6937\n",
      "Epoch 8689/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.4125 - val_loss: 92.5301\n",
      "Epoch 8690/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.7285 - val_loss: 92.9740\n",
      "Epoch 8691/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.3914 - val_loss: 92.1774\n",
      "Epoch 8692/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8530 - val_loss: 92.8397\n",
      "Epoch 8693/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.4741 - val_loss: 92.4572\n",
      "Epoch 8694/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.3413 - val_loss: 92.8570\n",
      "Epoch 8695/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3712 - val_loss: 92.3838\n",
      "Epoch 8696/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8609 - val_loss: 92.1875\n",
      "Epoch 8697/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2872 - val_loss: 92.1327\n",
      "Epoch 8698/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8222 - val_loss: 92.3193\n",
      "Epoch 8699/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.4139 - val_loss: 91.5663\n",
      "Epoch 8700/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 89.1924 - val_loss: 92.7692\n",
      "Epoch 8701/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8374 - val_loss: 93.8259\n",
      "Epoch 8702/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7969 - val_loss: 92.6740\n",
      "Epoch 8703/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.6177 - val_loss: 93.2924\n",
      "Epoch 8704/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 91.3050 - val_loss: 92.6989\n",
      "Epoch 8705/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.5192 - val_loss: 92.5539\n",
      "Epoch 8706/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.3774 - val_loss: 92.7963\n",
      "Epoch 8707/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.4822 - val_loss: 92.3746\n",
      "Epoch 8708/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 90.2827 - val_loss: 92.8124\n",
      "Epoch 8709/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 89.5320 - val_loss: 92.1066\n",
      "Epoch 8710/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.7082 - val_loss: 92.7515\n",
      "Epoch 8711/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4391 - val_loss: 93.2323\n",
      "Epoch 8712/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 94.7384 - val_loss: 92.5946\n",
      "Epoch 8713/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.7551 - val_loss: 93.5032\n",
      "Epoch 8714/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.3025 - val_loss: 92.2498\n",
      "Epoch 8715/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.9075 - val_loss: 92.7026\n",
      "Epoch 8716/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.4606 - val_loss: 92.7208\n",
      "Epoch 8717/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.8474 - val_loss: 93.3506\n",
      "Epoch 8718/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.3237 - val_loss: 92.9189\n",
      "Epoch 8719/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.3840 - val_loss: 91.6899\n",
      "Epoch 8720/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 89.8502 - val_loss: 92.7819\n",
      "Epoch 8721/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.5298 - val_loss: 91.9446\n",
      "Epoch 8722/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.9556 - val_loss: 92.9456\n",
      "Epoch 8723/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.7100 - val_loss: 92.1584\n",
      "Epoch 8724/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3171 - val_loss: 93.6370\n",
      "Epoch 8725/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 91.4752 - val_loss: 93.6509\n",
      "Epoch 8726/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.2388 - val_loss: 92.3138\n",
      "Epoch 8727/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.5544 - val_loss: 92.7590\n",
      "Epoch 8728/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.4346 - val_loss: 93.2874\n",
      "Epoch 8729/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.9305 - val_loss: 93.8789\n",
      "Epoch 8730/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.6107 - val_loss: 92.8763\n",
      "Epoch 8731/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7596 - val_loss: 93.0178\n",
      "Epoch 8732/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 90.9246 - val_loss: 93.0350\n",
      "Epoch 8733/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.3171 - val_loss: 92.8023\n",
      "Epoch 8734/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.5917 - val_loss: 93.1373\n",
      "Epoch 8735/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7558 - val_loss: 93.5540\n",
      "Epoch 8736/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.9959 - val_loss: 93.2488\n",
      "Epoch 8737/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.3743 - val_loss: 92.3077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8738/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 89.8445 - val_loss: 92.6221\n",
      "Epoch 8739/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.5561 - val_loss: 92.2336\n",
      "Epoch 8740/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.7173 - val_loss: 93.7673\n",
      "Epoch 8741/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.3935 - val_loss: 92.7998\n",
      "Epoch 8742/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.2617 - val_loss: 93.0761\n",
      "Epoch 8743/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.7335 - val_loss: 92.1290\n",
      "Epoch 8744/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.4808 - val_loss: 93.0630\n",
      "Epoch 8745/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.4695 - val_loss: 92.4125\n",
      "Epoch 8746/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.2964 - val_loss: 92.3221\n",
      "Epoch 8747/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1518 - val_loss: 92.5415\n",
      "Epoch 8748/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.4251 - val_loss: 92.5890\n",
      "Epoch 8749/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.8125 - val_loss: 92.7762\n",
      "Epoch 8750/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8074 - val_loss: 92.3741\n",
      "Epoch 8751/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3728 - val_loss: 92.5886\n",
      "Epoch 8752/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0911 - val_loss: 92.6872\n",
      "Epoch 8753/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5266 - val_loss: 92.2309\n",
      "Epoch 8754/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.2391 - val_loss: 92.6625\n",
      "Epoch 8755/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.8259 - val_loss: 92.8565\n",
      "Epoch 8756/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.1758 - val_loss: 92.3524\n",
      "Epoch 8757/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.5910 - val_loss: 92.5869\n",
      "Epoch 8758/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.9742 - val_loss: 92.9140\n",
      "Epoch 8759/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 89.9071 - val_loss: 92.6215\n",
      "Epoch 8760/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.7983 - val_loss: 92.6330\n",
      "Epoch 8761/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1235 - val_loss: 92.9220\n",
      "Epoch 8762/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.7533 - val_loss: 93.0902\n",
      "Epoch 8763/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 90.2132 - val_loss: 92.5879\n",
      "Epoch 8764/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 89.6517 - val_loss: 93.0603\n",
      "Epoch 8765/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.6748 - val_loss: 93.5250\n",
      "Epoch 8766/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.3495 - val_loss: 92.6975\n",
      "Epoch 8767/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.2539 - val_loss: 92.7976\n",
      "Epoch 8768/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.4826 - val_loss: 92.8396\n",
      "Epoch 8769/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.0121 - val_loss: 92.8484\n",
      "Epoch 8770/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3488 - val_loss: 92.6470\n",
      "Epoch 8771/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.6920 - val_loss: 93.4449\n",
      "Epoch 8772/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 92.2884 - val_loss: 93.6705\n",
      "Epoch 8773/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5208 - val_loss: 92.4499\n",
      "Epoch 8774/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.7486 - val_loss: 92.1139\n",
      "Epoch 8775/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.6666 - val_loss: 92.6779\n",
      "Epoch 8776/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.1357 - val_loss: 92.9859\n",
      "Epoch 8777/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 91.4188 - val_loss: 92.6780\n",
      "Epoch 8778/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9604 - val_loss: 93.1921\n",
      "Epoch 8779/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.4111 - val_loss: 92.2743\n",
      "Epoch 8780/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6598 - val_loss: 92.2834\n",
      "Epoch 8781/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 89.4324 - val_loss: 93.3501\n",
      "Epoch 8782/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1195 - val_loss: 93.2074\n",
      "Epoch 8783/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 96.2627 - val_loss: 92.8289\n",
      "Epoch 8784/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1910 - val_loss: 92.2912\n",
      "Epoch 8785/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.2681 - val_loss: 92.9389\n",
      "Epoch 8786/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9311 - val_loss: 93.6462\n",
      "Epoch 8787/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9113 - val_loss: 92.7972\n",
      "Epoch 8788/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.9195 - val_loss: 92.9515\n",
      "Epoch 8789/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3535 - val_loss: 92.9125\n",
      "Epoch 8790/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6034 - val_loss: 93.3350\n",
      "Epoch 8791/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.8450 - val_loss: 92.7597\n",
      "Epoch 8792/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.4552 - val_loss: 92.0407\n",
      "Epoch 8793/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.9544 - val_loss: 92.2799\n",
      "Epoch 8794/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.6874 - val_loss: 92.8071\n",
      "Epoch 8795/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 90.2988 - val_loss: 92.4636\n",
      "Epoch 8796/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.4477 - val_loss: 93.0674\n",
      "Epoch 8797/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.4893 - val_loss: 93.2462\n",
      "Epoch 8798/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.1182 - val_loss: 92.6061\n",
      "Epoch 8799/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.0198 - val_loss: 91.8629\n",
      "Epoch 8800/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.7033 - val_loss: 91.7994\n",
      "Epoch 8801/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2403 - val_loss: 92.7723\n",
      "Epoch 8802/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.5016 - val_loss: 93.1214\n",
      "Epoch 8803/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.1596 - val_loss: 93.0262\n",
      "Epoch 8804/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.5926 - val_loss: 92.6689\n",
      "Epoch 8805/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.2863 - val_loss: 93.6164\n",
      "Epoch 8806/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.7801 - val_loss: 92.7616\n",
      "Epoch 8807/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3187 - val_loss: 92.2854\n",
      "Epoch 8808/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.7007 - val_loss: 93.1107\n",
      "Epoch 8809/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.2834 - val_loss: 92.3626\n",
      "Epoch 8810/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.1059 - val_loss: 92.9740\n",
      "Epoch 8811/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.2184 - val_loss: 92.7107\n",
      "Epoch 8812/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.1946 - val_loss: 93.4031\n",
      "Epoch 8813/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.8946 - val_loss: 91.3716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8814/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.2688 - val_loss: 92.7737\n",
      "Epoch 8815/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.1654 - val_loss: 92.2312\n",
      "Epoch 8816/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6398 - val_loss: 92.5996\n",
      "Epoch 8817/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.5168 - val_loss: 92.4776\n",
      "Epoch 8818/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.5963 - val_loss: 92.6650\n",
      "Epoch 8819/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.8741 - val_loss: 92.2939\n",
      "Epoch 8820/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.7964 - val_loss: 92.9262\n",
      "Epoch 8821/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.5915 - val_loss: 92.7486\n",
      "Epoch 8822/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8577 - val_loss: 92.7990\n",
      "Epoch 8823/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.6673 - val_loss: 92.3407\n",
      "Epoch 8824/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0536 - val_loss: 92.0647\n",
      "Epoch 8825/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.2455 - val_loss: 91.8652\n",
      "Epoch 8826/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.5414 - val_loss: 93.0906\n",
      "Epoch 8827/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.3357 - val_loss: 92.6382\n",
      "Epoch 8828/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2684 - val_loss: 92.8522\n",
      "Epoch 8829/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.3888 - val_loss: 92.6289\n",
      "Epoch 8830/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.4836 - val_loss: 92.2004\n",
      "Epoch 8831/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.1132 - val_loss: 93.2132\n",
      "Epoch 8832/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.6244 - val_loss: 92.6605\n",
      "Epoch 8833/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.6196 - val_loss: 92.2270\n",
      "Epoch 8834/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.8727 - val_loss: 92.6163\n",
      "Epoch 8835/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9416 - val_loss: 92.9854\n",
      "Epoch 8836/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.9201 - val_loss: 93.2457\n",
      "Epoch 8837/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.9444 - val_loss: 91.9579\n",
      "Epoch 8838/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.4265 - val_loss: 93.1510\n",
      "Epoch 8839/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.2237 - val_loss: 92.6525\n",
      "Epoch 8840/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.4589 - val_loss: 91.9594\n",
      "Epoch 8841/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.4578 - val_loss: 92.6997\n",
      "Epoch 8842/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.3154 - val_loss: 93.3660\n",
      "Epoch 8843/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 93.1648 - val_loss: 93.8404\n",
      "Epoch 8844/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.3767 - val_loss: 93.8128\n",
      "Epoch 8845/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.5927 - val_loss: 93.3506\n",
      "Epoch 8846/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.4623 - val_loss: 93.1022\n",
      "Epoch 8847/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 90.3394 - val_loss: 93.3754\n",
      "Epoch 8848/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 90.9266 - val_loss: 93.2077\n",
      "Epoch 8849/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.9928 - val_loss: 92.8408\n",
      "Epoch 8850/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.2412 - val_loss: 92.0471\n",
      "Epoch 8851/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.0006 - val_loss: 92.6334\n",
      "Epoch 8852/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.4965 - val_loss: 94.0333\n",
      "Epoch 8853/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.5135 - val_loss: 92.8611\n",
      "Epoch 8854/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4016 - val_loss: 92.4400\n",
      "Epoch 8855/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3315 - val_loss: 92.0259\n",
      "Epoch 8856/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5960 - val_loss: 92.3204\n",
      "Epoch 8857/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.9373 - val_loss: 92.6167\n",
      "Epoch 8858/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.8518 - val_loss: 92.8469\n",
      "Epoch 8859/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 91.1495 - val_loss: 93.1426\n",
      "Epoch 8860/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.3028 - val_loss: 92.7462\n",
      "Epoch 8861/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.3960 - val_loss: 91.6015\n",
      "Epoch 8862/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 89.4297 - val_loss: 92.4012\n",
      "Epoch 8863/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.6400 - val_loss: 92.3259\n",
      "Epoch 8864/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.7947 - val_loss: 93.0422\n",
      "Epoch 8865/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.9725 - val_loss: 92.0540\n",
      "Epoch 8866/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.0875 - val_loss: 93.6155\n",
      "Epoch 8867/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.1069 - val_loss: 92.4657\n",
      "Epoch 8868/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.4681 - val_loss: 92.4442\n",
      "Epoch 8869/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.6153 - val_loss: 92.9065\n",
      "Epoch 8870/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.1640 - val_loss: 92.8909\n",
      "Epoch 8871/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.9559 - val_loss: 93.2373\n",
      "Epoch 8872/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 91.1527 - val_loss: 92.4855\n",
      "Epoch 8873/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.0250 - val_loss: 93.2026\n",
      "Epoch 8874/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.3062 - val_loss: 92.5411\n",
      "Epoch 8875/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.9903 - val_loss: 92.8671\n",
      "Epoch 8876/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0507 - val_loss: 92.6882\n",
      "Epoch 8877/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.7923 - val_loss: 92.9040\n",
      "Epoch 8878/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.4950 - val_loss: 93.6027\n",
      "Epoch 8879/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.1687 - val_loss: 93.2156\n",
      "Epoch 8880/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2123 - val_loss: 92.7609\n",
      "Epoch 8881/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.3767 - val_loss: 92.6267\n",
      "Epoch 8882/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.7843 - val_loss: 92.2759\n",
      "Epoch 8883/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.8803 - val_loss: 92.2969\n",
      "Epoch 8884/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.5597 - val_loss: 92.9834\n",
      "Epoch 8885/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.3841 - val_loss: 92.4093\n",
      "Epoch 8886/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 89.6732 - val_loss: 93.8011\n",
      "Epoch 8887/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.2845 - val_loss: 93.2988\n",
      "Epoch 8888/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6652 - val_loss: 93.8976\n",
      "Epoch 8889/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 89.2099 - val_loss: 92.5633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8890/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.1988 - val_loss: 92.6570\n",
      "Epoch 8891/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7278 - val_loss: 92.3585\n",
      "Epoch 8892/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 89.4730 - val_loss: 93.2311\n",
      "Epoch 8893/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.5025 - val_loss: 92.1636\n",
      "Epoch 8894/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.6255 - val_loss: 93.0218\n",
      "Epoch 8895/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 90.0441 - val_loss: 92.8877\n",
      "Epoch 8896/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5884 - val_loss: 93.0735\n",
      "Epoch 8897/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3148 - val_loss: 92.2647\n",
      "Epoch 8898/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.3856 - val_loss: 92.1349\n",
      "Epoch 8899/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.8141 - val_loss: 91.9555\n",
      "Epoch 8900/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.0235 - val_loss: 92.4064\n",
      "Epoch 8901/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.8360 - val_loss: 93.1889\n",
      "Epoch 8902/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2381 - val_loss: 92.6842\n",
      "Epoch 8903/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 89.9855 - val_loss: 93.1197\n",
      "Epoch 8904/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.1700 - val_loss: 93.1789\n",
      "Epoch 8905/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.7025 - val_loss: 92.7588\n",
      "Epoch 8906/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.8013 - val_loss: 93.6056\n",
      "Epoch 8907/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 95.3000 - val_loss: 92.8303\n",
      "Epoch 8908/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 89.9348 - val_loss: 92.8592\n",
      "Epoch 8909/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5858 - val_loss: 91.2354\n",
      "Epoch 8910/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.1163 - val_loss: 92.7859\n",
      "Epoch 8911/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.2206 - val_loss: 93.2574\n",
      "Epoch 8912/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.5887 - val_loss: 92.8289\n",
      "Epoch 8913/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 89.6596 - val_loss: 92.5097\n",
      "Epoch 8914/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.6781 - val_loss: 92.9863\n",
      "Epoch 8915/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.4417 - val_loss: 92.3870\n",
      "Epoch 8916/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.8412 - val_loss: 91.8164\n",
      "Epoch 8917/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.5177 - val_loss: 92.3108\n",
      "Epoch 8918/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 95.6230 - val_loss: 92.6125\n",
      "Epoch 8919/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.5619 - val_loss: 92.0724\n",
      "Epoch 8920/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.9986 - val_loss: 92.7845\n",
      "Epoch 8921/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.7891 - val_loss: 92.7097\n",
      "Epoch 8922/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.4810 - val_loss: 93.1005\n",
      "Epoch 8923/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.1949 - val_loss: 92.7627\n",
      "Epoch 8924/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7180 - val_loss: 92.6096\n",
      "Epoch 8925/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.7222 - val_loss: 92.3479\n",
      "Epoch 8926/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.8068 - val_loss: 92.3779\n",
      "Epoch 8927/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.0428 - val_loss: 92.7721\n",
      "Epoch 8928/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.9040 - val_loss: 92.8621\n",
      "Epoch 8929/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.6301 - val_loss: 93.0156\n",
      "Epoch 8930/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.9266 - val_loss: 92.5433\n",
      "Epoch 8931/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.3043 - val_loss: 92.9505\n",
      "Epoch 8932/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2285 - val_loss: 92.9319\n",
      "Epoch 8933/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.2195 - val_loss: 92.1667\n",
      "Epoch 8934/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 90.7078 - val_loss: 93.1335\n",
      "Epoch 8935/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 89.9464 - val_loss: 93.0407\n",
      "Epoch 8936/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.0345 - val_loss: 93.1849\n",
      "Epoch 8937/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.8293 - val_loss: 92.6286\n",
      "Epoch 8938/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.4710 - val_loss: 92.7186\n",
      "Epoch 8939/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.1995 - val_loss: 92.3332\n",
      "Epoch 8940/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.2355 - val_loss: 92.6452\n",
      "Epoch 8941/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.3469 - val_loss: 92.6573\n",
      "Epoch 8942/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.7826 - val_loss: 93.3160\n",
      "Epoch 8943/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.2436 - val_loss: 93.0377\n",
      "Epoch 8944/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 88.9815 - val_loss: 92.3566\n",
      "Epoch 8945/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.2804 - val_loss: 92.6833\n",
      "Epoch 8946/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.2611 - val_loss: 92.4987\n",
      "Epoch 8947/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2005 - val_loss: 93.0607\n",
      "Epoch 8948/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.6400 - val_loss: 92.4667\n",
      "Epoch 8949/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 89.6378 - val_loss: 93.3001\n",
      "Epoch 8950/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.5731 - val_loss: 92.6350\n",
      "Epoch 8951/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.5294 - val_loss: 91.9158\n",
      "Epoch 8952/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.4239 - val_loss: 92.2664\n",
      "Epoch 8953/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.0056 - val_loss: 92.6644\n",
      "Epoch 8954/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 93.4952 - val_loss: 92.7123\n",
      "Epoch 8955/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.1402 - val_loss: 92.8174\n",
      "Epoch 8956/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.2795 - val_loss: 93.4848\n",
      "Epoch 8957/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.6812 - val_loss: 91.4557\n",
      "Epoch 8958/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.6793 - val_loss: 93.1417\n",
      "Epoch 8959/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.8351 - val_loss: 93.0815\n",
      "Epoch 8960/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.1953 - val_loss: 93.0560\n",
      "Epoch 8961/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.9949 - val_loss: 92.6134\n",
      "Epoch 8962/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.5517 - val_loss: 92.0571\n",
      "Epoch 8963/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.1498 - val_loss: 92.6612\n",
      "Epoch 8964/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3600 - val_loss: 91.1573\n",
      "Epoch 8965/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.7097 - val_loss: 93.1008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8966/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.5397 - val_loss: 92.5742\n",
      "Epoch 8967/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.2539 - val_loss: 91.7537\n",
      "Epoch 8968/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.6928 - val_loss: 92.1339\n",
      "Epoch 8969/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.5789 - val_loss: 92.3819\n",
      "Epoch 8970/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1898 - val_loss: 93.1288\n",
      "Epoch 8971/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.4537 - val_loss: 92.8431\n",
      "Epoch 8972/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.2661 - val_loss: 92.7541\n",
      "Epoch 8973/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.1238 - val_loss: 93.2609\n",
      "Epoch 8974/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.1762 - val_loss: 92.6821\n",
      "Epoch 8975/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 89.7388 - val_loss: 92.5398\n",
      "Epoch 8976/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.4472 - val_loss: 91.7149\n",
      "Epoch 8977/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8408 - val_loss: 92.4226\n",
      "Epoch 8978/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.7658 - val_loss: 92.2172\n",
      "Epoch 8979/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.7659 - val_loss: 92.9928\n",
      "Epoch 8980/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.1627 - val_loss: 92.3694\n",
      "Epoch 8981/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.2027 - val_loss: 93.1699\n",
      "Epoch 8982/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.4319 - val_loss: 92.1931\n",
      "Epoch 8983/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.8734 - val_loss: 93.1820\n",
      "Epoch 8984/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.3331 - val_loss: 93.2673\n",
      "Epoch 8985/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 95.9548 - val_loss: 92.5030\n",
      "Epoch 8986/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.5998 - val_loss: 92.7325\n",
      "Epoch 8987/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4273 - val_loss: 93.1546\n",
      "Epoch 8988/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.4551 - val_loss: 92.5719\n",
      "Epoch 8989/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.8135 - val_loss: 92.7922\n",
      "Epoch 8990/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1646 - val_loss: 91.9844\n",
      "Epoch 8991/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2183 - val_loss: 92.7257\n",
      "Epoch 8992/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.8446 - val_loss: 92.2474\n",
      "Epoch 8993/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.7393 - val_loss: 93.1549\n",
      "Epoch 8994/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.8800 - val_loss: 92.3835\n",
      "Epoch 8995/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.7359 - val_loss: 92.5927\n",
      "Epoch 8996/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6464 - val_loss: 93.0412\n",
      "Epoch 8997/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.5641 - val_loss: 92.2334\n",
      "Epoch 8998/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.8680 - val_loss: 92.0839\n",
      "Epoch 8999/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.9498 - val_loss: 92.8063\n",
      "Epoch 9000/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.2041 - val_loss: 92.1970\n",
      "Epoch 9001/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.9799 - val_loss: 91.9397\n",
      "Epoch 9002/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.8909 - val_loss: 93.0389\n",
      "Epoch 9003/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.4166 - val_loss: 93.0966\n",
      "Epoch 9004/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.3835 - val_loss: 92.1794\n",
      "Epoch 9005/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 89.9811 - val_loss: 92.9414\n",
      "Epoch 9006/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.0382 - val_loss: 92.3183\n",
      "Epoch 9007/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.0826 - val_loss: 93.1228\n",
      "Epoch 9008/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9372 - val_loss: 92.5612\n",
      "Epoch 9009/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4642 - val_loss: 92.4611\n",
      "Epoch 9010/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.3030 - val_loss: 92.8589\n",
      "Epoch 9011/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2811 - val_loss: 93.3494\n",
      "Epoch 9012/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.3266 - val_loss: 92.7562\n",
      "Epoch 9013/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 92.5066 - val_loss: 92.6686\n",
      "Epoch 9014/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2269 - val_loss: 92.3584\n",
      "Epoch 9015/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.2840 - val_loss: 92.8394\n",
      "Epoch 9016/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.9532 - val_loss: 92.5172\n",
      "Epoch 9017/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.5697 - val_loss: 93.0960\n",
      "Epoch 9018/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 90.3310 - val_loss: 92.8656\n",
      "Epoch 9019/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.0810 - val_loss: 91.9665\n",
      "Epoch 9020/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.7130 - val_loss: 91.6813\n",
      "Epoch 9021/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.1248 - val_loss: 92.7507\n",
      "Epoch 9022/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 91.0437 - val_loss: 92.5640\n",
      "Epoch 9023/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.6743 - val_loss: 92.8560\n",
      "Epoch 9024/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.8837 - val_loss: 92.8943\n",
      "Epoch 9025/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7565 - val_loss: 93.1025\n",
      "Epoch 9026/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.7803 - val_loss: 93.6431\n",
      "Epoch 9027/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.4561 - val_loss: 92.3515\n",
      "Epoch 9028/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.5487 - val_loss: 92.6325\n",
      "Epoch 9029/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.9789 - val_loss: 92.2319\n",
      "Epoch 9030/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.0499 - val_loss: 92.1875\n",
      "Epoch 9031/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5763 - val_loss: 93.3659\n",
      "Epoch 9032/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9323 - val_loss: 91.7189\n",
      "Epoch 9033/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1086 - val_loss: 92.6803\n",
      "Epoch 9034/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.5112 - val_loss: 92.6431\n",
      "Epoch 9035/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.0888 - val_loss: 93.8927\n",
      "Epoch 9036/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.7795 - val_loss: 92.3763\n",
      "Epoch 9037/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.5194 - val_loss: 92.4846\n",
      "Epoch 9038/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.7815 - val_loss: 92.6905\n",
      "Epoch 9039/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.0550 - val_loss: 92.5961\n",
      "Epoch 9040/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.7647 - val_loss: 92.3663\n",
      "Epoch 9041/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 89.6376 - val_loss: 92.6325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9042/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1109 - val_loss: 92.8150\n",
      "Epoch 9043/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.1871 - val_loss: 92.2050\n",
      "Epoch 9044/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 89.5988 - val_loss: 92.5666\n",
      "Epoch 9045/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.0409 - val_loss: 93.1617\n",
      "Epoch 9046/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.4654 - val_loss: 91.5373\n",
      "Epoch 9047/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.0595 - val_loss: 92.6564\n",
      "Epoch 9048/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.1850 - val_loss: 93.2239\n",
      "Epoch 9049/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.2092 - val_loss: 93.0157\n",
      "Epoch 9050/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.0072 - val_loss: 92.8612\n",
      "Epoch 9051/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.0979 - val_loss: 93.4549\n",
      "Epoch 9052/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.8422 - val_loss: 92.9035\n",
      "Epoch 9053/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.6736 - val_loss: 92.8893\n",
      "Epoch 9054/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.7489 - val_loss: 93.6944\n",
      "Epoch 9055/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.2767 - val_loss: 92.4680\n",
      "Epoch 9056/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3279 - val_loss: 93.1562\n",
      "Epoch 9057/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.2593 - val_loss: 93.5217\n",
      "Epoch 9058/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2354 - val_loss: 92.6042\n",
      "Epoch 9059/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 95.3006 - val_loss: 92.5048\n",
      "Epoch 9060/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4037 - val_loss: 93.1886\n",
      "Epoch 9061/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.1607 - val_loss: 91.8591\n",
      "Epoch 9062/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 94.9625 - val_loss: 91.3443\n",
      "Epoch 9063/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0993 - val_loss: 92.4351\n",
      "Epoch 9064/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.5781 - val_loss: 92.6344\n",
      "Epoch 9065/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.0012 - val_loss: 92.8734\n",
      "Epoch 9066/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 90.1363 - val_loss: 92.7672\n",
      "Epoch 9067/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 89.9136 - val_loss: 92.6792\n",
      "Epoch 9068/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.7515 - val_loss: 93.0968\n",
      "Epoch 9069/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 92.1079 - val_loss: 92.7599\n",
      "Epoch 9070/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.0895 - val_loss: 92.4279\n",
      "Epoch 9071/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 89.7445 - val_loss: 92.8567\n",
      "Epoch 9072/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.6183 - val_loss: 92.0801\n",
      "Epoch 9073/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 89.4914 - val_loss: 93.7556\n",
      "Epoch 9074/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.9595 - val_loss: 93.1680\n",
      "Epoch 9075/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.1421 - val_loss: 92.7716\n",
      "Epoch 9076/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4834 - val_loss: 92.4945\n",
      "Epoch 9077/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.8392 - val_loss: 92.7756\n",
      "Epoch 9078/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 89.9625 - val_loss: 92.2293\n",
      "Epoch 9079/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.8245 - val_loss: 92.4255\n",
      "Epoch 9080/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.5821 - val_loss: 92.6433\n",
      "Epoch 9081/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6793 - val_loss: 92.7779\n",
      "Epoch 9082/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 95.4401 - val_loss: 92.4815\n",
      "Epoch 9083/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.3473 - val_loss: 93.0299\n",
      "Epoch 9084/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.4090 - val_loss: 92.8558\n",
      "Epoch 9085/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.5073 - val_loss: 92.6985\n",
      "Epoch 9086/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8407 - val_loss: 91.6438\n",
      "Epoch 9087/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6184 - val_loss: 93.1749\n",
      "Epoch 9088/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.1417 - val_loss: 93.4638\n",
      "Epoch 9089/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.0573 - val_loss: 92.2684\n",
      "Epoch 9090/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.7294 - val_loss: 92.6927\n",
      "Epoch 9091/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.1131 - val_loss: 92.9328\n",
      "Epoch 9092/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.6085 - val_loss: 92.3140\n",
      "Epoch 9093/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.6323 - val_loss: 92.6288\n",
      "Epoch 9094/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7639 - val_loss: 91.9184\n",
      "Epoch 9095/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.0641 - val_loss: 92.4693\n",
      "Epoch 9096/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.7779 - val_loss: 91.7122\n",
      "Epoch 9097/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1192 - val_loss: 93.1284\n",
      "Epoch 9098/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 95.3322 - val_loss: 91.9682\n",
      "Epoch 9099/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3765 - val_loss: 91.9298\n",
      "Epoch 9100/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.4835 - val_loss: 91.8692\n",
      "Epoch 9101/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8120 - val_loss: 92.7162\n",
      "Epoch 9102/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5782 - val_loss: 93.3938\n",
      "Epoch 9103/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.3666 - val_loss: 92.8835\n",
      "Epoch 9104/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.0123 - val_loss: 92.6525\n",
      "Epoch 9105/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 90.9433 - val_loss: 93.1001\n",
      "Epoch 9106/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.5335 - val_loss: 93.3601\n",
      "Epoch 9107/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.1567 - val_loss: 92.1805\n",
      "Epoch 9108/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.9843 - val_loss: 92.2015\n",
      "Epoch 9109/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.6837 - val_loss: 92.5615\n",
      "Epoch 9110/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.1414 - val_loss: 92.8867\n",
      "Epoch 9111/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 91.9421 - val_loss: 93.5011\n",
      "Epoch 9112/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.7720 - val_loss: 92.6138\n",
      "Epoch 9113/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.1856 - val_loss: 92.2766\n",
      "Epoch 9114/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.4441 - val_loss: 92.7961\n",
      "Epoch 9115/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 93.8134 - val_loss: 92.0901\n",
      "Epoch 9116/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 95.5186 - val_loss: 92.6133\n",
      "Epoch 9117/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.2101 - val_loss: 92.8937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9118/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.9967 - val_loss: 92.8989\n",
      "Epoch 9119/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.6058 - val_loss: 94.3785\n",
      "Epoch 9120/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.9555 - val_loss: 92.6490\n",
      "Epoch 9121/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 89.9083 - val_loss: 92.8391\n",
      "Epoch 9122/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.8717 - val_loss: 93.0607\n",
      "Epoch 9123/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 90.6967 - val_loss: 91.6900\n",
      "Epoch 9124/10000\n",
      "96/96 [==============================] - 0s 188us/step - loss: 89.5871 - val_loss: 92.3050\n",
      "Epoch 9125/10000\n",
      "96/96 [==============================] - 0s 192us/step - loss: 90.8164 - val_loss: 92.4358\n",
      "Epoch 9126/10000\n",
      "96/96 [==============================] - 0s 198us/step - loss: 92.0469 - val_loss: 92.4028\n",
      "Epoch 9127/10000\n",
      "96/96 [==============================] - 0s 186us/step - loss: 90.6417 - val_loss: 93.2840\n",
      "Epoch 9128/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 90.3032 - val_loss: 92.5768\n",
      "Epoch 9129/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 90.4345 - val_loss: 92.4215\n",
      "Epoch 9130/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 93.6239 - val_loss: 93.5406\n",
      "Epoch 9131/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 89.9607 - val_loss: 94.3866\n",
      "Epoch 9132/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 89.9045 - val_loss: 92.5981\n",
      "Epoch 9133/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 91.1572 - val_loss: 93.2113\n",
      "Epoch 9134/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 89.8976 - val_loss: 93.0380\n",
      "Epoch 9135/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.7346 - val_loss: 92.3670\n",
      "Epoch 9136/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1950 - val_loss: 92.6425\n",
      "Epoch 9137/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.3695 - val_loss: 92.8125\n",
      "Epoch 9138/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 93.8473 - val_loss: 92.7800\n",
      "Epoch 9139/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.3504 - val_loss: 92.7050\n",
      "Epoch 9140/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.4199 - val_loss: 93.2324\n",
      "Epoch 9141/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.2808 - val_loss: 93.4405\n",
      "Epoch 9142/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2102 - val_loss: 92.7284\n",
      "Epoch 9143/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.5564 - val_loss: 92.1652\n",
      "Epoch 9144/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.8399 - val_loss: 92.3188\n",
      "Epoch 9145/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 91.5168 - val_loss: 93.4417\n",
      "Epoch 9146/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 92.5784 - val_loss: 92.5832\n",
      "Epoch 9147/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 95.4497 - val_loss: 92.7589\n",
      "Epoch 9148/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 90.9985 - val_loss: 93.5197\n",
      "Epoch 9149/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 93.5633 - val_loss: 92.4670\n",
      "Epoch 9150/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.5165 - val_loss: 93.1164\n",
      "Epoch 9151/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.1323 - val_loss: 92.6800\n",
      "Epoch 9152/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 89.1073 - val_loss: 92.8353\n",
      "Epoch 9153/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 91.1682 - val_loss: 92.2981\n",
      "Epoch 9154/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 90.8532 - val_loss: 92.2463\n",
      "Epoch 9155/10000\n",
      "96/96 [==============================] - 0s 146us/step - loss: 90.4915 - val_loss: 92.7525\n",
      "Epoch 9156/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 91.1229 - val_loss: 93.3071\n",
      "Epoch 9157/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.5528 - val_loss: 92.3658\n",
      "Epoch 9158/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 90.3189 - val_loss: 92.7993\n",
      "Epoch 9159/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 92.0187 - val_loss: 93.1187\n",
      "Epoch 9160/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.6342 - val_loss: 93.0088\n",
      "Epoch 9161/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 90.9611 - val_loss: 93.4255\n",
      "Epoch 9162/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 89.4241 - val_loss: 91.8455\n",
      "Epoch 9163/10000\n",
      "96/96 [==============================] - 0s 159us/step - loss: 91.5278 - val_loss: 93.3201\n",
      "Epoch 9164/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 89.9128 - val_loss: 92.4768\n",
      "Epoch 9165/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 92.7660 - val_loss: 91.2301\n",
      "Epoch 9166/10000\n",
      "96/96 [==============================] - 0s 151us/step - loss: 89.8448 - val_loss: 92.3154\n",
      "Epoch 9167/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 90.9321 - val_loss: 93.2315\n",
      "Epoch 9168/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 90.5094 - val_loss: 92.9604\n",
      "Epoch 9169/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 90.7823 - val_loss: 92.0262\n",
      "Epoch 9170/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 90.2533 - val_loss: 91.8546\n",
      "Epoch 9171/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 89.7386 - val_loss: 92.6799\n",
      "Epoch 9172/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 90.6646 - val_loss: 93.5065\n",
      "Epoch 9173/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 90.6784 - val_loss: 92.2765\n",
      "Epoch 9174/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 90.3965 - val_loss: 92.4981\n",
      "Epoch 9175/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.3694 - val_loss: 92.9061\n",
      "Epoch 9176/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 92.0691 - val_loss: 93.0105\n",
      "Epoch 9177/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 90.9468 - val_loss: 92.3781\n",
      "Epoch 9178/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 89.4927 - val_loss: 92.5626\n",
      "Epoch 9179/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.9751 - val_loss: 92.6415\n",
      "Epoch 9180/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 91.8079 - val_loss: 92.6121\n",
      "Epoch 9181/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 95.6366 - val_loss: 92.7635\n",
      "Epoch 9182/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 90.8282 - val_loss: 93.0695\n",
      "Epoch 9183/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 89.7714 - val_loss: 93.0044\n",
      "Epoch 9184/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 91.3737 - val_loss: 92.9211\n",
      "Epoch 9185/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.1105 - val_loss: 92.5162\n",
      "Epoch 9186/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 90.6777 - val_loss: 93.7965\n",
      "Epoch 9187/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 92.3875 - val_loss: 92.3406\n",
      "Epoch 9188/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 90.5461 - val_loss: 92.4086\n",
      "Epoch 9189/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 90.7834 - val_loss: 92.5172\n",
      "Epoch 9190/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.3251 - val_loss: 92.5512\n",
      "Epoch 9191/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.5056 - val_loss: 93.2305\n",
      "Epoch 9192/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.1665 - val_loss: 93.3368\n",
      "Epoch 9193/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 91.9323 - val_loss: 92.4920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9194/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.0458 - val_loss: 93.1872\n",
      "Epoch 9195/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 90.6215 - val_loss: 92.6482\n",
      "Epoch 9196/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 90.9396 - val_loss: 92.1684\n",
      "Epoch 9197/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 90.9848 - val_loss: 92.4700\n",
      "Epoch 9198/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.1049 - val_loss: 91.6921\n",
      "Epoch 9199/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 89.0295 - val_loss: 92.4825\n",
      "Epoch 9200/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 90.7452 - val_loss: 92.8566\n",
      "Epoch 9201/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 91.6576 - val_loss: 92.2670\n",
      "Epoch 9202/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 93.3449 - val_loss: 92.7848\n",
      "Epoch 9203/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 90.9133 - val_loss: 92.1874\n",
      "Epoch 9204/10000\n",
      "96/96 [==============================] - 0s 149us/step - loss: 91.1534 - val_loss: 92.5960\n",
      "Epoch 9205/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 89.4262 - val_loss: 92.4656\n",
      "Epoch 9206/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 91.0937 - val_loss: 93.1724\n",
      "Epoch 9207/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.5985 - val_loss: 92.4644\n",
      "Epoch 9208/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.3035 - val_loss: 92.3664\n",
      "Epoch 9209/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.0518 - val_loss: 92.3231\n",
      "Epoch 9210/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.8179 - val_loss: 92.1394\n",
      "Epoch 9211/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 90.8224 - val_loss: 93.2267\n",
      "Epoch 9212/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 90.3944 - val_loss: 92.2426\n",
      "Epoch 9213/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 90.8385 - val_loss: 92.7620\n",
      "Epoch 9214/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 90.8633 - val_loss: 92.0694\n",
      "Epoch 9215/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 93.5152 - val_loss: 92.9785\n",
      "Epoch 9216/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 90.8860 - val_loss: 92.6092\n",
      "Epoch 9217/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 92.0831 - val_loss: 92.9998\n",
      "Epoch 9218/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.6722 - val_loss: 92.6370\n",
      "Epoch 9219/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.1264 - val_loss: 92.7392\n",
      "Epoch 9220/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.1380 - val_loss: 93.8210\n",
      "Epoch 9221/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.4740 - val_loss: 92.3275\n",
      "Epoch 9222/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.5351 - val_loss: 92.5431\n",
      "Epoch 9223/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3036 - val_loss: 93.5236\n",
      "Epoch 9224/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.1681 - val_loss: 93.2845\n",
      "Epoch 9225/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 90.3308 - val_loss: 93.0609\n",
      "Epoch 9226/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 92.2563 - val_loss: 92.3068\n",
      "Epoch 9227/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 90.2145 - val_loss: 92.5424\n",
      "Epoch 9228/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.5993 - val_loss: 91.7878\n",
      "Epoch 9229/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 90.2875 - val_loss: 92.3209\n",
      "Epoch 9230/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 91.8806 - val_loss: 93.2082\n",
      "Epoch 9231/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 93.2226 - val_loss: 92.4391\n",
      "Epoch 9232/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 90.8847 - val_loss: 92.6895\n",
      "Epoch 9233/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.3501 - val_loss: 93.0132\n",
      "Epoch 9234/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 89.7239 - val_loss: 91.7816\n",
      "Epoch 9235/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.3637 - val_loss: 93.2199\n",
      "Epoch 9236/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 91.5125 - val_loss: 93.0104\n",
      "Epoch 9237/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.8728 - val_loss: 92.5555\n",
      "Epoch 9238/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.3592 - val_loss: 92.3776\n",
      "Epoch 9239/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 91.9181 - val_loss: 92.1442\n",
      "Epoch 9240/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 91.3702 - val_loss: 92.8372\n",
      "Epoch 9241/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 92.4980 - val_loss: 92.9667\n",
      "Epoch 9242/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 90.9103 - val_loss: 92.5725\n",
      "Epoch 9243/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 91.4155 - val_loss: 92.5221\n",
      "Epoch 9244/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 96.0586 - val_loss: 92.8039\n",
      "Epoch 9245/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 92.5357 - val_loss: 92.8649\n",
      "Epoch 9246/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.9142 - val_loss: 93.6877\n",
      "Epoch 9247/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.8071 - val_loss: 92.9217\n",
      "Epoch 9248/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 89.6129 - val_loss: 92.9047\n",
      "Epoch 9249/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.5401 - val_loss: 92.5011\n",
      "Epoch 9250/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 90.7481 - val_loss: 92.0492\n",
      "Epoch 9251/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 89.1993 - val_loss: 92.8250\n",
      "Epoch 9252/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 89.6246 - val_loss: 92.9500\n",
      "Epoch 9253/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 90.7495 - val_loss: 91.8744\n",
      "Epoch 9254/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 90.1512 - val_loss: 93.0958\n",
      "Epoch 9255/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 90.2829 - val_loss: 92.0272\n",
      "Epoch 9256/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 91.7266 - val_loss: 92.7373\n",
      "Epoch 9257/10000\n",
      "96/96 [==============================] - 0s 151us/step - loss: 90.8394 - val_loss: 92.3243\n",
      "Epoch 9258/10000\n",
      "96/96 [==============================] - 0s 149us/step - loss: 90.4483 - val_loss: 93.1800\n",
      "Epoch 9259/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 92.1733 - val_loss: 93.4830\n",
      "Epoch 9260/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.5854 - val_loss: 91.9046\n",
      "Epoch 9261/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.2418 - val_loss: 92.2661\n",
      "Epoch 9262/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 89.6211 - val_loss: 93.2910\n",
      "Epoch 9263/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.9253 - val_loss: 93.1171\n",
      "Epoch 9264/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 91.3738 - val_loss: 92.3687\n",
      "Epoch 9265/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.3201 - val_loss: 92.2035\n",
      "Epoch 9266/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 90.9015 - val_loss: 92.9091\n",
      "Epoch 9267/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.1580 - val_loss: 92.6660\n",
      "Epoch 9268/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 91.2840 - val_loss: 92.5525\n",
      "Epoch 9269/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8261 - val_loss: 92.7545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9270/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 90.6605 - val_loss: 92.5185\n",
      "Epoch 9271/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.5272 - val_loss: 92.8038\n",
      "Epoch 9272/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 91.8521 - val_loss: 92.8270\n",
      "Epoch 9273/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.3170 - val_loss: 92.4181\n",
      "Epoch 9274/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.8125 - val_loss: 92.3003\n",
      "Epoch 9275/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.2470 - val_loss: 92.4363\n",
      "Epoch 9276/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 91.0378 - val_loss: 92.4843\n",
      "Epoch 9277/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4473 - val_loss: 92.5311\n",
      "Epoch 9278/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.0865 - val_loss: 92.2926\n",
      "Epoch 9279/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 89.7750 - val_loss: 92.4543\n",
      "Epoch 9280/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 92.1413 - val_loss: 91.9333\n",
      "Epoch 9281/10000\n",
      "96/96 [==============================] - 0s 147us/step - loss: 90.1875 - val_loss: 92.9121\n",
      "Epoch 9282/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.2842 - val_loss: 92.4463\n",
      "Epoch 9283/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.9104 - val_loss: 92.6902\n",
      "Epoch 9284/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.7098 - val_loss: 92.8517\n",
      "Epoch 9285/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 90.2029 - val_loss: 93.0907\n",
      "Epoch 9286/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.3451 - val_loss: 92.1721\n",
      "Epoch 9287/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.5805 - val_loss: 92.1132\n",
      "Epoch 9288/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.7518 - val_loss: 92.3167\n",
      "Epoch 9289/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.2070 - val_loss: 92.8548\n",
      "Epoch 9290/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.9213 - val_loss: 92.6888\n",
      "Epoch 9291/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 90.5111 - val_loss: 92.7159\n",
      "Epoch 9292/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 89.8877 - val_loss: 92.7004\n",
      "Epoch 9293/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.5501 - val_loss: 92.8533\n",
      "Epoch 9294/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 92.1444 - val_loss: 92.7326\n",
      "Epoch 9295/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.0878 - val_loss: 93.0044\n",
      "Epoch 9296/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.3843 - val_loss: 93.4040\n",
      "Epoch 9297/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.0942 - val_loss: 92.1030\n",
      "Epoch 9298/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.4354 - val_loss: 92.7306\n",
      "Epoch 9299/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4020 - val_loss: 92.8231\n",
      "Epoch 9300/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.7086 - val_loss: 92.3170\n",
      "Epoch 9301/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.1068 - val_loss: 92.8508\n",
      "Epoch 9302/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.2543 - val_loss: 92.5800\n",
      "Epoch 9303/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.7560 - val_loss: 92.5741\n",
      "Epoch 9304/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.7258 - val_loss: 93.2182\n",
      "Epoch 9305/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.4356 - val_loss: 92.4022\n",
      "Epoch 9306/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.2927 - val_loss: 93.1382\n",
      "Epoch 9307/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.5991 - val_loss: 92.6688\n",
      "Epoch 9308/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.3821 - val_loss: 92.9386\n",
      "Epoch 9309/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.3662 - val_loss: 92.6626\n",
      "Epoch 9310/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.3442 - val_loss: 93.3532\n",
      "Epoch 9311/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.8431 - val_loss: 91.9180\n",
      "Epoch 9312/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.6422 - val_loss: 93.1296\n",
      "Epoch 9313/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 90.2554 - val_loss: 91.9972\n",
      "Epoch 9314/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 91.3002 - val_loss: 93.7078\n",
      "Epoch 9315/10000\n",
      "96/96 [==============================] - 0s 158us/step - loss: 90.7703 - val_loss: 92.6721\n",
      "Epoch 9316/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 91.3529 - val_loss: 92.1561\n",
      "Epoch 9317/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 90.1427 - val_loss: 92.5711\n",
      "Epoch 9318/10000\n",
      "96/96 [==============================] - 0s 149us/step - loss: 91.0967 - val_loss: 93.6589\n",
      "Epoch 9319/10000\n",
      "96/96 [==============================] - 0s 146us/step - loss: 89.8190 - val_loss: 93.1422\n",
      "Epoch 9320/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 91.3428 - val_loss: 92.8273\n",
      "Epoch 9321/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 91.2330 - val_loss: 93.0760\n",
      "Epoch 9322/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 88.2760 - val_loss: 92.7199\n",
      "Epoch 9323/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 91.9872 - val_loss: 93.2150\n",
      "Epoch 9324/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 90.3479 - val_loss: 92.3294\n",
      "Epoch 9325/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 90.2944 - val_loss: 93.1309\n",
      "Epoch 9326/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 90.1806 - val_loss: 92.6825\n",
      "Epoch 9327/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 90.8506 - val_loss: 92.6514\n",
      "Epoch 9328/10000\n",
      "96/96 [==============================] - 0s 152us/step - loss: 91.1712 - val_loss: 92.8227\n",
      "Epoch 9329/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 90.5175 - val_loss: 92.8655\n",
      "Epoch 9330/10000\n",
      "96/96 [==============================] - 0s 149us/step - loss: 91.8552 - val_loss: 92.3220\n",
      "Epoch 9331/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 90.3848 - val_loss: 92.7609\n",
      "Epoch 9332/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 89.4865 - val_loss: 92.9053\n",
      "Epoch 9333/10000\n",
      "96/96 [==============================] - 0s 141us/step - loss: 93.0730 - val_loss: 92.6505\n",
      "Epoch 9334/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 89.2841 - val_loss: 92.4658\n",
      "Epoch 9335/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 93.1995 - val_loss: 93.1158\n",
      "Epoch 9336/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 90.2503 - val_loss: 93.2800\n",
      "Epoch 9337/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 90.0942 - val_loss: 91.5636\n",
      "Epoch 9338/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.8016 - val_loss: 92.9401\n",
      "Epoch 9339/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 90.7954 - val_loss: 92.6000\n",
      "Epoch 9340/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 91.3246 - val_loss: 92.2344\n",
      "Epoch 9341/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 90.9066 - val_loss: 93.1760\n",
      "Epoch 9342/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 90.3338 - val_loss: 92.6133\n",
      "Epoch 9343/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 90.7919 - val_loss: 92.4409\n",
      "Epoch 9344/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 90.2959 - val_loss: 92.7354\n",
      "Epoch 9345/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 93.2290 - val_loss: 92.6838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9346/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 92.4231 - val_loss: 93.0182\n",
      "Epoch 9347/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 90.3957 - val_loss: 92.0827\n",
      "Epoch 9348/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 90.0178 - val_loss: 92.1222\n",
      "Epoch 9349/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 89.8092 - val_loss: 91.9633\n",
      "Epoch 9350/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 96.1582 - val_loss: 91.7270\n",
      "Epoch 9351/10000\n",
      "96/96 [==============================] - 0s 146us/step - loss: 91.6019 - val_loss: 92.9650\n",
      "Epoch 9352/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 92.5554 - val_loss: 91.8505\n",
      "Epoch 9353/10000\n",
      "96/96 [==============================] - 0s 153us/step - loss: 96.5894 - val_loss: 92.9705\n",
      "Epoch 9354/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 90.1485 - val_loss: 92.7674\n",
      "Epoch 9355/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.7583 - val_loss: 92.7285\n",
      "Epoch 9356/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 89.7506 - val_loss: 93.7862\n",
      "Epoch 9357/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 91.0516 - val_loss: 92.9203\n",
      "Epoch 9358/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 91.5519 - val_loss: 92.6032\n",
      "Epoch 9359/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 91.3848 - val_loss: 92.9871\n",
      "Epoch 9360/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 91.1485 - val_loss: 92.4358\n",
      "Epoch 9361/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 89.0778 - val_loss: 92.2750\n",
      "Epoch 9362/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 90.5999 - val_loss: 93.7033\n",
      "Epoch 9363/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 90.9921 - val_loss: 92.7922\n",
      "Epoch 9364/10000\n",
      "96/96 [==============================] - 0s 147us/step - loss: 91.1722 - val_loss: 93.4179\n",
      "Epoch 9365/10000\n",
      "96/96 [==============================] - 0s 157us/step - loss: 92.5815 - val_loss: 93.3073\n",
      "Epoch 9366/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 90.7088 - val_loss: 92.6353\n",
      "Epoch 9367/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 91.2283 - val_loss: 91.7926\n",
      "Epoch 9368/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 91.5678 - val_loss: 92.7666\n",
      "Epoch 9369/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 89.3149 - val_loss: 93.0928\n",
      "Epoch 9370/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.4398 - val_loss: 92.7126\n",
      "Epoch 9371/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.4770 - val_loss: 92.5956\n",
      "Epoch 9372/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 90.0730 - val_loss: 92.5695\n",
      "Epoch 9373/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 89.4879 - val_loss: 92.8857\n",
      "Epoch 9374/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 92.6566 - val_loss: 92.0984\n",
      "Epoch 9375/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.6949 - val_loss: 93.1475\n",
      "Epoch 9376/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.1974 - val_loss: 92.2828\n",
      "Epoch 9377/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.2083 - val_loss: 92.4661\n",
      "Epoch 9378/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.4477 - val_loss: 92.8495\n",
      "Epoch 9379/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 94.4179 - val_loss: 92.2972\n",
      "Epoch 9380/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.1298 - val_loss: 93.1056\n",
      "Epoch 9381/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.3967 - val_loss: 92.7104\n",
      "Epoch 9382/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 90.5069 - val_loss: 92.9184\n",
      "Epoch 9383/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.3945 - val_loss: 92.7028\n",
      "Epoch 9384/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 89.7598 - val_loss: 92.4405\n",
      "Epoch 9385/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 90.8845 - val_loss: 92.2921\n",
      "Epoch 9386/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.3165 - val_loss: 93.0123\n",
      "Epoch 9387/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.9844 - val_loss: 93.2756\n",
      "Epoch 9388/10000\n",
      "96/96 [==============================] - 0s 197us/step - loss: 92.9865 - val_loss: 93.2340\n",
      "Epoch 9389/10000\n",
      "96/96 [==============================] - 0s 254us/step - loss: 92.4436 - val_loss: 92.3518\n",
      "Epoch 9390/10000\n",
      "96/96 [==============================] - 0s 256us/step - loss: 90.8339 - val_loss: 92.7007\n",
      "Epoch 9391/10000\n",
      "96/96 [==============================] - 0s 167us/step - loss: 91.5463 - val_loss: 92.2090\n",
      "Epoch 9392/10000\n",
      "96/96 [==============================] - 0s 253us/step - loss: 90.9464 - val_loss: 92.5492\n",
      "Epoch 9393/10000\n",
      "96/96 [==============================] - 0s 231us/step - loss: 89.9908 - val_loss: 93.2251\n",
      "Epoch 9394/10000\n",
      "96/96 [==============================] - 0s 213us/step - loss: 89.4207 - val_loss: 91.8567\n",
      "Epoch 9395/10000\n",
      "96/96 [==============================] - 0s 156us/step - loss: 89.1985 - val_loss: 92.7537\n",
      "Epoch 9396/10000\n",
      "96/96 [==============================] - 0s 173us/step - loss: 90.5933 - val_loss: 92.4943\n",
      "Epoch 9397/10000\n",
      "96/96 [==============================] - 0s 163us/step - loss: 90.4938 - val_loss: 92.3339\n",
      "Epoch 9398/10000\n",
      "96/96 [==============================] - 0s 165us/step - loss: 90.1681 - val_loss: 92.6700\n",
      "Epoch 9399/10000\n",
      "96/96 [==============================] - 0s 168us/step - loss: 89.5212 - val_loss: 92.2351\n",
      "Epoch 9400/10000\n",
      "96/96 [==============================] - 0s 160us/step - loss: 89.6747 - val_loss: 92.1889\n",
      "Epoch 9401/10000\n",
      "96/96 [==============================] - 0s 146us/step - loss: 91.8582 - val_loss: 92.7966\n",
      "Epoch 9402/10000\n",
      "96/96 [==============================] - 0s 151us/step - loss: 91.1789 - val_loss: 92.5765\n",
      "Epoch 9403/10000\n",
      "96/96 [==============================] - 0s 157us/step - loss: 90.6351 - val_loss: 92.5092\n",
      "Epoch 9404/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 90.9966 - val_loss: 92.4971\n",
      "Epoch 9405/10000\n",
      "96/96 [==============================] - 0s 159us/step - loss: 90.8516 - val_loss: 92.8631\n",
      "Epoch 9406/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 92.0689 - val_loss: 92.8270\n",
      "Epoch 9407/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 93.3797 - val_loss: 93.0168\n",
      "Epoch 9408/10000\n",
      "96/96 [==============================] - 0s 150us/step - loss: 91.5386 - val_loss: 92.5796\n",
      "Epoch 9409/10000\n",
      "96/96 [==============================] - 0s 158us/step - loss: 90.7099 - val_loss: 91.8134\n",
      "Epoch 9410/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 89.9483 - val_loss: 92.5666\n",
      "Epoch 9411/10000\n",
      "96/96 [==============================] - 0s 159us/step - loss: 90.8256 - val_loss: 92.4694\n",
      "Epoch 9412/10000\n",
      "96/96 [==============================] - 0s 151us/step - loss: 91.9460 - val_loss: 92.7893\n",
      "Epoch 9413/10000\n",
      "96/96 [==============================] - 0s 154us/step - loss: 93.5011 - val_loss: 92.5569\n",
      "Epoch 9414/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 89.3214 - val_loss: 93.5445\n",
      "Epoch 9415/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 90.1863 - val_loss: 92.4127\n",
      "Epoch 9416/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 91.9856 - val_loss: 92.6283\n",
      "Epoch 9417/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 89.8130 - val_loss: 93.1810\n",
      "Epoch 9418/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 91.4440 - val_loss: 92.9695\n",
      "Epoch 9419/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 90.5945 - val_loss: 92.6472\n",
      "Epoch 9420/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 90.2976 - val_loss: 93.2808\n",
      "Epoch 9421/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 91.4600 - val_loss: 92.9331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9422/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 90.5544 - val_loss: 92.4748\n",
      "Epoch 9423/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 90.9165 - val_loss: 92.7266\n",
      "Epoch 9424/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 91.5547 - val_loss: 92.5059\n",
      "Epoch 9425/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.6256 - val_loss: 93.4467\n",
      "Epoch 9426/10000\n",
      "96/96 [==============================] - 0s 148us/step - loss: 92.1503 - val_loss: 92.6743\n",
      "Epoch 9427/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 90.1026 - val_loss: 93.5860\n",
      "Epoch 9428/10000\n",
      "96/96 [==============================] - 0s 136us/step - loss: 91.6534 - val_loss: 92.7494\n",
      "Epoch 9429/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 91.2377 - val_loss: 92.3350\n",
      "Epoch 9430/10000\n",
      "96/96 [==============================] - 0s 139us/step - loss: 91.3769 - val_loss: 93.1578\n",
      "Epoch 9431/10000\n",
      "96/96 [==============================] - 0s 137us/step - loss: 90.2625 - val_loss: 92.6568\n",
      "Epoch 9432/10000\n",
      "96/96 [==============================] - 0s 147us/step - loss: 92.0655 - val_loss: 92.9353\n",
      "Epoch 9433/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 89.9130 - val_loss: 92.5149\n",
      "Epoch 9434/10000\n",
      "96/96 [==============================] - 0s 146us/step - loss: 93.1976 - val_loss: 92.3082\n",
      "Epoch 9435/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 91.7032 - val_loss: 91.7101\n",
      "Epoch 9436/10000\n",
      "96/96 [==============================] - 0s 155us/step - loss: 89.6111 - val_loss: 93.2935\n",
      "Epoch 9437/10000\n",
      "96/96 [==============================] - 0s 163us/step - loss: 90.4105 - val_loss: 92.4981\n",
      "Epoch 9438/10000\n",
      "96/96 [==============================] - 0s 153us/step - loss: 90.9774 - val_loss: 92.4170\n",
      "Epoch 9439/10000\n",
      "96/96 [==============================] - 0s 150us/step - loss: 88.5865 - val_loss: 92.6835\n",
      "Epoch 9440/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 93.2563 - val_loss: 92.0496\n",
      "Epoch 9441/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 92.6804 - val_loss: 93.2846\n",
      "Epoch 9442/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 90.1676 - val_loss: 92.7467\n",
      "Epoch 9443/10000\n",
      "96/96 [==============================] - 0s 148us/step - loss: 90.4198 - val_loss: 92.8186\n",
      "Epoch 9444/10000\n",
      "96/96 [==============================] - 0s 143us/step - loss: 89.9963 - val_loss: 92.4637\n",
      "Epoch 9445/10000\n",
      "96/96 [==============================] - 0s 148us/step - loss: 88.1638 - val_loss: 92.6845\n",
      "Epoch 9446/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 89.8810 - val_loss: 92.9595\n",
      "Epoch 9447/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 89.4114 - val_loss: 92.9091\n",
      "Epoch 9448/10000\n",
      "96/96 [==============================] - 0s 153us/step - loss: 90.0152 - val_loss: 92.5902\n",
      "Epoch 9449/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 90.3464 - val_loss: 92.5089\n",
      "Epoch 9450/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 89.5833 - val_loss: 93.1486\n",
      "Epoch 9451/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.9382 - val_loss: 92.9251\n",
      "Epoch 9452/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.9511 - val_loss: 92.8884\n",
      "Epoch 9453/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 90.7373 - val_loss: 93.0543\n",
      "Epoch 9454/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.6581 - val_loss: 92.1358\n",
      "Epoch 9455/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.3348 - val_loss: 91.1339\n",
      "Epoch 9456/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.3048 - val_loss: 93.0968\n",
      "Epoch 9457/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.0480 - val_loss: 91.9067\n",
      "Epoch 9458/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.7098 - val_loss: 92.2402\n",
      "Epoch 9459/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.2319 - val_loss: 92.7639\n",
      "Epoch 9460/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 89.2836 - val_loss: 92.8978\n",
      "Epoch 9461/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.9429 - val_loss: 92.6295\n",
      "Epoch 9462/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.5926 - val_loss: 91.6573\n",
      "Epoch 9463/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1620 - val_loss: 93.1361\n",
      "Epoch 9464/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.8204 - val_loss: 92.5016\n",
      "Epoch 9465/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6987 - val_loss: 92.2179\n",
      "Epoch 9466/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.0910 - val_loss: 92.9976\n",
      "Epoch 9467/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.6142 - val_loss: 92.1321\n",
      "Epoch 9468/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.5703 - val_loss: 92.6646\n",
      "Epoch 9469/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.4969 - val_loss: 92.7813\n",
      "Epoch 9470/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.3264 - val_loss: 92.9432\n",
      "Epoch 9471/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 89.8325 - val_loss: 92.9994\n",
      "Epoch 9472/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5850 - val_loss: 92.1447\n",
      "Epoch 9473/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9943 - val_loss: 91.9207\n",
      "Epoch 9474/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 91.7436 - val_loss: 92.7658\n",
      "Epoch 9475/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.9453 - val_loss: 92.9007\n",
      "Epoch 9476/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 98.4701 - val_loss: 92.4966\n",
      "Epoch 9477/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.5331 - val_loss: 91.8041\n",
      "Epoch 9478/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.4380 - val_loss: 92.4851\n",
      "Epoch 9479/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.3161 - val_loss: 92.9384\n",
      "Epoch 9480/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2438 - val_loss: 91.5670\n",
      "Epoch 9481/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1983 - val_loss: 92.5261\n",
      "Epoch 9482/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.6389 - val_loss: 92.7140\n",
      "Epoch 9483/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.7791 - val_loss: 91.9766\n",
      "Epoch 9484/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.3431 - val_loss: 93.2014\n",
      "Epoch 9485/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.4775 - val_loss: 93.7198\n",
      "Epoch 9486/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.9435 - val_loss: 92.4539\n",
      "Epoch 9487/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.5942 - val_loss: 92.4727\n",
      "Epoch 9488/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 89.2258 - val_loss: 92.6086\n",
      "Epoch 9489/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.9626 - val_loss: 93.3991\n",
      "Epoch 9490/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 89.7668 - val_loss: 92.9203\n",
      "Epoch 9491/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.3662 - val_loss: 93.0037\n",
      "Epoch 9492/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1291 - val_loss: 91.5745\n",
      "Epoch 9493/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2891 - val_loss: 93.0613\n",
      "Epoch 9494/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.8273 - val_loss: 92.1652\n",
      "Epoch 9495/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.5701 - val_loss: 93.1822\n",
      "Epoch 9496/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.0115 - val_loss: 92.7666\n",
      "Epoch 9497/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.8614 - val_loss: 92.8251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9498/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8557 - val_loss: 91.8143\n",
      "Epoch 9499/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.0807 - val_loss: 93.5063\n",
      "Epoch 9500/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.2953 - val_loss: 92.6072\n",
      "Epoch 9501/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.3156 - val_loss: 93.0413\n",
      "Epoch 9502/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.4789 - val_loss: 92.3767\n",
      "Epoch 9503/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.4443 - val_loss: 92.1170\n",
      "Epoch 9504/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.9035 - val_loss: 91.8756\n",
      "Epoch 9505/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.5532 - val_loss: 92.2820\n",
      "Epoch 9506/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.0976 - val_loss: 92.6945\n",
      "Epoch 9507/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.4553 - val_loss: 92.2369\n",
      "Epoch 9508/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.9844 - val_loss: 93.2003\n",
      "Epoch 9509/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.9812 - val_loss: 92.1014\n",
      "Epoch 9510/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.8062 - val_loss: 91.8934\n",
      "Epoch 9511/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.6692 - val_loss: 92.9203\n",
      "Epoch 9512/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.2082 - val_loss: 93.8513\n",
      "Epoch 9513/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.6902 - val_loss: 92.7936\n",
      "Epoch 9514/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.5745 - val_loss: 92.6515\n",
      "Epoch 9515/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 90.7631 - val_loss: 93.4439\n",
      "Epoch 9516/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.6037 - val_loss: 92.4951\n",
      "Epoch 9517/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.3092 - val_loss: 92.6074\n",
      "Epoch 9518/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 94.3518 - val_loss: 93.2120\n",
      "Epoch 9519/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6585 - val_loss: 92.3923\n",
      "Epoch 9520/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 93.9096 - val_loss: 92.9145\n",
      "Epoch 9521/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.8609 - val_loss: 92.7197\n",
      "Epoch 9522/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.2493 - val_loss: 92.0208\n",
      "Epoch 9523/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 94.0679 - val_loss: 92.7163\n",
      "Epoch 9524/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.1013 - val_loss: 90.4783\n",
      "Epoch 9525/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.5200 - val_loss: 92.5848\n",
      "Epoch 9526/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3908 - val_loss: 92.1541\n",
      "Epoch 9527/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.9519 - val_loss: 92.7953\n",
      "Epoch 9528/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.4580 - val_loss: 92.6143\n",
      "Epoch 9529/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.3459 - val_loss: 92.6573\n",
      "Epoch 9530/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.3060 - val_loss: 93.1892\n",
      "Epoch 9531/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4627 - val_loss: 92.8442\n",
      "Epoch 9532/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.2808 - val_loss: 92.6287\n",
      "Epoch 9533/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 96.1747 - val_loss: 92.4310\n",
      "Epoch 9534/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.7489 - val_loss: 92.5269\n",
      "Epoch 9535/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 95.7286 - val_loss: 92.4530\n",
      "Epoch 9536/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.2926 - val_loss: 92.9432\n",
      "Epoch 9537/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.4208 - val_loss: 92.4473\n",
      "Epoch 9538/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.5371 - val_loss: 92.8487\n",
      "Epoch 9539/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.2289 - val_loss: 92.6392\n",
      "Epoch 9540/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.5227 - val_loss: 92.6592\n",
      "Epoch 9541/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.7698 - val_loss: 92.2543\n",
      "Epoch 9542/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 90.0254 - val_loss: 92.6378\n",
      "Epoch 9543/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.3738 - val_loss: 92.8072\n",
      "Epoch 9544/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.9388 - val_loss: 93.3558\n",
      "Epoch 9545/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 93.1895 - val_loss: 92.4249\n",
      "Epoch 9546/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.4430 - val_loss: 92.9608\n",
      "Epoch 9547/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 89.8925 - val_loss: 93.2068\n",
      "Epoch 9548/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 88.9378 - val_loss: 92.6450\n",
      "Epoch 9549/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.0226 - val_loss: 92.3857\n",
      "Epoch 9550/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6076 - val_loss: 90.9369\n",
      "Epoch 9551/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 89.7151 - val_loss: 91.8730\n",
      "Epoch 9552/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.0941 - val_loss: 94.2666\n",
      "Epoch 9553/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.5905 - val_loss: 93.0759\n",
      "Epoch 9554/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 91.3570 - val_loss: 92.7267\n",
      "Epoch 9555/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.4611 - val_loss: 92.7755\n",
      "Epoch 9556/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.4288 - val_loss: 92.5078\n",
      "Epoch 9557/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.1285 - val_loss: 93.1370\n",
      "Epoch 9558/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.4612 - val_loss: 92.4668\n",
      "Epoch 9559/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3001 - val_loss: 93.1742\n",
      "Epoch 9560/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.5150 - val_loss: 93.2101\n",
      "Epoch 9561/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.1523 - val_loss: 92.3473\n",
      "Epoch 9562/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.8859 - val_loss: 93.2955\n",
      "Epoch 9563/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3548 - val_loss: 92.4192\n",
      "Epoch 9564/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1166 - val_loss: 92.0882\n",
      "Epoch 9565/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.3557 - val_loss: 92.8113\n",
      "Epoch 9566/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 89.7126 - val_loss: 92.6149\n",
      "Epoch 9567/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.5015 - val_loss: 92.6110\n",
      "Epoch 9568/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.4463 - val_loss: 92.8104\n",
      "Epoch 9569/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 90.0311 - val_loss: 92.2373\n",
      "Epoch 9570/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.8126 - val_loss: 92.6824\n",
      "Epoch 9571/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.0316 - val_loss: 91.6805\n",
      "Epoch 9572/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.4566 - val_loss: 93.0843\n",
      "Epoch 9573/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.7678 - val_loss: 92.2621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9574/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.3225 - val_loss: 91.7297\n",
      "Epoch 9575/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 93.0118 - val_loss: 93.2404\n",
      "Epoch 9576/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.2374 - val_loss: 91.4892\n",
      "Epoch 9577/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.7813 - val_loss: 92.8009\n",
      "Epoch 9578/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.9700 - val_loss: 92.8282\n",
      "Epoch 9579/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.2888 - val_loss: 92.5104\n",
      "Epoch 9580/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.1624 - val_loss: 92.2637\n",
      "Epoch 9581/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.5560 - val_loss: 92.3535\n",
      "Epoch 9582/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.9990 - val_loss: 92.5914\n",
      "Epoch 9583/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.5018 - val_loss: 92.7835\n",
      "Epoch 9584/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.2181 - val_loss: 92.7895\n",
      "Epoch 9585/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9163 - val_loss: 92.2811\n",
      "Epoch 9586/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.7438 - val_loss: 93.1195\n",
      "Epoch 9587/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.6762 - val_loss: 93.4707\n",
      "Epoch 9588/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.3835 - val_loss: 92.1329\n",
      "Epoch 9589/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 89.9831 - val_loss: 92.6822\n",
      "Epoch 9590/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 90.5418 - val_loss: 92.7368\n",
      "Epoch 9591/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.5268 - val_loss: 93.1223\n",
      "Epoch 9592/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.9409 - val_loss: 91.6699\n",
      "Epoch 9593/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.5454 - val_loss: 92.3190\n",
      "Epoch 9594/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.4061 - val_loss: 92.1772\n",
      "Epoch 9595/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1622 - val_loss: 92.7005\n",
      "Epoch 9596/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.3208 - val_loss: 92.3675\n",
      "Epoch 9597/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 92.7530 - val_loss: 92.5278\n",
      "Epoch 9598/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 89.6407 - val_loss: 93.4595\n",
      "Epoch 9599/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.1024 - val_loss: 92.0666\n",
      "Epoch 9600/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.1620 - val_loss: 92.7103\n",
      "Epoch 9601/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.6294 - val_loss: 93.0452\n",
      "Epoch 9602/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.4846 - val_loss: 92.7323\n",
      "Epoch 9603/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.4961 - val_loss: 92.8412\n",
      "Epoch 9604/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.6673 - val_loss: 92.8277\n",
      "Epoch 9605/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.5147 - val_loss: 92.5153\n",
      "Epoch 9606/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.5033 - val_loss: 92.5159\n",
      "Epoch 9607/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.3301 - val_loss: 92.7728\n",
      "Epoch 9608/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.3400 - val_loss: 92.3896\n",
      "Epoch 9609/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 89.9571 - val_loss: 92.1066\n",
      "Epoch 9610/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.5416 - val_loss: 92.4885\n",
      "Epoch 9611/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.6790 - val_loss: 93.0040\n",
      "Epoch 9612/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 91.3484 - val_loss: 92.4020\n",
      "Epoch 9613/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.1901 - val_loss: 92.6956\n",
      "Epoch 9614/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.4356 - val_loss: 92.6733\n",
      "Epoch 9615/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 89.9884 - val_loss: 91.7314\n",
      "Epoch 9616/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.2776 - val_loss: 92.7711\n",
      "Epoch 9617/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.3891 - val_loss: 92.5830\n",
      "Epoch 9618/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.6622 - val_loss: 92.5764\n",
      "Epoch 9619/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.3872 - val_loss: 92.6805\n",
      "Epoch 9620/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1569 - val_loss: 92.7638\n",
      "Epoch 9621/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.3470 - val_loss: 92.5820\n",
      "Epoch 9622/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.4684 - val_loss: 92.9230\n",
      "Epoch 9623/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.5155 - val_loss: 92.8086\n",
      "Epoch 9624/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2341 - val_loss: 92.4298\n",
      "Epoch 9625/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.2146 - val_loss: 92.5629\n",
      "Epoch 9626/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.5617 - val_loss: 91.8922\n",
      "Epoch 9627/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.1851 - val_loss: 92.5727\n",
      "Epoch 9628/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 89.5222 - val_loss: 92.4523\n",
      "Epoch 9629/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.6390 - val_loss: 92.6578\n",
      "Epoch 9630/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 93.2771 - val_loss: 92.2708\n",
      "Epoch 9631/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.0298 - val_loss: 92.6320\n",
      "Epoch 9632/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5018 - val_loss: 92.5294\n",
      "Epoch 9633/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.3839 - val_loss: 92.2359\n",
      "Epoch 9634/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3384 - val_loss: 92.8947\n",
      "Epoch 9635/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.2818 - val_loss: 92.8917\n",
      "Epoch 9636/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.1557 - val_loss: 92.4557\n",
      "Epoch 9637/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.9526 - val_loss: 92.4237\n",
      "Epoch 9638/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.9001 - val_loss: 92.0940\n",
      "Epoch 9639/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 89.7278 - val_loss: 92.5566\n",
      "Epoch 9640/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.4120 - val_loss: 92.7451\n",
      "Epoch 9641/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3368 - val_loss: 92.0939\n",
      "Epoch 9642/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.9699 - val_loss: 92.5186\n",
      "Epoch 9643/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.0757 - val_loss: 92.4632\n",
      "Epoch 9644/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 89.8680 - val_loss: 91.9336\n",
      "Epoch 9645/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 89.6450 - val_loss: 92.5414\n",
      "Epoch 9646/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.3577 - val_loss: 92.0639\n",
      "Epoch 9647/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.7092 - val_loss: 92.6273\n",
      "Epoch 9648/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 90.9234 - val_loss: 92.5316\n",
      "Epoch 9649/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.9650 - val_loss: 93.4878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9650/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.5639 - val_loss: 92.2879\n",
      "Epoch 9651/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.4653 - val_loss: 92.0959\n",
      "Epoch 9652/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.2765 - val_loss: 92.5299\n",
      "Epoch 9653/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.1520 - val_loss: 93.4765\n",
      "Epoch 9654/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5737 - val_loss: 92.1583\n",
      "Epoch 9655/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 91.1272 - val_loss: 92.2599\n",
      "Epoch 9656/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.1637 - val_loss: 92.4731\n",
      "Epoch 9657/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.2808 - val_loss: 92.2141\n",
      "Epoch 9658/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.7091 - val_loss: 93.7870\n",
      "Epoch 9659/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3632 - val_loss: 92.4972\n",
      "Epoch 9660/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.5675 - val_loss: 93.8037\n",
      "Epoch 9661/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.6840 - val_loss: 92.4606\n",
      "Epoch 9662/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.9250 - val_loss: 91.8673\n",
      "Epoch 9663/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2561 - val_loss: 92.4624\n",
      "Epoch 9664/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8529 - val_loss: 92.0073\n",
      "Epoch 9665/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7712 - val_loss: 92.8683\n",
      "Epoch 9666/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.8573 - val_loss: 91.7189\n",
      "Epoch 9667/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8451 - val_loss: 92.9533\n",
      "Epoch 9668/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.2263 - val_loss: 92.3261\n",
      "Epoch 9669/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.7972 - val_loss: 92.3463\n",
      "Epoch 9670/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 89.8636 - val_loss: 92.2099\n",
      "Epoch 9671/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.0319 - val_loss: 92.2372\n",
      "Epoch 9672/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.8257 - val_loss: 93.2897\n",
      "Epoch 9673/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 89.7176 - val_loss: 92.2304\n",
      "Epoch 9674/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.6685 - val_loss: 92.5209\n",
      "Epoch 9675/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.7249 - val_loss: 92.2225\n",
      "Epoch 9676/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.6568 - val_loss: 92.3606\n",
      "Epoch 9677/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.3783 - val_loss: 92.1168\n",
      "Epoch 9678/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.0400 - val_loss: 92.4397\n",
      "Epoch 9679/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.6279 - val_loss: 92.7173\n",
      "Epoch 9680/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.9641 - val_loss: 92.1373\n",
      "Epoch 9681/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 89.9964 - val_loss: 92.5571\n",
      "Epoch 9682/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 89.6247 - val_loss: 92.4319\n",
      "Epoch 9683/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.7434 - val_loss: 91.7614\n",
      "Epoch 9684/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 89.4946 - val_loss: 93.5353\n",
      "Epoch 9685/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6319 - val_loss: 92.4242\n",
      "Epoch 9686/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9818 - val_loss: 92.1036\n",
      "Epoch 9687/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.4942 - val_loss: 92.1153\n",
      "Epoch 9688/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.3015 - val_loss: 92.4624\n",
      "Epoch 9689/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3296 - val_loss: 92.9390\n",
      "Epoch 9690/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.9969 - val_loss: 91.7741\n",
      "Epoch 9691/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.5921 - val_loss: 92.3042\n",
      "Epoch 9692/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 89.6591 - val_loss: 92.7754\n",
      "Epoch 9693/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.6109 - val_loss: 92.9665\n",
      "Epoch 9694/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 88.9566 - val_loss: 92.2874\n",
      "Epoch 9695/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.3417 - val_loss: 92.2060\n",
      "Epoch 9696/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.1692 - val_loss: 93.3754\n",
      "Epoch 9697/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.6661 - val_loss: 92.7926\n",
      "Epoch 9698/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.4650 - val_loss: 91.8567\n",
      "Epoch 9699/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.0359 - val_loss: 92.8168\n",
      "Epoch 9700/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 89.6388 - val_loss: 91.9283\n",
      "Epoch 9701/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.3310 - val_loss: 93.0437\n",
      "Epoch 9702/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.3814 - val_loss: 92.7820\n",
      "Epoch 9703/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.9662 - val_loss: 92.5806\n",
      "Epoch 9704/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1157 - val_loss: 92.0205\n",
      "Epoch 9705/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.0336 - val_loss: 92.5476\n",
      "Epoch 9706/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.2560 - val_loss: 92.2711\n",
      "Epoch 9707/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.0927 - val_loss: 92.2699\n",
      "Epoch 9708/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.3015 - val_loss: 91.8192\n",
      "Epoch 9709/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.9033 - val_loss: 92.3390\n",
      "Epoch 9710/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.3479 - val_loss: 92.0694\n",
      "Epoch 9711/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 93.0497 - val_loss: 92.7582\n",
      "Epoch 9712/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.3347 - val_loss: 92.6786\n",
      "Epoch 9713/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.0798 - val_loss: 92.1571\n",
      "Epoch 9714/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.9055 - val_loss: 92.4327\n",
      "Epoch 9715/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.1452 - val_loss: 92.2406\n",
      "Epoch 9716/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2695 - val_loss: 93.1825\n",
      "Epoch 9717/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 89.6362 - val_loss: 92.4716\n",
      "Epoch 9718/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.3854 - val_loss: 93.2538\n",
      "Epoch 9719/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.5865 - val_loss: 92.2041\n",
      "Epoch 9720/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.8102 - val_loss: 92.2135\n",
      "Epoch 9721/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 89.3269 - val_loss: 93.0631\n",
      "Epoch 9722/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.4097 - val_loss: 92.1617\n",
      "Epoch 9723/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.0702 - val_loss: 93.0346\n",
      "Epoch 9724/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.9012 - val_loss: 92.6656\n",
      "Epoch 9725/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.6747 - val_loss: 92.4623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9726/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.2315 - val_loss: 92.2416\n",
      "Epoch 9727/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 89.2565 - val_loss: 92.5363\n",
      "Epoch 9728/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.0606 - val_loss: 92.6074\n",
      "Epoch 9729/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.4877 - val_loss: 92.2014\n",
      "Epoch 9730/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.5326 - val_loss: 92.6337\n",
      "Epoch 9731/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.6700 - val_loss: 92.5744\n",
      "Epoch 9732/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.6393 - val_loss: 91.7146\n",
      "Epoch 9733/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.1033 - val_loss: 92.7249\n",
      "Epoch 9734/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 88.6420 - val_loss: 92.6821\n",
      "Epoch 9735/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.2330 - val_loss: 92.2618\n",
      "Epoch 9736/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 88.8179 - val_loss: 92.9179\n",
      "Epoch 9737/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.7078 - val_loss: 92.5710\n",
      "Epoch 9738/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 94.7098 - val_loss: 92.2953\n",
      "Epoch 9739/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.9965 - val_loss: 92.7655\n",
      "Epoch 9740/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 89.8637 - val_loss: 92.4462\n",
      "Epoch 9741/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.0640 - val_loss: 92.1599\n",
      "Epoch 9742/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 89.8953 - val_loss: 92.1874\n",
      "Epoch 9743/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.0125 - val_loss: 92.5587\n",
      "Epoch 9744/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.5377 - val_loss: 92.3985\n",
      "Epoch 9745/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.4641 - val_loss: 92.3992\n",
      "Epoch 9746/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.2508 - val_loss: 92.8636\n",
      "Epoch 9747/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.9913 - val_loss: 92.4418\n",
      "Epoch 9748/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.3286 - val_loss: 91.7161\n",
      "Epoch 9749/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2321 - val_loss: 92.4930\n",
      "Epoch 9750/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 89.4205 - val_loss: 92.2175\n",
      "Epoch 9751/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 89.3627 - val_loss: 92.4595\n",
      "Epoch 9752/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 89.1998 - val_loss: 92.8956\n",
      "Epoch 9753/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2285 - val_loss: 91.9347\n",
      "Epoch 9754/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2040 - val_loss: 93.5393\n",
      "Epoch 9755/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 90.8749 - val_loss: 92.2199\n",
      "Epoch 9756/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.9189 - val_loss: 92.6272\n",
      "Epoch 9757/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 89.8634 - val_loss: 92.2824\n",
      "Epoch 9758/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.5197 - val_loss: 92.1111\n",
      "Epoch 9759/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.4322 - val_loss: 92.8008\n",
      "Epoch 9760/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 89.9645 - val_loss: 92.4663\n",
      "Epoch 9761/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 91.9188 - val_loss: 91.8190\n",
      "Epoch 9762/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.6115 - val_loss: 93.3091\n",
      "Epoch 9763/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.4443 - val_loss: 92.8615\n",
      "Epoch 9764/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.4881 - val_loss: 92.0764\n",
      "Epoch 9765/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 95.9172 - val_loss: 92.5929\n",
      "Epoch 9766/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 89.4450 - val_loss: 92.7338\n",
      "Epoch 9767/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.5777 - val_loss: 92.1887\n",
      "Epoch 9768/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 93.7483 - val_loss: 92.1026\n",
      "Epoch 9769/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.3874 - val_loss: 92.4578\n",
      "Epoch 9770/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.3255 - val_loss: 91.6294\n",
      "Epoch 9771/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.2497 - val_loss: 92.6472\n",
      "Epoch 9772/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.1968 - val_loss: 92.5092\n",
      "Epoch 9773/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 88.8556 - val_loss: 92.6964\n",
      "Epoch 9774/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.0340 - val_loss: 93.5197\n",
      "Epoch 9775/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 90.5503 - val_loss: 92.5196\n",
      "Epoch 9776/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.3911 - val_loss: 92.0411\n",
      "Epoch 9777/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 90.7633 - val_loss: 92.3136\n",
      "Epoch 9778/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.4869 - val_loss: 93.7365\n",
      "Epoch 9779/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.3302 - val_loss: 91.8598\n",
      "Epoch 9780/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 89.6158 - val_loss: 91.5649\n",
      "Epoch 9781/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.8962 - val_loss: 92.3912\n",
      "Epoch 9782/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 89.6893 - val_loss: 92.5963\n",
      "Epoch 9783/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.4924 - val_loss: 92.3223\n",
      "Epoch 9784/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.5789 - val_loss: 91.5192\n",
      "Epoch 9785/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.5925 - val_loss: 92.6753\n",
      "Epoch 9786/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.8433 - val_loss: 92.4642\n",
      "Epoch 9787/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.6217 - val_loss: 91.8920\n",
      "Epoch 9788/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 90.3095 - val_loss: 92.6151\n",
      "Epoch 9789/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.6017 - val_loss: 91.5927\n",
      "Epoch 9790/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.0141 - val_loss: 92.0649\n",
      "Epoch 9791/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.2091 - val_loss: 92.6319\n",
      "Epoch 9792/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 92.2825 - val_loss: 92.8078\n",
      "Epoch 9793/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 90.3909 - val_loss: 92.8637\n",
      "Epoch 9794/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.0871 - val_loss: 92.3850\n",
      "Epoch 9795/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.4683 - val_loss: 92.7864\n",
      "Epoch 9796/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.8454 - val_loss: 92.4469\n",
      "Epoch 9797/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 89.8241 - val_loss: 92.8579\n",
      "Epoch 9798/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.0925 - val_loss: 91.5366\n",
      "Epoch 9799/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 89.2517 - val_loss: 92.2262\n",
      "Epoch 9800/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.0546 - val_loss: 92.1699\n",
      "Epoch 9801/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.5041 - val_loss: 91.8404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9802/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.5742 - val_loss: 92.4596\n",
      "Epoch 9803/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.3960 - val_loss: 92.1947\n",
      "Epoch 9804/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.6319 - val_loss: 92.7323\n",
      "Epoch 9805/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.5169 - val_loss: 92.9361\n",
      "Epoch 9806/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8561 - val_loss: 92.7671\n",
      "Epoch 9807/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.1300 - val_loss: 92.5960\n",
      "Epoch 9808/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 89.3889 - val_loss: 92.6090\n",
      "Epoch 9809/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 88.9566 - val_loss: 92.7231\n",
      "Epoch 9810/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.3962 - val_loss: 92.3455\n",
      "Epoch 9811/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 93.8254 - val_loss: 93.0211\n",
      "Epoch 9812/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2114 - val_loss: 92.2322\n",
      "Epoch 9813/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2251 - val_loss: 92.6618\n",
      "Epoch 9814/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.1273 - val_loss: 92.4573\n",
      "Epoch 9815/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.3961 - val_loss: 92.1133\n",
      "Epoch 9816/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 89.8752 - val_loss: 92.8303\n",
      "Epoch 9817/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 89.5474 - val_loss: 92.1619\n",
      "Epoch 9818/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.8169 - val_loss: 93.1124\n",
      "Epoch 9819/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4118 - val_loss: 92.7000\n",
      "Epoch 9820/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.6649 - val_loss: 92.2261\n",
      "Epoch 9821/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.2565 - val_loss: 92.1162\n",
      "Epoch 9822/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.9813 - val_loss: 92.1922\n",
      "Epoch 9823/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.3830 - val_loss: 91.8776\n",
      "Epoch 9824/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.4846 - val_loss: 92.7351\n",
      "Epoch 9825/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.0424 - val_loss: 92.0014\n",
      "Epoch 9826/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.9472 - val_loss: 91.9115\n",
      "Epoch 9827/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 89.5253 - val_loss: 92.4529\n",
      "Epoch 9828/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.1589 - val_loss: 92.5914\n",
      "Epoch 9829/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 92.4409 - val_loss: 92.5267\n",
      "Epoch 9830/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.1592 - val_loss: 92.5968\n",
      "Epoch 9831/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.7261 - val_loss: 92.3628\n",
      "Epoch 9832/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.2685 - val_loss: 92.0591\n",
      "Epoch 9833/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3788 - val_loss: 92.6531\n",
      "Epoch 9834/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.3248 - val_loss: 92.2218\n",
      "Epoch 9835/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.1552 - val_loss: 92.2903\n",
      "Epoch 9836/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.3187 - val_loss: 92.4770\n",
      "Epoch 9837/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.9451 - val_loss: 94.2149\n",
      "Epoch 9838/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 89.9444 - val_loss: 91.7519\n",
      "Epoch 9839/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.8290 - val_loss: 91.9189\n",
      "Epoch 9840/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.5064 - val_loss: 92.2393\n",
      "Epoch 9841/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.9359 - val_loss: 92.9084\n",
      "Epoch 9842/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2154 - val_loss: 92.4374\n",
      "Epoch 9843/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.4115 - val_loss: 92.9635\n",
      "Epoch 9844/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.0822 - val_loss: 92.7488\n",
      "Epoch 9845/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.8436 - val_loss: 92.5908\n",
      "Epoch 9846/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.0310 - val_loss: 91.9617\n",
      "Epoch 9847/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.2378 - val_loss: 92.1546\n",
      "Epoch 9848/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 92.3171 - val_loss: 92.4021\n",
      "Epoch 9849/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 92.0584 - val_loss: 91.4435\n",
      "Epoch 9850/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 88.6034 - val_loss: 91.9847\n",
      "Epoch 9851/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.7593 - val_loss: 92.4455\n",
      "Epoch 9852/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.3189 - val_loss: 92.0015\n",
      "Epoch 9853/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 92.3690 - val_loss: 92.3421\n",
      "Epoch 9854/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 89.1898 - val_loss: 92.7833\n",
      "Epoch 9855/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.7419 - val_loss: 93.0898\n",
      "Epoch 9856/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.7136 - val_loss: 91.1964\n",
      "Epoch 9857/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 90.9478 - val_loss: 92.7013\n",
      "Epoch 9858/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.5046 - val_loss: 92.2335\n",
      "Epoch 9859/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 91.0393 - val_loss: 94.0268\n",
      "Epoch 9860/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 89.6011 - val_loss: 92.6235\n",
      "Epoch 9861/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 90.7042 - val_loss: 92.6833\n",
      "Epoch 9862/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.2699 - val_loss: 92.5190\n",
      "Epoch 9863/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 92.6464 - val_loss: 92.4420\n",
      "Epoch 9864/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.2306 - val_loss: 92.3419\n",
      "Epoch 9865/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 90.0111 - val_loss: 92.1227\n",
      "Epoch 9866/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.3349 - val_loss: 92.5997\n",
      "Epoch 9867/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.7967 - val_loss: 92.5945\n",
      "Epoch 9868/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.3419 - val_loss: 91.7775\n",
      "Epoch 9869/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 89.8080 - val_loss: 92.1008\n",
      "Epoch 9870/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 89.8689 - val_loss: 93.1911\n",
      "Epoch 9871/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.1896 - val_loss: 92.1952\n",
      "Epoch 9872/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.2064 - val_loss: 92.0436\n",
      "Epoch 9873/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.3530 - val_loss: 92.4546\n",
      "Epoch 9874/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.4198 - val_loss: 92.8883\n",
      "Epoch 9875/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.7005 - val_loss: 92.5955\n",
      "Epoch 9876/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.3842 - val_loss: 92.6077\n",
      "Epoch 9877/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 89.6900 - val_loss: 91.0872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9878/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.4042 - val_loss: 92.1943\n",
      "Epoch 9879/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 89.9019 - val_loss: 92.3699\n",
      "Epoch 9880/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.0943 - val_loss: 92.1457\n",
      "Epoch 9881/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.0038 - val_loss: 92.3525\n",
      "Epoch 9882/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.1189 - val_loss: 92.5115\n",
      "Epoch 9883/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.1829 - val_loss: 92.6343\n",
      "Epoch 9884/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.0253 - val_loss: 92.7062\n",
      "Epoch 9885/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.6295 - val_loss: 92.0525\n",
      "Epoch 9886/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.1129 - val_loss: 93.3301\n",
      "Epoch 9887/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.6564 - val_loss: 92.6195\n",
      "Epoch 9888/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.0523 - val_loss: 92.7021\n",
      "Epoch 9889/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.2530 - val_loss: 92.3221\n",
      "Epoch 9890/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 90.9724 - val_loss: 93.1414\n",
      "Epoch 9891/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 93.0380 - val_loss: 92.3245\n",
      "Epoch 9892/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.8985 - val_loss: 92.3163\n",
      "Epoch 9893/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.5876 - val_loss: 92.9953\n",
      "Epoch 9894/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.4379 - val_loss: 92.2650\n",
      "Epoch 9895/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.5181 - val_loss: 92.3355\n",
      "Epoch 9896/10000\n",
      "96/96 [==============================] - 0s 124us/step - loss: 91.3893 - val_loss: 93.2535\n",
      "Epoch 9897/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 90.9235 - val_loss: 92.8578\n",
      "Epoch 9898/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.2010 - val_loss: 91.6511\n",
      "Epoch 9899/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 90.7103 - val_loss: 92.6227\n",
      "Epoch 9900/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.0831 - val_loss: 91.9352\n",
      "Epoch 9901/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.1656 - val_loss: 92.4766\n",
      "Epoch 9902/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.0753 - val_loss: 93.0161\n",
      "Epoch 9903/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 91.7217 - val_loss: 92.1552\n",
      "Epoch 9904/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 92.1846 - val_loss: 92.7739\n",
      "Epoch 9905/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 93.5463 - val_loss: 92.4592\n",
      "Epoch 9906/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 91.3814 - val_loss: 92.7370\n",
      "Epoch 9907/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 89.5126 - val_loss: 92.1915\n",
      "Epoch 9908/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 92.7006 - val_loss: 93.1445\n",
      "Epoch 9909/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 91.6726 - val_loss: 92.1596\n",
      "Epoch 9910/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 93.0685 - val_loss: 92.2780\n",
      "Epoch 9911/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.4368 - val_loss: 92.4517\n",
      "Epoch 9912/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.4817 - val_loss: 93.0135\n",
      "Epoch 9913/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.0852 - val_loss: 93.3654\n",
      "Epoch 9914/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 91.2474 - val_loss: 91.4814\n",
      "Epoch 9915/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.1389 - val_loss: 93.1930\n",
      "Epoch 9916/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.9035 - val_loss: 92.7214\n",
      "Epoch 9917/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 89.3409 - val_loss: 92.8667\n",
      "Epoch 9918/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 90.3337 - val_loss: 92.3909\n",
      "Epoch 9919/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.1093 - val_loss: 92.4952\n",
      "Epoch 9920/10000\n",
      "96/96 [==============================] - 0s 123us/step - loss: 89.8480 - val_loss: 92.3435\n",
      "Epoch 9921/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.3314 - val_loss: 92.8145\n",
      "Epoch 9922/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 90.0779 - val_loss: 92.1913\n",
      "Epoch 9923/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.2440 - val_loss: 93.0920\n",
      "Epoch 9924/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 90.6541 - val_loss: 92.6826\n",
      "Epoch 9925/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 91.5020 - val_loss: 92.1801\n",
      "Epoch 9926/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.9935 - val_loss: 93.3276\n",
      "Epoch 9927/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.2290 - val_loss: 92.2308\n",
      "Epoch 9928/10000\n",
      "96/96 [==============================] - 0s 120us/step - loss: 91.5544 - val_loss: 92.3864\n",
      "Epoch 9929/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 90.6392 - val_loss: 92.4999\n",
      "Epoch 9930/10000\n",
      "96/96 [==============================] - 0s 118us/step - loss: 89.1824 - val_loss: 92.2857\n",
      "Epoch 9931/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 91.9312 - val_loss: 93.4022\n",
      "Epoch 9932/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 89.7465 - val_loss: 92.0341\n",
      "Epoch 9933/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 90.1172 - val_loss: 92.6148\n",
      "Epoch 9934/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.4619 - val_loss: 92.6793\n",
      "Epoch 9935/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 92.2894 - val_loss: 92.2644\n",
      "Epoch 9936/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 88.7213 - val_loss: 92.7495\n",
      "Epoch 9937/10000\n",
      "96/96 [==============================] - 0s 119us/step - loss: 91.9789 - val_loss: 92.0157\n",
      "Epoch 9938/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 96.7767 - val_loss: 92.1268\n",
      "Epoch 9939/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.9506 - val_loss: 92.6040\n",
      "Epoch 9940/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 90.8909 - val_loss: 93.0166\n",
      "Epoch 9941/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 97.3173 - val_loss: 92.3866\n",
      "Epoch 9942/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 90.9843 - val_loss: 92.4334\n",
      "Epoch 9943/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 90.6429 - val_loss: 92.4592\n",
      "Epoch 9944/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 89.7427 - val_loss: 92.6436\n",
      "Epoch 9945/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.0900 - val_loss: 92.2013\n",
      "Epoch 9946/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.1620 - val_loss: 92.3493\n",
      "Epoch 9947/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 91.0384 - val_loss: 92.4374\n",
      "Epoch 9948/10000\n",
      "96/96 [==============================] - 0s 115us/step - loss: 90.9339 - val_loss: 93.0440\n",
      "Epoch 9949/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 89.6076 - val_loss: 92.4100\n",
      "Epoch 9950/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.6023 - val_loss: 92.6026\n",
      "Epoch 9951/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 92.1713 - val_loss: 92.5582\n",
      "Epoch 9952/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 90.0772 - val_loss: 91.5515\n",
      "Epoch 9953/10000\n",
      "96/96 [==============================] - 0s 116us/step - loss: 90.2936 - val_loss: 92.7930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9954/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.0720 - val_loss: 92.4917\n",
      "Epoch 9955/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 90.1700 - val_loss: 92.7753\n",
      "Epoch 9956/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.1086 - val_loss: 91.9370\n",
      "Epoch 9957/10000\n",
      "96/96 [==============================] - 0s 135us/step - loss: 90.5252 - val_loss: 92.1866\n",
      "Epoch 9958/10000\n",
      "96/96 [==============================] - 0s 138us/step - loss: 91.0375 - val_loss: 92.6165\n",
      "Epoch 9959/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 90.9199 - val_loss: 92.6353\n",
      "Epoch 9960/10000\n",
      "96/96 [==============================] - 0s 140us/step - loss: 91.5220 - val_loss: 92.8293\n",
      "Epoch 9961/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 92.0555 - val_loss: 92.1467\n",
      "Epoch 9962/10000\n",
      "96/96 [==============================] - 0s 146us/step - loss: 89.9149 - val_loss: 92.9650\n",
      "Epoch 9963/10000\n",
      "96/96 [==============================] - 0s 149us/step - loss: 90.8940 - val_loss: 93.2509\n",
      "Epoch 9964/10000\n",
      "96/96 [==============================] - 0s 151us/step - loss: 92.1538 - val_loss: 92.9226\n",
      "Epoch 9965/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 92.5577 - val_loss: 92.4151\n",
      "Epoch 9966/10000\n",
      "96/96 [==============================] - 0s 155us/step - loss: 91.0650 - val_loss: 92.3546\n",
      "Epoch 9967/10000\n",
      "96/96 [==============================] - 0s 150us/step - loss: 90.2497 - val_loss: 91.7806\n",
      "Epoch 9968/10000\n",
      "96/96 [==============================] - 0s 155us/step - loss: 90.8388 - val_loss: 92.1219\n",
      "Epoch 9969/10000\n",
      "96/96 [==============================] - 0s 163us/step - loss: 90.5248 - val_loss: 92.4973\n",
      "Epoch 9970/10000\n",
      "96/96 [==============================] - 0s 159us/step - loss: 90.8265 - val_loss: 91.4508\n",
      "Epoch 9971/10000\n",
      "96/96 [==============================] - 0s 147us/step - loss: 90.9257 - val_loss: 92.9344\n",
      "Epoch 9972/10000\n",
      "96/96 [==============================] - 0s 144us/step - loss: 90.8812 - val_loss: 92.4234\n",
      "Epoch 9973/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 91.0961 - val_loss: 92.4612\n",
      "Epoch 9974/10000\n",
      "96/96 [==============================] - 0s 122us/step - loss: 90.3865 - val_loss: 91.9910\n",
      "Epoch 9975/10000\n",
      "96/96 [==============================] - 0s 117us/step - loss: 89.8153 - val_loss: 92.6124\n",
      "Epoch 9976/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 91.1776 - val_loss: 92.4634\n",
      "Epoch 9977/10000\n",
      "96/96 [==============================] - 0s 354us/step - loss: 91.7924 - val_loss: 92.2969\n",
      "Epoch 9978/10000\n",
      "96/96 [==============================] - 0s 121us/step - loss: 90.3669 - val_loss: 92.7846\n",
      "Epoch 9979/10000\n",
      "96/96 [==============================] - 0s 114us/step - loss: 89.5948 - val_loss: 93.6969\n",
      "Epoch 9980/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 93.6350 - val_loss: 92.7880\n",
      "Epoch 9981/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 90.7787 - val_loss: 92.2598\n",
      "Epoch 9982/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.4642 - val_loss: 92.2385\n",
      "Epoch 9983/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.4417 - val_loss: 91.9787\n",
      "Epoch 9984/10000\n",
      "96/96 [==============================] - 0s 126us/step - loss: 90.1131 - val_loss: 93.1525\n",
      "Epoch 9985/10000\n",
      "96/96 [==============================] - 0s 127us/step - loss: 90.7446 - val_loss: 92.5905\n",
      "Epoch 9986/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.6225 - val_loss: 93.1538\n",
      "Epoch 9987/10000\n",
      "96/96 [==============================] - 0s 130us/step - loss: 90.2263 - val_loss: 92.5149\n",
      "Epoch 9988/10000\n",
      "96/96 [==============================] - 0s 131us/step - loss: 90.0953 - val_loss: 92.5050\n",
      "Epoch 9989/10000\n",
      "96/96 [==============================] - 0s 125us/step - loss: 90.6875 - val_loss: 93.1404\n",
      "Epoch 9990/10000\n",
      "96/96 [==============================] - 0s 129us/step - loss: 89.9260 - val_loss: 93.2231\n",
      "Epoch 9991/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 89.7395 - val_loss: 92.7458\n",
      "Epoch 9992/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 89.6861 - val_loss: 92.5020\n",
      "Epoch 9993/10000\n",
      "96/96 [==============================] - 0s 163us/step - loss: 88.6484 - val_loss: 92.4532\n",
      "Epoch 9994/10000\n",
      "96/96 [==============================] - 0s 132us/step - loss: 90.4461 - val_loss: 92.4135\n",
      "Epoch 9995/10000\n",
      "96/96 [==============================] - 0s 128us/step - loss: 91.6561 - val_loss: 91.8478\n",
      "Epoch 9996/10000\n",
      "96/96 [==============================] - 0s 133us/step - loss: 89.6303 - val_loss: 92.3142\n",
      "Epoch 9997/10000\n",
      "96/96 [==============================] - 0s 142us/step - loss: 89.6161 - val_loss: 91.6197\n",
      "Epoch 9998/10000\n",
      "96/96 [==============================] - 0s 160us/step - loss: 89.4663 - val_loss: 92.5241\n",
      "Epoch 9999/10000\n",
      "96/96 [==============================] - 0s 134us/step - loss: 91.4230 - val_loss: 92.3058\n",
      "Epoch 10000/10000\n",
      "96/96 [==============================] - 0s 145us/step - loss: 91.5094 - val_loss: 92.4217\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X[:-2], y[:-2], epochs=10000, validation_data=(X_test, y_test), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f595c548940>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzsnXmYHVWZ/z/n3rprL+lushACIYCsIiAEBEEWUURR1AF1cEZQR3AERx0dl/E3Du7LuM3gguLCoqKCKyqrQFD2NYSwhIQlG1k6Sa93rVu3fn+cOlWnltu53Um6k+Z8nydPuqvr3tpOnfe83++7CNd1MTAwMDAw0JGa6hMwMDAwMNj5YIyDgYGBgUEMxjgYGBgYGMRgjIOBgYGBQQzGOBgYGBgYxGCMg4GBgYFBDMY4GBgYGBjEYIyDgYGBgUEMxjgYGBgYGMRgTfUJTBQzZ850FyxYMNWnYWBgYLDL4KGHHtrkuu6sdvbdZY3DggULePDBB6f6NAwMDAx2GQghVra7r6GVDAwMDAxiMMbBwMDAwCAGYxwMDAwMDGIwxsHAwMDAIAZjHAwMDAwMYjDGwcDAwMAgBmMcDAwMDAxiMMbBwGCC2DhS5abH10/1aRgY7BAY42BgMEFc++AaPvDzh6g3mlN9KgYG2x3GOBgYTBBV26HpgtN0p/pUDAy2O4xxMDCYIOqO9BgaTeM5GEw/GONgYDBB2A3pMRjPwWA6whgHA4MJwvY9B2McDKYfjHEwMJgglHEwnoPBdIQxDgYGE0TdeA4G0xjGOBgYTBC242kOjjEOBtMPWzUOQogDhRCLtX/DQoiPCCH6hBC3CCGWe//3evsLIcQlQogVQoglQogjte86z9t/uRDiPG37UUKIx7zPXCKEEDvmcg0Mth/sholWMpi+2KpxcF13meu6R7iuewRwFFAGfg98CrjVdd39gVu93wFeD+zv/bsAuBRACNEHXAy8AjgGuFgZFG+f87XPnb5drs7AYAfCaA4G0xnjpZVOBZ5xXXcl8GbgSm/7lcBbvJ/fDFzlStwL9Agh5gKvA25xXXeL67oDwC3A6d7ful3Xvdd1XRe4SvsuA4OdFkZzMJjOGK9x+Efgl97Pc1zXXef9vB6Y4/08D1itfWaNt22s7WsSthsY7NQwnoPBdEbbxkEIkQXOBK6N/s1b8e/wN0QIcYEQ4kEhxIP9/f07+nAGBmNCCdLGczCYjhiP5/B64GHXdTd4v2/wKCG8/zd629cCe2mf29PbNtb2PRO2x+C67mWu6y50XXfhrFmzxnHqBgbbH4HnYARpg+mH8RiHcwgoJYDrABVxdB7wR237uV7U0rHAkEc/3QScJoTo9YTo04CbvL8NCyGO9aKUztW+y8Bgp4WqxmqbUFaDaQirnZ2EEB3Aa4H3a5u/ClwjhPgXYCXwdm/79cAbgBXIyKb3ALiuu0UI8QXgAW+/z7uuu8X7+ULgCqAA3OD9MzDYqWE0B4PpjLaMg+u6JWC3yLbNyOil6L4ucFGL7/kp8NOE7Q8Ch7ZzLgYGOwuM5mAwnWEypA0MJgijORhMZxjjYGAwQfhVWY3mYDANYYyDgcEEoQRpozkYTEcY42BgMEEYzcFgOsMYBwODCcJEKxlMZxjjYGAwATSbru8xGM/BYDrCGAcDgwnA1iKUTLSSwXSEMQ4GBhOAnhVtPAeD6QhjHAwMJgDV6AeM5mAwPWGMg4HBBKDEaDB5DgbTE8Y4GBhMAHXHeA4G0xvGOBgYTABGczCY7jDGwcBgArAdE61kML1hjIOBwQRQ1wRp08/BYDrCGAcDgwnANpqDwTSHMQ4GBhOA0RwMpjuMcTAwmACM5mAw3WGMg4HBBKCHshrPwWA6whgHA4MJwGRIG0x3GONgYDABGM3BYLrDGAcDgwkgpDmYUFaDaQhjHAwMJgCjORhMdxjjYGAwASjPoZBJm2glg2kJYxwMDCYAJUgXsmnjORhMSxjjYGAwAShBWnoOxjgYTD8Y42BgMAEozSGfSRnPwWBawhgHA4MJwPaNg/EcDKYnjHEwMJgAbKeJlRJk0sZzMJieMMbBwGACsB2XTDqFlRI0HBOtZDD9YIyDgcEEUG80yaQF6ZQwnoPBtIQxDgYGW8FP7nyOK+56LrTNdppkrRRWWhjNwWBawhgHA4Ot4LpHX+DPS9aFttlOk0w6RTplNAeDycMz/aM8vWEE193xY84YBwODraBad6g2nNA2XXMwGdIGk4XL7niWt//wnkk5ljUpRzEw2IVRsR0cN7yOqjua5mAK7xlMEh5eNcDL9+pBCLHDj2U8BwODraBiO1TtiOfQaGqegzEOBjseQxWb5RtHOXJ+76Qcz3gOBgZbQbXuxDjeQJBOGeNgMClYvHoQgCP3NsbBwGCnQMV2iE7/jaaW52CMg8Ek4OGVA6QEHL5Xz6Qcry1aSQjRI4T4jRDiKSHEk0KI44QQfUKIW4QQy73/e719hRDiEiHECiHEEiHEkdr3nOftv1wIcZ62/SghxGPeZy4Rk0GoGRi0Adtp0mi6MVpJz3MwnoPBZODhVQMcMKeLztzkrOnb1Rz+D7jRdd2DgMOBJ4FPAbe6rrs/cKv3O8Drgf29fxcAlwIIIfqAi4FXAMcAFyuD4u1zvva507ftsgwMtg8qnlFoNN1Q9zcVyio9BxOtZLBj0Wy6LF49OGmUErRhHIQQM4ATgZ8AuK5bd113EHgzcKW325XAW7yf3wxc5UrcC/QIIeYCrwNucV13i+u6A8AtwOne37pd173XlcTuVdp3GRhMKar1wGPQvQfbccmmU8ZzMJgUrOgfZaTamDQxGtrzHPYB+oHLhRCPCCF+LIToAOa4rqsyg9YDc7yf5wGrtc+v8baNtX1NwvYYhBAXCCEeFEI82N/f38apGxhsGyq2bhxaeQ7GOBjsWDy8cgCAI+dPjt4A7RkHCzgSuNR13ZcDJQIKCQBvxb/D3xDXdS9zXXeh67oLZ82ataMPZ2AQMQ7Bz3WnScaSGdKOyXMw2MF4eNUAvcUM+8zsmLRjtmMc1gBrXNe9z/v9N0hjscGjhPD+3+j9fS2wl/b5Pb1tY23fM2G7gcGUo9KSVpKCtJU2noPBjsfDqwZ5+fzeSUl+U9iqcXBddz2wWghxoLfpVOAJ4DpARRydB/zR+/k64FwvaulYYMijn24CThNC9HpC9GnATd7fhoUQx3pRSudq32VgMKVoSSs1jOZgMDkYKtus2Dg6qZQStJ/n8G/AL4QQWeBZ4D1Iw3KNEOJfgJXA2719rwfeAKwAyt6+uK67RQjxBeABb7/Pu667xfv5QuAKoADc4P0zMJhy6N6CXl/JdppYaYGVEtgmWslgB+KR1UpvmDwxGto0Dq7rLgYWJvzp1IR9XeCiFt/zU+CnCdsfBA5t51wMDCYTlXpT+zmiOXieg+vKUMNUyqTnTCVsp8l7r3iAj7zmAI6axJDPHY2HVw1OavKbgqmtZGAwBloJ0rbTJOtFKwFGd9gJsKVU5+/LN3Hn8k1TfSrbFUvXDnHAnC46Jin5TcEYBwODMRAyDg09lNX1+zkARnfYCaCM94aR6hSfyfbFcMVmt87spB/XGAcDgzFQqTf8n9Xk4zRdHK22EmCypHcCqICBjcO1KT6T7YuK7VDIpCf9uMY4GBiMAV1zUMZBldHIWLK2EhjPYWeAej4bp5nnULEdcsY4GBjsXEjSHJRxyKZlD2kwmsPOAPWsppvnUK0bz8HAYKdD1XboyKa9n6VRsL2MaEkrGc1hZ4Ey3v2jtV3meawfqm61H3S10TTGwcBgZ0Ol7tCZt8ikRZxWMtFKOxWU8XaaLptLO7/3sHGkyvFfu43bl20cc79K3aGQNcbBwGCnghID81bapy3qDWUcNM3B1FeactS0JMVdgVraPFrHabo8219quY/rulRsh7w1+VO1MQ4GBmOgYjvkM2ny2bRGK3mag6VrDtsvWqkZ6R1h0B70PJRdQZSueYuM/pHWhkztkzeeg4HBzoWqLV36fCZFzaeVAs1hR0Qr/eK+lZz89UXb7fteLNBrX+0KnoMaT2MZB2XwjOZgYLCToVKP00o7WnN4fnOZtYMVGsZ7GBd0z2HDrmAclOcw2vpc1ZjLG+NgsKthqGJz0dUPM1CqT/Wp7BD4mkMm7U8+dUfXHLZ/tJI6jp6Rvb2xekuZr9/01FYjZXYlqIl0RiEzbWglVc/LeA4GuxyWrh3iL0vWsWTt0FSfyg5BxXbIZ9MUMprm0NDyHHaA56AmOb3Q3/bGLU9s4Hu3PzNpK2zXdfnNQ2tCq/vtjaote2zMnZHfRTwHeS82jeE5qDFnPAeDXQ5qAtuRL/1UQiUg5TIpv2S3rzlYuuaw/Vb5vuewA+9pZRKOoWPZhhH+49pHWbSVsM1tQdV2yFtp5nTndw3PwZv4N5fqLSnEgFYy0UoGOykWrx7kf258Kra97A3e2g6kQKYSOq2kDGGS5mBvx1DWyTC4SgytTJJxKNVkjaryDvSGag3p5c3uyu0agrT3zriurCibBCNIG+z0uGHpOr6/6JkYt171XvbaNPUcKl60UiGT9l/msOaw/aOVKpMwcU/GMULH82pU7chFRNVuks+kmNOd3yWypEN5GS10B984mFBWg50VygWOTiaVaew5NJuuN+HIUNbtXVvpKzc8yaWLnoltr6h7vQNX2T6ttAOPkXS8HbmIULTS7O4cTtNtuRrfWaC/M60ilirGczDY2VFtIZJONnc9mVAvr08rJYSyBtFK4zeOdyzr584V/bHtasLekdFK1RbGfkfBzy7fgeG5KmFxdlcegA3DO7fuUNPyMja18BzU+2YEaYOdFi2NQ336eg7Bqi0VCmW1G4Eg7UcrTUBzqNhOIgc/GdFKk00rVX3PYUcaPMejlXLAzp8lrdNKrTwHtUAwxsFgp0WryaQ6jWmlisb35r1QVtd1t5vmULWdRAMwGdFKtUkwQDomMk6+d/sK3nP5/eM4hqQAZ3dLz2FnF6VrjSYd2TSdOatlroPyIo3mYLDTQtEQZa0zGkwOlzxV0F16FUpYazTDmsM25DlU6mN7DuMxDq7rjiuhbVvpwKrtjOuzgYfZ/meeWDfMY+PIn6l6tNKsTuk57Oy5DrWGd75duZbGwQ9lNYX3DHZWVFt4DuXpTCtp2al5S/V0cMKhrOmJZ0hXG81E49DqXo+FU76xiKvuWdn2/uraJkorfezaR/ng1Y+0f7wJeA6tjGcr1BrSc8haKfo6sjs/rWQ3yVkpZnXmWibCVWyHTFr442wyYU36EQ12SSjus5UgPZ4V4a4CnVYqaA1/ws1+JuY5NJsu9UYz1KMapNitvr/didtpujy/ucxzm1qXfo7CF6TrEzPqj68dGhcPXpmA5lCuN6jYDq7rIoTY6v5VrbT17K7cLuA5NMll0szsyrJs/UjiPsobmgoYz8GgLSjuM7qSC/IcpqHnoIURKlqpajvJ/RzGGa2ksq3V5OdvD7Ulbe87JyJgT8Q7UXCaLmsGKuP6rBof41lEVOwmrtv+fdAn0tm7QJZ0reH4nkNLzcGemhahYIyDQZvQJzMdPnc9HT0HTQxUtFLFo5UyaYEQYsKeg5rwmm6YaknqWb01lL3s4/FM1tsieq8bqtBouuOifCYS1aa8qqjO1XJ/L1oJYM4ukCVda3i0UleO4Woj8VlU6sZzMNjJsbU8h+noOVRDnkNYc8h4HPBEo5X0iVy/p1WN5mnXEyhPQD/YlnDZVVvK4/7shDQHO9lbTYLrBgmLwC6RJS01BylIQ3IBvqo9Nf2jwRgHgzbRKmnqRZHnkNWNg9QElHGwvCS48eY56KvEsm4oWvw8FtTkOa7ooW2glVZ7xqFcb7QdITURbarSgspMQi2SD7ArZEnXGg65TEozDvFzVVWBpwLGOGwDNgxXp1U9/LHQahVXncBLv6sgFK2kNIeGQ133HNIT8xyqIc8hoE0mRCt5n293Ja9W2fKzEzEOFSBOiY2FiSTBBcZh67RSLVLaelfIkla00kwv9DZJd5iq/tFgjMOE0T9S44Sv3cZtT+24EsQ7C1RkDcQnrCBefvp6DnmdVqo72I0mWc8oTFxz0DyHerJBaN84tL/ChvCEPhHNQdFK0L5BGm8SnOu6vkfVzjGU5qWM+Ozu1hPujsLn/vQ4F139cNv7S+MQ0EpJ56ra1E4FTCjrBNE/UsN2XF4Y2nlXJtsL+gsdXcUFeQ7Tz3Oo2g5CQM5KBcah4WkOVlhzGG9LT92Y6pOfMkhWSoyDVmr459vescdPXenQjUPZduht4zPjpZVqDRmpBO0ZPXVNKnBgTvfkew4PrxpkpGq3vb+KVtqtYyvGwWgOuxZKvivfXiRFFOPNaJ1KhCmQZuLfpoPmcP9zWxjWXm7VP1oI4b+gUc0hLSbmOegGQdccVGhwTzHbtjc2XkG6lRjeLlZvKdOZs7zPtxlJ5B2n3uY4aXV/WiHaMW0qsqQ3Dlcp18YX3pvLpMhaKXqLGfpH44asYozDrodRL3ywNI7BoOOGpes56ot/3SVW3GGRNJgM9IStXb0qa6XucM6P7uUX964KtmkvpqIrKvWw5pBKCVJiAppDI3mCVve6t5hJnLibCccZt3FoQWO1g1KtweZSnQPmdIaOvTVU/TyHNg1eC02mFaId07JWiu68xZbS5BgHp+mycaTmNzVqB4pWApjZmWPTSIIgXZeJclMBYxw8bBqt8cJgpe39R6vjjy3X8dymEltKdYYrE/M8JhPVFitNffuu7jmMVG2cpsu6oWAMqBLQQIxWUpoDyIilbfIckoxDRzY2cT/bP8qBn7mBFRvD2bTjFaTVRN2dt8Y9flcPSErpwN27Y+c+FsYbytrq/rSCTytpE2kxO/7rmyg2l2TYbGkcEVyKVgJkfaWEUNaa8RymHl/48xPjEpOCtocTm9z92jbjceu3lYba8PiEvkOnN5ImsmI2vV3yHIarNjcuXb/N3zMRKE9Q5311MVC9xJJWCjwHkLrD+DOkdc1Bi1aqB55D1Dg8v7mE7bg8t6kc2q7Xt0ryLKLQDdB4J89Vmz3j4HkO7Y5fP+R5At7NxI1D2m+ctKOhEu7ajeByXdePVgJaFt+TnQhNtNKUon+kNq6YaDWZjIdj1DHu8gVNB37wKrj1CxM6Hs/9HS59JTx27bg/qiiQlIhE03j6Q28xS63hbLOGct3iF/jXnz/UsgjZjoSagPQXVGkOlLcgKgPkMylqtoPdcEPGwUqJ9jyHp2+G7x4DN/8XNc0gJEUr9SVM3KPeWItSF0m01Fio+dRVdtyagxKjx+M5uK7rj6G2PYdQNNfWF2CB5hA8F73v947Gei0wZbQNasl2XFwXnzJSJTT0d8h2mjSari+yTzaMcfBQqjXGFfOttIZxxYkv/iX86SNA63IULbHsetjwGDx2DbguA6U6g+VxJPgs+ZX8/4Efj7nbHxev5a9PbAht00XSJM+hp5ih6eLrDxOFeqkUZTeZ8D0HzTBVbIeZ6TL84AT42VvIWykqtqc5aLHn6bQYW3MYWgu/fhdc/TYo9cPd3+GlzwTPIZr4lk4JuvKZuHHw7stIZPIptciTaIWKZoCi3sZQxeaEr93GI6sGEj+7ekuZrpzFvJ4C4E3cI+th1X0tj6cij/IZSb+1E9lVbmE8Wx8j7jkUsulJ08I2aHWcyjUHnroe/vRhaCQvdNT5Ks9hZleOiu1QSni/piqUtS3jIIR4XgjxmBBisRDiQW9bnxDiFiHEcu//Xm+7EEJcIoRYIYRYIoQ4Uvue87z9lwshztO2H+V9/wrvs1svwbidMVprjGuVUarZnJB6jGYtuZpiDBuekIPlocth45N+1E/bx7z3Uvn/4CroX8bHf/MoH//NkvY+a1fhiesgNwNW3yfppRa4dNEzXH73c6FtypD1FjOhF1W9wD3FDLDt4azbUgxuW1GqNeikTL/2klfsJheMfh+G18K6R1mYfsYvn9Gu5tCoDDNyyStpPn0TvPoz8NEn4WVv57iV3+ccaxE5KxVe+debXonwlN9cSD9H/X+FkMFuYzzpRh3C4vjagQprBio8vSF5XK/aUmavvmJQpbZWhZ+fBT89De7+buJn1HPtKWSB9lqFVsZ5TeoYHaVV0stGJi9O1ljSo6Kqw/3wxwvhoSvkYjDBo1YelE8redFVfrtQx6ZalfrXrlBb6RTXdY9wXXeh9/ungFtd190fuNX7HeD1wP7evwuAS0EaE+Bi4BXAMcDFyqB4+5yvfe70CV/RRGBXeVP5D7zXuRa3OtzWR45d+QN+nv0KF6+7ENYvDf3t0kXP8JM7nwtWZI0a/O4CyHWCSMOSa/wXsq2VzQuLYeVdcOyF8vflN7FhuNZ+gs/ym6E2DG/8FqRz8ODlLXct1+pUqmGPRLnsfRGRtGI77Cn6Oa90OXuwaZtF6Yn0Nf7V/au4a8WmbTouQHHVbTyY+wCXuV+gtOUFAI4dvY3jyrfD8R+BXDdv56aw5mBXYMPjWCmB08JrGnn+YbqcQf568BfhxP+ATB7e/D1WdL+CL6Z/xJsy94cmd7te4Sup73P+Q2dygFgduqej1ToHiVVUS6Phg5QHOTN1N/uJtW2Np6rd5KTUo/zjpu8yi4HQ5Gv3r+DL1o/Yb8XlMBDvD7F6oML8viJFzzgcsOJy2LAU5h0FN/8/uOW/Y5NhxXYQNDkqu4osdlv6VMV2eFt6EV/P/JC+4SeDP5Q207z5Yka//2pYdkNwTbU6n7B+xe5XHAc3fwaYXFppg0Yr9dz3P1AdhsPfCY9eDXf9b2z/wDh4tJJKhButwcDz8NPTyS/6IsCUCdLbkgT3ZuBk7+crgUXAJ73tV7lyyXOvEKJHCDHX2/cW13W3AAghbgFOF0IsArpd173X234V8BYgePLbEcvWj9BdsJg7oyAH8ZPXwc2f4SPOSrDA/d6d8PqvwcFnQisH5r4f8uqNV3KTs5CF4ln48alwxjdhv1Nh4Dk23Xk9T5S7uWf5iXzjHUfSc+cXJSV0zq/hgR/BY9dS65H2r62J8N5LIdsJJ38Knr0Dlt9CuX54iPceE49dCx2z4ZC3SEOx5Nfw2s9BtiO83/Jb+G3lfDLVJvztQ3D0+6DQQ2bLMv7d+g1Hlqp8t/5K4LUAdD17Pddn/5PugTIvz91Ac9W+cMir2junBPiVQsfxQn/nthUcMb+H418yM7R97WCFFwYrHL2gb+tfsuwGjrn333jencnC1NNYPz4JTv8SH6xcyvP5Q1jw6s9AvcTJD1zB9dUt2E6aTErA786Hp2+iy7q8pefgrnsUgOfyhwQbrSw/n/8Fzlp6Ed9wv8VdK5+F+qXg1DjvmY/yEncxNaeLq7Nforb2WPILjoDyFs54/N/599zd2A/kYOhU2OdEWHUPX3v+ejJZm7qbZuCelXDGf0GmAM0m9D8F9RLMPlguTkbWs/CBj3F29kbYCDflbqD5ZA4Wng2Lf8FL//JxDkrXyS27HZZ9C/Z4uRzb846i2XRZvaXMKQfOopBJs59Yy1HP/wgOeTOcfTlc/3G46/8Y2biSrtM+DbMOBMBZ8wi/y36Wl4+u4Nns7rgrsnD4G8Z8JLOfv46vZy6jiSC18g648hew+8vgoSsQ9RLDbi+dv/xHOOKf4aSP8+qHLmIv616aPQtI3fcDOOKdFLKa57D+MUhn/XPaZrguPH0T3Po56NmbkfL5CAEH8zyzll0NR58v5xGnBn/9LOz2Ejj4Tf7Hle6TywSCNED6yT/A4osBwcgh7wOmznNo1zi4wM1CCBf4oeu6lwFzXNdd5/19PTDH+3kesFr77Bpv21jb1yRs3/5oOvzs+5/njHll5s4YhP5lsHk57uxDeFf905TcHL+ZfS3pa86FWQeBSEF9VK729zsFDjoDSpvghk/ycPF4Ltzyrxze1eR3c34Cf7zIP8xnALKw8fnvcdc3j+QNzu2Io94DB54O1SH4/QXMzy4B9pArG9eFVffClmdgaA2UN8OhZ8P8V0g+d+lvYeF7IT8DDjgN7v4OKWuYWlaKgjx7B6xfAsd9MG7QqkNyEC98D6Qt+T1Lfi2/88hz5T61Ubj5v+Chyxlw92RzahbH3fYFuOv/oHser+1/klPTAruU59fp63F/8idE7z68bMmvWOzuy4qD/42jn/gKe/32LGh8Dw57W/gcaqOweTn07A3F1pO1bxzKJVjxKKy+H5w6uE35LBYcD/ucBOmM/5mKrfVhLm2GjY/D4GqW3fcQKzYMc/TbzoS5h8ljJxn7J/8E176bzZ0H8tb+DzNXbOEPPT/C+t35pMlz7d7/zce9+5Z94EccM3gjDzRez9GlRbD2TwAcmlmB00wesun1S+h3Z7DRDecRDzdzfDj/RT7gXM3bBv4AP1wKIsXe5ef4auGjvOzoUzjqjnPp/fVb4bQvwO1fZt+RjXzLPpuj57i8av39Uocq7sYtxTdw1eBhvC29iLMWfxdW/kVe8/N3yrEEgIC+faG0iT3tCt+0z+bgk/+ReX//BIf/5X3wwLdg4xMMzDyGM9e8i389YS/O610C93wffv8B+MBd9Jccao0m8/uKpHD5WvbH2Kkc1uu/Dqk0nPFNFg/mednT34flf4C5h8PMA5j32G/IiS5+3/teDt98PX2/PweefCO85rMwc//4TVvxV1752Ge4t3kw357xn7wldQfnbLoenvsbvPQtXNfzz/zHrSX+9or7mfvo92Hxz5krMnzSPp8vn/9p+N5C+MtHKXZ/TY6N5/4Gv3ibpJtO+TQc/2F5vlGUNsO6R8DKy4VTOge1EagMyPfIykK2CwRw93fg2UVyXK34K//FYoa7PsmHqldSz/aQO+XTcry9+XvSA7vmXNjjSNj3ZFhwPG6tm1kMUqQC6x5lz+cf4ZLMrzjyvntg3kI4+ydsGu0B7pqyaKV2jcMJruuuFULMBm4RQjyl/9F1XdczHDsUQogLkFQV8+fPn8AXpPhU6ufk19fB3lda8+MupHLoO7nzs7cCsPGcG5n79NVyhZ0pyBV7dRge/RU8+FP5PXufwLfrn8DZMsoauwjv+gMsvhoaVdzefXjN5c/zoZfWOKl+B69ZeRsvpOcx73Vfkp896AzIFHnFyK38lHfJlc3fvg63fyk4TysP918GB70RCj3QbMAr3i//tv9pcOcshzrsAAAgAElEQVS3Odx+hLvECVAZhN+8R04CjSqc+PHwNT/5J7l6ednb5e97vQJmHSyv5dCz4OGfyYE+vBbn2A/ypkULKRY7eOT9c6RxGNnAPQd8kg8tmc8/nnAIw/dcwcUjdyBW38ey/d7D2x4/hU/s8TLesrjAnfMup+N374MbPg7F3aDQB6PrpU4CcuV3/u2hyd1HdYjD+6/j9MzNvOoPT0HTo8xSGWkYmg2481vyew95s7yGbAcn1p/mVZs2wmWfgBceQa5j4NXASa6Aa7zorEIv7HkM7vxj6c/OZ/bQYnhmkfTo9jyaa+f9D8N3rGPY7eTvJ1/Daf1X8Mm7Cszs9MbZnEN4KvtSTh79Cz8Tx/AP6/8X5rwMNjzGoc2neaR5YuKQy/Qv5eHmgliWb9V2SGULXJW5gOfSx/OJ6v9BvcQ35nyFe5yDObhvH95R/wy3dn1dLjx69+Ebe32HHzzdzev7dudV//QTST/M2JPvfvdeNhSqfKx0MPu+5nxe/vhXYe3DcqwseJUcQ+uXymsVaX5ZeBffuavGpbsfyln1z3LXcQ8x54nL4TWf4+b0maxb8yQbM3Ph+FPkO/Krd8KDl7NqztlyCPUV4YEfs1As4zd7fJqzu7x1oRD8ddZ5nL/0pdz6us10L/8dLP0dGw8+j9c+cjynzzuQT647ibtOepJZD18CT/0ZDjhdLmpmHwyjG2HTMvjDRWwu7sv5mz/GoV1zuKZxFudc8FU5QXfOYs3tK7BZxlOHfJi5R78V7v4OV/NGfv9oJ1/r6IPXfh7+eBHH7X0zL9SLcPVXoHcBzDxArvSX3wKv/i95zo0KbFohz2XlXXIh0g7yPXD61+Ri64WHyfz0HVxR/w+sVIMHD/o8Cws93gAowD9dC/f9QBqTO78Nf/8G+wEP5IE/yN26gNekctw/7zyOee83IZ2hMiAN+07tObiuu9b7f6MQ4vdIzWCDEGKu67rrPNpIVaBbC+ylfXxPb9taAhpKbV/kbd8zYf+k87gMuAxg4cKF4zdGQvDu4nfZY8+9ueSdC/3NJY27LzeEnIjVZKxgV+QKfd1iOPYDDPxIisGVuiNXIUe+C5CUyDPNG1mzx4H0nHIRH77y7zy3ucR1isLJdcJBZ/DKpTeQ4Rx6Nt4Hj3xFTtSv/gx07yEnwnu+Jyfn+igc+AbYbT/v7hwD+R6OKz/ErfZxcMfXoLxFrkhu+yL0LAiv3B+7Fnr3gXlH+veAhe+VE/g3D4baEMw/Ds76EcMzF1JbdAtu3ZGrvrOlMXz49hX0L1lGV/cMvuO8jg//y9foS1e4f8kI9uNLmVHMMEA3y0/7GUes/y1seRbKm+R57Xk0vPxcudq67Ytw9yXwqo8F57fxSVj0VVh2A+c4NZ4Xc3hm77dxwCvfAnsfD9mid/+rsOKv0uNZ/Ev5UgP/mwZnNAW9R8tV4Z5HQ898LvzzBm59cgNPXjSf1IZH5WS5+j7E8puYDbipDGL+sXDqxXDM+QzcEji166sW7mu/wPWLbuD92ot5R9ebeP/mr/JdPktOlOGsH8M17+Klg0/zYBKtZFcpDC7ncfeNfhiqgqrTX8imeYjD4IMPQKPG4p8tI5+SE8JKd3eefeO1HNB/Mxz9Pp765TKgX0ZWCQF9+8gxaDv0dWTZXKqzvu8YuPCe+LkcdIb/47obn8JKPUtXPkMDi5WHfYQ5Z34OhGDkjmcArXrqgW+Q3tqiL7PuxFfKrxq+C278FPekjuTujtM4WzvMaK1BPz1sOORMuk/5EDQdVjw7wPAj99FTzFInw/qXXcisV71PRs498CO48o3hc+1dwK/3+TbV+4bpylsMbKnLVXvnLAC/xMlwxYYDF8Lbr+TZ6x4nl/EIiMPfCQ//jNPWfo9T3AZ0zoZz/widc+Qi7/qPwxURWmvWQXDCR2Hfk6Qnb5flO5/vlguL3AzpxdZGwC5Juq0gvcHaHkdzRvVLXLPbZbwwbLN89ptYqH93sU+OzVM+LReaLzzC8uee58rbHuH8o2ey90sOgd0P44TvLeeMWXtxjLd40gs/TgW2ahyEEB1AynXdEe/n04DPA9cB5wFf9f7/o/eR64APCiF+hRSfhzwDchPwZU2EPg34T9d1twghhoUQxwL3AecC39l+lxhGrTCHkVp4daBHf7QUsDIFSQsdeLr3GS+U1Q73uFUhkar2TCrfxUAjUozrsHfQ9di1/EP675y89A/S3X/T/0Guy9shByd9Ao56Dzx8Bbz0H4LPpi2a+53KiUtvYU97pfQwjjoPXv8/8LO3yiiJQi907U55/dPkn/0b4sT/CPfgPfwdcP8PYeaB0sWe/wp57l4Me70RTvSqeQXoZhTkoC03XPq6+qjWBwEZLw9QaVpw3IUt7z3rlsAiT8+Zub8U2q96s/zbkefyuVWHcfnKPj5/wKEcsP+CyP3Pw8FvlP8cG6rDVEtDvPnbN9Exaz6/+5czQruPNDZRI0ttzhEU9jpKGkTg+nuX8uM/3sInzjuLYw8KvM9SvcHMziwDZdsvqug03ZAYuLTnZAa3fJ/93DXcuseFnDr7INjzaA7a/Ofk8MyNTyBch6XNfbATchPymRTFbFrm12Q7INtB1XboKWb9CWGkMM83pmqcRuPoS7UG+87qYPnG9jQs3TCB9xlvfKjCcX4EkxBw+lfgByew15JLODa1P3Nu+gbMPYyvjHyCPSLispq4/XDbVCAKq/FTazhyoj/lP+GEj8Djv5eTZucsqY3tcQSbblpFIVOimE3HQllHvJDeYS3kudbQOqalUnDGN8n/4EQG6KHzXX8k3bW7/NsR58iF1LpH5ZiyCtJ4eIZ2IugfqbGRXu44/iouvm4pnxhLcM93w74nsa5xCD93ZvLWlx/H3ntLqnVGxxoGtPB0pb3tzIL0HOD33uRiAVe7rnujEOIB4BohxL8AKwGPt+B64A3ACqAMvAfAMwJfAB7w9vu8EqeBC4ErgAJSiN4hYjRAV96KvVz67+1GyqjPOE2XuhPUSFEvcIdnHArZhIiJfU9hgG6+bP0Y187A2b/TDIOGzllxmgio73Mqsx7/LZeIb+BmiohXfwasHLzj5/CT0+AXZwFQBGw3zao93sh++hfkZ7DhvLvYrSOLpYnaerx8ueYwo5jy70neSlPIymtS2kDZz39oM5T1Dd+A5+6A6z4Ep30Rfv5WuSJ795+gdwGP//AeYMvWI0zSGejYjTJdLHPns6ddiO2i1xvS48SHUt087B7AYCMb2n+05tCVz5ASgv6RWmKMuZUtcKV1NofUHuPhef/EqQB7LqRn8S/oq78QP09PjH7c3Zt5kUSuasOhM2dRyKRjuSNzM+lQoT8FNSkmJcHt5oVCtpvnkNOOod9vlUsRKvo356Vw1Ls57MEr+Uk2g+hdAP/0W8TlT8boMj8Xoxp/p4Jxon13pgBHvDN2juV6g0JWjrloEpxvHCrBokt2gdO4+d0P5bqjfsyX7ypze8c8QqEX3XPlvzbRcJp85o9Lef+J+7FgZkfs76ry6/zdiqSEaCsxNhqtBNJ4DunX1Jha47BVpcN13Wdd1z3c+/dS13W/5G3f7Lruqa7r7u+67mvURO9KXOS67n6u677Mdd0Hte/6qeu6L/H+Xa5tf9B13UO9z3zQ3YHlSjtzVmjgwtieQ73RZNn6eMx3qdYg48W664Mh8BzkAy0khdOlLW4Ux5MWLjfu+SEpHo4DI3udRNMV7JNaj3PSf0KHF6lT7IPz/iQn4bddyVWH/pSja99nIL9X6POlWoOTv76IPyx+Ibbd/1l7IdWLV/QGqT7xZtMpP6xxq6GsXXPgtC/BqrtlXHx+Brz7z5IPJojgGG+9niRj0qoYndo3Wlq5VGvQkUv7ZQyqCcYhn0nzM/cMLrD/nYzl6SbzJIGwoPokMax7lLrVxWp3dqxAo+oNHF08KGOWNHGrZ6InCaq+B7t1ZBPvxebRWih7F1RZkFSQq6DdI7Xijz3LU/4fVZGnlJ4B7/o9dOzmnXvyxK2fo/p+5Tm0U5m1YjcpZi06Ej0HjVZS+9edWCbxyMwj2UjvNuc6rB2s8Mv7V/O35f2Jf1c5Drt35+nIWqF3pxWiSXAg84h0z0HlQu3USXDTCZ35BOMwRjbmnx59gTdc8vdQaY2m11xdJa7oq6eo51DMpn3qSce3Gm/n/fV/584ZZ477GipWHw+4B/JUcy/Kh787/MfuuXDM+fDSt3BffR8G6Yp5SoMVm4rtsGYgXKNH58XL9fDLnddpCK0tZT6T8lc/beVsvPyfpVDasze8+3ro3Tu4Ljv43nYwVhvJSouS6uoYw5ExMFpr0JG1/AJolQSXPp9JUao1cF2CMOLZh1Alxz5JxmH9EjZ3HQiI2IRRa8h+x5I20RcnTa+5UOC1KSgDoz/PWqOJ03T9ngDRe/e5P8VrhqkeAb4B0j4TeA6Re9oxk3dnv8n/7nuZ1MWQhe1iE3fN9s5RX9Un0EpbQaXeoODdn0rk/QlopfAqO8rN5xMM7ESg7nt03lBQxndOd55iLt1WZVal6eieQ28xy0BJM3iRHhWTjRedcejOZxJoJX3lFv7bxhFZbVFPOFMvuopN1icg9TelOeQzadxIMS7XddnUyHFT82gqE0gcK9sNzq9/jLfX/5tqs/UjVL1+o6vWJNdf7qddh/aZaiPMUesdumR/ZU+baOdahIBzfgUX3Q89YY9mvElwekZ1tOCc7zlE+k+M5Tl05iy/xo1PK2XCnoPa7pfPSFs8mz2A/eoR4+A0YMPjbOg4SJ5PTJB2KGRSscqhqgpnfoyJe7QWVP5U19NdsLBSIjZZrx+uxjwHVW02yTtRYyLpWT7nzEQUd/N/T6JMx6SVvAzpdsaJ70FlLVw3Sq/JZxeiYBKql4Y0lW2AMt6tJv0NI1WyadmToSNnhUpgtIJPK2lU2IxiJnZNAHlTeG9y0JmzQi8XRGml8MBVA0OvY6QmTmUcSiFayfGPA/iUi/4S6V2uJrKqKdcdhulgmI4xs01VkbToqlUZx+gEORoyDmGqTeeoqxplU8ik/dVP2xnSqbTMuYigqhmddhDqtxxZjVZa0UrKc6jEDWNHTnoOm0Zr/nPXm7vrk4+egPhM9iAW2M+E6+hsehoaVdYVZBx/9BnoE3TVDuobqSqcamJTVFvN613dlbNoapOlMtQdWSuxXMRotRFaYUPgCapJJ2SAaq27ykl6MbgHyZRPgnHw3infc2gjQ7pcdyhm0/77o3tXgeYQpj71iRaS372JQE32rQrqbRyuMbs7hxCCjqxFuR3PIZFWyjJaa4Ra8qYEZNtNdt3OePEZh7yF03Qj7rpOKyVPpIOaRVfblHHQX5CYIJ2wAtRfjomsatpp1jJUsRks26Fzip7jmJ5DyJgFkTUQ5vPzmbT/UrZbjrkV1L1op/MXtK6/E+o/HPmucgvPYbTm+JqD7bisH5ITfZRWUtBrKz2bP4QMDRmNpeCJ0atynnGILEjUSreorW71KpzRcaMWIHNmyPaXir5RE1EhmyafUGhupGYzWmuEPKuKF62UTadilXZHW2gOruvGupIVk8TihLHla1M5tYhoh1YKU5nlBO8mRCvZY9BK2+o5tIgSU1g/VPXbkhaz6bYagCUJ0r2eYD9YkQtRdQ+moNQc8GI0Dt6krQtm+kOPvlyKDhgqBwOx5BsHOSB0KiopWgmS+yAkHa8dhEs8J6/CVmt9fidiHGKaQ8KEVbXl6k6tfqITyvt/9iDXP7aOdqGMZrvlM8JlnYOf607Tr5IaXTWq+x1dTZc0zQECrytKKynonsPK/MHyhzUP+NtYvwSsAqtTMnNar/Pvuq630g1PfnqEVMCXN/3zA5jTHfZW1XUXPRE7er0jVamR6JVcq14YrWp/GqaV5H2JGnoV2qsbyFB5CjzvxrvGqOaQy6RajpMkVOyw56CO4zRd/33VKRil4ehIWpiNheUbRvjXnz0UM16+59BCc9gwUmV3zzh05toUpL2xnrV0WknSbmquqTamrtEPvAiNQ1deTtojEQoln0mRScc521FFK1UCWkkNztmJtJJnHLxBXYyEf0Z/npDnEHkhk6A3gY8mYLWmleLipzzfpsf/ht30sqc5ZNMphAhPKM2my02Pb+CB57fQDlRIcPT6FFZsHImVxW51H8fyrJTR0w2j8iQ7PM0BNOOgRytZycZhJDuLjWImrPUD86TnsPuhlLV5ohRZleczqRDvr4xiPpMmnRJk0ymfLhv1jYOchNREpSaiYtbyxVsF13X9/YYjIZLquNEJvpXmUE0oiV3MpLEdF9t7bvrkORKJVhov/RinleLUznCEn89b4enMj8Zqc7Fxz7ObufHx9bwwGNZofM2hxaSvaCWAYi4u0ieh1nDIpAXpVOAVKM9hwDMOKjhhqvCiNQ76QC7VZcx5PhPnUNULPVhuTSuFwg09Q6PyB9RLGFrtey9aOiUmxIdWImGmSVCTW85KxTwHdf7RiJ1QeG6kR4A+kQVir3zphRDkrFTopVcvUrue0VgGc8NwldO+/TduifSZaNUtrJWXJn+X56h7DnoQgXqmq5M8B81Q6P0crJTgyfSBgefQbEqKaffDImMj8LjUd6vFQ9luxETwfCYo562emVqhqt8rmucQHb9Vu+kXBNSvV9EV8hiBcag3mv4zjD433XApRL3ikRbeuBKXM2kRW0S0QlXRShnv/tTDC5pZXTmGq5own0ArjddzaJVH4keJJXgOo7UGo7WGb7Q7sukY/WQ7TZ7bVApfn90MUUoQJJMqfVPvRDgVeNEZh86ctM76QFZiZDGBs1W0kq45lCKegz6RjtYcn7qC5IgJvRXkRAVphVaT76otZfo6sszqyo1BK8WpFbVyjlJteSuNlU6RTadCtJJ6IXNWOpG7br+vcfLKH2QGatMl1GsBorRScjhyjFZKmMh0KjBKK+kTjr4yDfVzSKd4InWArCF1ycvhuwuhPgJzDw9rWyq81g4mWl00jSbe5TPBPfWNw4ywcYjSSuGcheD5hsXb4LnpnylFnnnovnlGNao5qHOH8D0NC9KtFxFJUJpRyHOIhJPO6yl4PZv1sRjxHMZpHFploPuGKUFzUAlwiu7ryMUF6d8/spbXfftvERrMCYnREAj2aiFaSbimycSLzjj4nkMtPNmraI8YreR7DvXQ/qAJ0hFxu0M3Dj5tEF/t9xbH38MXkr2QKFZ7TVlUdFb4mhTvHvcougsZ8plUbAWaC61mg0lOX+WGPIdaeCLcGqLd0HT4TW4iz6YVlTRW28yyHadZAuOQ9jzIFGsHZe2maBKcQrRN6M2pV8kqt/OOkkmNh70DDnxDYnMkfaLVqbpoboXeyUytWtUKNdrDvJizWlJEEBVvm/5x9XBUtb/qEKcjqc5PNJJIGaOZndnQwqMSWURszTjopUs6cuHwaXWOe/bKrPjhiu1rOFF+Ph+hQbcG9Z608hySQlkD4xB4DuVIaPWagQp1p8mAliul949W6PUSGQd0z2EKaaVt6eewS0Kt6qMucGfOwiUpuiWJVpL77NaRIyXCMezK0ChEBTUIVmW9xSyrI4lo7SCsXyS/aCs3lzl8rx5eGKzEuFI1yCWN4Pju7ahn2DqyVuhFqGmDVI/L10tTRF96dX+TGrzbTpOm64bcanUdxYTYeXX+0RVZtYURiIrpOvSJUNXE0sOPhRDM6sqxeovXhcsKC7AKunFIpwQb6YUz4yXBKvVldOcthqsN/zgVn6IJR4CpqBR9VV+JrOoVrTQS9RwycUFap0GUMVTajtJPdFpJn9xXbBwN1QwLMsbj9yNKK+0+Ix/KC6pppS2k5zD2ZO0byazll2ypRGileco4VG36vEk114pWioynb9y0jGUbRvjRuaHyeEGb2mh5Em9BkUQrRY1DMeflZTQc37NSC0t9zqk1mrHz7fCoN8VSVOwwCzHZeNF6DmFaQYYxJk1M6oUejEQrpYR6ucMClDI0CknRSr5x6MiEYtzbxdZopYbTZO1ghfl9BTpyVkyQ1if+RHotF6/3o15uvRCaogsg/tL7/HrCqu3i6x7n/KseCm3TDWYsVl+t3MbwHEIr9DEoKvV7QwtnjkaYKWotm06Fak+1EqStlKDRTDbSFdthpvIwVQ6BinHXckfKthMr2ZHLpH3jGhWkS1HjkEt7NFTcQEPgJUYn+YJ+DG+f3TpyNF1CDYySsnWjCx/1+bkzCrFQVn+cZFJbzXPQtZdoyRbfc/B6WA+Vg85yUc0hk5ZBJtHxtPSFIZauHYodV08y1FHSxl/0XVWlM3TPIfodqrqC7k3V7DitJIRgRiHrGxNdG5oKvOiMgx/KGolWKvpF0JL5xqFInkOHt8qUrrwubjd8VxiIRfhAMPiVADXe9prlejCwkozDuqEqTtNl774OOhPS+XXudDhyXZ25dMhzaKjYe03ArHpuc63R1CayVGhiUrRdEu21ekuZVZujAp3cr68j2zLrNv5sZJIQtKaVojkTFdvBSqkKpOHJQI0NRRdG+d5QnoMVaA7plIhFUunnMtMzNj4/rtFHwfiIC9KFTCqmOczszJIS4XuiIpui0Ur6ZDSsrUbltWiaQ0T0VsYsKcJOF+WjkUTqeHvMyFOuO0E48RgeJsA3b14WCjbwqbJsPM8h7jk0tEiq+HSme0YKo9VGYikM5aHGe3Q3YvsobBiu0pmz/LGjFhg6m6AWlsNRz8GKn29vMeOX0Kg14lTZZOJFZxystIy60Y3DaK1BZ1ZxtsHAVTWUIK456BnQpUgIaLLmEKeCelSp6wSOfd1QhZWbSzzbPxpbrVTqDd+VTjIsSkzdq69IMUIRqe9XiHpQnZ4w70cbaWGX6npDMfnKaFjpkOcQpVDC5+8k9jeQ9yRDoxmER+rnGyteZzu+gU2KVhIi7rlUbMcPJFATZsxz8P4ejRQZS3No1Sa0Ygc1uEoRzyGvRSvpmoO611GxuJCRQQEdmo5UrjsUPbG3EKnTFFoEqFLcUeOgGZQR33OIj61q5HnLn8OUjzqnud6qXg9KyIc8zPCYvfLu5/nzkqAIpHp+BS+HJiWCcTTsaw6yz8dwxQ7uW0INoqhIr64zmhgoz9cJ/a8Qfb91bBiu+mGsEIj0uhFRGkK4JpYTi1YCuWDUk+CM5jDJiBbfU3SKbDkZXyV0efvXG02yVipEHW2NVlKubYjq8FfJ4aYeABuHq5zwP7eHKld+8S2H8s/HBgXqynWH7nyG9cPVxLBAZRzm75YsSJdqDX+ST7oPHVrl2uikUPBC9aKRNVG6YLQaXqnqKNedmBdQ0TwH9buagIOJMKI52A4zihk2l+qh56Z+jlJUzaYULmd353lhqOpPNLogDTCrMx+6ZoVWxiGdSuE4ycahXG+wW6e8plKCIO2vvm0nFv6c1zQE5a0CdOnGoeb4mceKVmo2XVIp4T/DzpzlRytFn2dYcwgHWiRpW8mCdGBcclbKf4YjVZsZhUxIWI3Sj64rk9r0wpb6OQohQu/YSFWGWyuDO1SxE3MwFBLrPyltod6gOx90JVRjdizPQXrEef/3DcM15nQFv6sxpBuRgSRaqdFM1BNmFDN+GHW03Pxk40XnOYB8udSDanrhcJ25dMwtVwNynuI3K6riZCNUdTWaId0Reej6Sw7B4Pc9B+1vawYr1BtN3nv8PnzzbYdjpYQfOaOgBk3eSvsrex0rN5fJpIUsIezRSnrphtFag7mqDIN3H1zXpVSXhq1DK4ugzjWn0RDhVW4yXTB2jR6HcoS/1TUHCK/4W3oOdScxykz93NcRNg5qElFhh2o1Ha2HFdBKUeMQvC4hzyGd7DkoY9RbzIYCF3QPIWfJBMJK3YlRN7ogLUOk5faOXOANypDPcKkW9RwU9bRHT96/VpVxnUgrVQPqCsLBDkmFCKPGYbjaoCtv0RWhbvXQ2WzEc6jYDk0XNo8GxkEPzwVC1O1I1aYrn/G1w+GqHRjbhAJ1SfWm1JiPVwgIa1D+dm0FH/3MptGaP14g8D7DnkP8eLWEPAcIl+2u2E6sXtRk4sVpHLSGP37hMi8UMOoFQBA2N+S5e61oJUVDRY1DVOiOdlYLxaZ7A+iMw3bnrKP2pLuQieUjVOqOl/SUSpx8V28ps1dvkXRK0BEp1CbP32HujELoeOolVYJ0NGErSkPEV6DhcxmLVlL3uBxamYY9h1C9qnqy56CEzlZGva+YTdQilHioJz2lRHAt7dBK2Ui0UpLmoIxRMZsOUUH6SlcIQdEzbtFQVlkrKSif0elNiJ3a+FWlreXnwoX0Rqo2xWyanmLWp9CiDWQK2ZRmgGyslBRFIZx9H4yDeLSS7t105TP+eaqIsJAg3WIRoXsOUV1ED4IY8QyQlU75HpF/bgmTbTQxUHkqEI8+ahWtVK41YmVLFAZKdT+zGfAjFcva+6M/D4VaI3ni7y1mGSzbNJsudaM5TD4681Yg6Gl8czQUUD1g5TkM+IXsHN991Cf+INM2HlIXjePPW/G6MRDw4F2eu9uV0H9ClRaIJp4prPJyHOS5xFcyo7WGn0wVrJ6D+xD2HLyVpjW25hB/6QNaKdrLIqkEcuA5xKm2gAuOGocm+Wxyw5x8RhZ6qyYYDWUchnVPMGv5YZtq5Rx9MTPplC9mZzRB2koJ7IRopSAkMx26p1Gvq6DdUyslfK8kb4XzHNTEo1OF4bFohe6dmki78xmfQgvOKaCwlMYzUpUGSBmAJM8hTCup7OVg8uvMWf7YHa02qDtNmq5GP1qpEBWq3sMtpXqsDHlR86CCngq27zWozmlR71ZHVHMo1+UiSH2XgmQQkgXpUt1hdpdKPgw+4zRdhqsNvyaSfs7qO/TmPbFQ1gRBekYxQ63R9D9njMMkQ+8Gp0eqFLPpkBiq/raHZxxU1EGYVrL8xCo1gKOeQyGiS/i1iiJhehAMoG7NOOgRRaBoJctbrScL0vM946AmFDVYFX2k4uUD3l1RK2mPtgh7DqGsXX2Vq7/0CaGsTVcmNenQV8PRbRbL9kkAACAASURBVEkifTRs0/9MXfVEiNJKDYoJJazVtShBWvcc9GemPIdiNnklCvE8B9elZU+JQiYtvTHv91pE5Fcd1aJVT8Or+oY/KXZGaCWVCxBN+lL6V3chGEP+RKrlOYC836NVuX9QA0m7d3Xp7eoTWjolyFopf/yPesZILUiGqzbVCI2Vy6RDepp6x+pO078/6pqV8ZF6oKKVGnTltHdDo5WSopWiXqW+wNAna9mQS/4c9Q7K9YYvOutitbqnIc8hshjTm/fEjUOyIA0y4lBekzEOk4oureGPPqHnI5O1egFV2JyKWFLcPHirvsjKNio0FSKUi/QcUjG3HILVjJoIuvOZBM+hQSGTCpVXUBgq2wxV7MA4RPjfcl2+BIobHomIcB1ea8a606TeaMaESNXZrmzHjUOrGHv95Ww4Tb/Anv4SRgVpXXMYrSav6Coe317IWiGKqlwPOp0l0U29xSyZtPC9pmj4sQo9TXox1QQUjVYCYrqDblg7tbIK0Tr9xYzlUXXNWP8Ix1us6AuSjlzY8+3QVtigR/ZIfl56DuEAAT1DGuT9HvFoIb95k/Y8qw2ZOBctH617ziOecenOB2Mu7mGGNQed2tni6Q66UVXH0Avv+e9GIcNwxfaNWOLziniVurcQLb6ZtL3eaGI7buA5aJ9XyWo9IeMQnkP0KMfhreQ5QGBoVIMm4zlMMjq1STGgU4KwQj98sB72HIa00Effc9A4TX2C1RGte++33fQevD7BD1dt0inhr1qTaCWpOVjkMnFBWo9UUtcqzy1yjjkr9N0hDyqnhVdG+NxiVvbDUKsmPeol7DnEKSOIeASRe2KlhP/iJ2k/0rCFE7PkfQxKegT3J+1NDHFqpJhN05XPaGMgnImaz6TpzluJL6Za7WUj0UpATHfQhVVdm4rW6Ve0UrRcgr6q13UunVZSlXEhXktITaTdhYwfuhnTkLTPjFRtulp4DvKcE1bmmXSIVtI1h9Gqpge0yJDWJ+LNpZp3rIZ/X9Q56gZI0VaKVorSdDpUMyX/eKH6T/FVvZUSiaXrleegJ2KqiV91uAMvcVL7ji3ePrO7cpFQ1nhzInlNynPwMvRNtNLkQgnSruuGJnTFwwaTvfx/9+486ZRgsCxXKbbjBoK0FwLbbLqhiVdHPhPOn4j1ZI4I0qqMgzzXTKz3QBCtFBekfePgew5hDlQ3AvoEqZ+7WomW6vGXW72ASkCMrgjV5B2qytkiQS1KK4VaVyYYkYZW1hsUrZSOhRMrTSbKN/uGLisnfxXemRRh9sW3vozzXrmAKNQzy6TDmoM8v2gXwWDS0hvPR/sdq9V3dAJW+1TrTihEWu9mKL2nQFzWrzPQHCy/p0O0gJ5+v0drrTWHVnV+QrWZPGNUyMiS4yPVJM8hHQl51jyHUhClk04J/x7rnsOwpjkorzoQpLcerdSqOKBeTDOp6VVvMUM2nQp9RtHMMzTPQYbeBomnSqec31f0P+u6bmtayQtvX2c8h6lBpxfBU647/gsrBelweWD1f2feYkYhw2ClrnHz4Tag1YbTklYqepyyQtWrNdOqh293Ifh8lFayHenmFjNpb7UenpDWeLWaVIRVNCM8bByCCVIXpJXnUA4Zh8DFB804aKUeXE1fGK0F5b9blbnQV2HKC0jq3hVayUWoKNVSM2qAlKajC+K60Bk1jFHjcObhe3DovBlEIZvkEKrDr35u1W+imJW5I3pf62hIqBKkw0lmXvhkrUGt0fTPsTMfRKDptbzykfEkPYEM3V5U3HDFjicvatSmrzkkeLR68Twdyituek14uvJyYaMMmG6QwcuHSdAcADaXAlpJJfZBoNmpY3T7tJLUUpT3nJQTEB0boeTXhBLjs7vzoX0UFSifYTokSKtktZ5CYBxANfzxaCXvmvbqK/rjTS1wkmgl5YWs9zUHE8o6qfCjKWqNmCANwUuh/lbMpOkpZBgs2/6A6ogYh1JNNzRbj1YKdfsKrWxsX3CT5ypfMjXx6Nmj+UwqlgQ3VLHJpEUsnT/qOfi0Ui1Mr8lSAGlvm6NlSIdXmso46JmvQKg5jeLuW1VcjRb3y2cCHSZa/ltxseoeq5aaqv5OyADZniCdTYebCGlcdnfB8sX4aOLiWMhbaTLpVIh7t9LJmoPOncuJJfAcchEPoeKFPCb1S9jkFbGLPtOhih0qYRKlRUe96CMV3CDFWyVIhxPulOfQlbf8FXg4Qzq58YyixJSgq4vmUpCOaw51J6gnpk/EKlmsajuxMh2VeoNSveEdI6CVRmoNfwJvFcqqLxBGW9BKer+MWqNJQ+lidaVJpunMWyGdTHkOvVq0EqiGPwGt1Jmz6OvIxhopJRoHb5y/4NFKxnOYZOhx2Hp2bLSOi964Z0ZR8pvBJBoJH6wHJSFignQ22oRFhrKqBKiQ5lAJBDeINyfSRc6kUFa1ClaTV1SQ1j2fLs0r0e+DH6JYa2hNXoLIGkiglVQCllbELSnTNqnfNgTdyaKJVQ1HiuJKENT7V8vzSidEKwWeA+BHzOgielcu6jm09xKqznc6rBaag65xFLXG87WIh1D0yl5Em7uoe94/GjYOKslMVT5VnoM+0ateB1Jz8KKHKnIln7NSpDxvR9e9hv1Q1riBTuqXoM5d6RXyHINIotEWtBIEq+eRaoNsOkXWSvljStGCoftjB9n8Oq2k7k82HVyTjuD6wk2eolpeUBI90oZV9xyyViKt1B3xHGTDH8ffp6cok/bKdYeG0/TfkaTQW0WtrjfRSlMD9XKNVG3/IapMWwhWmCUvAxek6zhQrodoKNCyRO1GS80h2kRIrYySevgOV+3QYPMpAW9Q6yJnUijrSC1cMrxD82zk/4ER6C7ognRwH/zwVz1rN8Fz0HnhoD+w47fdVJ5DqxapoWglT6RVqz/9GYAmCPrJX8FEX8haMVqpmKDpVKOeQyUwmNFn1go5Kx3SG6B1tJKiEvMZGR6s6vxHPYSid/7RWjpqH5U9rBY16lz7R6v+PVDXBXKM6J5gyHOoJ3snQxWbeqPpCdJJnkNyKQdlmEcjE7fyeKPRbv53+9VmpYbQV8z6tFL0PhSykrLc5BlJ5Tmod2PjcK1lJnGscqx3X+bOyIejlbxnNUc1U/JbgwbvRVc+XKdssFynO2+FKEYIN/wZKNfp68iG2AolyCd5DiC9BxPKOkUIGv40/DpDqZSITSY6F60yF0cjBqCgTb6lWgMh4vHxhUy4127VDurpJwlmuufQrXk5EOgghYxc4UWrnupRLSALDeaslD/4kwRpJcyr+1D0w/FkxUs9MUvXHHReOKgS2wxVEIWgZIP8Tn2FH9dhUinhhcWGjZnyHPSIH3X/ChkZequoAF2QhuB56qtYJfTXGzK0tjPbnnGQvcbDr42vOTitPYcOb4LzQ1YjmcaKVkrSHNSk6GsO3v8bvXLR6pnks0GlXrWS785n/Ez84YrMCUg6Rr9GXVlexE1Uc0gUpDPSsCmKrlOjlRIF6UywiICA+urryIYEad0QqbLdajWtJ8EBbBiptpxEo2NgpCrHeU8h3JBIvV9zulo1U0rH6pQNVmw/L0dHMRtoDgOlOj3FbKhVwFi0Eshcn9oYOspk4UVpHPRQu3K9EfcCtGgl9bcZxQxDZdsfNL4grXkb0UxbhdgKVnPRo3WXhqt2qBhYVz7sOUR7BsdpJce/Pv96tUHtG4e8XAnZjusLm52RyadUc2Jcs/p5oFwP8cJqe00T5v0e23ayQYj2GU6qFBoIhSqUsBH6zmjxOnWPZJJg2Asp1x2/R0N3PkO57vjhye16DvN6Cn52uUKgObSOVlIif6neiK2Mi97iYaTaSLynwcSd9v73jMOIMg7y92w6qGCqP+fAc2jEJt7YMbx9o/kIVdtJpEEUJRYYI+U5ZCKCtAplVeMkSDTtzFns1pkNC9K6cfCud8OI8hwUreRtH661FG5jiYEqUS8fnuhLtQZWStDXqYoGhmnYDi+oIOw52KEEOIWOXFAdd8DbR1/k+bRSgkYC4aQ6U5V1kuF3g6s1QjHuRb8EseY5eIOrp5BlpNaITSZ+/fZ6oyV3rSe7deczoVWYPhFGozEg4FUDz0EzDl7imd6xa6TW8FdUCvqg1usIdfnfbUcqzQaeQ8UOc82656DKJgMhKqLkew5x46CMWSYtYjqMn5mcCSc9Qbxftx4Fo99f5UXovQCSjLLi4VUnr3YF6Y+edkAs47tVtJLi99Mp4U/s5ZrjCdLJ1M5YnoPi85Xx3+j11FbPxKcpI/y82n/YKzWhr1jVsTdGJt7owiOpDac6tk5jqTElKx8nC9IQGAcVut3XkWXlZq8aad0JTZDq+jYOV0PHULTS5tEaL5ndGTs3/bh6kImK1HumP7xQ6dSyu5M8BxnAEfYcZrTyHDRaqbeY1XrX22S9e9CKCuvZSYzDi9Jz6NImXH1Cz0fixHWvwo8i8CqkKhpCF7Fbcdd6E3nZ7zZYJevp/dFoDHmugT6in1s+kw5E4Ejv5mhtJ9046N6NMkLD/n0ISoKofeVkEp/Imm544PorQrvpT0xqsq+FPAf5824d8aQgv6GQfk9a0Er6pKPfX914RicGFeIq72v4eY5Hc4gaktaaQ7ACDt3TBE7d/zlhe7+nOXT4VVnVZBmmleRnrJBA3JXPkE4JurzooZjn4E1UynNQelyS55C0Oi9kZTi1WjT5onk+TCvFNActqq0rkVYK7rG6PmXIg1BW+QybbmtuPkorDVdtOr2qrtFWwR3ZuHFQ462YkbkqerTTULkeC2OV90AmPKp6Vb0ToJUUWu0zGXhRGgc/9t9r+qG75emUiNTQ9zwHzzisHVCTSTC5QyACJq1AdaEwWohMp5Wi0Rj6z34nrwitBEnGIUorBWGUuhHo1jwHvYBbOiXIZ1KU6w41rRk9BN4VEKFAglDWmOcQyWcAaTh0ikmu+rUmN5GscxVFEvUcohFOwf2JBxjo5a3VBLPem3DajVZKQqsMaZ0+8qt11mV4cJI3BuG4dqVLbfIn7kzo/yitBDIRrloPPAc1FmSpiUYsmc1Kp8imUxqtlOw5tNIcglV92PPoylnUGk2GK3ZIs8pGPAdVFmS3jqwv1qqAAv+aMgF9JI8RhLJG71UUfmKg9k535y06c5lwnkOkLlRQtVn2qLDSKTrzlh8JBpIy6kmglVSfdRWa29sRlBgfqdmacRibVtKjyqYCL0rjoMpTjNbs0GSq3PKQ5pANi19rByt+eCvolSkboYQkHX6TdE+MhHCceVVb1UDUc2hFKwWZrNEql9FVsJ6ApRsBfTUTNWyqVWh0xRhe5Qbbdc9BvVg9xQxWKtzDt1KXdYV6iplQETM9azjaxwACL8QXpLWw1OD+NgIaIJuOZQzrHckCz2F8tFISWnkOZW2VrkR+lXWuT2YF3eDq+o53/oEgrRYUUlvoH0nwHDLhJk6BBhAUqYuusgvZdCxcNpcJyk6oTOzkPIdA/xCCUOVYde6tPEzAT7zr9WpqbSnVZe2wSCgrSM/B8hYuICPx1NzZiqKJ1ktTNFZX3qLuNEOlclSjK9BoJY0N0A2HrMiaLEir57TG80ql5xC8x+p9bek5eIlwUylGw4vUOEAQTRHNji1EaJ6AVpIPbM1AJTSR6JTGaC0+MUO4IFq0yqk+EfovtJYhnbVS5DMpn+v0o5W8PAcg9BLrRQEV9F4CI7WGLzrqYrd+reozKmtXn8gyaeFz7KGXXotC0SOikorfFTLhQnTq3ug6TLRelUpSjJa9VtFK6rvDSYJhSqFiN/znpe7x+qHx0UpJUPej4YQFaZ3G0umKWDROiBYKflYCsyrvrBYkKgPZNw65sHFQCW0QeAKqSF2SB1DIpP2Y/a6QIC3vW91p4ragbtQKf+Nwlc6s5a90u7QcBN3DjEYrjXhcv2pNunm03vL+bByp+RnY6j4oamlrtFIoMTBnxaIA1Xa/3Ew9mAOKCc9QRvnFs6MhGEtrBnTjEKeVWonoyhtp5Q1NFl60xkGJS3qHLQiyMVV4p08rqbC54WpoIlFtQEteKY4o36++E5Tn4HGwVjARBsYh7jmo36MllwuZdIjKUd/fdOOr4M5sWJDuTPAcokZS1YepRlaMqjmN3CfYX49fH9UojXw2gZ6IRH74tWY0zyGapd6Rs0Ihgnr+RYhWiiSe6fvqBkhRai8M7TjPQdIjYZF/oGzHJtroyl9BCKF5OvEINJVIpj8HlRE84hVw1K932KtDFF1l6xNxIEgHbV+jZbd1RCdu//yUaD4c9RwCWqnWcPzcir4O6Rn2j9SwHTdRe9lSqsfejRlbMQ7RHhfRhkR6FJ8qV55Ji4BW0tgD/TPKmCbRSmp/RUH3FDPkMzJ5crjaDq1kPIcpRWdeco5RKki55bVG0++MBsEDa7rxqqsyka3RUpAOXNt4UpDUHLzszUpccwBJDei0kuXV0c9bkVVRiyQ8vT+Dfr262B2jlXKWR4HEaQi1EkwKcdU9h6QGSnI1naIju5X+BpogbXm5Dx25tO9t6EZAj1ZKEqSDrOrgWpRxULHz28NziFVltYMyEOqeb/bom6RENIhHp/iaRcIzTfqMSriMFnD06xAleA7qXNR9BtW8yTPEke5xofPTKJ/OBK2svxWt1GiGsvVVqXbVEjecIR3/XgX1HJOK7unnXKlLrUBFJXVp0UMQLqESiu6rN3zPTK82oJrxJGsOHq3k1TlT16ZE8HaS4GBqE+DgRWwcunKWLPdrO4m0kj/BaROpSl9Iom3GEqT1mk0BVx7XHPTEpdC5apVZ9RLNQZmDcM/gJEG65HlDoZcgK69psCy5aN3oqRLTUgsID5NoFVCIh7LmLJksFi2ZrFbT6gV0XTfWHlM3KIqqU43mg/ajgZHVPTO/3HMmyPRV31XRKQIlSA9tuyDdqrZSVRNW1RgLalJpOk5IZ0ieuGPPVFvh6xm6qtCcElgVVE+HaI6FPH7K/05lTPTse73ndRRq4t40WgtrZd7kO1i2Q58LPMwgcKEzn/FpJUXFtPKsYsahEI4ajEKPQFQ5Mt1ejg+E8xk6tPdiVFuM+XOAFsgS9HKIaw7qWem0krxO+b1by3NQ3zmVRffgxWwc8pYfN90ZWYVV6sHAVQMmlRK+CxsrrJdN++UHxtIcVM1+0GmllF8YbDghWkn9rrd59DNi/dLKgdisn7NCR06WbC5716VfU1fOSpwgVVvLaLkF/XparQhV6WYIh6VCsJruyFl+Ce5qpFmLXkkzXKo6HYpWyno5BEW/mm7Yc0h54qVPK2mrZpl7IKmZTFq0fFHbQRCtFEmCswNhNWtJ+lGVwtD55GLEc9Xhi68xg6/oKiuyv7zfw9XwQkX2Im8kCstRXQTa9xzUWIzSmboXoR8v0ByaoYiqGYUMKZHsOeSslC88j5dWUrpN1XZCiye9vlpT8yjU3/XoPnUuuucwpGilBM1BJe2tHayQ05p6dXm5Hz6t1GLyV9FKU5njAC9i49CZs/yMyyjXLpurqMzI4AH1+MYh/EIWs2ktUSkpWklb2WrJWyAHgKocOly1yaZTsYEuX2zPc9DCMaOhrK1KhusRGPpLAPJlUxUgY7RSLR52qV+PPngzaeG/hLoBylupkHFQq2m95lN0ZaqL2LruE9UcdAEbpGegG4fod+lel7z2gEbYFviaQ7R8Rj0SBpy1/HESopLGopUSJm7996RSLTI4IpxprwRYPYw6ekx94k30HBJW50n3M/pzITTRB2M2SJyTQvb/b+/Mg+S46jv+/U33HDuzh7QrWdZlHbaMkHzFKI4cG3BMAjIQRBWXAQcXOHEOMCQklZj8k6IS/iCVikkqhCoXJjGpBONySBCGgiKxU8k/MchxDmzHFcU22IqNJVnXao+Z3X35473X/et39M7szuxc71Ol0k5PT8/r6e73e797fbWUmGL4eWmt0TyuPK98sxKvX5YII5Y1fn6ukWTWj7I8Eq1lzNTd0UpJox9XtJIa+/HTs4lJCYAq9pialcwCjhot8PpGOBBRRERPENHD6vUuInqMiI4R0VeJqKS2l9XrY+r9newYn1LbnyGit7Dth9S2Y0R0d/tOz89oJU5sxHzFrIu4mQX2ACTZkHa/hjiJOXcJB119dba+mISx6ZVj2tBlyerloOGNaXg4pqk5+IVD6gzljYoA+bC57O66BIAZdgmkN23FeIDlanMpifwA5MSQqcqqIoaqTGC5GtDoXt4XWAZ7xufAzCM6gornObi6iJnF4/QE4Qo/bgVvhnR9IfOAj5Zjp1kpm+fg/q19At8pHJTPIWNWYitc09adOL3Zd/C6XYmfzKFd+Uw+pjZufrf0TWUT5yZrpcSJ69NuxozfQZ+Xq7QH/+xMI/2+sUoxY1ZKNQqd3Z2GWfOwWp4fpc1K4xX73kk6KTYWM8Ij9TkseavIAjL3ZKwSd7ULHNCa5vAJAE+z158FcI8Q4jIApwHcobbfAeC02n6P2g9EtA/ArQD2AzgE4M+VwIkAfB7ALQD2AXi/2rej8Jss65AuJOYXIDthas3B1cznhEML0egIHx5NY9bgTx9oW03ljWl4OGbZcEjziqscfX46w7RmTJAvOSJ2ZAmARWc1Tr4q55RVfwluCjId0jN1aVZKwgJVcT/5eUMTaGTDg3nHN55DwH9fngcCpGYt3SSJj5n3HlgNrmgls0ubHFOU1A+qZLSuQlLp1ecs9pmVRszgiFIqHEYNnwPfJ/MZLYDY/uWYRSsZ9yyHJ0VmNY+0tLlTOLBMev29k7WSM7FPvra1G2B5s5J+b44XB2T5DLyni35uZIYz80UYZqULKlppvBIn4cUcXsSRlwHRz/F8Y2nZzOfLN41hx2Q1d59O05RwIKJtAN4G4IvqNQG4GcBDapf7AbxT/X1YvYZ6/01q/8MAHhBCzAshngNwDMB16t8xIcSzQog6gAfUvh2F32Q1Y1LMmJXYRKujCFxmJW3a8Tk29UObRivpyJzUYXaetUDMjLUcJ6F/vChZOQllNcxKjrBHgAkHQ3PQY+efq5WiJJvbXsWlEz+nrGo9WcLBYVbiTZLM+js8+ognKcoIJ1uD0p+ZrS9iprGQ+CL0MXkgAJ+sxz0+pFbREwTXHHS0W8asVI6TKBefhlApeVb1nmtaMyf6kqz+emp63tAc/H4Nl+lKaw663Ivc5i6foTFX9fp+4qt6IhlpV19kZiX1uanRdJXti9pyRfL5xqbRpmKtIYxX4iRYQkfq8WPrBFBdej6poBDLCsfarOQyKZm/yfqaS3Oww4lNvnrnQfzWm1+Tu0+naVZz+ByA3wagPW5TAM4IIXQW04sAtqq/twJ4AQDU+2fV/sl24zO+7R1l1KMC63IWvLe0xq85uI/F0ZNXYr9lJbsBOdmdm3ULBz2JnZ/LVvRMfA7LmpWywsFnG870gcg4Jw2fg9H4RyPbli5m/Boyz4GV7FYrfp5QZDmki1nhkGgO5ThpEyp9DlnTjIxWyq7WtXBKmxZxranNPgcmHHg+ima0LCduwDbRuPw4/LVp+tK/r8usBMgkrtEy9zlkV/Wuz4wZmoNu+2qW3eaU4kJy/r5FiWsRkcmHYZpDMiaPtmpqDuNN2Od9iYF6sjafdZ00qs+bLx60s1qW67a1fED+JlprymoOMabruuFS/oIkzjE7rRXLCgciejuAV4QQj6/BeJYby51EdJSIjp44cWJVx8o6X7Oqv3YOy/eYcFArBZfmkB7LIxyU0EkiP4xwVG0KMMNYgWyyGtcczDwHXnGVkwoH2/TFbdG+38ScTKo5moMOZa2xiSGTBKecwrzsiKVNsd9k2tAc6otLqC8sWVm0us+w7j+cbldC2THBJRVEVykcUp+D3bfCF69vT356BWwK3DTMlJP2EzHMSg6zGZCtQ+QzXbkijOYWWBCFZwI2ixma3z9iaEM6EmrauF8n2UrcFHpeh3STZiXpkM76OEZVIqxp3hpVYdZaaGQWf6rU95mZhlX9mKOfn/WGz0EI2eOhmwX1mqWZEd4A4B1E9DykyedmAH8CYB0R6V9tG4Dj6u/jALYDgHp/AsApvt34jG+7hRDiXiHEASHEgY0bNzYxdD8+55m+KU8alTABblZyr/rMY2X3kZ3AXLWVAN2gZcFtVmKRFXxSTCOEWBEzRz+J5cxKmqyQ9JsheMgpRzukz88tJKaCSjEN1V1ckpnQunyGHDNzIhuObp07wqOVADi7pknNYSFTskKPdbax5JystamlE5rDjENTqWXGZWpjMjPXbCSUOqSzv/UYE5iZ/T0O4lzNweHs5W1fzcRNE7O8hCZXc1D3CU/Uy2gOvjEaz4cu5e4KKeWf1aGsvP6TbpNr9miplWMsCbumFZCanM7ONqze0Rx9r2Yd0nKMJ6frSQHCXmbZEQohPiWE2CaE2AnpUH5ECPFBAI8CeLfa7XYAX1d/H1Gvod5/RMju3kcA3KqimXYB2APgewC+D2CPin4qqe840pazyyE7KdorphPn5634dy0cLLNS0X2s7D7SKaYL2ekHgtvXz801nA7ppLT2rDYryde6vALXHMwVphyT/I4fOyKqfL6XfM3BbQKpFAuqsuZSZmJYXBIZ84SMVtICwDYr6ZX/mZl6Jktdj2m6bpf10JntM/WFjGDTGcNmCDE/9/ZpDrZZqeox1Zm/Ha+yy0kyrD2mQpdpT+MqZyH38Qkg25Q4x0u+eFtxys+ZkTv69zXPSwsHXcpCM6mq+LrOy2dW2r9lAg/9yvW4/tIp59j0+SW5H6z+03glxvRcwzLHakHsco6PKlPU6Zm616wEpPfqZM0uv39yej43uqpXWI34+h0AnySiY5A+hfvU9vsATKntnwRwNwAIIZ4E8CCApwB8G8BHhRCLyi/xMQDfgYyGelDt21G0PdZs65lqDvNWxISu/2I1FOcrixyH9EzDPakBctU/U1/0mJVYTDbL8gW0nT9b/thEr5ReydEceOkEwKzX455M7Ic+SspDJHkOzESUhplmSyNbEgtUogAAGbZJREFUfphSKqCBdEWbmKJ08TqXQ9rQHKTQWLC0E4CFsq7WIa2S4Hiew4zj+1yLED5+Z3kKTygrz3LnVDNabHov6Z4O+rtc32FGGwEy5HSuISvp+uLyXZ+Xr92mslKsotrmss2xdJY0ke3nSgomOhY/B3ZOWtqyOb6kgoFhMdAViQG+CJH/n9BmWMOneG5Oag552opZsBPgmsN8X5iVWloyCSH+CcA/qb+fhYw0MveZA/Aez+c/A+Azju3fAvCtVsayWnhkAr+x9I1+cnreeiBvuHQKf/iuq3DtJesz27mZx+do4g7pTLmEojEROs1K6YrDjICpsH7L057aTgVVovxEjuagS1RoTCe963xts1IhCdUcrWQnornGYhIaOVKMkqzXmflFCPX8VEpZU5uvd/IF5UMwK3fO1GXWNDdPaHvzrMOs1K4kuCiyNQczZNn8btfk59QclhEOvokecNvnz88vWM5wrZ2YoayANFnqyDDfBGyWJLHGaIU8ywXN4lI250Zft6rju6rMFNQquvijWVJEO6Sn5xfUs5vNRk867RkO6ZfPzkIIOLvAabRAMX0OgHTyD5xwGCRGk4nBPfGdnJ63VvFxVMB7f3I7TMz0ehepQzpbyI4XLgPcwmE8qQg7nxxLUy5GSSjrhfkFK5xQI+s/6ZWQPYG4cjc05qR1zfZ1uPaSddgyMZLZXi4WkmqV5sQwx/wt1ZJ8+HVUCJFcLeqVaSURDtrvEyefA5Tm4PA5aOGwbX32951rLDkd0uOe6LNWcfkc0hpPtj+KmxU17/vJ7TiuektwXDkI/LXpuPWZlfhrr+bgWBDML8ggitxoII8/wPd9uhz4bGMp49TVwsGZT+H5jmZIMqTnG9bCaHo+rUisr8lYIhwcmkMlxml1j7v6R5vj5U728Yzw7X2z0tAKBx62xtEPwanpOjYbk58PvarJy7TloZaZHr7FrD/ANFkB6aRi9gwGdG5BWqt+w6h7NaPr//NGRYDftJI1gWRXOVdsncDXfu0G6zv4itQUDhmzElsNz9Rl1dVKnK4W9fmZDWj0mM4rv0bWPBdL30K9AN48Z6QYZWLqneUz2pQhzfs5uPIqfBFJAHDz3k3OY0+NlhAVKKMNAcDF4xVcdtEo9m+ZyGzP5h24wz7N779y6wQO7p7E3s3jyTaerDZbtyvzcrwO6Ur2HuDHnp5fwPRcA9vWpc9YXqnqn9l7Ec7MNCxh2Az62dMtO5PxqcXJudmGM4zb1YaVn2O+z0GZlZjPgZv5lstz6AWGVjhEBUKtFDlstvL1gqHy5uF7ODja7jm/YNvEAd483T6GLhL38jm7Lg93SPt8DgDP/jRMDR7TSp7m4IPf8JbPwVHaQld+LbHiZPx9M+tcj0mXoDB9RTN1aR5w9UfQn+ET1e4NNezeWMP+remkuBIi8kcr8fPS16CVmjmH9l+Mb3/i9UkPbc1IKcI/fPKN1v65ZqXEQZydmC6eqOCBO6/PbOOhrK7KvJxqKXbWBPM7pCOcmq5bVYxLsSwbwQM8NNdest4y5zZLpRhhScgF33aWdax/n5fPzTkDVPRijD8bfLwTIzlmpXKEmPl5+PcB3e0N3SxDKxwAefPmhaU2u0pJzUr5qrfOAeAr7IJyBOuJ0OWQltvTKrKms1jb8i/Uc8xKHk3JF7FjNpBpBq4qm85IXnSQaw4X6guoFKNM4TSzd3JaEE0e85SneN2SAM7OZleXFUOg8M+sq5bwyG/e1NS55VEoyJDijM/BJRxyNAcfcVTAnk1jTe+fMWM5ylvnOZY5aQvaJczV7RIqnKlaKWnjyvE5wMtFaVaanrOj66ZqpbbXFNK/yYnp+czzwXt6TLFIKf0saM3Vl8eUpzm87cotmKyVM+bDailCVCAsLolgVup1JkaK1mTcTM6CiTZjLOdzAIAzsw3snMquOEZKUeJz8AmHsUox2cdc1b96oS57NTj6R2tMp256XLdpRXcR8zWWd8FXQ6bDdK6xaOUaVFXMeK2cLTKWCMxpt+agfRGuqC/TYa+362bvnWq9GBcKhs/B9nFox2YnV41pcmXBypnYMFrG+EgxN7JHk9Tt0ppDzu921817cNvBHdb2q7ZN4MqtE9g5la0RVI4LqrjlonU/blk30vbfR/8m9YUlZ3jvS2fnsGOqlmzXi7xXzs0jMqL4uHDJy3O4/tIpK7xWt3c9O9sImkOv8/vvvMJSvauOh3k5fCabzHFL6SS19+LsSnCkyHv4uo8xVonxzI/lPpkObLE0K80vLGFhSSwrHFzmo6hA7hDYcqTq/zd3I/PYbZ4hDcjy1WYET60c4//OzGLeUfm1WooSxx8vvAcgiYgyHdKuvxOz0kwdlWLnShLIFWHW52AmtWkB3Mn2j7oC8GjZXmTc+YbduOWKi5s6DtccZuvuKDjNRLWICccqevfGUXzjrhsdY4zwqqoxZd7vf/juq1BoQni1QjYowM474Hk5QHqd5pUw4cI0U11gBc7xsYoSDsHn0Ntct2vS2jbiUSHz4JOdjwrTHHy1bQDbFKDhjuqs5pCWrAD8AmrUI8CICDsmq7jEUQFSTsb1plfbTs2BRSuZq2ldN98lgEaKEU5DCQf1sEaqeY82N5l5DunfzCGtwmNfvVDvaH38uECWz8FXxqSTjeN1/wLXxLVhtIwNo7b5xwWv+DvXWMJkrX1j1gUaAft+3La+/ZVIfRFcPCyWj0OHfvMucMl+rC6TqyLrcsjvnA1mpX5Er7yEo1e0D72izdcc5Htmc3kgW3DONAVo+E2djVaKVIOdtB+vC5/PAQCO3HWjU82tlqLcuvMmvCSIjuDhbRrNctpJQ6HGkrWariSmpyjTBrPmaZiTyYo2opgAKRzMpMZ2EkVk+RzMc0oc0h3UHAD5+68k5JOT9ApZWHKWbV8NfNXsWwy1k4yp2FM2x5WBPlNftKwH+jN5/oY89HXpB7NS749wjdG9AYDmNYdqUdbFySvExYuP+Spyuhr9aHydtXTHrvPzdqFATupzsB/y0XLsFEqj5bhpk5Ici9JOXAXcmEM6STYqybr5s3V/QyHLDFZ290TwmpWYQ7qTPXktnwMr9ZyOSzukO/vYVYrRqifdpGObyhHxdVpbzbGB1eeYNEMm8dFjFjJ/r7R2laE5aOGQE6mUR1pzLGgOfclIKVLOsuYuYKFA+PJHfgqXbxr1H5PH3pdsEwqQn/3JHdVmJFEzmkNaN6b5S14txy3dxHrS5w8gL7+tO6NpTaRWjpMoJl/9Jqv8eClOGre7Ms0Bt0PaLKvRbuICYdEon2Gek04+7KRZCQA2jJawyQh9bZWoQChGJB3S7dYcuIN3LTSHjFmp6PzbfNZ9nfa0EBkGzSEIBwdml7ZmyCv8xY8J2JODr6ELh9/IZoa0LGKmMpM9x/A5pPOoeUo6+Cg7VvvFSNb710IgExaoHczT815Tmys5T/dE8IUd+yrKuuLn20Vk+BzmjHMFZFhqOS50vP3jF257XVtWppU4SjWHNq50s76plU2yrcDHbhYg1KGl5jjSSsCGcHBUW20F/RwH4dCn+IqdrQaXqcN87QtjBVLBwbucAamJ4pQK7/RpO81EVJl86PqdePncbNP76xve/A4dEmuuprUQOTe34C3ul1dYrploJd5ZrZOTchyR0c9hwbm4eOuVm3Fwd/5CYrVsWddcZv9ylFW59blGfoZ068ddPgCjnfjKmBMRxioxzsw0rEWIvoerli9C7pdXdC+PRHMIZqX+REe7rCRV33tMY7XPqTahOehoJauWjtJCtB3epxmYJSiaYTltyMTX71h3gzO7tPEHspnWlUBWWPijlTwCpIMPpKk5zDaWMDVqf98977umY2NoN+U4SppetdNPUo7sqLZOkjErOfJ8zszYHRgTTduh/f30pVM4sHNl2dpBc+hzWnVIN4NvwuLv5fkc9M1rTnBJkbrzdpc3zkrMSq2S+BwqtuagHdKujGHA3zPC5ZBOPsO0ArOHg+u4Hfc5GIX3Ohk6uxaUiwWcVbkm7TyXTJmVDpvYgPxQcWlOmrU0VDO3hvM3v3RwxWPpJ59D74+wC/hWravBzE3g6AksL1opaaRuVeFUZiWtOXj8JJdMVlGOC7h0Y835fjtISx7bY0x6LXiSDH2/idUeU52fWQbC51soRgUUVUntTkaIRM5opf4WDpU4wplZO+FwtfCQ55XkCqz0+3TWPyepSuwpN77aXh8mqXDo/XsjaA4OEod0G28M7oT2hW3m+Rz0e+aEU2ZmJTMngLNl3Qie+YNbWh94C6S9B4ySJMrnMNdYzFQXze0Z4fH7VFnxOp65GhUIpbiA+oIjZ6IYobG40NHJ2tQcXNFK/Ua5mNb8aq9D2i34O0VBCYVSbJdK14sun/my3bkxuoDi+lrnHfGrJWgODjrhkC6o7F7A5ZB2m2M42uRkRtykDun5jpqMmsFnVqowh7QvqsgnHCx1P6cERbUkK2Ga/Xn1sTpp5mkmWqnfqMRRYlbqRLSSr0hkJxgpRc7n2RfirTWGdpu9Du6exDc+diP2Xry6SsBrQRAODmQzmvZPJml7TV+ew/JJcL6ewSeNipPdIAlldURjzTd0Fzx3hqqv25yp1usJ1zVZVYtRbqOYTvscdD+HxuISGoui730OlWIB51VZlrY6pNWx1kpzAPxZ47wTYna7O1pptRARrtw2sfyOPUAQDg6u3DqB65bpS7sSfM1eKk2YlbTJyO78lYaydltzuHi8gtuv34Gf2XtRZrs2K0mHNHMiryBaKQkxdEz0I6XIub2yxpqDq5dDP8Lt4u31ObRfM1+OSrHgFA5b1o1gslayHMRptNLwWt6H98xzeM+B7XjPAbsd6GrRE7lvlZwXraRjsu2uWvK1rMja3ckoKhA+ffgKa3ua55CN/c9GK3nyHCyfQ5x5P/NeKQaz7KTHWgvNIaKkmJxuvtTJWk5rAb8mnTArraVw2DRecRYd/PANO/Gua7daC8EkWqnLz1Q36e+7t8/Qk4U5se3fMoH9W8Zx2UX+8hsA8MtvuBSv3Zwt953J/lyDbNOVUC5GmJm3k6l0ldW5xpKtOXj8PrUcs9JIKco4hc1jddIHIKOVpFBINYf+VswzmkNby2esrUMaAD7/gWudBSQrRXcVgH2bx3HF1nG8tg98A50iCIc1JPU5ZG/GXRtq+ObHX7/s53/1pkutbXx112wtqLVmpBjhzKx0bFrlCMox5hp1KzFQ9wdYb/ROruY4pG96zUbMqBpT5vcDnQ1ljVk/h7Q0eX8/XhnNoY2hl9rnsJY+MvM+Wo5N4xU8fNfyz+Qg0993b5/Bu3S1C76667bPwcdIqZCs6E0NIekZYfwmB3dN4f6PXIerDeddXh/mX7vpMuf369yQjvscVOE9XVJ8/QqLs/UKXGBX2qgFJWalNdQcAq0Trs4akqxg27gKy2oOvXk5fWUuALtjnKZQILzx8o3WsXymuWa+f63yHJ4/dQGA1Aj7mUxf7zYK1orKOdjYZOOhQHfozdlkQKmW5EPRzlaVWZ9Db15OX98FIN+H4EJrDq0U0VsTs1KUakfPnbyAainCxrH+nvy45tBOratSjPDwXTc6uw8GeofenE0GlNFK3PYJvBilZYd716zkn2SS1plNTj4r0hzWKs9Baw4nL2DHVK3todBrDS874etQuFIu3zS2/E6BrtKbs8mA8kuv340372uuwXsrVOKCak7Um5ezkhP1kudDcFFTCYqtmIgqaxKtlJqVfnhqBns39//ktxb5IYHepTdnkwFl+2QV2zugSleKqnNdjzr4cjWHUmtVKuOogM+97xq8bkfzJZOra+CQlprDEhYWl/CjV2dw6Ir2LwLWGn1N+r1GVGBl9OZsEmgJXx+FXiHblMcujVxu0Q9z+JqtLX3/z1+9BaWosOLuXc2gNYfjZ2axsCSws8+d0UB6X3W653WgN+nN2STQEkmtmh7Nc6jkODY/8FOXYP+WziYabV03go/cuKuj36F9Ds+dHIxIJSBbWjswfAThMADoXIee1RxyWqRevmlsIJyTUaGAxUWB55Vw2DHV/5E4lTWI8gr0LkFfHAAqxbWvVdMKvl7Pg0QcSc3h+VMzqJWigYjhDw7p4SYIhwGg0oUql63QqQJuvUSkHNLPn7qAnRv6P4wVSM1K5eBzGErCVR8A9OTbs2YlJRDKccHbqa7f0T6H509eGAhnNBA0h2EnCIcBoFKMUI4LbU9UaheVNUhC6zZRgSAE8MLpWeyaGgzhEEJZh5venE0CLVEpulsg9gpJbaMBnmRipREtLomBcEYDQXMYdnp3Rgk0zc++dhM2jVe6PQwvxaiAuEAt1UPqN6JCus4ahDBWgGsOYQ05jCx71YmoQkTfI6L/IKIniejTavsuInqMiI4R0VeJqKS2l9XrY+r9nexYn1LbnyGit7Dth9S2Y0R0d/tPc7B521Wbcfcte7s9jFxGiu4WnoNCzHwpg+ZzGGShHvDTzJJgHsDNQoirAVwD4BARHQTwWQD3CCEuA3AawB1q/zsAnFbb71H7gYj2AbgVwH4AhwD8ORFFRBQB+DyAWwDsA/B+tW9ggCgXI1T7vPlNHtrRPlaOMdViY5lepRwXUCtF2FDr/7DcQOssKxyEZFq9LKp/AsDNAB5S2+8H8E7192H1Gur9N5GM6zsM4AEhxLwQ4jkAxwBcp/4dE0I8K4SoA3hA7RsYIEZKhYFegcaRFA6DEsYKyJ4a3/z463HbwR3dHkqgCzRlTFQr/H8H8AqA7wL4XwBnhBALapcXAeiCN1sBvAAA6v2zAKb4duMzvu2ucdxJREeJ6OiJEyeaGXqgR7horIJNfd7fII9Y+RwGxRmt2bmhNtBRZgE/Ten5QohFANcQ0ToAfwegKwZuIcS9AO4FgAMHDtid5AM9y72/8DoUm6y82o9on8OgOKMDgZaMwEKIM0T0KIDrAawjolhpB9sAHFe7HQewHcCLRBQDmABwim3X8M/4tgcGhKkBKCeRh/Y57ByQHIdAoJlopY1KYwARjQD4OQBPA3gUwLvVbrcD+Lr6+4h6DfX+I0IIobbfqqKZdgHYA+B7AL4PYI+KfipBOq2PtOPkAoG1gvscAoFBoBnNYTOA+1VUUQHAg0KIh4noKQAPENEfAHgCwH1q//sA/BURHQPwKuRkDyHEk0T0IICnACwA+KgyV4GIPgbgOwAiAF8SQjzZtjMMBNaAGy/bgF9546W4attEt4cSCLQFkov6/uPAgQPi6NGj3R5GIBAI9A1E9LgQ4kAz+w6uhzAQCAQCKyYIh0AgEAhYBOEQCAQCAYsgHAKBQCBgEYRDIBAIBCyCcAgEAoGARRAOgUAgELAIwiEQCAQCFn2bBEdEJwD8cIUf3wDgZBuH008M67kP63kD4dzDuafsEEJsbObDfSscVgMRHW02S3DQGNZzH9bzBsK5h3NfGcGsFAgEAgGLIBwCgUAgYDGswuHebg+giwzruQ/reQPh3IeVVZ37UPocAoFAIJDPsGoOgUAgEMhhqIQDER0iomeI6BgR3d3t8XQSItpORI8S0VNE9CQRfUJtnySi7xLR/6j/13d7rJ2AiCIieoKIHlavdxHRY+raf1V1HRw4iGgdET1ERP9NRE8T0fVDdM1/Q93rPyCirxBRZVCvOxF9iYheIaIfsG3O60ySP1W/wX8S0bXNfMfQCAfVye7zAG4BsA/A+4loX3dH1VEWAPymEGIfgIMAPqrO924A/yiE2APgH9XrQeQTkO1sNZ8FcI8Q4jIApwHc0ZVRdZ4/AfBtIcReAFdD/gYDf82JaCuAjwM4IIS4ArKr5K0Y3Ov+lwAOGdt81/kWyLbMewDcCeALzXzB0AgHANcBOCaEeFYIUQfwAIDDXR5TxxBCvCSE+Df193nISWIr5Dnfr3a7H8A7uzPCzkFE2wC8DcAX1WsCcDOAh9Qug3reEwDeANWyVwhRF0KcwRBcc0UMYISIYgBVAC9hQK+7EOKfIdswc3zX+TCALwvJvwJYR0Sbl/uOYRIOWwG8wF6/qLYNPES0E8BPAHgMwCYhxEvqrZcBbOrSsDrJ5wD8NoAl9XoKwBkhxIJ6PajXfheAEwD+QpnUvkhENQzBNRdCHAfwRwB+BCkUzgJ4HMNx3TW+67yiuW+YhMNQQkSjAP4WwK8LIc7x94QMVRuocDUiejuAV4QQj3d7LF0gBnAtgC8IIX4CwAUYJqRBvOYAoOzrhyEF5BYANdhml6GhHdd5mITDcQDb2ettatvAQkRFSMHw10KIr6nNP9Yqpfr/lW6Nr0PcAOAdRPQ8pOnwZkg7/DplbgAG99q/COBFIcRj6vVDkMJi0K85APwsgOeEECeEEA0AX4O8F4bhumt813lFc98wCYfvA9ijohdKkM6qI10eU8dQdvb7ADwthPhj9tYRALerv28H8PW1HlsnEUJ8SgixTQixE/IaPyKE+CCARwG8W+02cOcNAEKIlwG8QESvUZveBOApDPg1V/wIwEEiqqp7X5/7wF93hu86HwHwIRW1dBDAWWZ+8jJUSXBE9FZIe3QE4EtCiM90eUgdg4huBPAvAP4Lqe39dyH9Dg8CuASyqu17hRCmY2sgIKKbAPyWEOLtRLQbUpOYBPAEgNuEEPPdHF8nIKJrIB3xJQDPAvgw5CJw4K85EX0awPsgI/WeAPCLkLb1gbvuRPQVADdBVl79MYDfA/D3cFxnJSz/DNLMNgPgw0KIo8t+xzAJh0AgEAg0xzCZlQKBQCDQJEE4BAKBQMAiCIdAIBAIWAThEAgEAgGLIBwCgUAgYBGEQyAQCAQsgnAIBAKBgEUQDoFAIBCw+H8CL8VCOL57JAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(scaler.inverse_transform(y))\n",
    "plt.plot(scaler.inverse_transform(model.predict(X[:-2], batch_size=32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6325cd1b70>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_hat = scaler.inverse_transform(model.predict(X_test, batch_size=32))\n",
    "y_real = scaler.inverse_transform(y_test[:-1])\n",
    "\n",
    "plt.plot(y_hat)\n",
    "plt.plot(y_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.858636325140955"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(y_true=y_real, y_pred=y_hat[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f63259b7278>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['loss'])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
