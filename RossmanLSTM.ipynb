{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation, LSTM, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "Store = 67\n",
    "\n",
    "data = pd.read_csv('train.csv', index_col=\"Date\", parse_dates=[\"Date\"])\n",
    "data.drop([\"DayOfWeek\", \"Customers\", \"Open\", \"Promo\", \"StateHoliday\", \"SchoolHoliday\"], inplace=True, axis=1)\n",
    "store = data[data.Store == Store].Sales\n",
    "store_ts = store.resample(\"W\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:4: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "look_back = 3\n",
    "\n",
    "scaler = MinMaxScaler((-1,1))\n",
    "data_scaled = scaler.fit_transform(store_ts.reshape(-1, 1))\n",
    "\n",
    "convert_to_step = lambda interval: data_scaled[interval[0]:interval[1]]\n",
    "intervals = zip(range(len(data_scaled) - look_back), range(look_back, len(data_scaled)))\n",
    "\n",
    "train_set, test_set = train_test_split(np.array(list(map(convert_to_step, intervals))), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train_set[:-1], train_set[1:, -1]\n",
    "X_test, y_test = test_set[:-1], test_set[1:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (32, 3, 64)               16896     \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (32, 3, 64)               33024     \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (32, 3, 64)               33024     \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (32, 64)                  33024     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (32, 1)                   65        \n",
      "=================================================================\n",
      "Total params: 116,033\n",
      "Trainable params: 116,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(look_back, 1), batch_size=32, return_sequences=True, stateful=True),\n",
    "    LSTM(64, stateful=True, return_sequences=True),\n",
    "    LSTM(64, stateful=True, return_sequences=True),\n",
    "    LSTM(64, stateful=True),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(loss='mape', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96 samples, validate on 32 samples\n",
      "Epoch 1/10000\n",
      "96/96 [==============================] - 1s 13ms/step - loss: 105.9250 - val_loss: 103.4944\n",
      "Epoch 2/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 101.5695 - val_loss: 94.1294\n",
      "Epoch 3/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 104.3783 - val_loss: 94.8066\n",
      "Epoch 4/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 100.7280 - val_loss: 99.5036\n",
      "Epoch 5/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 100.7978 - val_loss: 101.1453\n",
      "Epoch 6/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 100.7760 - val_loss: 98.7927\n",
      "Epoch 7/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 98.4718 - val_loss: 96.7627\n",
      "Epoch 8/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 100.2383 - val_loss: 95.5721\n",
      "Epoch 9/10000\n",
      "96/96 [==============================] - 0s 482us/step - loss: 100.7553 - val_loss: 95.8075\n",
      "Epoch 10/10000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 99.8783 - val_loss: 98.6335\n",
      "Epoch 11/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 99.4021 - val_loss: 99.2517\n",
      "Epoch 12/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 99.7319 - val_loss: 98.7685\n",
      "Epoch 13/10000\n",
      "96/96 [==============================] - 0s 516us/step - loss: 99.2686 - val_loss: 98.3017\n",
      "Epoch 14/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 98.6133 - val_loss: 97.2202\n",
      "Epoch 15/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 99.5379 - val_loss: 96.4406\n",
      "Epoch 16/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 99.6607 - val_loss: 97.5480\n",
      "Epoch 17/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 98.7000 - val_loss: 98.2619\n",
      "Epoch 18/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 98.7894 - val_loss: 97.8367\n",
      "Epoch 19/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 98.8542 - val_loss: 97.6089\n",
      "Epoch 20/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 98.7804 - val_loss: 98.2473\n",
      "Epoch 21/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 98.8847 - val_loss: 97.9695\n",
      "Epoch 22/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 98.7669 - val_loss: 97.4294\n",
      "Epoch 23/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 98.7013 - val_loss: 98.4690\n",
      "Epoch 24/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 99.8755 - val_loss: 99.2784\n",
      "Epoch 25/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 99.5179 - val_loss: 99.2306\n",
      "Epoch 26/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 99.0790 - val_loss: 98.0008\n",
      "Epoch 27/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 98.9214 - val_loss: 95.8268\n",
      "Epoch 28/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 101.2298 - val_loss: 94.9985\n",
      "Epoch 29/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 101.0818 - val_loss: 95.9758\n",
      "Epoch 30/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 100.0143 - val_loss: 97.8405\n",
      "Epoch 31/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 98.8149 - val_loss: 98.2427\n",
      "Epoch 32/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 99.0833 - val_loss: 98.2187\n",
      "Epoch 33/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 98.8955 - val_loss: 97.2784\n",
      "Epoch 34/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 99.2099 - val_loss: 97.9039\n",
      "Epoch 35/10000\n",
      "96/96 [==============================] - 0s 495us/step - loss: 98.8157 - val_loss: 97.4917\n",
      "Epoch 36/10000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 98.8982 - val_loss: 97.6127\n",
      "Epoch 37/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 98.5218 - val_loss: 97.8081\n",
      "Epoch 38/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 99.1835 - val_loss: 97.6172\n",
      "Epoch 39/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 99.0252 - val_loss: 98.9597\n",
      "Epoch 40/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 99.9195 - val_loss: 99.6443\n",
      "Epoch 41/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 99.7599 - val_loss: 99.3117\n",
      "Epoch 42/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 99.3570 - val_loss: 98.4805\n",
      "Epoch 43/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 99.2794 - val_loss: 97.0695\n",
      "Epoch 44/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 99.1946 - val_loss: 96.8677\n",
      "Epoch 45/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 99.0708 - val_loss: 97.7250\n",
      "Epoch 46/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 98.5476 - val_loss: 97.8601\n",
      "Epoch 47/10000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 98.7946 - val_loss: 97.4673\n",
      "Epoch 48/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 98.9414 - val_loss: 97.2634\n",
      "Epoch 49/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 98.7342 - val_loss: 98.1129\n",
      "Epoch 50/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 99.3647 - val_loss: 98.8959\n",
      "Epoch 51/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 99.5968 - val_loss: 98.9795\n",
      "Epoch 52/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 99.2781 - val_loss: 98.0836\n",
      "Epoch 53/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 98.5698 - val_loss: 97.4741\n",
      "Epoch 54/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 98.5889 - val_loss: 97.5463\n",
      "Epoch 55/10000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 98.2883 - val_loss: 98.1168\n",
      "Epoch 56/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 98.6903 - val_loss: 98.0959\n",
      "Epoch 57/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 98.7469 - val_loss: 97.7037\n",
      "Epoch 58/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 98.4627 - val_loss: 97.5502\n",
      "Epoch 59/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 98.7212 - val_loss: 98.3232\n",
      "Epoch 60/10000\n",
      "96/96 [==============================] - 0s 516us/step - loss: 99.2880 - val_loss: 98.8238\n",
      "Epoch 61/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 99.0668 - val_loss: 97.9506\n",
      "Epoch 62/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 98.3232 - val_loss: 97.3497\n",
      "Epoch 63/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 99.2097 - val_loss: 96.6590\n",
      "Epoch 64/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 98.8496 - val_loss: 97.2712\n",
      "Epoch 65/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 99.3559 - val_loss: 98.9622\n",
      "Epoch 66/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 99.4227 - val_loss: 98.8232\n",
      "Epoch 67/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 99.0820 - val_loss: 98.2583\n",
      "Epoch 68/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 99.0080 - val_loss: 97.2361\n",
      "Epoch 69/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 98.8188 - val_loss: 97.3501\n",
      "Epoch 70/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 99.3434 - val_loss: 98.1542\n",
      "Epoch 71/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 98.6525 - val_loss: 97.8232\n",
      "Epoch 72/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 98.5318 - val_loss: 96.7813\n",
      "Epoch 73/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 99.2777 - val_loss: 96.7284\n",
      "Epoch 74/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 99.0107 - val_loss: 97.7948\n",
      "Epoch 75/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 98.4021 - val_loss: 98.5095\n",
      "Epoch 76/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 99.9283 - val_loss: 98.7903\n",
      "Epoch 77/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 99.4121 - val_loss: 98.6807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 99.0382 - val_loss: 97.8421\n",
      "Epoch 79/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 99.2216 - val_loss: 97.0444\n",
      "Epoch 80/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 98.3056 - val_loss: 97.0650\n",
      "Epoch 81/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 99.1901 - val_loss: 98.4200\n",
      "Epoch 82/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 98.7020 - val_loss: 99.5244\n",
      "Epoch 83/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 99.5513 - val_loss: 98.7690\n",
      "Epoch 84/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 98.6686 - val_loss: 98.7043\n",
      "Epoch 85/10000\n",
      "96/96 [==============================] - 0s 499us/step - loss: 98.3190 - val_loss: 97.0428\n",
      "Epoch 86/10000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 100.4348 - val_loss: 96.0116\n",
      "Epoch 87/10000\n",
      "96/96 [==============================] - 0s 572us/step - loss: 99.8087 - val_loss: 96.9052\n",
      "Epoch 88/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 98.7659 - val_loss: 97.9578\n",
      "Epoch 89/10000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 98.9011 - val_loss: 97.6653\n",
      "Epoch 90/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 99.0901 - val_loss: 98.7287\n",
      "Epoch 91/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 98.9664 - val_loss: 98.3560\n",
      "Epoch 92/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 98.7713 - val_loss: 96.4162\n",
      "Epoch 93/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 100.0999 - val_loss: 95.9224\n",
      "Epoch 94/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 99.2463 - val_loss: 96.7036\n",
      "Epoch 95/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 99.9961 - val_loss: 100.4159\n",
      "Epoch 96/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 99.9212 - val_loss: 100.0766\n",
      "Epoch 97/10000\n",
      "96/96 [==============================] - 0s 508us/step - loss: 99.0626 - val_loss: 97.4659\n",
      "Epoch 98/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 98.4692 - val_loss: 95.5195\n",
      "Epoch 99/10000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 97.9307 - val_loss: 95.1965\n",
      "Epoch 100/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 101.1616 - val_loss: 96.6180\n",
      "Epoch 101/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 101.7932 - val_loss: 100.7579\n",
      "Epoch 102/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 100.7995 - val_loss: 100.4136\n",
      "Epoch 103/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 100.0105 - val_loss: 100.2636\n",
      "Epoch 104/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 99.9309 - val_loss: 98.6254\n",
      "Epoch 105/10000\n",
      "96/96 [==============================] - 0s 511us/step - loss: 98.2821 - val_loss: 96.6223\n",
      "Epoch 106/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 100.6841 - val_loss: 95.9452\n",
      "Epoch 107/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 100.1993 - val_loss: 96.7737\n",
      "Epoch 108/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 99.4700 - val_loss: 97.0216\n",
      "Epoch 109/10000\n",
      "96/96 [==============================] - 0s 482us/step - loss: 98.5874 - val_loss: 98.1085\n",
      "Epoch 110/10000\n",
      "96/96 [==============================] - 0s 502us/step - loss: 99.0853 - val_loss: 97.9142\n",
      "Epoch 111/10000\n",
      "96/96 [==============================] - 0s 482us/step - loss: 98.5942 - val_loss: 97.9010\n",
      "Epoch 112/10000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 98.6872 - val_loss: 97.3029\n",
      "Epoch 113/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 98.8791 - val_loss: 97.5457\n",
      "Epoch 114/10000\n",
      "96/96 [==============================] - 0s 354us/step - loss: 98.9190 - val_loss: 97.6938\n",
      "Epoch 115/10000\n",
      "96/96 [==============================] - 0s 485us/step - loss: 98.5455 - val_loss: 97.0693\n",
      "Epoch 116/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 98.4110 - val_loss: 97.6761\n",
      "Epoch 117/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 98.4138 - val_loss: 98.1371\n",
      "Epoch 118/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 99.1463 - val_loss: 98.3897\n",
      "Epoch 119/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 99.2596 - val_loss: 97.7804\n",
      "Epoch 120/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 98.7459 - val_loss: 97.5473\n",
      "Epoch 121/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 99.6993 - val_loss: 96.6009\n",
      "Epoch 122/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 100.0568 - val_loss: 97.1863\n",
      "Epoch 123/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 98.9310 - val_loss: 97.6024\n",
      "Epoch 124/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 99.2444 - val_loss: 98.2871\n",
      "Epoch 125/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 99.4277 - val_loss: 98.0634\n",
      "Epoch 126/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 98.9287 - val_loss: 97.5370\n",
      "Epoch 127/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 99.2279 - val_loss: 96.8043\n",
      "Epoch 128/10000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 98.3004 - val_loss: 98.2705\n",
      "Epoch 129/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 99.1891 - val_loss: 97.4866\n",
      "Epoch 130/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 99.5385 - val_loss: 97.3006\n",
      "Epoch 131/10000\n",
      "96/96 [==============================] - 0s 488us/step - loss: 98.1763 - val_loss: 97.0638\n",
      "Epoch 132/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 98.8822 - val_loss: 97.0055\n",
      "Epoch 133/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 98.9458 - val_loss: 97.7549\n",
      "Epoch 134/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 98.3839 - val_loss: 97.5674\n",
      "Epoch 135/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 98.5220 - val_loss: 98.1137\n",
      "Epoch 136/10000\n",
      "96/96 [==============================] - 0s 507us/step - loss: 99.0967 - val_loss: 98.6315\n",
      "Epoch 137/10000\n",
      "96/96 [==============================] - 0s 518us/step - loss: 98.7916 - val_loss: 98.1745\n",
      "Epoch 138/10000\n",
      "96/96 [==============================] - 0s 502us/step - loss: 99.5020 - val_loss: 97.4145\n",
      "Epoch 139/10000\n",
      "96/96 [==============================] - 0s 504us/step - loss: 98.9651 - val_loss: 97.8916\n",
      "Epoch 140/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 98.0338 - val_loss: 98.3944\n",
      "Epoch 141/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 99.3909 - val_loss: 99.9413\n",
      "Epoch 142/10000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 100.4828 - val_loss: 98.7668\n",
      "Epoch 143/10000\n",
      "96/96 [==============================] - 0s 485us/step - loss: 99.2962 - val_loss: 98.1119\n",
      "Epoch 144/10000\n",
      "96/96 [==============================] - 0s 510us/step - loss: 98.2086 - val_loss: 97.6217\n",
      "Epoch 145/10000\n",
      "96/96 [==============================] - 0s 486us/step - loss: 97.6800 - val_loss: 97.5286\n",
      "Epoch 146/10000\n",
      "96/96 [==============================] - 0s 507us/step - loss: 98.8840 - val_loss: 97.7772\n",
      "Epoch 147/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 98.8051 - val_loss: 98.5966\n",
      "Epoch 148/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 98.2062 - val_loss: 98.3983\n",
      "Epoch 149/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 99.1192 - val_loss: 97.7541\n",
      "Epoch 150/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 98.7041 - val_loss: 97.8393\n",
      "Epoch 151/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 98.5834 - val_loss: 98.2884\n",
      "Epoch 152/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 99.4814 - val_loss: 98.7423\n",
      "Epoch 153/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 98.9488 - val_loss: 99.4859\n",
      "Epoch 154/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 98.9998 - val_loss: 97.8124\n",
      "Epoch 155/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 98.7097 - val_loss: 97.2898\n",
      "Epoch 156/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 98.3847 - val_loss: 100.0215\n",
      "Epoch 157/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 104.7665 - val_loss: 99.2278\n",
      "Epoch 158/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 99.6753 - val_loss: 96.1301\n",
      "Epoch 159/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 98.9543 - val_loss: 97.1057\n",
      "Epoch 160/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 98.5405 - val_loss: 98.5318\n",
      "Epoch 161/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 99.0452 - val_loss: 98.9957\n",
      "Epoch 162/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 100.0067 - val_loss: 99.2916\n",
      "Epoch 163/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 100.2414 - val_loss: 97.9707\n",
      "Epoch 164/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 99.1030 - val_loss: 96.7343\n",
      "Epoch 165/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 99.5240 - val_loss: 96.7767\n",
      "Epoch 166/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 100.0929 - val_loss: 97.8278\n",
      "Epoch 167/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 99.6696 - val_loss: 98.4443\n",
      "Epoch 168/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 99.0103 - val_loss: 98.2321\n",
      "Epoch 169/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 98.8034 - val_loss: 98.1468\n",
      "Epoch 170/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 99.7554 - val_loss: 97.5801\n",
      "Epoch 171/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 99.2478 - val_loss: 98.0835\n",
      "Epoch 172/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 98.8526 - val_loss: 98.0086\n",
      "Epoch 173/10000\n",
      "96/96 [==============================] - 0s 487us/step - loss: 98.8919 - val_loss: 97.1437\n",
      "Epoch 174/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 99.1100 - val_loss: 96.9327\n",
      "Epoch 175/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 99.5989 - val_loss: 96.8294\n",
      "Epoch 176/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 99.0791 - val_loss: 98.1111\n",
      "Epoch 177/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 99.6237 - val_loss: 98.9346\n",
      "Epoch 178/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 99.7299 - val_loss: 98.7729\n",
      "Epoch 179/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 99.1401 - val_loss: 98.8425\n",
      "Epoch 180/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 99.2083 - val_loss: 97.9708\n",
      "Epoch 181/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 98.3786 - val_loss: 97.6280\n",
      "Epoch 182/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 98.4288 - val_loss: 97.9912\n",
      "Epoch 183/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 98.9726 - val_loss: 96.9774\n",
      "Epoch 184/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 97.9647 - val_loss: 96.8250\n",
      "Epoch 185/10000\n",
      "96/96 [==============================] - 0s 355us/step - loss: 99.3185 - val_loss: 97.0018\n",
      "Epoch 186/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 98.7947 - val_loss: 97.7697\n",
      "Epoch 187/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 98.8664 - val_loss: 98.4594\n",
      "Epoch 188/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 99.1887 - val_loss: 98.4886\n",
      "Epoch 189/10000\n",
      "96/96 [==============================] - 0s 357us/step - loss: 98.7044 - val_loss: 98.1112\n",
      "Epoch 190/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 99.0118 - val_loss: 97.6205\n",
      "Epoch 191/10000\n",
      "96/96 [==============================] - 0s 356us/step - loss: 98.8260 - val_loss: 97.7541\n",
      "Epoch 192/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 98.1074 - val_loss: 98.1413\n",
      "Epoch 193/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 99.0479 - val_loss: 98.8290\n",
      "Epoch 194/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 99.3413 - val_loss: 99.2342\n",
      "Epoch 195/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 98.5186 - val_loss: 99.0346\n",
      "Epoch 196/10000\n",
      "96/96 [==============================] - 0s 356us/step - loss: 98.6576 - val_loss: 98.5385\n",
      "Epoch 197/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 98.1877 - val_loss: 98.0387\n",
      "Epoch 198/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 99.1160 - val_loss: 98.7686\n",
      "Epoch 199/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 98.9770 - val_loss: 98.9889\n",
      "Epoch 200/10000\n",
      "96/96 [==============================] - 0s 523us/step - loss: 98.4864 - val_loss: 98.9806\n",
      "Epoch 201/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 99.5355 - val_loss: 97.7123\n",
      "Epoch 202/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 97.9791 - val_loss: 97.6741\n",
      "Epoch 203/10000\n",
      "96/96 [==============================] - 0s 355us/step - loss: 99.0405 - val_loss: 96.6805\n",
      "Epoch 204/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 99.5801 - val_loss: 96.9796\n",
      "Epoch 205/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 98.1300 - val_loss: 97.9592\n",
      "Epoch 206/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 98.9789 - val_loss: 98.7280\n",
      "Epoch 207/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 99.0135 - val_loss: 100.1694\n",
      "Epoch 208/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 98.9950 - val_loss: 98.1008\n",
      "Epoch 209/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 98.3099 - val_loss: 97.9804\n",
      "Epoch 210/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 97.1445 - val_loss: 98.3287\n",
      "Epoch 211/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 98.8623 - val_loss: 97.5663\n",
      "Epoch 212/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 99.4915 - val_loss: 95.7350\n",
      "Epoch 213/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 101.6074 - val_loss: 95.9647\n",
      "Epoch 214/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 100.1235 - val_loss: 97.1662\n",
      "Epoch 215/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 98.8692 - val_loss: 98.4919\n",
      "Epoch 216/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 100.4839 - val_loss: 99.7030\n",
      "Epoch 217/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 98.9988 - val_loss: 98.5322\n",
      "Epoch 218/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 98.1292 - val_loss: 97.2161\n",
      "Epoch 219/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 99.1744 - val_loss: 96.3260\n",
      "Epoch 220/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 99.3042 - val_loss: 97.5129\n",
      "Epoch 221/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 99.8587 - val_loss: 98.8361\n",
      "Epoch 222/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 99.6272 - val_loss: 97.9954\n",
      "Epoch 223/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 99.7706 - val_loss: 98.1624\n",
      "Epoch 224/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 98.6358 - val_loss: 98.0285\n",
      "Epoch 225/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 98.4057 - val_loss: 97.6508\n",
      "Epoch 226/10000\n",
      "96/96 [==============================] - 0s 349us/step - loss: 98.4463 - val_loss: 97.8049\n",
      "Epoch 227/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 98.0516 - val_loss: 99.2975\n",
      "Epoch 228/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 98.6745 - val_loss: 99.7022\n",
      "Epoch 229/10000\n",
      "96/96 [==============================] - 0s 516us/step - loss: 99.0134 - val_loss: 99.8598\n",
      "Epoch 230/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 399us/step - loss: 99.0548 - val_loss: 99.8695\n",
      "Epoch 231/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 98.2651 - val_loss: 98.7037\n",
      "Epoch 232/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 98.3826 - val_loss: 97.9548\n",
      "Epoch 233/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 97.9319 - val_loss: 96.6362\n",
      "Epoch 234/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 100.9770 - val_loss: 99.5633\n",
      "Epoch 235/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 98.6162 - val_loss: 99.7534\n",
      "Epoch 236/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 98.3532 - val_loss: 99.0410\n",
      "Epoch 237/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 96.9433 - val_loss: 98.3200\n",
      "Epoch 238/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 97.8452 - val_loss: 98.9677\n",
      "Epoch 239/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 97.3600 - val_loss: 98.9006\n",
      "Epoch 240/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 98.9737 - val_loss: 97.6819\n",
      "Epoch 241/10000\n",
      "96/96 [==============================] - 0s 353us/step - loss: 98.0257 - val_loss: 99.0509\n",
      "Epoch 242/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 97.9753 - val_loss: 99.8762\n",
      "Epoch 243/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 97.9596 - val_loss: 99.4950\n",
      "Epoch 244/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 97.7552 - val_loss: 97.1890\n",
      "Epoch 245/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 97.3780 - val_loss: 98.1844\n",
      "Epoch 246/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 97.8857 - val_loss: 99.7172\n",
      "Epoch 247/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 100.0395 - val_loss: 97.9817\n",
      "Epoch 248/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 97.9134 - val_loss: 98.1409\n",
      "Epoch 249/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 97.0599 - val_loss: 99.0413\n",
      "Epoch 250/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 97.7955 - val_loss: 98.1345\n",
      "Epoch 251/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 97.6208 - val_loss: 99.3152\n",
      "Epoch 252/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 96.2273 - val_loss: 99.3497\n",
      "Epoch 253/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 98.4620 - val_loss: 100.4583\n",
      "Epoch 254/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 98.6089 - val_loss: 98.7220\n",
      "Epoch 255/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 96.6833 - val_loss: 98.2830\n",
      "Epoch 256/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 101.1378 - val_loss: 98.5669\n",
      "Epoch 257/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 98.1824 - val_loss: 98.0995\n",
      "Epoch 258/10000\n",
      "96/96 [==============================] - 0s 346us/step - loss: 99.3548 - val_loss: 97.1019\n",
      "Epoch 259/10000\n",
      "96/96 [==============================] - 0s 343us/step - loss: 98.4059 - val_loss: 97.0463\n",
      "Epoch 260/10000\n",
      "96/96 [==============================] - 0s 346us/step - loss: 98.3261 - val_loss: 97.4387\n",
      "Epoch 261/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 99.0276 - val_loss: 98.1805\n",
      "Epoch 262/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 97.9680 - val_loss: 98.7071\n",
      "Epoch 263/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 99.3487 - val_loss: 99.9767\n",
      "Epoch 264/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 98.3218 - val_loss: 99.5419\n",
      "Epoch 265/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 99.4169 - val_loss: 98.2544\n",
      "Epoch 266/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 98.5430 - val_loss: 97.9809\n",
      "Epoch 267/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 98.6154 - val_loss: 98.7327\n",
      "Epoch 268/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 99.9821 - val_loss: 98.1734\n",
      "Epoch 269/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 98.5401 - val_loss: 98.1303\n",
      "Epoch 270/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 97.5018 - val_loss: 98.3693\n",
      "Epoch 271/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 97.3058 - val_loss: 98.7250\n",
      "Epoch 272/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 97.4377 - val_loss: 99.2183\n",
      "Epoch 273/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 98.8102 - val_loss: 99.0783\n",
      "Epoch 274/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 98.2800 - val_loss: 97.5223\n",
      "Epoch 275/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 96.2950 - val_loss: 96.8939\n",
      "Epoch 276/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 99.4530 - val_loss: 95.8421\n",
      "Epoch 277/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 99.9725 - val_loss: 95.6098\n",
      "Epoch 278/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 99.6702 - val_loss: 96.7528\n",
      "Epoch 279/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 98.3967 - val_loss: 97.6270\n",
      "Epoch 280/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 97.6036 - val_loss: 98.6507\n",
      "Epoch 281/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 98.9590 - val_loss: 98.0421\n",
      "Epoch 282/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 96.7367 - val_loss: 98.4536\n",
      "Epoch 283/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 97.0844 - val_loss: 98.8966\n",
      "Epoch 284/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 96.9750 - val_loss: 99.7578\n",
      "Epoch 285/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 97.7923 - val_loss: 98.2225\n",
      "Epoch 286/10000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 97.8185 - val_loss: 97.5413\n",
      "Epoch 287/10000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 98.4829 - val_loss: 99.6091\n",
      "Epoch 288/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 97.9784 - val_loss: 96.3800\n",
      "Epoch 289/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 102.7369 - val_loss: 97.8078\n",
      "Epoch 290/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 97.8774 - val_loss: 97.7902\n",
      "Epoch 291/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 97.8219 - val_loss: 97.2199\n",
      "Epoch 292/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 98.6122 - val_loss: 98.5511\n",
      "Epoch 293/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 97.7853 - val_loss: 98.0503\n",
      "Epoch 294/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 95.9655 - val_loss: 97.3640\n",
      "Epoch 295/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 98.4533 - val_loss: 98.4127\n",
      "Epoch 296/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 98.1453 - val_loss: 97.8677\n",
      "Epoch 297/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 99.2377 - val_loss: 98.6707\n",
      "Epoch 298/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 96.8545 - val_loss: 99.7353\n",
      "Epoch 299/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 99.3375 - val_loss: 98.7618\n",
      "Epoch 300/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 100.3068 - val_loss: 98.6130\n",
      "Epoch 301/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 98.6339 - val_loss: 98.4856\n",
      "Epoch 302/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 97.1343 - val_loss: 97.5793\n",
      "Epoch 303/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 96.4951 - val_loss: 98.1151\n",
      "Epoch 304/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 97.9267 - val_loss: 97.8843\n",
      "Epoch 305/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 99.8857 - val_loss: 97.2245\n",
      "Epoch 306/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 97.6748 - val_loss: 97.7038\n",
      "Epoch 307/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 97.3131 - val_loss: 98.3295\n",
      "Epoch 308/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 97.8820 - val_loss: 99.6730\n",
      "Epoch 309/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 98.9559 - val_loss: 98.5803\n",
      "Epoch 310/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 99.5091 - val_loss: 97.8601\n",
      "Epoch 311/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 98.0924 - val_loss: 97.1303\n",
      "Epoch 312/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 97.9391 - val_loss: 97.2797\n",
      "Epoch 313/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 97.8431 - val_loss: 98.0420\n",
      "Epoch 314/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 97.9946 - val_loss: 99.3287\n",
      "Epoch 315/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 96.6880 - val_loss: 99.3561\n",
      "Epoch 316/10000\n",
      "96/96 [==============================] - 0s 502us/step - loss: 97.0212 - val_loss: 99.2486\n",
      "Epoch 317/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 97.1705 - val_loss: 97.7645\n",
      "Epoch 318/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 97.0116 - val_loss: 96.9685\n",
      "Epoch 319/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 96.7778 - val_loss: 95.8683\n",
      "Epoch 320/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 96.3885 - val_loss: 97.5217\n",
      "Epoch 321/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 97.0496 - val_loss: 96.6950\n",
      "Epoch 322/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 98.0007 - val_loss: 97.6645\n",
      "Epoch 323/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 96.0161 - val_loss: 97.6222\n",
      "Epoch 324/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 96.4912 - val_loss: 98.7345\n",
      "Epoch 325/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 94.9363 - val_loss: 96.2246\n",
      "Epoch 326/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 98.2094 - val_loss: 96.3891\n",
      "Epoch 327/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 100.1113 - val_loss: 96.7533\n",
      "Epoch 328/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 96.6037 - val_loss: 97.2809\n",
      "Epoch 329/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 98.6174 - val_loss: 95.8254\n",
      "Epoch 330/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 99.1060 - val_loss: 97.3238\n",
      "Epoch 331/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 97.9010 - val_loss: 97.6462\n",
      "Epoch 332/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 96.8045 - val_loss: 99.5288\n",
      "Epoch 333/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 101.7436 - val_loss: 97.6330\n",
      "Epoch 334/10000\n",
      "96/96 [==============================] - 0s 353us/step - loss: 98.4492 - val_loss: 96.6964\n",
      "Epoch 335/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 98.2147 - val_loss: 96.4775\n",
      "Epoch 336/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 96.9240 - val_loss: 96.7429\n",
      "Epoch 337/10000\n",
      "96/96 [==============================] - 0s 349us/step - loss: 98.1172 - val_loss: 97.7806\n",
      "Epoch 338/10000\n",
      "96/96 [==============================] - 0s 354us/step - loss: 96.1027 - val_loss: 98.4919\n",
      "Epoch 339/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 98.8203 - val_loss: 96.9254\n",
      "Epoch 340/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 99.1465 - val_loss: 96.8476\n",
      "Epoch 341/10000\n",
      "96/96 [==============================] - 0s 357us/step - loss: 100.8978 - val_loss: 99.5265\n",
      "Epoch 342/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 101.4944 - val_loss: 98.4415\n",
      "Epoch 343/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 99.3767 - val_loss: 97.7228\n",
      "Epoch 344/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 97.3875 - val_loss: 95.9757\n",
      "Epoch 345/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 97.1134 - val_loss: 95.6265\n",
      "Epoch 346/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 99.7013 - val_loss: 95.7324\n",
      "Epoch 347/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 96.2577 - val_loss: 95.0930\n",
      "Epoch 348/10000\n",
      "96/96 [==============================] - 0s 506us/step - loss: 97.4838 - val_loss: 97.4096\n",
      "Epoch 349/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 98.2132 - val_loss: 95.9749\n",
      "Epoch 350/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 100.4248 - val_loss: 94.7854\n",
      "Epoch 351/10000\n",
      "96/96 [==============================] - 0s 351us/step - loss: 96.6417 - val_loss: 95.8112\n",
      "Epoch 352/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 100.3275 - val_loss: 96.2719\n",
      "Epoch 353/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 96.1474 - val_loss: 97.6275\n",
      "Epoch 354/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 99.4642 - val_loss: 96.8304\n",
      "Epoch 355/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 97.9571 - val_loss: 97.6984\n",
      "Epoch 356/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 99.6488 - val_loss: 98.7641\n",
      "Epoch 357/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 98.4337 - val_loss: 98.5817\n",
      "Epoch 358/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 98.2054 - val_loss: 98.0638\n",
      "Epoch 359/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 97.6067 - val_loss: 97.9784\n",
      "Epoch 360/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 96.3366 - val_loss: 98.1933\n",
      "Epoch 361/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 98.9714 - val_loss: 97.5549\n",
      "Epoch 362/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 96.1823 - val_loss: 97.5885\n",
      "Epoch 363/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 95.9131 - val_loss: 98.0853\n",
      "Epoch 364/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 100.5394 - val_loss: 97.0808\n",
      "Epoch 365/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 100.4846 - val_loss: 98.2445\n",
      "Epoch 366/10000\n",
      "96/96 [==============================] - 0s 354us/step - loss: 95.5636 - val_loss: 98.1117\n",
      "Epoch 367/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 94.7833 - val_loss: 97.7732\n",
      "Epoch 368/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 94.6385 - val_loss: 97.5716\n",
      "Epoch 369/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 96.4439 - val_loss: 97.7413\n",
      "Epoch 370/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 97.0787 - val_loss: 97.6032\n",
      "Epoch 371/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 97.0022 - val_loss: 97.7393\n",
      "Epoch 372/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 96.6009 - val_loss: 98.1615\n",
      "Epoch 373/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 98.5269 - val_loss: 99.2640\n",
      "Epoch 374/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 96.9235 - val_loss: 98.0052\n",
      "Epoch 375/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 96.3549 - val_loss: 97.1571\n",
      "Epoch 376/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 98.4427 - val_loss: 95.6548\n",
      "Epoch 377/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 94.9098 - val_loss: 95.0187\n",
      "Epoch 378/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 97.6926 - val_loss: 96.3519\n",
      "Epoch 379/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 99.5565 - val_loss: 96.1537\n",
      "Epoch 380/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 98.7116 - val_loss: 96.5594\n",
      "Epoch 381/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 96.8488 - val_loss: 97.1471\n",
      "Epoch 382/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 390us/step - loss: 96.5529 - val_loss: 98.5509\n",
      "Epoch 383/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 98.8705 - val_loss: 99.5077\n",
      "Epoch 384/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 99.2797 - val_loss: 98.7698\n",
      "Epoch 385/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 96.8129 - val_loss: 97.4560\n",
      "Epoch 386/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 100.2632 - val_loss: 95.1058\n",
      "Epoch 387/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 98.0398 - val_loss: 95.6668\n",
      "Epoch 388/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 100.6695 - val_loss: 97.2536\n",
      "Epoch 389/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 96.7291 - val_loss: 97.1523\n",
      "Epoch 390/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 98.2354 - val_loss: 97.7094\n",
      "Epoch 391/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 97.5479 - val_loss: 97.7858\n",
      "Epoch 392/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 96.5129 - val_loss: 97.0350\n",
      "Epoch 393/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 97.5703 - val_loss: 97.0132\n",
      "Epoch 394/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 98.0684 - val_loss: 97.5417\n",
      "Epoch 395/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 96.5365 - val_loss: 97.9573\n",
      "Epoch 396/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 104.3218 - val_loss: 97.8240\n",
      "Epoch 397/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 96.3197 - val_loss: 97.8763\n",
      "Epoch 398/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 97.1381 - val_loss: 97.3785\n",
      "Epoch 399/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 96.2459 - val_loss: 97.2131\n",
      "Epoch 400/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 97.2976 - val_loss: 96.5891\n",
      "Epoch 401/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 96.9901 - val_loss: 96.0904\n",
      "Epoch 402/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 94.5425 - val_loss: 96.4846\n",
      "Epoch 403/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 96.4547 - val_loss: 97.0010\n",
      "Epoch 404/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 97.8121 - val_loss: 95.1959\n",
      "Epoch 405/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 95.9762 - val_loss: 95.3338\n",
      "Epoch 406/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 96.6260 - val_loss: 98.1047\n",
      "Epoch 407/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 98.9324 - val_loss: 97.9494\n",
      "Epoch 408/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 96.5796 - val_loss: 97.5907\n",
      "Epoch 409/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 97.5162 - val_loss: 97.4460\n",
      "Epoch 410/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 94.7318 - val_loss: 97.1012\n",
      "Epoch 411/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 96.5816 - val_loss: 97.3585\n",
      "Epoch 412/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 100.2684 - val_loss: 97.7552\n",
      "Epoch 413/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 99.6724 - val_loss: 99.1415\n",
      "Epoch 414/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 97.8748 - val_loss: 98.5742\n",
      "Epoch 415/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 98.2516 - val_loss: 98.1428\n",
      "Epoch 416/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 97.9987 - val_loss: 97.6558\n",
      "Epoch 417/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 96.4426 - val_loss: 96.9827\n",
      "Epoch 418/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 97.0965 - val_loss: 96.0015\n",
      "Epoch 419/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 97.4114 - val_loss: 97.1419\n",
      "Epoch 420/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 96.5969 - val_loss: 98.3077\n",
      "Epoch 421/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 97.1523 - val_loss: 98.3337\n",
      "Epoch 422/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 96.9486 - val_loss: 98.8744\n",
      "Epoch 423/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 97.2428 - val_loss: 97.2397\n",
      "Epoch 424/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 98.3239 - val_loss: 95.4613\n",
      "Epoch 425/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 95.8595 - val_loss: 96.5380\n",
      "Epoch 426/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 97.3045 - val_loss: 96.4062\n",
      "Epoch 427/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 96.5073 - val_loss: 96.2564\n",
      "Epoch 428/10000\n",
      "96/96 [==============================] - 0s 525us/step - loss: 96.5613 - val_loss: 96.5500\n",
      "Epoch 429/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 97.4312 - val_loss: 98.9252\n",
      "Epoch 430/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 97.5279 - val_loss: 98.2591\n",
      "Epoch 431/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 97.6785 - val_loss: 97.6646\n",
      "Epoch 432/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 96.5640 - val_loss: 96.2719\n",
      "Epoch 433/10000\n",
      "96/96 [==============================] - 0s 487us/step - loss: 97.4324 - val_loss: 96.7177\n",
      "Epoch 434/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 97.7012 - val_loss: 96.0893\n",
      "Epoch 435/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 97.0549 - val_loss: 97.6426\n",
      "Epoch 436/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 96.6145 - val_loss: 97.5799\n",
      "Epoch 437/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 97.0592 - val_loss: 97.9541\n",
      "Epoch 438/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 97.4591 - val_loss: 96.8202\n",
      "Epoch 439/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 99.6346 - val_loss: 96.4546\n",
      "Epoch 440/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 99.4638 - val_loss: 97.0002\n",
      "Epoch 441/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 97.4917 - val_loss: 97.3587\n",
      "Epoch 442/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 97.0160 - val_loss: 97.7057\n",
      "Epoch 443/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 96.4527 - val_loss: 98.0087\n",
      "Epoch 444/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 97.0595 - val_loss: 96.8626\n",
      "Epoch 445/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 97.6999 - val_loss: 96.9710\n",
      "Epoch 446/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 96.1446 - val_loss: 96.9160\n",
      "Epoch 447/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 96.5551 - val_loss: 97.4641\n",
      "Epoch 448/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 98.5338 - val_loss: 98.0289\n",
      "Epoch 449/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 96.3388 - val_loss: 98.2947\n",
      "Epoch 450/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 96.9719 - val_loss: 97.9611\n",
      "Epoch 451/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 102.0295 - val_loss: 98.2424\n",
      "Epoch 452/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 95.8542 - val_loss: 96.4819\n",
      "Epoch 453/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 96.2483 - val_loss: 97.1322\n",
      "Epoch 454/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 97.1144 - val_loss: 97.3017\n",
      "Epoch 455/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 97.3962 - val_loss: 97.9816\n",
      "Epoch 456/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 97.6575 - val_loss: 97.6106\n",
      "Epoch 457/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 96.2730 - val_loss: 97.8892\n",
      "Epoch 458/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 97.4724 - val_loss: 98.2652\n",
      "Epoch 459/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 95.7009 - val_loss: 97.3071\n",
      "Epoch 460/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 96.1105 - val_loss: 96.8297\n",
      "Epoch 461/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 96.2073 - val_loss: 97.0216\n",
      "Epoch 462/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 98.2860 - val_loss: 96.6552\n",
      "Epoch 463/10000\n",
      "96/96 [==============================] - 0s 495us/step - loss: 95.2344 - val_loss: 97.4037\n",
      "Epoch 464/10000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 95.9556 - val_loss: 97.4106\n",
      "Epoch 465/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 95.5773 - val_loss: 96.5923\n",
      "Epoch 466/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 95.3756 - val_loss: 96.5793\n",
      "Epoch 467/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 99.6318 - val_loss: 95.5169\n",
      "Epoch 468/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 97.1181 - val_loss: 96.9718\n",
      "Epoch 469/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 95.8253 - val_loss: 98.7813\n",
      "Epoch 470/10000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 96.9112 - val_loss: 97.7148\n",
      "Epoch 471/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 97.0356 - val_loss: 98.2020\n",
      "Epoch 472/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 98.0615 - val_loss: 97.9412\n",
      "Epoch 473/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 95.8557 - val_loss: 96.4979\n",
      "Epoch 474/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 95.5348 - val_loss: 96.3058\n",
      "Epoch 475/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 97.5092 - val_loss: 96.6604\n",
      "Epoch 476/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 95.9771 - val_loss: 96.7375\n",
      "Epoch 477/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 98.6594 - val_loss: 95.8984\n",
      "Epoch 478/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 99.7365 - val_loss: 96.9878\n",
      "Epoch 479/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 97.9132 - val_loss: 96.9273\n",
      "Epoch 480/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 95.8821 - val_loss: 95.5254\n",
      "Epoch 481/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 101.7273 - val_loss: 96.5301\n",
      "Epoch 482/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 95.5214 - val_loss: 97.1419\n",
      "Epoch 483/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 95.9748 - val_loss: 97.3681\n",
      "Epoch 484/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 96.9726 - val_loss: 96.8333\n",
      "Epoch 485/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 96.6259 - val_loss: 97.2784\n",
      "Epoch 486/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 96.2415 - val_loss: 96.9362\n",
      "Epoch 487/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 95.1412 - val_loss: 97.2861\n",
      "Epoch 488/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 95.4865 - val_loss: 97.9923\n",
      "Epoch 489/10000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 95.7733 - val_loss: 97.4857\n",
      "Epoch 490/10000\n",
      "96/96 [==============================] - 0s 514us/step - loss: 96.7394 - val_loss: 97.5922\n",
      "Epoch 491/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 98.6681 - val_loss: 96.0659\n",
      "Epoch 492/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 96.5655 - val_loss: 96.4507\n",
      "Epoch 493/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 95.3119 - val_loss: 96.3653\n",
      "Epoch 494/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 96.6067 - val_loss: 96.1325\n",
      "Epoch 495/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 97.3970 - val_loss: 95.9715\n",
      "Epoch 496/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 96.4089 - val_loss: 96.6664\n",
      "Epoch 497/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 95.5515 - val_loss: 96.8215\n",
      "Epoch 498/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 96.9816 - val_loss: 97.5965\n",
      "Epoch 499/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 94.1659 - val_loss: 95.4313\n",
      "Epoch 500/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 97.8191 - val_loss: 95.8219\n",
      "Epoch 501/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 95.5931 - val_loss: 95.2294\n",
      "Epoch 502/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 96.0792 - val_loss: 97.6154\n",
      "Epoch 503/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 97.1064 - val_loss: 96.8969\n",
      "Epoch 504/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 95.4667 - val_loss: 96.1281\n",
      "Epoch 505/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 97.0592 - val_loss: 95.2450\n",
      "Epoch 506/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 95.5334 - val_loss: 95.5372\n",
      "Epoch 507/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 96.2561 - val_loss: 96.4068\n",
      "Epoch 508/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 99.2791 - val_loss: 97.6236\n",
      "Epoch 509/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 96.3223 - val_loss: 98.4190\n",
      "Epoch 510/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 95.8198 - val_loss: 96.7886\n",
      "Epoch 511/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 93.9764 - val_loss: 96.3961\n",
      "Epoch 512/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 97.9094 - val_loss: 95.0897\n",
      "Epoch 513/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 101.4744 - val_loss: 95.0482\n",
      "Epoch 514/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 95.8603 - val_loss: 96.8739\n",
      "Epoch 515/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 96.0312 - val_loss: 97.3517\n",
      "Epoch 516/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 97.2104 - val_loss: 97.6940\n",
      "Epoch 517/10000\n",
      "96/96 [==============================] - 0s 488us/step - loss: 96.5007 - val_loss: 95.8087\n",
      "Epoch 518/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 95.9705 - val_loss: 95.0490\n",
      "Epoch 519/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 97.7921 - val_loss: 95.3155\n",
      "Epoch 520/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 94.9218 - val_loss: 95.5939\n",
      "Epoch 521/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 95.6080 - val_loss: 96.4292\n",
      "Epoch 522/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 97.5397 - val_loss: 95.8931\n",
      "Epoch 523/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 96.8519 - val_loss: 95.8929\n",
      "Epoch 524/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 94.0059 - val_loss: 97.1848\n",
      "Epoch 525/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 95.0737 - val_loss: 95.7166\n",
      "Epoch 526/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 95.4492 - val_loss: 95.5993\n",
      "Epoch 527/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 96.4415 - val_loss: 96.7401\n",
      "Epoch 528/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 93.9502 - val_loss: 97.4990\n",
      "Epoch 529/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 93.8139 - val_loss: 94.9251\n",
      "Epoch 530/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 94.9112 - val_loss: 95.1571\n",
      "Epoch 531/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 97.9414 - val_loss: 98.3013\n",
      "Epoch 532/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 98.1275 - val_loss: 95.0344\n",
      "Epoch 533/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 96.2066 - val_loss: 94.3186\n",
      "Epoch 534/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 470us/step - loss: 95.2349 - val_loss: 94.4052\n",
      "Epoch 535/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 94.9696 - val_loss: 95.8934\n",
      "Epoch 536/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 95.1246 - val_loss: 94.9718\n",
      "Epoch 537/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 95.4848 - val_loss: 96.5342\n",
      "Epoch 538/10000\n",
      "96/96 [==============================] - 0s 497us/step - loss: 95.5308 - val_loss: 96.8071\n",
      "Epoch 539/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 95.0083 - val_loss: 95.9692\n",
      "Epoch 540/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 95.9183 - val_loss: 96.1800\n",
      "Epoch 541/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 96.4149 - val_loss: 94.8981\n",
      "Epoch 542/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 94.9036 - val_loss: 94.5325\n",
      "Epoch 543/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 95.6850 - val_loss: 95.4913\n",
      "Epoch 544/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 95.6699 - val_loss: 95.1514\n",
      "Epoch 545/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 96.6251 - val_loss: 95.3492\n",
      "Epoch 546/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 94.8185 - val_loss: 95.7079\n",
      "Epoch 547/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 97.6179 - val_loss: 95.7865\n",
      "Epoch 548/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 95.1794 - val_loss: 96.9421\n",
      "Epoch 549/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 94.8202 - val_loss: 96.0232\n",
      "Epoch 550/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 93.6432 - val_loss: 94.9798\n",
      "Epoch 551/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 95.6296 - val_loss: 94.4135\n",
      "Epoch 552/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 95.7973 - val_loss: 94.9080\n",
      "Epoch 553/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 95.1721 - val_loss: 95.1150\n",
      "Epoch 554/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 95.5765 - val_loss: 96.1268\n",
      "Epoch 555/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 94.0383 - val_loss: 95.8233\n",
      "Epoch 556/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 94.3565 - val_loss: 95.9151\n",
      "Epoch 557/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 95.8756 - val_loss: 94.2817\n",
      "Epoch 558/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 95.7306 - val_loss: 95.1871\n",
      "Epoch 559/10000\n",
      "96/96 [==============================] - 0s 572us/step - loss: 94.6278 - val_loss: 95.8022\n",
      "Epoch 560/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 96.6959 - val_loss: 95.3480\n",
      "Epoch 561/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 93.4620 - val_loss: 95.0530\n",
      "Epoch 562/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 97.0084 - val_loss: 94.8415\n",
      "Epoch 563/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 93.2619 - val_loss: 93.7828\n",
      "Epoch 564/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 95.2903 - val_loss: 94.6376\n",
      "Epoch 565/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 95.8333 - val_loss: 95.1857\n",
      "Epoch 566/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 93.9606 - val_loss: 96.5660\n",
      "Epoch 567/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 95.1902 - val_loss: 95.0210\n",
      "Epoch 568/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 94.1772 - val_loss: 94.2898\n",
      "Epoch 569/10000\n",
      "96/96 [==============================] - 0s 354us/step - loss: 96.4927 - val_loss: 94.5350\n",
      "Epoch 570/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 95.8717 - val_loss: 94.4115\n",
      "Epoch 571/10000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 94.9974 - val_loss: 94.4230\n",
      "Epoch 572/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 95.9450 - val_loss: 95.0843\n",
      "Epoch 573/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 93.7843 - val_loss: 95.4563\n",
      "Epoch 574/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 95.5733 - val_loss: 95.1777\n",
      "Epoch 575/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 93.9306 - val_loss: 96.1459\n",
      "Epoch 576/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 95.4563 - val_loss: 95.6422\n",
      "Epoch 577/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 95.0861 - val_loss: 95.9442\n",
      "Epoch 578/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 95.3344 - val_loss: 96.2408\n",
      "Epoch 579/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 95.7873 - val_loss: 95.3244\n",
      "Epoch 580/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 95.7165 - val_loss: 95.1617\n",
      "Epoch 581/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 96.1692 - val_loss: 95.2444\n",
      "Epoch 582/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 95.1653 - val_loss: 95.6635\n",
      "Epoch 583/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 94.1833 - val_loss: 95.4541\n",
      "Epoch 584/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 93.5492 - val_loss: 95.4743\n",
      "Epoch 585/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 94.6802 - val_loss: 96.8421\n",
      "Epoch 586/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 95.2236 - val_loss: 95.5370\n",
      "Epoch 587/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 94.6176 - val_loss: 95.0214\n",
      "Epoch 588/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 95.8670 - val_loss: 96.1068\n",
      "Epoch 589/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 93.8191 - val_loss: 94.5441\n",
      "Epoch 590/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 94.3006 - val_loss: 94.6169\n",
      "Epoch 591/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 97.2975 - val_loss: 95.7621\n",
      "Epoch 592/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 95.8791 - val_loss: 95.7771\n",
      "Epoch 593/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 95.6517 - val_loss: 96.4713\n",
      "Epoch 594/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 95.6811 - val_loss: 95.1942\n",
      "Epoch 595/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 95.1151 - val_loss: 94.8274\n",
      "Epoch 596/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 97.3959 - val_loss: 94.3829\n",
      "Epoch 597/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 97.5181 - val_loss: 94.9565\n",
      "Epoch 598/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 96.0358 - val_loss: 95.9779\n",
      "Epoch 599/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 93.5574 - val_loss: 96.7396\n",
      "Epoch 600/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 94.8190 - val_loss: 94.8980\n",
      "Epoch 601/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 94.9677 - val_loss: 95.0800\n",
      "Epoch 602/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 94.8695 - val_loss: 94.6683\n",
      "Epoch 603/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 94.9021 - val_loss: 95.0777\n",
      "Epoch 604/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 93.3236 - val_loss: 94.3427\n",
      "Epoch 605/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 94.4281 - val_loss: 94.1506\n",
      "Epoch 606/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 97.4253 - val_loss: 94.8914\n",
      "Epoch 607/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 96.6572 - val_loss: 94.7146\n",
      "Epoch 608/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 95.2407 - val_loss: 96.3845\n",
      "Epoch 609/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 95.5782 - val_loss: 94.5657\n",
      "Epoch 610/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 94.6801 - val_loss: 94.5212\n",
      "Epoch 611/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 93.6407 - val_loss: 95.0414\n",
      "Epoch 612/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 95.3560 - val_loss: 94.8611\n",
      "Epoch 613/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 95.1996 - val_loss: 95.2871\n",
      "Epoch 614/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 97.8906 - val_loss: 95.6220\n",
      "Epoch 615/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 94.6999 - val_loss: 95.7885\n",
      "Epoch 616/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 93.0469 - val_loss: 94.6296\n",
      "Epoch 617/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 94.4748 - val_loss: 94.4736\n",
      "Epoch 618/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 95.8434 - val_loss: 93.8469\n",
      "Epoch 619/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 93.0419 - val_loss: 94.7029\n",
      "Epoch 620/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 92.5021 - val_loss: 96.0131\n",
      "Epoch 621/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 95.8624 - val_loss: 95.8141\n",
      "Epoch 622/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 96.1809 - val_loss: 94.6789\n",
      "Epoch 623/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 94.9853 - val_loss: 94.1268\n",
      "Epoch 624/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 95.3757 - val_loss: 94.6011\n",
      "Epoch 625/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 92.8858 - val_loss: 95.7555\n",
      "Epoch 626/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 95.4712 - val_loss: 94.6725\n",
      "Epoch 627/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 96.3857 - val_loss: 95.4050\n",
      "Epoch 628/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 94.4584 - val_loss: 95.4826\n",
      "Epoch 629/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 94.3472 - val_loss: 96.2784\n",
      "Epoch 630/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 95.1680 - val_loss: 95.8259\n",
      "Epoch 631/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 94.9318 - val_loss: 95.5095\n",
      "Epoch 632/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 94.5386 - val_loss: 95.1191\n",
      "Epoch 633/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 95.3496 - val_loss: 95.3239\n",
      "Epoch 634/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 94.1595 - val_loss: 95.4205\n",
      "Epoch 635/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 94.1440 - val_loss: 94.4388\n",
      "Epoch 636/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 94.9718 - val_loss: 94.3717\n",
      "Epoch 637/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 93.1677 - val_loss: 94.8467\n",
      "Epoch 638/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 95.2917 - val_loss: 94.5571\n",
      "Epoch 639/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 94.1110 - val_loss: 95.6922\n",
      "Epoch 640/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 94.2168 - val_loss: 95.3903\n",
      "Epoch 641/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 94.7269 - val_loss: 95.4327\n",
      "Epoch 642/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 97.6051 - val_loss: 95.7634\n",
      "Epoch 643/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 93.1361 - val_loss: 94.9254\n",
      "Epoch 644/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 92.8195 - val_loss: 94.5716\n",
      "Epoch 645/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 94.3338 - val_loss: 96.1236\n",
      "Epoch 646/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 92.5980 - val_loss: 95.6926\n",
      "Epoch 647/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 95.8086 - val_loss: 95.3691\n",
      "Epoch 648/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 93.1830 - val_loss: 95.0643\n",
      "Epoch 649/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 93.6147 - val_loss: 96.2108\n",
      "Epoch 650/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 93.7947 - val_loss: 96.4088\n",
      "Epoch 651/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 98.2514 - val_loss: 94.6530\n",
      "Epoch 652/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 94.5831 - val_loss: 94.4030\n",
      "Epoch 653/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 94.7454 - val_loss: 95.3524\n",
      "Epoch 654/10000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 96.1648 - val_loss: 96.2508\n",
      "Epoch 655/10000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 95.3716 - val_loss: 94.2508\n",
      "Epoch 656/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 94.4802 - val_loss: 94.6454\n",
      "Epoch 657/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 97.3340 - val_loss: 95.2901\n",
      "Epoch 658/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 93.2609 - val_loss: 96.8120\n",
      "Epoch 659/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 96.5423 - val_loss: 96.0328\n",
      "Epoch 660/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 94.9226 - val_loss: 96.4003\n",
      "Epoch 661/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 98.7559 - val_loss: 96.5614\n",
      "Epoch 662/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 97.3908 - val_loss: 95.6966\n",
      "Epoch 663/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 100.4492 - val_loss: 95.0598\n",
      "Epoch 664/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 93.1659 - val_loss: 95.5556\n",
      "Epoch 665/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 95.9622 - val_loss: 95.2016\n",
      "Epoch 666/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 93.7412 - val_loss: 95.4018\n",
      "Epoch 667/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 95.1367 - val_loss: 94.8322\n",
      "Epoch 668/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 94.7910 - val_loss: 95.4008\n",
      "Epoch 669/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 93.7954 - val_loss: 95.0581\n",
      "Epoch 670/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 94.5538 - val_loss: 95.2710\n",
      "Epoch 671/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 93.7081 - val_loss: 94.9077\n",
      "Epoch 672/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 96.3238 - val_loss: 94.1966\n",
      "Epoch 673/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 95.0486 - val_loss: 95.4000\n",
      "Epoch 674/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 93.9016 - val_loss: 95.3355\n",
      "Epoch 675/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 95.0326 - val_loss: 96.4336\n",
      "Epoch 676/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 95.9745 - val_loss: 97.1243\n",
      "Epoch 677/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 95.5072 - val_loss: 96.9567\n",
      "Epoch 678/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 94.7481 - val_loss: 94.4574\n",
      "Epoch 679/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 93.6070 - val_loss: 95.0551\n",
      "Epoch 680/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 94.6620 - val_loss: 94.7719\n",
      "Epoch 681/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 96.4952 - val_loss: 95.8712\n",
      "Epoch 682/10000\n",
      "96/96 [==============================] - 0s 488us/step - loss: 97.1434 - val_loss: 95.1852\n",
      "Epoch 683/10000\n",
      "96/96 [==============================] - 0s 508us/step - loss: 97.2468 - val_loss: 94.7914\n",
      "Epoch 684/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 93.4616 - val_loss: 95.0791\n",
      "Epoch 685/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 94.6391 - val_loss: 96.4669\n",
      "Epoch 686/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 360us/step - loss: 93.8140 - val_loss: 94.9574\n",
      "Epoch 687/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 94.2437 - val_loss: 95.7016\n",
      "Epoch 688/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 94.1075 - val_loss: 94.7474\n",
      "Epoch 689/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 98.8852 - val_loss: 94.1049\n",
      "Epoch 690/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 94.7177 - val_loss: 95.2301\n",
      "Epoch 691/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 95.3792 - val_loss: 96.5823\n",
      "Epoch 692/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 95.5953 - val_loss: 95.1329\n",
      "Epoch 693/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 93.4922 - val_loss: 94.3474\n",
      "Epoch 694/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 94.5145 - val_loss: 94.7505\n",
      "Epoch 695/10000\n",
      "96/96 [==============================] - 0s 510us/step - loss: 92.7704 - val_loss: 94.9750\n",
      "Epoch 696/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 95.4185 - val_loss: 94.1895\n",
      "Epoch 697/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 93.8602 - val_loss: 96.1192\n",
      "Epoch 698/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 94.4966 - val_loss: 96.4148\n",
      "Epoch 699/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 95.2440 - val_loss: 96.9857\n",
      "Epoch 700/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 96.3504 - val_loss: 96.1233\n",
      "Epoch 701/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 96.4786 - val_loss: 96.1476\n",
      "Epoch 702/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 96.1071 - val_loss: 95.3147\n",
      "Epoch 703/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 96.0266 - val_loss: 96.0711\n",
      "Epoch 704/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 94.6851 - val_loss: 96.5430\n",
      "Epoch 705/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 94.1998 - val_loss: 94.4571\n",
      "Epoch 706/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 96.9728 - val_loss: 94.9486\n",
      "Epoch 707/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 98.7856 - val_loss: 95.2197\n",
      "Epoch 708/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 94.5393 - val_loss: 94.7187\n",
      "Epoch 709/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 95.3519 - val_loss: 95.0843\n",
      "Epoch 710/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 94.3744 - val_loss: 94.8209\n",
      "Epoch 711/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 94.1939 - val_loss: 95.4359\n",
      "Epoch 712/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 95.5061 - val_loss: 94.4653\n",
      "Epoch 713/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 94.3193 - val_loss: 95.8915\n",
      "Epoch 714/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 94.8522 - val_loss: 94.3158\n",
      "Epoch 715/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 91.9352 - val_loss: 94.5086\n",
      "Epoch 716/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 95.4989 - val_loss: 97.6693\n",
      "Epoch 717/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 93.8492 - val_loss: 94.5795\n",
      "Epoch 718/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 95.1431 - val_loss: 94.0138\n",
      "Epoch 719/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 98.5284 - val_loss: 94.0698\n",
      "Epoch 720/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 95.8297 - val_loss: 96.0512\n",
      "Epoch 721/10000\n",
      "96/96 [==============================] - 0s 351us/step - loss: 96.0551 - val_loss: 94.0026\n",
      "Epoch 722/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 95.1354 - val_loss: 96.0475\n",
      "Epoch 723/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 94.5195 - val_loss: 95.4999\n",
      "Epoch 724/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 94.2688 - val_loss: 94.4852\n",
      "Epoch 725/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 93.6988 - val_loss: 95.9704\n",
      "Epoch 726/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 97.5653 - val_loss: 96.9954\n",
      "Epoch 727/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 95.8892 - val_loss: 94.9834\n",
      "Epoch 728/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 94.4481 - val_loss: 95.2482\n",
      "Epoch 729/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 97.3466 - val_loss: 94.3778\n",
      "Epoch 730/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 94.3745 - val_loss: 94.3926\n",
      "Epoch 731/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 96.8284 - val_loss: 94.2750\n",
      "Epoch 732/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 94.0711 - val_loss: 93.2689\n",
      "Epoch 733/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 95.9045 - val_loss: 94.2097\n",
      "Epoch 734/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 95.0142 - val_loss: 94.7827\n",
      "Epoch 735/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 95.3168 - val_loss: 95.0139\n",
      "Epoch 736/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 93.0839 - val_loss: 94.1044\n",
      "Epoch 737/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 97.9469 - val_loss: 94.3077\n",
      "Epoch 738/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 93.1398 - val_loss: 93.9469\n",
      "Epoch 739/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 96.6728 - val_loss: 94.7226\n",
      "Epoch 740/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 100.0544 - val_loss: 96.0526\n",
      "Epoch 741/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 94.9950 - val_loss: 94.3854\n",
      "Epoch 742/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 95.4295 - val_loss: 94.0585\n",
      "Epoch 743/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 93.4248 - val_loss: 94.9852\n",
      "Epoch 744/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 96.3215 - val_loss: 95.2223\n",
      "Epoch 745/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 93.1748 - val_loss: 95.3829\n",
      "Epoch 746/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 95.4397 - val_loss: 95.5177\n",
      "Epoch 747/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 95.8885 - val_loss: 96.6902\n",
      "Epoch 748/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 96.1044 - val_loss: 95.7582\n",
      "Epoch 749/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 96.2833 - val_loss: 94.4241\n",
      "Epoch 750/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 97.2102 - val_loss: 94.6497\n",
      "Epoch 751/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 94.7498 - val_loss: 94.5688\n",
      "Epoch 752/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 94.6821 - val_loss: 95.2749\n",
      "Epoch 753/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 95.3610 - val_loss: 95.0526\n",
      "Epoch 754/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 95.8948 - val_loss: 95.1918\n",
      "Epoch 755/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 94.8065 - val_loss: 94.8804\n",
      "Epoch 756/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 95.1967 - val_loss: 96.1883\n",
      "Epoch 757/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 93.5213 - val_loss: 95.0452\n",
      "Epoch 758/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 92.7187 - val_loss: 94.6201\n",
      "Epoch 759/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 96.4850 - val_loss: 94.4319\n",
      "Epoch 760/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 94.6126 - val_loss: 94.6491\n",
      "Epoch 761/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 97.4902 - val_loss: 94.5410\n",
      "Epoch 762/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 96.5191 - val_loss: 94.1884\n",
      "Epoch 763/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 97.7247 - val_loss: 94.8846\n",
      "Epoch 764/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 95.3225 - val_loss: 95.8888\n",
      "Epoch 765/10000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 93.7963 - val_loss: 95.0817\n",
      "Epoch 766/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 96.5901 - val_loss: 95.4680\n",
      "Epoch 767/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 96.6484 - val_loss: 94.3871\n",
      "Epoch 768/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 103.1894 - val_loss: 94.1840\n",
      "Epoch 769/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 95.2466 - val_loss: 94.9679\n",
      "Epoch 770/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 95.7266 - val_loss: 95.2038\n",
      "Epoch 771/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 97.2150 - val_loss: 95.8636\n",
      "Epoch 772/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 95.9824 - val_loss: 96.1575\n",
      "Epoch 773/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 95.9964 - val_loss: 95.1808\n",
      "Epoch 774/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 94.9469 - val_loss: 93.9070\n",
      "Epoch 775/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 93.9302 - val_loss: 94.4320\n",
      "Epoch 776/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 94.5637 - val_loss: 94.5464\n",
      "Epoch 777/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 96.3724 - val_loss: 95.7715\n",
      "Epoch 778/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 93.5970 - val_loss: 93.5346\n",
      "Epoch 779/10000\n",
      "96/96 [==============================] - 0s 482us/step - loss: 97.5745 - val_loss: 95.0814\n",
      "Epoch 780/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 96.2025 - val_loss: 94.9230\n",
      "Epoch 781/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 98.4522 - val_loss: 95.7391\n",
      "Epoch 782/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 99.2879 - val_loss: 94.1368\n",
      "Epoch 783/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 96.8346 - val_loss: 95.1209\n",
      "Epoch 784/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 95.3228 - val_loss: 95.7519\n",
      "Epoch 785/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 96.0360 - val_loss: 96.1125\n",
      "Epoch 786/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 96.0084 - val_loss: 95.8817\n",
      "Epoch 787/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 96.0385 - val_loss: 95.6315\n",
      "Epoch 788/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 94.3941 - val_loss: 95.4956\n",
      "Epoch 789/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 96.7460 - val_loss: 96.6836\n",
      "Epoch 790/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 95.0631 - val_loss: 94.6235\n",
      "Epoch 791/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 96.0545 - val_loss: 94.2641\n",
      "Epoch 792/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 96.5277 - val_loss: 95.3458\n",
      "Epoch 793/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 96.0098 - val_loss: 95.2963\n",
      "Epoch 794/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 96.9005 - val_loss: 94.2669\n",
      "Epoch 795/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 93.2058 - val_loss: 94.8574\n",
      "Epoch 796/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 95.3537 - val_loss: 95.3891\n",
      "Epoch 797/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 94.8154 - val_loss: 94.5583\n",
      "Epoch 798/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 93.5367 - val_loss: 94.5559\n",
      "Epoch 799/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 95.6479 - val_loss: 96.2472\n",
      "Epoch 800/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 95.4678 - val_loss: 95.1498\n",
      "Epoch 801/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 95.0956 - val_loss: 95.2527\n",
      "Epoch 802/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 93.7860 - val_loss: 94.2354\n",
      "Epoch 803/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 95.0342 - val_loss: 95.7251\n",
      "Epoch 804/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 93.6844 - val_loss: 95.9059\n",
      "Epoch 805/10000\n",
      "96/96 [==============================] - 0s 357us/step - loss: 95.5694 - val_loss: 96.4202\n",
      "Epoch 806/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 97.0367 - val_loss: 94.7886\n",
      "Epoch 807/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 93.6402 - val_loss: 94.3803\n",
      "Epoch 808/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 94.7888 - val_loss: 94.8018\n",
      "Epoch 809/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 96.5737 - val_loss: 95.0085\n",
      "Epoch 810/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 97.3954 - val_loss: 94.8372\n",
      "Epoch 811/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 96.6066 - val_loss: 94.2899\n",
      "Epoch 812/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 95.7288 - val_loss: 94.0553\n",
      "Epoch 813/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 96.8619 - val_loss: 95.0480\n",
      "Epoch 814/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 94.2918 - val_loss: 94.9177\n",
      "Epoch 815/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 95.5935 - val_loss: 95.3570\n",
      "Epoch 816/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 95.2889 - val_loss: 96.1444\n",
      "Epoch 817/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 96.3946 - val_loss: 96.2235\n",
      "Epoch 818/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 93.7926 - val_loss: 94.9274\n",
      "Epoch 819/10000\n",
      "96/96 [==============================] - 0s 482us/step - loss: 93.0594 - val_loss: 95.7864\n",
      "Epoch 820/10000\n",
      "96/96 [==============================] - 0s 488us/step - loss: 94.5280 - val_loss: 97.8843\n",
      "Epoch 821/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 96.2393 - val_loss: 96.1856\n",
      "Epoch 822/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 99.6504 - val_loss: 95.8656\n",
      "Epoch 823/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 94.8870 - val_loss: 96.3656\n",
      "Epoch 824/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 95.5118 - val_loss: 95.5503\n",
      "Epoch 825/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 95.1420 - val_loss: 93.8925\n",
      "Epoch 826/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 94.6069 - val_loss: 93.4927\n",
      "Epoch 827/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 95.5792 - val_loss: 95.4616\n",
      "Epoch 828/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 95.2689 - val_loss: 94.9957\n",
      "Epoch 829/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 96.3879 - val_loss: 95.3960\n",
      "Epoch 830/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 94.8452 - val_loss: 95.0852\n",
      "Epoch 831/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 94.2223 - val_loss: 95.0959\n",
      "Epoch 832/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 95.6786 - val_loss: 93.6051\n",
      "Epoch 833/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 95.6432 - val_loss: 94.9266\n",
      "Epoch 834/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 97.7814 - val_loss: 94.5050\n",
      "Epoch 835/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 95.8816 - val_loss: 95.7628\n",
      "Epoch 836/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 94.6512 - val_loss: 95.4746\n",
      "Epoch 837/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 93.4462 - val_loss: 94.1882\n",
      "Epoch 838/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 393us/step - loss: 92.8995 - val_loss: 93.6507\n",
      "Epoch 839/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 94.4293 - val_loss: 95.1126\n",
      "Epoch 840/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 94.2488 - val_loss: 94.2216\n",
      "Epoch 841/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 95.6868 - val_loss: 95.3736\n",
      "Epoch 842/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 94.5002 - val_loss: 94.5639\n",
      "Epoch 843/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 95.7877 - val_loss: 93.6568\n",
      "Epoch 844/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 94.5618 - val_loss: 94.1007\n",
      "Epoch 845/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 95.6166 - val_loss: 94.6676\n",
      "Epoch 846/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 94.2360 - val_loss: 94.4864\n",
      "Epoch 847/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 98.2675 - val_loss: 93.6426\n",
      "Epoch 848/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 96.1588 - val_loss: 95.0533\n",
      "Epoch 849/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 95.6237 - val_loss: 94.6123\n",
      "Epoch 850/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 95.3606 - val_loss: 94.3389\n",
      "Epoch 851/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 93.6233 - val_loss: 93.9867\n",
      "Epoch 852/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 97.0153 - val_loss: 94.4812\n",
      "Epoch 853/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 93.4951 - val_loss: 95.3860\n",
      "Epoch 854/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 94.8697 - val_loss: 95.9155\n",
      "Epoch 855/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 92.1897 - val_loss: 94.4431\n",
      "Epoch 856/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 91.5526 - val_loss: 94.1923\n",
      "Epoch 857/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 94.5454 - val_loss: 95.3184\n",
      "Epoch 858/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 93.9478 - val_loss: 96.6672\n",
      "Epoch 859/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 95.8305 - val_loss: 95.0836\n",
      "Epoch 860/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 93.7544 - val_loss: 95.6783\n",
      "Epoch 861/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 93.3348 - val_loss: 94.1228\n",
      "Epoch 862/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 93.5670 - val_loss: 93.9233\n",
      "Epoch 863/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 95.0671 - val_loss: 94.9115\n",
      "Epoch 864/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 96.4583 - val_loss: 95.2607\n",
      "Epoch 865/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 95.5667 - val_loss: 95.9446\n",
      "Epoch 866/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 96.9199 - val_loss: 94.7528\n",
      "Epoch 867/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 96.7152 - val_loss: 96.0794\n",
      "Epoch 868/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 94.2379 - val_loss: 95.5738\n",
      "Epoch 869/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 94.2202 - val_loss: 95.6656\n",
      "Epoch 870/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 94.2691 - val_loss: 96.0501\n",
      "Epoch 871/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 93.8636 - val_loss: 94.1058\n",
      "Epoch 872/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 94.6175 - val_loss: 95.4278\n",
      "Epoch 873/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 96.0711 - val_loss: 93.6713\n",
      "Epoch 874/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 96.8649 - val_loss: 94.8668\n",
      "Epoch 875/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 97.3868 - val_loss: 95.5467\n",
      "Epoch 876/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 94.4476 - val_loss: 94.4989\n",
      "Epoch 877/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 96.6179 - val_loss: 95.5571\n",
      "Epoch 878/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 96.5855 - val_loss: 95.4311\n",
      "Epoch 879/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 95.7062 - val_loss: 96.2734\n",
      "Epoch 880/10000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 94.5638 - val_loss: 96.5459\n",
      "Epoch 881/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 95.1654 - val_loss: 97.2683\n",
      "Epoch 882/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 95.7399 - val_loss: 95.1392\n",
      "Epoch 883/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 93.9482 - val_loss: 96.3392\n",
      "Epoch 884/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 93.9976 - val_loss: 94.7288\n",
      "Epoch 885/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 96.9930 - val_loss: 94.2498\n",
      "Epoch 886/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 97.4902 - val_loss: 94.9473\n",
      "Epoch 887/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 94.7443 - val_loss: 94.3728\n",
      "Epoch 888/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 95.5391 - val_loss: 96.7389\n",
      "Epoch 889/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 96.1510 - val_loss: 95.4924\n",
      "Epoch 890/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 95.4557 - val_loss: 96.2149\n",
      "Epoch 891/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 95.3705 - val_loss: 96.2169\n",
      "Epoch 892/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 94.4837 - val_loss: 94.9533\n",
      "Epoch 893/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 94.3558 - val_loss: 95.9095\n",
      "Epoch 894/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 95.6690 - val_loss: 95.6118\n",
      "Epoch 895/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 92.8476 - val_loss: 95.1184\n",
      "Epoch 896/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 93.3557 - val_loss: 94.9919\n",
      "Epoch 897/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 94.7880 - val_loss: 94.5027\n",
      "Epoch 898/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 95.4514 - val_loss: 97.0493\n",
      "Epoch 899/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 95.3855 - val_loss: 96.0560\n",
      "Epoch 900/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 96.3448 - val_loss: 96.1232\n",
      "Epoch 901/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 98.0800 - val_loss: 95.3381\n",
      "Epoch 902/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 93.9800 - val_loss: 94.8743\n",
      "Epoch 903/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 96.9538 - val_loss: 95.6471\n",
      "Epoch 904/10000\n",
      "96/96 [==============================] - 0s 481us/step - loss: 95.2287 - val_loss: 95.6840\n",
      "Epoch 905/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 95.6577 - val_loss: 94.8710\n",
      "Epoch 906/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 98.2616 - val_loss: 94.8658\n",
      "Epoch 907/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 96.7446 - val_loss: 95.2026\n",
      "Epoch 908/10000\n",
      "96/96 [==============================] - 0s 494us/step - loss: 95.3002 - val_loss: 96.3810\n",
      "Epoch 909/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 96.4648 - val_loss: 96.3666\n",
      "Epoch 910/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 96.1986 - val_loss: 94.9725\n",
      "Epoch 911/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 96.8405 - val_loss: 94.9176\n",
      "Epoch 912/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 92.8655 - val_loss: 95.6856\n",
      "Epoch 913/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 95.4690 - val_loss: 95.7379\n",
      "Epoch 914/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 94.8205 - val_loss: 95.2874\n",
      "Epoch 915/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 94.8986 - val_loss: 95.6584\n",
      "Epoch 916/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 93.6563 - val_loss: 95.5414\n",
      "Epoch 917/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 97.0326 - val_loss: 95.2521\n",
      "Epoch 918/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 96.4219 - val_loss: 95.0182\n",
      "Epoch 919/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 94.0777 - val_loss: 93.8493\n",
      "Epoch 920/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 95.7056 - val_loss: 93.9271\n",
      "Epoch 921/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 93.5020 - val_loss: 94.7601\n",
      "Epoch 922/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 97.3611 - val_loss: 95.2310\n",
      "Epoch 923/10000\n",
      "96/96 [==============================] - 0s 355us/step - loss: 97.0945 - val_loss: 95.7890\n",
      "Epoch 924/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 92.8618 - val_loss: 94.5805\n",
      "Epoch 925/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 92.9339 - val_loss: 93.8982\n",
      "Epoch 926/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 92.9557 - val_loss: 95.9584\n",
      "Epoch 927/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 94.4338 - val_loss: 95.1873\n",
      "Epoch 928/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 94.4087 - val_loss: 95.9658\n",
      "Epoch 929/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 94.5373 - val_loss: 95.6778\n",
      "Epoch 930/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 92.3147 - val_loss: 96.3398\n",
      "Epoch 931/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 93.4988 - val_loss: 94.9838\n",
      "Epoch 932/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 94.6320 - val_loss: 95.7162\n",
      "Epoch 933/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 93.5197 - val_loss: 93.9123\n",
      "Epoch 934/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 92.7667 - val_loss: 94.7472\n",
      "Epoch 935/10000\n",
      "96/96 [==============================] - 0s 510us/step - loss: 92.4274 - val_loss: 96.5210\n",
      "Epoch 936/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 93.4904 - val_loss: 94.1329\n",
      "Epoch 937/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 95.1530 - val_loss: 95.3042\n",
      "Epoch 938/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 95.9071 - val_loss: 95.3492\n",
      "Epoch 939/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 93.0620 - val_loss: 94.3826\n",
      "Epoch 940/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 95.2400 - val_loss: 96.1139\n",
      "Epoch 941/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 94.0439 - val_loss: 96.1876\n",
      "Epoch 942/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 92.9253 - val_loss: 94.8745\n",
      "Epoch 943/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 92.9682 - val_loss: 95.5933\n",
      "Epoch 944/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 92.1256 - val_loss: 93.7910\n",
      "Epoch 945/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 94.8377 - val_loss: 94.6556\n",
      "Epoch 946/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 96.2581 - val_loss: 94.4816\n",
      "Epoch 947/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 91.5543 - val_loss: 93.6079\n",
      "Epoch 948/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 93.1404 - val_loss: 95.6304\n",
      "Epoch 949/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 95.3012 - val_loss: 96.3541\n",
      "Epoch 950/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 93.5678 - val_loss: 94.9000\n",
      "Epoch 951/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 94.3557 - val_loss: 95.6561\n",
      "Epoch 952/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 96.1327 - val_loss: 97.0431\n",
      "Epoch 953/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 93.6230 - val_loss: 94.9826\n",
      "Epoch 954/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 93.1901 - val_loss: 95.2161\n",
      "Epoch 955/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 94.0750 - val_loss: 95.3580\n",
      "Epoch 956/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 94.8602 - val_loss: 93.6510\n",
      "Epoch 957/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 95.7644 - val_loss: 95.5057\n",
      "Epoch 958/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 91.9524 - val_loss: 94.7655\n",
      "Epoch 959/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 95.7865 - val_loss: 95.1226\n",
      "Epoch 960/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 92.5032 - val_loss: 93.9978\n",
      "Epoch 961/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 90.8534 - val_loss: 94.8408\n",
      "Epoch 962/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 95.2157 - val_loss: 95.8855\n",
      "Epoch 963/10000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 93.1275 - val_loss: 94.8078\n",
      "Epoch 964/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 95.2269 - val_loss: 93.6835\n",
      "Epoch 965/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 95.8016 - val_loss: 94.4562\n",
      "Epoch 966/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 95.4918 - val_loss: 94.5679\n",
      "Epoch 967/10000\n",
      "96/96 [==============================] - 0s 492us/step - loss: 96.6702 - val_loss: 95.1846\n",
      "Epoch 968/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 91.6449 - val_loss: 94.8218\n",
      "Epoch 969/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 94.6603 - val_loss: 94.6791\n",
      "Epoch 970/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 93.5062 - val_loss: 94.7421\n",
      "Epoch 971/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 94.7778 - val_loss: 95.0740\n",
      "Epoch 972/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 92.9568 - val_loss: 94.7761\n",
      "Epoch 973/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 93.9499 - val_loss: 96.5664\n",
      "Epoch 974/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 92.6672 - val_loss: 94.4078\n",
      "Epoch 975/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 92.7609 - val_loss: 94.6983\n",
      "Epoch 976/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 92.5772 - val_loss: 94.1011\n",
      "Epoch 977/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 98.6667 - val_loss: 95.2050\n",
      "Epoch 978/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 93.4217 - val_loss: 96.1225\n",
      "Epoch 979/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 92.4016 - val_loss: 96.1055\n",
      "Epoch 980/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 93.5346 - val_loss: 94.1084\n",
      "Epoch 981/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 92.5427 - val_loss: 93.8336\n",
      "Epoch 982/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 92.5514 - val_loss: 93.8578\n",
      "Epoch 983/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 92.3111 - val_loss: 93.8528\n",
      "Epoch 984/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 95.8655 - val_loss: 96.8440\n",
      "Epoch 985/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 92.3990 - val_loss: 97.3301\n",
      "Epoch 986/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 95.4383 - val_loss: 95.0924\n",
      "Epoch 987/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 89.9625 - val_loss: 95.2482\n",
      "Epoch 988/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 90.8686 - val_loss: 94.8313\n",
      "Epoch 989/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 94.0682 - val_loss: 95.9004\n",
      "Epoch 990/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 381us/step - loss: 94.3454 - val_loss: 96.7725\n",
      "Epoch 991/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 93.0964 - val_loss: 97.8981\n",
      "Epoch 992/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 102.0747 - val_loss: 96.3429\n",
      "Epoch 993/10000\n",
      "96/96 [==============================] - 0s 475us/step - loss: 93.5933 - val_loss: 95.6772\n",
      "Epoch 994/10000\n",
      "96/96 [==============================] - 0s 513us/step - loss: 90.9851 - val_loss: 94.1704\n",
      "Epoch 995/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 94.5549 - val_loss: 94.2255\n",
      "Epoch 996/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 95.3124 - val_loss: 95.1818\n",
      "Epoch 997/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 95.1176 - val_loss: 95.4870\n",
      "Epoch 998/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 95.8339 - val_loss: 95.2945\n",
      "Epoch 999/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 93.5734 - val_loss: 96.4338\n",
      "Epoch 1000/10000\n",
      "96/96 [==============================] - 0s 485us/step - loss: 96.9608 - val_loss: 94.9783\n",
      "Epoch 1001/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 92.6787 - val_loss: 98.3077\n",
      "Epoch 1002/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 96.9373 - val_loss: 95.4503\n",
      "Epoch 1003/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 99.7834 - val_loss: 96.3001\n",
      "Epoch 1004/10000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 94.7985 - val_loss: 95.2165\n",
      "Epoch 1005/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 94.0910 - val_loss: 94.9418\n",
      "Epoch 1006/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 94.5293 - val_loss: 96.0117\n",
      "Epoch 1007/10000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 92.9087 - val_loss: 95.3508\n",
      "Epoch 1008/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 96.8485 - val_loss: 94.8759\n",
      "Epoch 1009/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 93.7063 - val_loss: 95.3521\n",
      "Epoch 1010/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 90.9281 - val_loss: 93.6339\n",
      "Epoch 1011/10000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 92.1981 - val_loss: 94.9318\n",
      "Epoch 1012/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 93.5441 - val_loss: 94.2801\n",
      "Epoch 1013/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 93.1634 - val_loss: 96.3997\n",
      "Epoch 1014/10000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 92.4630 - val_loss: 93.3824\n",
      "Epoch 1015/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 92.1103 - val_loss: 94.5955\n",
      "Epoch 1016/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 94.6992 - val_loss: 95.1881\n",
      "Epoch 1017/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 93.1407 - val_loss: 94.9728\n",
      "Epoch 1018/10000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 91.3991 - val_loss: 94.1922\n",
      "Epoch 1019/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 92.1919 - val_loss: 94.2178\n",
      "Epoch 1020/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 91.6010 - val_loss: 95.4301\n",
      "Epoch 1021/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 94.1390 - val_loss: 96.8758\n",
      "Epoch 1022/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 95.1659 - val_loss: 94.9432\n",
      "Epoch 1023/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 92.5103 - val_loss: 94.1211\n",
      "Epoch 1024/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 91.4449 - val_loss: 94.4227\n",
      "Epoch 1025/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 92.3727 - val_loss: 92.5676\n",
      "Epoch 1026/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 92.4753 - val_loss: 95.8413\n",
      "Epoch 1027/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 92.4022 - val_loss: 95.9772\n",
      "Epoch 1028/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 93.9774 - val_loss: 94.7810\n",
      "Epoch 1029/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 93.9839 - val_loss: 93.5693\n",
      "Epoch 1030/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 94.7141 - val_loss: 94.3935\n",
      "Epoch 1031/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 95.9078 - val_loss: 95.3165\n",
      "Epoch 1032/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 96.9486 - val_loss: 94.8156\n",
      "Epoch 1033/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 92.1917 - val_loss: 93.5365\n",
      "Epoch 1034/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 92.3255 - val_loss: 94.2373\n",
      "Epoch 1035/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 93.3341 - val_loss: 95.9025\n",
      "Epoch 1036/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 96.5001 - val_loss: 94.5502\n",
      "Epoch 1037/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 92.5686 - val_loss: 95.6083\n",
      "Epoch 1038/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 92.7719 - val_loss: 94.7864\n",
      "Epoch 1039/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 92.7711 - val_loss: 97.2679\n",
      "Epoch 1040/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 93.6497 - val_loss: 96.9338\n",
      "Epoch 1041/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 90.3014 - val_loss: 95.9885\n",
      "Epoch 1042/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 92.7909 - val_loss: 95.6758\n",
      "Epoch 1043/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 93.0349 - val_loss: 96.0331\n",
      "Epoch 1044/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 91.7893 - val_loss: 95.7800\n",
      "Epoch 1045/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 93.9277 - val_loss: 94.2688\n",
      "Epoch 1046/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 99.3716 - val_loss: 95.6236\n",
      "Epoch 1047/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 90.6161 - val_loss: 93.4170\n",
      "Epoch 1048/10000\n",
      "96/96 [==============================] - 0s 503us/step - loss: 95.4094 - val_loss: 95.6320\n",
      "Epoch 1049/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 92.7770 - val_loss: 95.3997\n",
      "Epoch 1050/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 92.0350 - val_loss: 94.6355\n",
      "Epoch 1051/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 93.4520 - val_loss: 94.0444\n",
      "Epoch 1052/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 91.0870 - val_loss: 95.6356\n",
      "Epoch 1053/10000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 92.6151 - val_loss: 94.1456\n",
      "Epoch 1054/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 94.3166 - val_loss: 95.6199\n",
      "Epoch 1055/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 93.0945 - val_loss: 95.3714\n",
      "Epoch 1056/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 94.7935 - val_loss: 96.0805\n",
      "Epoch 1057/10000\n",
      "96/96 [==============================] - 0s 512us/step - loss: 93.6654 - val_loss: 95.8311\n",
      "Epoch 1058/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 92.8733 - val_loss: 94.9001\n",
      "Epoch 1059/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 93.2616 - val_loss: 95.4522\n",
      "Epoch 1060/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 93.4813 - val_loss: 95.4364\n",
      "Epoch 1061/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 95.0771 - val_loss: 93.4554\n",
      "Epoch 1062/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 93.8078 - val_loss: 91.2398\n",
      "Epoch 1063/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 94.1745 - val_loss: 92.5767\n",
      "Epoch 1064/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 93.2870 - val_loss: 95.5157\n",
      "Epoch 1065/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 94.1316 - val_loss: 95.7398\n",
      "Epoch 1066/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 380us/step - loss: 95.3793 - val_loss: 94.0408\n",
      "Epoch 1067/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 94.8864 - val_loss: 95.7471\n",
      "Epoch 1068/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 92.4259 - val_loss: 93.7010\n",
      "Epoch 1069/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 93.7818 - val_loss: 95.3744\n",
      "Epoch 1070/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 90.6109 - val_loss: 96.4409\n",
      "Epoch 1071/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 90.3473 - val_loss: 96.2278\n",
      "Epoch 1072/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 94.2209 - val_loss: 94.5127\n",
      "Epoch 1073/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 93.3803 - val_loss: 94.3750\n",
      "Epoch 1074/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 90.6743 - val_loss: 96.0681\n",
      "Epoch 1075/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 94.1268 - val_loss: 95.0734\n",
      "Epoch 1076/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 91.4147 - val_loss: 95.5752\n",
      "Epoch 1077/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 94.2080 - val_loss: 95.1705\n",
      "Epoch 1078/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 91.7975 - val_loss: 96.1943\n",
      "Epoch 1079/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 96.4616 - val_loss: 95.2092\n",
      "Epoch 1080/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 93.6278 - val_loss: 94.8275\n",
      "Epoch 1081/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 94.0060 - val_loss: 97.6762\n",
      "Epoch 1082/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 94.7727 - val_loss: 102.0309\n",
      "Epoch 1083/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 94.1243 - val_loss: 99.9292\n",
      "Epoch 1084/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 95.5471 - val_loss: 94.8478\n",
      "Epoch 1085/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 94.9319 - val_loss: 96.3779\n",
      "Epoch 1086/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 96.2661 - val_loss: 95.6109\n",
      "Epoch 1087/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 94.4919 - val_loss: 93.9692\n",
      "Epoch 1088/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 94.7052 - val_loss: 95.2507\n",
      "Epoch 1089/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 94.7643 - val_loss: 94.2016\n",
      "Epoch 1090/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 91.4605 - val_loss: 94.6064\n",
      "Epoch 1091/10000\n",
      "96/96 [==============================] - 0s 355us/step - loss: 93.1158 - val_loss: 93.6180\n",
      "Epoch 1092/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 93.8886 - val_loss: 95.2912\n",
      "Epoch 1093/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 91.0565 - val_loss: 93.6026\n",
      "Epoch 1094/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 91.9520 - val_loss: 93.5805\n",
      "Epoch 1095/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 94.2840 - val_loss: 95.2676\n",
      "Epoch 1096/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 90.2438 - val_loss: 94.3730\n",
      "Epoch 1097/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 91.5701 - val_loss: 94.9814\n",
      "Epoch 1098/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 91.4895 - val_loss: 94.7210\n",
      "Epoch 1099/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 92.5977 - val_loss: 97.1742\n",
      "Epoch 1100/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 91.0556 - val_loss: 95.7043\n",
      "Epoch 1101/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 88.7541 - val_loss: 95.2502\n",
      "Epoch 1102/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 95.5767 - val_loss: 95.7219\n",
      "Epoch 1103/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 93.0793 - val_loss: 94.8646\n",
      "Epoch 1104/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 90.3962 - val_loss: 95.9263\n",
      "Epoch 1105/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 89.6519 - val_loss: 94.8300\n",
      "Epoch 1106/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 92.9873 - val_loss: 94.7501\n",
      "Epoch 1107/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 92.6383 - val_loss: 94.4752\n",
      "Epoch 1108/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 90.7713 - val_loss: 93.9037\n",
      "Epoch 1109/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 90.6251 - val_loss: 95.3849\n",
      "Epoch 1110/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 90.0953 - val_loss: 95.0168\n",
      "Epoch 1111/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 93.7553 - val_loss: 93.6209\n",
      "Epoch 1112/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 90.1347 - val_loss: 95.2730\n",
      "Epoch 1113/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 90.5365 - val_loss: 95.4364\n",
      "Epoch 1114/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 95.2876 - val_loss: 97.2062\n",
      "Epoch 1115/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 95.7710 - val_loss: 94.1191\n",
      "Epoch 1116/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 90.6338 - val_loss: 94.1377\n",
      "Epoch 1117/10000\n",
      "96/96 [==============================] - 0s 490us/step - loss: 94.3892 - val_loss: 94.4124\n",
      "Epoch 1118/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 94.2420 - val_loss: 95.1981\n",
      "Epoch 1119/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 91.6089 - val_loss: 97.0439\n",
      "Epoch 1120/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 94.1846 - val_loss: 94.1446\n",
      "Epoch 1121/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 91.7038 - val_loss: 95.7158\n",
      "Epoch 1122/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 90.9512 - val_loss: 93.2755\n",
      "Epoch 1123/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 91.6110 - val_loss: 97.0993\n",
      "Epoch 1124/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 92.7223 - val_loss: 94.9032\n",
      "Epoch 1125/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 91.3691 - val_loss: 94.0784\n",
      "Epoch 1126/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 89.3897 - val_loss: 91.4066\n",
      "Epoch 1127/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 94.0225 - val_loss: 91.8893\n",
      "Epoch 1128/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 91.9869 - val_loss: 93.6710\n",
      "Epoch 1129/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 91.3426 - val_loss: 92.7506\n",
      "Epoch 1130/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 89.6231 - val_loss: 98.1649\n",
      "Epoch 1131/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 93.5053 - val_loss: 95.6985\n",
      "Epoch 1132/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 92.2256 - val_loss: 93.9106\n",
      "Epoch 1133/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 92.4596 - val_loss: 98.0349\n",
      "Epoch 1134/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 95.5322 - val_loss: 96.5851\n",
      "Epoch 1135/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 94.6326 - val_loss: 96.4912\n",
      "Epoch 1136/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 99.3404 - val_loss: 96.7111\n",
      "Epoch 1137/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 92.4764 - val_loss: 96.5161\n",
      "Epoch 1138/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 90.8721 - val_loss: 96.5909\n",
      "Epoch 1139/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 93.6184 - val_loss: 95.6148\n",
      "Epoch 1140/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 91.8037 - val_loss: 94.8570\n",
      "Epoch 1141/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 92.7002 - val_loss: 95.6917\n",
      "Epoch 1142/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 395us/step - loss: 90.7053 - val_loss: 92.9591\n",
      "Epoch 1143/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 91.8269 - val_loss: 92.4662\n",
      "Epoch 1144/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 92.0308 - val_loss: 96.2254\n",
      "Epoch 1145/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 90.2294 - val_loss: 93.4749\n",
      "Epoch 1146/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 98.0673 - val_loss: 97.0957\n",
      "Epoch 1147/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 93.8222 - val_loss: 96.2819\n",
      "Epoch 1148/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 92.2400 - val_loss: 96.4701\n",
      "Epoch 1149/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 89.3554 - val_loss: 95.2740\n",
      "Epoch 1150/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 97.0560 - val_loss: 95.3061\n",
      "Epoch 1151/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 95.3013 - val_loss: 94.1395\n",
      "Epoch 1152/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 92.8534 - val_loss: 93.5030\n",
      "Epoch 1153/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 89.7762 - val_loss: 92.9289\n",
      "Epoch 1154/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 95.0325 - val_loss: 95.7142\n",
      "Epoch 1155/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 96.2034 - val_loss: 92.9464\n",
      "Epoch 1156/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 89.3166 - val_loss: 96.0526\n",
      "Epoch 1157/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 95.2229 - val_loss: 95.3718\n",
      "Epoch 1158/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 94.9624 - val_loss: 95.9248\n",
      "Epoch 1159/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 93.4112 - val_loss: 94.4140\n",
      "Epoch 1160/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 90.6507 - val_loss: 96.5655\n",
      "Epoch 1161/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 91.1563 - val_loss: 94.4235\n",
      "Epoch 1162/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 96.2789 - val_loss: 91.2859\n",
      "Epoch 1163/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 95.9872 - val_loss: 96.7685\n",
      "Epoch 1164/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 94.1492 - val_loss: 96.5552\n",
      "Epoch 1165/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 95.9845 - val_loss: 95.5046\n",
      "Epoch 1166/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 96.3002 - val_loss: 95.3032\n",
      "Epoch 1167/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 90.0974 - val_loss: 94.8908\n",
      "Epoch 1168/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 93.5165 - val_loss: 94.6074\n",
      "Epoch 1169/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 90.6494 - val_loss: 94.7912\n",
      "Epoch 1170/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 91.1013 - val_loss: 95.6168\n",
      "Epoch 1171/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 92.8957 - val_loss: 95.2347\n",
      "Epoch 1172/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 92.8703 - val_loss: 94.6523\n",
      "Epoch 1173/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 94.7879 - val_loss: 96.1600\n",
      "Epoch 1174/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 91.7573 - val_loss: 94.9104\n",
      "Epoch 1175/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 90.7476 - val_loss: 96.1485\n",
      "Epoch 1176/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 90.9361 - val_loss: 94.3465\n",
      "Epoch 1177/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 91.8457 - val_loss: 94.9418\n",
      "Epoch 1178/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 92.1465 - val_loss: 94.0896\n",
      "Epoch 1179/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 90.4970 - val_loss: 94.1777\n",
      "Epoch 1180/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 89.5733 - val_loss: 96.4213\n",
      "Epoch 1181/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 92.9718 - val_loss: 94.1081\n",
      "Epoch 1182/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 88.0846 - val_loss: 93.1882\n",
      "Epoch 1183/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 87.8324 - val_loss: 96.0517\n",
      "Epoch 1184/10000\n",
      "96/96 [==============================] - 0s 351us/step - loss: 94.3967 - val_loss: 90.6800\n",
      "Epoch 1185/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 90.6412 - val_loss: 96.4781\n",
      "Epoch 1186/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 89.1730 - val_loss: 95.9012\n",
      "Epoch 1187/10000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 94.6664 - val_loss: 94.3438\n",
      "Epoch 1188/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 91.7146 - val_loss: 93.5455\n",
      "Epoch 1189/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 90.3228 - val_loss: 98.3672\n",
      "Epoch 1190/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 90.3633 - val_loss: 94.5661\n",
      "Epoch 1191/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 93.0073 - val_loss: 93.9568\n",
      "Epoch 1192/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 90.3368 - val_loss: 92.9387\n",
      "Epoch 1193/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 93.3435 - val_loss: 92.8396\n",
      "Epoch 1194/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 91.9002 - val_loss: 95.8358\n",
      "Epoch 1195/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 90.0869 - val_loss: 93.8716\n",
      "Epoch 1196/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 88.2062 - val_loss: 95.3586\n",
      "Epoch 1197/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 94.8023 - val_loss: 95.3344\n",
      "Epoch 1198/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 89.8206 - val_loss: 95.6990\n",
      "Epoch 1199/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 90.3029 - val_loss: 93.8101\n",
      "Epoch 1200/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 88.7545 - val_loss: 93.5519\n",
      "Epoch 1201/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 87.7510 - val_loss: 94.7534\n",
      "Epoch 1202/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 88.1870 - val_loss: 96.5741\n",
      "Epoch 1203/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 89.3040 - val_loss: 95.1864\n",
      "Epoch 1204/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 89.4542 - val_loss: 95.9379\n",
      "Epoch 1205/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 89.0384 - val_loss: 94.9713\n",
      "Epoch 1206/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 90.7171 - val_loss: 95.4471\n",
      "Epoch 1207/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 91.6470 - val_loss: 94.5893\n",
      "Epoch 1208/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 94.2723 - val_loss: 95.6701\n",
      "Epoch 1209/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 90.3521 - val_loss: 94.9468\n",
      "Epoch 1210/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 90.1308 - val_loss: 96.0225\n",
      "Epoch 1211/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 91.2482 - val_loss: 91.5265\n",
      "Epoch 1212/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 90.8321 - val_loss: 94.7470\n",
      "Epoch 1213/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 90.4736 - val_loss: 94.8278\n",
      "Epoch 1214/10000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 92.8421 - val_loss: 93.6371\n",
      "Epoch 1215/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 92.2228 - val_loss: 93.1673\n",
      "Epoch 1216/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 90.0997 - val_loss: 93.3600\n",
      "Epoch 1217/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 88.9791 - val_loss: 95.8880\n",
      "Epoch 1218/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 411us/step - loss: 88.7216 - val_loss: 96.5739\n",
      "Epoch 1219/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 88.5227 - val_loss: 91.6065\n",
      "Epoch 1220/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 88.4407 - val_loss: 95.0436\n",
      "Epoch 1221/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 94.7555 - val_loss: 96.7442\n",
      "Epoch 1222/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 91.2292 - val_loss: 96.7845\n",
      "Epoch 1223/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 90.1601 - val_loss: 89.7535\n",
      "Epoch 1224/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 94.7980 - val_loss: 93.2008\n",
      "Epoch 1225/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 89.4679 - val_loss: 95.6043\n",
      "Epoch 1226/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 90.5355 - val_loss: 98.2508\n",
      "Epoch 1227/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 88.6812 - val_loss: 96.0612\n",
      "Epoch 1228/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 87.7798 - val_loss: 94.5558\n",
      "Epoch 1229/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 88.2715 - val_loss: 95.1688\n",
      "Epoch 1230/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 88.8319 - val_loss: 96.6256\n",
      "Epoch 1231/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 89.2879 - val_loss: 92.4077\n",
      "Epoch 1232/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 94.2058 - val_loss: 92.9664\n",
      "Epoch 1233/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 95.1148 - val_loss: 91.1707\n",
      "Epoch 1234/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 88.5365 - val_loss: 91.0051\n",
      "Epoch 1235/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 88.7716 - val_loss: 96.0870\n",
      "Epoch 1236/10000\n",
      "96/96 [==============================] - 0s 498us/step - loss: 91.5045 - val_loss: 92.3937\n",
      "Epoch 1237/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 89.4188 - val_loss: 94.1658\n",
      "Epoch 1238/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 90.0934 - val_loss: 95.6522\n",
      "Epoch 1239/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 89.1837 - val_loss: 94.0022\n",
      "Epoch 1240/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 92.3998 - val_loss: 92.3353\n",
      "Epoch 1241/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 88.6013 - val_loss: 93.7957\n",
      "Epoch 1242/10000\n",
      "96/96 [==============================] - 0s 513us/step - loss: 88.0027 - val_loss: 92.5481\n",
      "Epoch 1243/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 90.0611 - val_loss: 93.5559\n",
      "Epoch 1244/10000\n",
      "96/96 [==============================] - 0s 355us/step - loss: 88.6269 - val_loss: 93.2594\n",
      "Epoch 1245/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 90.0763 - val_loss: 94.5972\n",
      "Epoch 1246/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 88.8387 - val_loss: 95.9287\n",
      "Epoch 1247/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 88.6343 - val_loss: 97.7176\n",
      "Epoch 1248/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 86.9726 - val_loss: 95.5098\n",
      "Epoch 1249/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 87.4559 - val_loss: 93.3603\n",
      "Epoch 1250/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 88.8052 - val_loss: 92.1208\n",
      "Epoch 1251/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 88.1461 - val_loss: 93.7145\n",
      "Epoch 1252/10000\n",
      "96/96 [==============================] - 0s 355us/step - loss: 88.3707 - val_loss: 92.3025\n",
      "Epoch 1253/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 91.6294 - val_loss: 91.1907\n",
      "Epoch 1254/10000\n",
      "96/96 [==============================] - 0s 349us/step - loss: 91.3693 - val_loss: 92.8629\n",
      "Epoch 1255/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 88.4999 - val_loss: 92.0848\n",
      "Epoch 1256/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 90.8657 - val_loss: 95.6272\n",
      "Epoch 1257/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 91.4034 - val_loss: 93.4630\n",
      "Epoch 1258/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 87.1467 - val_loss: 93.1137\n",
      "Epoch 1259/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 91.5484 - val_loss: 92.2660\n",
      "Epoch 1260/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 88.7721 - val_loss: 96.5997\n",
      "Epoch 1261/10000\n",
      "96/96 [==============================] - 0s 351us/step - loss: 88.4314 - val_loss: 97.5710\n",
      "Epoch 1262/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 87.4055 - val_loss: 91.5189\n",
      "Epoch 1263/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 86.3436 - val_loss: 92.4352\n",
      "Epoch 1264/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 86.7748 - val_loss: 91.3462\n",
      "Epoch 1265/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 86.9191 - val_loss: 94.1929\n",
      "Epoch 1266/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 87.0971 - val_loss: 92.0260\n",
      "Epoch 1267/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 87.5225 - val_loss: 95.6513\n",
      "Epoch 1268/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 89.7026 - val_loss: 91.1532\n",
      "Epoch 1269/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 87.7428 - val_loss: 97.9480\n",
      "Epoch 1270/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 93.5259 - val_loss: 98.9140\n",
      "Epoch 1271/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 92.0317 - val_loss: 94.6595\n",
      "Epoch 1272/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 89.0052 - val_loss: 95.3050\n",
      "Epoch 1273/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 87.4814 - val_loss: 92.6046\n",
      "Epoch 1274/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 87.3413 - val_loss: 92.8060\n",
      "Epoch 1275/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 90.1191 - val_loss: 96.0490\n",
      "Epoch 1276/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 88.1934 - val_loss: 99.7891\n",
      "Epoch 1277/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 90.8549 - val_loss: 97.1007\n",
      "Epoch 1278/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 89.3364 - val_loss: 99.5392\n",
      "Epoch 1279/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 92.1963 - val_loss: 95.6631\n",
      "Epoch 1280/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 85.8072 - val_loss: 91.1304\n",
      "Epoch 1281/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 87.7226 - val_loss: 90.4427\n",
      "Epoch 1282/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 93.5349 - val_loss: 94.5212\n",
      "Epoch 1283/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 88.5212 - val_loss: 92.4315\n",
      "Epoch 1284/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 87.4077 - val_loss: 92.5625\n",
      "Epoch 1285/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 87.6373 - val_loss: 94.9319\n",
      "Epoch 1286/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 87.1629 - val_loss: 97.8110\n",
      "Epoch 1287/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 88.1516 - val_loss: 92.2958\n",
      "Epoch 1288/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 90.9266 - val_loss: 93.3986\n",
      "Epoch 1289/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 89.7468 - val_loss: 96.8231\n",
      "Epoch 1290/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 88.9762 - val_loss: 95.2628\n",
      "Epoch 1291/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 90.6059 - val_loss: 95.2361\n",
      "Epoch 1292/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 88.0056 - val_loss: 93.0158\n",
      "Epoch 1293/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 86.6068 - val_loss: 95.4081\n",
      "Epoch 1294/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 379us/step - loss: 88.0779 - val_loss: 92.6395\n",
      "Epoch 1295/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 92.0099 - val_loss: 94.5856\n",
      "Epoch 1296/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 90.0075 - val_loss: 93.1594\n",
      "Epoch 1297/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 85.9480 - val_loss: 95.6906\n",
      "Epoch 1298/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 84.8788 - val_loss: 90.7911\n",
      "Epoch 1299/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 88.4569 - val_loss: 95.8629\n",
      "Epoch 1300/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 88.9656 - val_loss: 100.4787\n",
      "Epoch 1301/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 86.3043 - val_loss: 100.0648\n",
      "Epoch 1302/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 86.2183 - val_loss: 91.6832\n",
      "Epoch 1303/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 87.1271 - val_loss: 91.0205\n",
      "Epoch 1304/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 86.4365 - val_loss: 96.7800\n",
      "Epoch 1305/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 87.9040 - val_loss: 94.7440\n",
      "Epoch 1306/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 85.6551 - val_loss: 98.6166\n",
      "Epoch 1307/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 89.0168 - val_loss: 89.2225\n",
      "Epoch 1308/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 90.2455 - val_loss: 93.1913\n",
      "Epoch 1309/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 87.6962 - val_loss: 96.3972\n",
      "Epoch 1310/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 86.9358 - val_loss: 88.0018\n",
      "Epoch 1311/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 88.7615 - val_loss: 94.0972\n",
      "Epoch 1312/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 94.8121 - val_loss: 93.6408\n",
      "Epoch 1313/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 90.4686 - val_loss: 93.8215\n",
      "Epoch 1314/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 94.2995 - val_loss: 95.6768\n",
      "Epoch 1315/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 90.7802 - val_loss: 96.4849\n",
      "Epoch 1316/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 94.6690 - val_loss: 90.4074\n",
      "Epoch 1317/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 86.9138 - val_loss: 89.6917\n",
      "Epoch 1318/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 87.3401 - val_loss: 88.7306\n",
      "Epoch 1319/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 90.9963 - val_loss: 89.3614\n",
      "Epoch 1320/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 88.2899 - val_loss: 91.7452\n",
      "Epoch 1321/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 90.0350 - val_loss: 96.3315\n",
      "Epoch 1322/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 87.0414 - val_loss: 94.8881\n",
      "Epoch 1323/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 89.8510 - val_loss: 95.5028\n",
      "Epoch 1324/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 91.9511 - val_loss: 94.3825\n",
      "Epoch 1325/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 90.3186 - val_loss: 85.9352\n",
      "Epoch 1326/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 93.0564 - val_loss: 85.2519\n",
      "Epoch 1327/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 94.9188 - val_loss: 87.7090\n",
      "Epoch 1328/10000\n",
      "96/96 [==============================] - 0s 357us/step - loss: 84.6908 - val_loss: 90.0795\n",
      "Epoch 1329/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 91.0106 - val_loss: 88.4680\n",
      "Epoch 1330/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 93.2590 - val_loss: 89.4736\n",
      "Epoch 1331/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 88.8328 - val_loss: 87.4749\n",
      "Epoch 1332/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 87.1419 - val_loss: 91.2791\n",
      "Epoch 1333/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 83.5329 - val_loss: 88.5775\n",
      "Epoch 1334/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 89.1763 - val_loss: 94.3416\n",
      "Epoch 1335/10000\n",
      "96/96 [==============================] - 0s 352us/step - loss: 89.9825 - val_loss: 89.4277\n",
      "Epoch 1336/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 83.6795 - val_loss: 90.9932\n",
      "Epoch 1337/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 88.6492 - val_loss: 89.5553\n",
      "Epoch 1338/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 87.0084 - val_loss: 93.8309\n",
      "Epoch 1339/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 84.6857 - val_loss: 87.3151\n",
      "Epoch 1340/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 87.8566 - val_loss: 103.1950\n",
      "Epoch 1341/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 92.0267 - val_loss: 93.0761\n",
      "Epoch 1342/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 93.3545 - val_loss: 97.4562\n",
      "Epoch 1343/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 86.9326 - val_loss: 95.8848\n",
      "Epoch 1344/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 87.2469 - val_loss: 95.1623\n",
      "Epoch 1345/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 85.2173 - val_loss: 92.3394\n",
      "Epoch 1346/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 85.6116 - val_loss: 91.8070\n",
      "Epoch 1347/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 84.6472 - val_loss: 90.9011\n",
      "Epoch 1348/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 84.1903 - val_loss: 91.5382\n",
      "Epoch 1349/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 88.4812 - val_loss: 92.2952\n",
      "Epoch 1350/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 82.5281 - val_loss: 92.0763\n",
      "Epoch 1351/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 82.0159 - val_loss: 86.3977\n",
      "Epoch 1352/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 85.3889 - val_loss: 87.6319\n",
      "Epoch 1353/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 90.5964 - val_loss: 86.7221\n",
      "Epoch 1354/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 93.0009 - val_loss: 92.8065\n",
      "Epoch 1355/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 90.5743 - val_loss: 89.3343\n",
      "Epoch 1356/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 86.1636 - val_loss: 92.8552\n",
      "Epoch 1357/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 89.3923 - val_loss: 89.6810\n",
      "Epoch 1358/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 88.9710 - val_loss: 86.8311\n",
      "Epoch 1359/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 85.2809 - val_loss: 86.9389\n",
      "Epoch 1360/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 88.7292 - val_loss: 83.9731\n",
      "Epoch 1361/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 86.2330 - val_loss: 93.1067\n",
      "Epoch 1362/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 84.6855 - val_loss: 94.7904\n",
      "Epoch 1363/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 87.6811 - val_loss: 87.0501\n",
      "Epoch 1364/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 91.3184 - val_loss: 91.6139\n",
      "Epoch 1365/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 86.9640 - val_loss: 91.2162\n",
      "Epoch 1366/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 84.9257 - val_loss: 89.9780\n",
      "Epoch 1367/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 84.8110 - val_loss: 91.8983\n",
      "Epoch 1368/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 88.8282 - val_loss: 92.5267\n",
      "Epoch 1369/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 80.3198 - val_loss: 84.7197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1370/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 92.5588 - val_loss: 94.2239\n",
      "Epoch 1371/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 87.8690 - val_loss: 90.5436\n",
      "Epoch 1372/10000\n",
      "96/96 [==============================] - 0s 355us/step - loss: 91.0156 - val_loss: 87.9313\n",
      "Epoch 1373/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 86.0883 - val_loss: 91.5916\n",
      "Epoch 1374/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 85.3193 - val_loss: 87.4135\n",
      "Epoch 1375/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 95.9344 - val_loss: 85.5717\n",
      "Epoch 1376/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 88.2770 - val_loss: 91.1134\n",
      "Epoch 1377/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 84.4633 - val_loss: 90.2525\n",
      "Epoch 1378/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 84.7412 - val_loss: 89.6720\n",
      "Epoch 1379/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 86.1433 - val_loss: 93.0542\n",
      "Epoch 1380/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 84.7769 - val_loss: 88.6659\n",
      "Epoch 1381/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 81.5601 - val_loss: 93.7137\n",
      "Epoch 1382/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 83.5253 - val_loss: 94.2806\n",
      "Epoch 1383/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 86.9776 - val_loss: 90.5553\n",
      "Epoch 1384/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 85.4069 - val_loss: 94.7798\n",
      "Epoch 1385/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 83.6697 - val_loss: 89.5781\n",
      "Epoch 1386/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 86.8852 - val_loss: 83.9131\n",
      "Epoch 1387/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 83.8892 - val_loss: 86.9571\n",
      "Epoch 1388/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 81.2921 - val_loss: 89.8745\n",
      "Epoch 1389/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 84.3285 - val_loss: 88.6007\n",
      "Epoch 1390/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 82.2116 - val_loss: 87.3562\n",
      "Epoch 1391/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 89.6512 - val_loss: 87.9294\n",
      "Epoch 1392/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 87.8402 - val_loss: 90.2672\n",
      "Epoch 1393/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 89.2091 - val_loss: 94.7262\n",
      "Epoch 1394/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 90.3873 - val_loss: 96.0238\n",
      "Epoch 1395/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 84.9521 - val_loss: 93.5681\n",
      "Epoch 1396/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 84.5676 - val_loss: 98.6684\n",
      "Epoch 1397/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 81.6974 - val_loss: 89.1073\n",
      "Epoch 1398/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 81.0187 - val_loss: 84.6142\n",
      "Epoch 1399/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 86.1954 - val_loss: 99.6800\n",
      "Epoch 1400/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 83.7409 - val_loss: 89.8323\n",
      "Epoch 1401/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 93.2920 - val_loss: 89.3534\n",
      "Epoch 1402/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 85.7379 - val_loss: 90.6537\n",
      "Epoch 1403/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 89.3993 - val_loss: 91.7662\n",
      "Epoch 1404/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 81.6888 - val_loss: 87.2681\n",
      "Epoch 1405/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 83.6614 - val_loss: 94.8764\n",
      "Epoch 1406/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 83.9843 - val_loss: 99.1299\n",
      "Epoch 1407/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 88.4052 - val_loss: 86.8564\n",
      "Epoch 1408/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 86.8983 - val_loss: 91.2155\n",
      "Epoch 1409/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 87.5992 - val_loss: 92.9654\n",
      "Epoch 1410/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 91.0714 - val_loss: 90.7204\n",
      "Epoch 1411/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 85.7276 - val_loss: 88.4970\n",
      "Epoch 1412/10000\n",
      "96/96 [==============================] - 0s 353us/step - loss: 87.2293 - val_loss: 95.6521\n",
      "Epoch 1413/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 84.8151 - val_loss: 98.3679\n",
      "Epoch 1414/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 88.1253 - val_loss: 106.4069\n",
      "Epoch 1415/10000\n",
      "96/96 [==============================] - 0s 352us/step - loss: 88.9572 - val_loss: 97.3419\n",
      "Epoch 1416/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 88.0501 - val_loss: 96.1097\n",
      "Epoch 1417/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 80.5441 - val_loss: 89.7632\n",
      "Epoch 1418/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 80.0076 - val_loss: 94.6725\n",
      "Epoch 1419/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 83.2093 - val_loss: 97.8027\n",
      "Epoch 1420/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 84.7310 - val_loss: 79.3732\n",
      "Epoch 1421/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 80.1531 - val_loss: 94.3123\n",
      "Epoch 1422/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 84.3930 - val_loss: 82.9537\n",
      "Epoch 1423/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 82.2975 - val_loss: 91.8207\n",
      "Epoch 1424/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 81.3398 - val_loss: 86.9170\n",
      "Epoch 1425/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 85.1779 - val_loss: 85.0417\n",
      "Epoch 1426/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 92.4686 - val_loss: 83.0806\n",
      "Epoch 1427/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 86.3295 - val_loss: 93.4233\n",
      "Epoch 1428/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 82.5784 - val_loss: 89.3792\n",
      "Epoch 1429/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 89.4975 - val_loss: 96.5432\n",
      "Epoch 1430/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 87.6496 - val_loss: 91.5570\n",
      "Epoch 1431/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 84.5416 - val_loss: 91.6318\n",
      "Epoch 1432/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 84.0311 - val_loss: 96.0099\n",
      "Epoch 1433/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 89.3857 - val_loss: 88.1467\n",
      "Epoch 1434/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 86.7623 - val_loss: 89.0605\n",
      "Epoch 1435/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 85.2299 - val_loss: 93.9808\n",
      "Epoch 1436/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 84.9300 - val_loss: 87.1812\n",
      "Epoch 1437/10000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 85.7997 - val_loss: 91.3418\n",
      "Epoch 1438/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 81.5686 - val_loss: 91.1848\n",
      "Epoch 1439/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 88.9056 - val_loss: 93.1877\n",
      "Epoch 1440/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 82.0619 - val_loss: 80.6298\n",
      "Epoch 1441/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 87.8045 - val_loss: 87.2604\n",
      "Epoch 1442/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 85.8372 - val_loss: 87.3811\n",
      "Epoch 1443/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 84.4307 - val_loss: 95.4310\n",
      "Epoch 1444/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 82.6425 - val_loss: 94.9713\n",
      "Epoch 1445/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 83.3030 - val_loss: 86.4207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1446/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 82.2127 - val_loss: 94.2318\n",
      "Epoch 1447/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 86.9565 - val_loss: 94.9930\n",
      "Epoch 1448/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 85.9213 - val_loss: 82.6096\n",
      "Epoch 1449/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 83.1902 - val_loss: 97.1577\n",
      "Epoch 1450/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 86.7465 - val_loss: 81.7555\n",
      "Epoch 1451/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 85.3473 - val_loss: 102.0470\n",
      "Epoch 1452/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 87.0382 - val_loss: 81.7872\n",
      "Epoch 1453/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 87.5677 - val_loss: 99.0536\n",
      "Epoch 1454/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 79.0278 - val_loss: 91.3128\n",
      "Epoch 1455/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 81.9674 - val_loss: 89.9295\n",
      "Epoch 1456/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 82.6082 - val_loss: 89.7350\n",
      "Epoch 1457/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 82.1942 - val_loss: 87.5018\n",
      "Epoch 1458/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 78.8061 - val_loss: 94.7344\n",
      "Epoch 1459/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 79.7941 - val_loss: 82.0266\n",
      "Epoch 1460/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 85.6384 - val_loss: 91.4687\n",
      "Epoch 1461/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 82.6559 - val_loss: 95.5294\n",
      "Epoch 1462/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 80.2882 - val_loss: 94.8069\n",
      "Epoch 1463/10000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 77.7008 - val_loss: 91.7413\n",
      "Epoch 1464/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 81.5437 - val_loss: 90.3970\n",
      "Epoch 1465/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 88.9017 - val_loss: 83.1115\n",
      "Epoch 1466/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 83.7056 - val_loss: 91.0398\n",
      "Epoch 1467/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 84.3089 - val_loss: 90.7083\n",
      "Epoch 1468/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 80.2753 - val_loss: 96.7680\n",
      "Epoch 1469/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 81.9584 - val_loss: 93.7230\n",
      "Epoch 1470/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 79.4896 - val_loss: 96.8344\n",
      "Epoch 1471/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 82.1038 - val_loss: 87.6467\n",
      "Epoch 1472/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 79.1140 - val_loss: 86.5367\n",
      "Epoch 1473/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 81.1843 - val_loss: 91.1969\n",
      "Epoch 1474/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 82.7697 - val_loss: 88.4323\n",
      "Epoch 1475/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 83.0146 - val_loss: 96.1737\n",
      "Epoch 1476/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 87.0753 - val_loss: 88.0058\n",
      "Epoch 1477/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 80.2522 - val_loss: 95.7859\n",
      "Epoch 1478/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 76.1701 - val_loss: 91.0603\n",
      "Epoch 1479/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 80.6626 - val_loss: 93.3582\n",
      "Epoch 1480/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 84.3157 - val_loss: 84.6511\n",
      "Epoch 1481/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 77.2441 - val_loss: 82.3752\n",
      "Epoch 1482/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 81.2076 - val_loss: 81.6213\n",
      "Epoch 1483/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 80.4801 - val_loss: 81.3419\n",
      "Epoch 1484/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 86.8492 - val_loss: 98.3173\n",
      "Epoch 1485/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 85.1124 - val_loss: 95.6771\n",
      "Epoch 1486/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 85.7165 - val_loss: 94.3191\n",
      "Epoch 1487/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 80.7464 - val_loss: 96.0813\n",
      "Epoch 1488/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 83.3825 - val_loss: 91.6735\n",
      "Epoch 1489/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 81.9596 - val_loss: 88.7545\n",
      "Epoch 1490/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 78.7461 - val_loss: 81.1005\n",
      "Epoch 1491/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 84.4206 - val_loss: 96.4624\n",
      "Epoch 1492/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 76.2514 - val_loss: 95.0457\n",
      "Epoch 1493/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 79.5883 - val_loss: 101.2610\n",
      "Epoch 1494/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 83.8250 - val_loss: 95.1764\n",
      "Epoch 1495/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 81.3217 - val_loss: 93.9529\n",
      "Epoch 1496/10000\n",
      "96/96 [==============================] - 0s 353us/step - loss: 83.3928 - val_loss: 90.3177\n",
      "Epoch 1497/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 81.2000 - val_loss: 88.4661\n",
      "Epoch 1498/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 77.3569 - val_loss: 96.4759\n",
      "Epoch 1499/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 80.5490 - val_loss: 97.6091\n",
      "Epoch 1500/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 81.5059 - val_loss: 91.6446\n",
      "Epoch 1501/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 82.6746 - val_loss: 96.0515\n",
      "Epoch 1502/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 82.5250 - val_loss: 96.2196\n",
      "Epoch 1503/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 81.2847 - val_loss: 85.3610\n",
      "Epoch 1504/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 78.0527 - val_loss: 94.2310\n",
      "Epoch 1505/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 84.5848 - val_loss: 96.8575\n",
      "Epoch 1506/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 80.0521 - val_loss: 87.9081\n",
      "Epoch 1507/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 78.2466 - val_loss: 90.9211\n",
      "Epoch 1508/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 78.2781 - val_loss: 91.2073\n",
      "Epoch 1509/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 75.0768 - val_loss: 98.5104\n",
      "Epoch 1510/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 79.4303 - val_loss: 84.9434\n",
      "Epoch 1511/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 79.8107 - val_loss: 101.7009\n",
      "Epoch 1512/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 84.6790 - val_loss: 94.1949\n",
      "Epoch 1513/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 87.1414 - val_loss: 96.9715\n",
      "Epoch 1514/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 82.7076 - val_loss: 86.0034\n",
      "Epoch 1515/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 85.9380 - val_loss: 90.7959\n",
      "Epoch 1516/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 84.4074 - val_loss: 82.5650\n",
      "Epoch 1517/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 84.4167 - val_loss: 90.9124\n",
      "Epoch 1518/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 83.0008 - val_loss: 89.3870\n",
      "Epoch 1519/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 81.7072 - val_loss: 90.1863\n",
      "Epoch 1520/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 82.0631 - val_loss: 99.5168\n",
      "Epoch 1521/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 79.2641 - val_loss: 98.3507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1522/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 80.5789 - val_loss: 85.3811\n",
      "Epoch 1523/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 79.2765 - val_loss: 99.5520\n",
      "Epoch 1524/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 82.1651 - val_loss: 88.5388\n",
      "Epoch 1525/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 79.8634 - val_loss: 94.6301\n",
      "Epoch 1526/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 82.7766 - val_loss: 94.5275\n",
      "Epoch 1527/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 86.1037 - val_loss: 89.4623\n",
      "Epoch 1528/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 78.4702 - val_loss: 92.0609\n",
      "Epoch 1529/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 85.1828 - val_loss: 90.5470\n",
      "Epoch 1530/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 80.7069 - val_loss: 90.8844\n",
      "Epoch 1531/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 88.3645 - val_loss: 94.9968\n",
      "Epoch 1532/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 82.0392 - val_loss: 97.8585\n",
      "Epoch 1533/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 80.4725 - val_loss: 88.9997\n",
      "Epoch 1534/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 83.7960 - val_loss: 88.6306\n",
      "Epoch 1535/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 78.2466 - val_loss: 93.6417\n",
      "Epoch 1536/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 77.1931 - val_loss: 88.0775\n",
      "Epoch 1537/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 81.0871 - val_loss: 92.2968\n",
      "Epoch 1538/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 87.6754 - val_loss: 88.9072\n",
      "Epoch 1539/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 81.5139 - val_loss: 87.4348\n",
      "Epoch 1540/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 77.6270 - val_loss: 91.4314\n",
      "Epoch 1541/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 79.3866 - val_loss: 91.7129\n",
      "Epoch 1542/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 83.4566 - val_loss: 89.2311\n",
      "Epoch 1543/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 80.7810 - val_loss: 86.9304\n",
      "Epoch 1544/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 77.8893 - val_loss: 89.8639\n",
      "Epoch 1545/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 82.1585 - val_loss: 85.4461\n",
      "Epoch 1546/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 82.6408 - val_loss: 84.7863\n",
      "Epoch 1547/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 83.6820 - val_loss: 83.8436\n",
      "Epoch 1548/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 79.5341 - val_loss: 93.4706\n",
      "Epoch 1549/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 78.2371 - val_loss: 91.9523\n",
      "Epoch 1550/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 81.0899 - val_loss: 87.8271\n",
      "Epoch 1551/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 78.9736 - val_loss: 86.8566\n",
      "Epoch 1552/10000\n",
      "96/96 [==============================] - 0s 356us/step - loss: 78.8703 - val_loss: 99.8425\n",
      "Epoch 1553/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 76.2934 - val_loss: 88.9356\n",
      "Epoch 1554/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 83.5706 - val_loss: 94.8769\n",
      "Epoch 1555/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 77.2806 - val_loss: 92.8673\n",
      "Epoch 1556/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 80.9598 - val_loss: 95.9650\n",
      "Epoch 1557/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 79.0876 - val_loss: 87.3462\n",
      "Epoch 1558/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 81.6942 - val_loss: 90.2577\n",
      "Epoch 1559/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 79.8355 - val_loss: 89.0373\n",
      "Epoch 1560/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 76.9333 - val_loss: 90.4161\n",
      "Epoch 1561/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 81.5414 - val_loss: 82.4556\n",
      "Epoch 1562/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 79.0056 - val_loss: 92.7892\n",
      "Epoch 1563/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 78.9113 - val_loss: 87.4282\n",
      "Epoch 1564/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 78.7439 - val_loss: 90.9586\n",
      "Epoch 1565/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 78.3481 - val_loss: 87.1058\n",
      "Epoch 1566/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 81.3022 - val_loss: 86.7879\n",
      "Epoch 1567/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 76.8917 - val_loss: 91.0971\n",
      "Epoch 1568/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 79.4607 - val_loss: 93.0232\n",
      "Epoch 1569/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 79.5510 - val_loss: 90.1924\n",
      "Epoch 1570/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 77.1080 - val_loss: 95.9415\n",
      "Epoch 1571/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 80.4401 - val_loss: 89.8052\n",
      "Epoch 1572/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 75.9535 - val_loss: 89.2850\n",
      "Epoch 1573/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 80.3823 - val_loss: 86.5053\n",
      "Epoch 1574/10000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 81.4020 - val_loss: 95.4494\n",
      "Epoch 1575/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 77.1337 - val_loss: 96.2485\n",
      "Epoch 1576/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 74.8433 - val_loss: 82.4714\n",
      "Epoch 1577/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 79.4334 - val_loss: 86.3810\n",
      "Epoch 1578/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 79.9000 - val_loss: 90.0305\n",
      "Epoch 1579/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 79.5116 - val_loss: 88.9101\n",
      "Epoch 1580/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 78.1274 - val_loss: 94.6018\n",
      "Epoch 1581/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 80.0880 - val_loss: 98.5321\n",
      "Epoch 1582/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 78.2044 - val_loss: 89.8234\n",
      "Epoch 1583/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 79.5002 - val_loss: 105.2238\n",
      "Epoch 1584/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 76.9711 - val_loss: 101.5942\n",
      "Epoch 1585/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 83.9465 - val_loss: 84.5121\n",
      "Epoch 1586/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 78.4629 - val_loss: 93.1215\n",
      "Epoch 1587/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 78.7423 - val_loss: 93.3757\n",
      "Epoch 1588/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 80.6930 - val_loss: 90.6487\n",
      "Epoch 1589/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 80.5085 - val_loss: 97.5034\n",
      "Epoch 1590/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 79.1410 - val_loss: 92.2190\n",
      "Epoch 1591/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 83.8408 - val_loss: 95.2529\n",
      "Epoch 1592/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 75.3969 - val_loss: 88.9352\n",
      "Epoch 1593/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 78.4742 - val_loss: 92.7811\n",
      "Epoch 1594/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 77.2400 - val_loss: 98.4435\n",
      "Epoch 1595/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 81.0709 - val_loss: 94.4140\n",
      "Epoch 1596/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 79.5226 - val_loss: 91.7142\n",
      "Epoch 1597/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 79.9932 - val_loss: 98.1115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1598/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 77.9485 - val_loss: 99.6751\n",
      "Epoch 1599/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 79.5430 - val_loss: 94.1412\n",
      "Epoch 1600/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 76.1257 - val_loss: 88.3187\n",
      "Epoch 1601/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 78.8939 - val_loss: 91.2149\n",
      "Epoch 1602/10000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 81.7250 - val_loss: 85.4636\n",
      "Epoch 1603/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 82.4121 - val_loss: 93.5876\n",
      "Epoch 1604/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 84.9799 - val_loss: 98.4952\n",
      "Epoch 1605/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 83.4686 - val_loss: 99.5791\n",
      "Epoch 1606/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 82.2171 - val_loss: 90.6725\n",
      "Epoch 1607/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 73.7138 - val_loss: 89.0244\n",
      "Epoch 1608/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 83.5019 - val_loss: 99.7456\n",
      "Epoch 1609/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 75.6222 - val_loss: 93.7416\n",
      "Epoch 1610/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 76.5150 - val_loss: 85.1560\n",
      "Epoch 1611/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 76.8763 - val_loss: 88.2712\n",
      "Epoch 1612/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 77.5562 - val_loss: 92.0215\n",
      "Epoch 1613/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 77.6211 - val_loss: 82.4981\n",
      "Epoch 1614/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 77.1481 - val_loss: 89.1079\n",
      "Epoch 1615/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 76.6333 - val_loss: 93.5942\n",
      "Epoch 1616/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 80.6614 - val_loss: 87.1618\n",
      "Epoch 1617/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 80.0509 - val_loss: 95.5303\n",
      "Epoch 1618/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 82.2158 - val_loss: 88.0972\n",
      "Epoch 1619/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 79.8502 - val_loss: 89.1517\n",
      "Epoch 1620/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 79.1250 - val_loss: 91.8278\n",
      "Epoch 1621/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 81.2045 - val_loss: 85.7191\n",
      "Epoch 1622/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 76.6092 - val_loss: 89.8164\n",
      "Epoch 1623/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 77.9894 - val_loss: 92.6519\n",
      "Epoch 1624/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 80.2410 - val_loss: 89.8249\n",
      "Epoch 1625/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 79.5638 - val_loss: 94.6955\n",
      "Epoch 1626/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 75.9886 - val_loss: 89.2702\n",
      "Epoch 1627/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 75.5718 - val_loss: 87.4875\n",
      "Epoch 1628/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 80.1379 - val_loss: 88.2963\n",
      "Epoch 1629/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 78.6432 - val_loss: 92.1564\n",
      "Epoch 1630/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 78.6271 - val_loss: 99.5542\n",
      "Epoch 1631/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 76.7631 - val_loss: 94.7728\n",
      "Epoch 1632/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 76.0898 - val_loss: 96.7379\n",
      "Epoch 1633/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 77.7452 - val_loss: 88.9859\n",
      "Epoch 1634/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 82.0483 - val_loss: 87.9799\n",
      "Epoch 1635/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 76.4748 - val_loss: 89.4002\n",
      "Epoch 1636/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 89.3024 - val_loss: 93.2591\n",
      "Epoch 1637/10000\n",
      "96/96 [==============================] - 0s 355us/step - loss: 76.5288 - val_loss: 77.1008\n",
      "Epoch 1638/10000\n",
      "96/96 [==============================] - 0s 346us/step - loss: 77.9418 - val_loss: 82.6288\n",
      "Epoch 1639/10000\n",
      "96/96 [==============================] - 0s 348us/step - loss: 80.8509 - val_loss: 98.3622\n",
      "Epoch 1640/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 80.2077 - val_loss: 85.7232\n",
      "Epoch 1641/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 77.4729 - val_loss: 91.1114\n",
      "Epoch 1642/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 76.6696 - val_loss: 94.4110\n",
      "Epoch 1643/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 78.5091 - val_loss: 93.9591\n",
      "Epoch 1644/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 79.5483 - val_loss: 89.9604\n",
      "Epoch 1645/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 79.5159 - val_loss: 91.0464\n",
      "Epoch 1646/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 76.4168 - val_loss: 90.8093\n",
      "Epoch 1647/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 77.4931 - val_loss: 95.0850\n",
      "Epoch 1648/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 78.7565 - val_loss: 92.6486\n",
      "Epoch 1649/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 77.9142 - val_loss: 87.9633\n",
      "Epoch 1650/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 78.1552 - val_loss: 89.9492\n",
      "Epoch 1651/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 79.3999 - val_loss: 96.4635\n",
      "Epoch 1652/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 81.2937 - val_loss: 92.4722\n",
      "Epoch 1653/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 79.2995 - val_loss: 101.3555\n",
      "Epoch 1654/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 83.1751 - val_loss: 98.4653\n",
      "Epoch 1655/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 79.2462 - val_loss: 95.6368\n",
      "Epoch 1656/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 79.6922 - val_loss: 85.8350\n",
      "Epoch 1657/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 75.1665 - val_loss: 89.3009\n",
      "Epoch 1658/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 80.3304 - val_loss: 98.1622\n",
      "Epoch 1659/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 77.8853 - val_loss: 86.5480\n",
      "Epoch 1660/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 80.4091 - val_loss: 96.7501\n",
      "Epoch 1661/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 82.1969 - val_loss: 94.3966\n",
      "Epoch 1662/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 78.9136 - val_loss: 98.4384\n",
      "Epoch 1663/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 78.3158 - val_loss: 93.5811\n",
      "Epoch 1664/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 78.1264 - val_loss: 89.1909\n",
      "Epoch 1665/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 81.3468 - val_loss: 97.2763\n",
      "Epoch 1666/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 79.0113 - val_loss: 80.4954\n",
      "Epoch 1667/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 75.5529 - val_loss: 98.0261\n",
      "Epoch 1668/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 76.9539 - val_loss: 86.4624\n",
      "Epoch 1669/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 76.1814 - val_loss: 84.5119\n",
      "Epoch 1670/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 79.5556 - val_loss: 98.1178\n",
      "Epoch 1671/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 81.8967 - val_loss: 92.0015\n",
      "Epoch 1672/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 74.9559 - val_loss: 83.0281\n",
      "Epoch 1673/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 77.4562 - val_loss: 86.9160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1674/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 78.8769 - val_loss: 88.1880\n",
      "Epoch 1675/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 77.6888 - val_loss: 89.3713\n",
      "Epoch 1676/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 79.7802 - val_loss: 85.7841\n",
      "Epoch 1677/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 76.4958 - val_loss: 81.6259\n",
      "Epoch 1678/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 76.4610 - val_loss: 94.9322\n",
      "Epoch 1679/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 74.1580 - val_loss: 87.3247\n",
      "Epoch 1680/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 79.1666 - val_loss: 90.1548\n",
      "Epoch 1681/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 73.5733 - val_loss: 93.0630\n",
      "Epoch 1682/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 74.1469 - val_loss: 88.2628\n",
      "Epoch 1683/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 83.7186 - val_loss: 89.6159\n",
      "Epoch 1684/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 82.7950 - val_loss: 94.9091\n",
      "Epoch 1685/10000\n",
      "96/96 [==============================] - 0s 493us/step - loss: 79.0207 - val_loss: 90.4824\n",
      "Epoch 1686/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 81.6976 - val_loss: 86.1889\n",
      "Epoch 1687/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 74.4034 - val_loss: 81.1355\n",
      "Epoch 1688/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 76.1735 - val_loss: 84.8761\n",
      "Epoch 1689/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 74.7404 - val_loss: 86.0057\n",
      "Epoch 1690/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 76.5045 - val_loss: 84.8738\n",
      "Epoch 1691/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 75.2276 - val_loss: 94.1291\n",
      "Epoch 1692/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 78.6749 - val_loss: 79.0199\n",
      "Epoch 1693/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 72.8441 - val_loss: 83.0170\n",
      "Epoch 1694/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 76.3448 - val_loss: 82.7609\n",
      "Epoch 1695/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 76.3873 - val_loss: 83.8441\n",
      "Epoch 1696/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 75.9568 - val_loss: 86.3105\n",
      "Epoch 1697/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 76.0523 - val_loss: 92.2998\n",
      "Epoch 1698/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 82.5073 - val_loss: 89.0242\n",
      "Epoch 1699/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 79.5621 - val_loss: 92.8748\n",
      "Epoch 1700/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 76.5936 - val_loss: 89.3464\n",
      "Epoch 1701/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 75.1274 - val_loss: 79.2649\n",
      "Epoch 1702/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 77.0630 - val_loss: 89.5615\n",
      "Epoch 1703/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 82.6595 - val_loss: 87.1829\n",
      "Epoch 1704/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 75.8823 - val_loss: 83.0992\n",
      "Epoch 1705/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 75.0954 - val_loss: 96.3683\n",
      "Epoch 1706/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 78.2812 - val_loss: 98.2220\n",
      "Epoch 1707/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 75.5371 - val_loss: 91.9571\n",
      "Epoch 1708/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 74.1473 - val_loss: 84.7201\n",
      "Epoch 1709/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 76.2049 - val_loss: 95.8533\n",
      "Epoch 1710/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 75.2553 - val_loss: 96.8073\n",
      "Epoch 1711/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 76.2129 - val_loss: 95.2228\n",
      "Epoch 1712/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 76.0209 - val_loss: 94.4921\n",
      "Epoch 1713/10000\n",
      "96/96 [==============================] - 0s 482us/step - loss: 75.6984 - val_loss: 91.2016\n",
      "Epoch 1714/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 75.8722 - val_loss: 92.1222\n",
      "Epoch 1715/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 77.7286 - val_loss: 90.6148\n",
      "Epoch 1716/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 74.7340 - val_loss: 98.7733\n",
      "Epoch 1717/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 73.7497 - val_loss: 97.6302\n",
      "Epoch 1718/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 74.0740 - val_loss: 89.8382\n",
      "Epoch 1719/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 74.4204 - val_loss: 99.1668\n",
      "Epoch 1720/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 75.4586 - val_loss: 86.3530\n",
      "Epoch 1721/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 76.5930 - val_loss: 87.8274\n",
      "Epoch 1722/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 79.1544 - val_loss: 89.0875\n",
      "Epoch 1723/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 81.7229 - val_loss: 99.0103\n",
      "Epoch 1724/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 78.2588 - val_loss: 91.5187\n",
      "Epoch 1725/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 76.1041 - val_loss: 86.8976\n",
      "Epoch 1726/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 74.1965 - val_loss: 84.9788\n",
      "Epoch 1727/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 74.0579 - val_loss: 87.4085\n",
      "Epoch 1728/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 82.4113 - val_loss: 84.6697\n",
      "Epoch 1729/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 74.6100 - val_loss: 86.9106\n",
      "Epoch 1730/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 76.8541 - val_loss: 87.5558\n",
      "Epoch 1731/10000\n",
      "96/96 [==============================] - 0s 350us/step - loss: 78.2142 - val_loss: 89.4275\n",
      "Epoch 1732/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 74.7142 - val_loss: 96.4394\n",
      "Epoch 1733/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 79.4852 - val_loss: 95.0907\n",
      "Epoch 1734/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 76.1358 - val_loss: 98.4729\n",
      "Epoch 1735/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 79.3767 - val_loss: 89.5323\n",
      "Epoch 1736/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 75.2630 - val_loss: 94.7490\n",
      "Epoch 1737/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 75.3337 - val_loss: 85.3679\n",
      "Epoch 1738/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 75.2355 - val_loss: 92.0420\n",
      "Epoch 1739/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 75.3514 - val_loss: 94.3804\n",
      "Epoch 1740/10000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 80.0496 - val_loss: 91.6890\n",
      "Epoch 1741/10000\n",
      "96/96 [==============================] - 0s 496us/step - loss: 77.7607 - val_loss: 87.2572\n",
      "Epoch 1742/10000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 75.7131 - val_loss: 83.1695\n",
      "Epoch 1743/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 76.9047 - val_loss: 82.0331\n",
      "Epoch 1744/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 73.5554 - val_loss: 84.6917\n",
      "Epoch 1745/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 74.7235 - val_loss: 92.0776\n",
      "Epoch 1746/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 74.8146 - val_loss: 95.1121\n",
      "Epoch 1747/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 78.8444 - val_loss: 84.5135\n",
      "Epoch 1748/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 76.7915 - val_loss: 89.7852\n",
      "Epoch 1749/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 78.1179 - val_loss: 91.1868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1750/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 76.0819 - val_loss: 88.2567\n",
      "Epoch 1751/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 78.2155 - val_loss: 78.5602\n",
      "Epoch 1752/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 75.6761 - val_loss: 91.2048\n",
      "Epoch 1753/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 73.8101 - val_loss: 89.8591\n",
      "Epoch 1754/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 76.4378 - val_loss: 88.6676\n",
      "Epoch 1755/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 70.3880 - val_loss: 100.6549\n",
      "Epoch 1756/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 81.8152 - val_loss: 92.9086\n",
      "Epoch 1757/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 80.4909 - val_loss: 92.8719\n",
      "Epoch 1758/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 74.8428 - val_loss: 107.6477\n",
      "Epoch 1759/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 74.9147 - val_loss: 89.5475\n",
      "Epoch 1760/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 76.3070 - val_loss: 99.6894\n",
      "Epoch 1761/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 75.3702 - val_loss: 94.1186\n",
      "Epoch 1762/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 72.2328 - val_loss: 93.3080\n",
      "Epoch 1763/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 73.8290 - val_loss: 93.3473\n",
      "Epoch 1764/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 78.5670 - val_loss: 92.8444\n",
      "Epoch 1765/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 77.9614 - val_loss: 92.7514\n",
      "Epoch 1766/10000\n",
      "96/96 [==============================] - 0s 355us/step - loss: 74.9629 - val_loss: 97.2172\n",
      "Epoch 1767/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 75.7484 - val_loss: 86.9050\n",
      "Epoch 1768/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 79.3470 - val_loss: 98.9310\n",
      "Epoch 1769/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 75.0245 - val_loss: 83.2026\n",
      "Epoch 1770/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 82.9720 - val_loss: 92.7904\n",
      "Epoch 1771/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 76.9835 - val_loss: 96.6404\n",
      "Epoch 1772/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 75.9554 - val_loss: 83.7218\n",
      "Epoch 1773/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 74.9001 - val_loss: 95.2713\n",
      "Epoch 1774/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 73.7887 - val_loss: 99.0475\n",
      "Epoch 1775/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 78.9428 - val_loss: 102.8468\n",
      "Epoch 1776/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 77.7374 - val_loss: 87.1353\n",
      "Epoch 1777/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 76.2659 - val_loss: 87.6519\n",
      "Epoch 1778/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 81.4395 - val_loss: 87.8120\n",
      "Epoch 1779/10000\n",
      "96/96 [==============================] - 0s 352us/step - loss: 81.2231 - val_loss: 88.1605\n",
      "Epoch 1780/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 77.1167 - val_loss: 95.3783\n",
      "Epoch 1781/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 75.2636 - val_loss: 82.0206\n",
      "Epoch 1782/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 73.3967 - val_loss: 97.8330\n",
      "Epoch 1783/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 74.8243 - val_loss: 99.5593\n",
      "Epoch 1784/10000\n",
      "96/96 [==============================] - 0s 356us/step - loss: 77.6708 - val_loss: 87.0729\n",
      "Epoch 1785/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 76.3104 - val_loss: 92.4943\n",
      "Epoch 1786/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 79.6974 - val_loss: 85.6923\n",
      "Epoch 1787/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 79.6907 - val_loss: 96.5069\n",
      "Epoch 1788/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 87.3953 - val_loss: 94.3311\n",
      "Epoch 1789/10000\n",
      "96/96 [==============================] - 0s 357us/step - loss: 75.7073 - val_loss: 83.9780\n",
      "Epoch 1790/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 76.2225 - val_loss: 90.3054\n",
      "Epoch 1791/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 76.0180 - val_loss: 92.3645\n",
      "Epoch 1792/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 76.8700 - val_loss: 95.5116\n",
      "Epoch 1793/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 73.1739 - val_loss: 93.7872\n",
      "Epoch 1794/10000\n",
      "96/96 [==============================] - 0s 350us/step - loss: 76.2045 - val_loss: 93.0661\n",
      "Epoch 1795/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 75.6743 - val_loss: 89.3609\n",
      "Epoch 1796/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 73.1886 - val_loss: 97.6676\n",
      "Epoch 1797/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 75.4484 - val_loss: 96.7294\n",
      "Epoch 1798/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 76.5122 - val_loss: 100.0667\n",
      "Epoch 1799/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 75.8990 - val_loss: 91.1033\n",
      "Epoch 1800/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 80.2766 - val_loss: 83.4970\n",
      "Epoch 1801/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 75.6604 - val_loss: 107.0532\n",
      "Epoch 1802/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 74.4593 - val_loss: 99.8258\n",
      "Epoch 1803/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 73.1216 - val_loss: 83.9711\n",
      "Epoch 1804/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 75.3588 - val_loss: 89.1931\n",
      "Epoch 1805/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 76.7839 - val_loss: 90.7372\n",
      "Epoch 1806/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 73.8660 - val_loss: 97.2468\n",
      "Epoch 1807/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 74.9589 - val_loss: 93.0630\n",
      "Epoch 1808/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 73.8170 - val_loss: 87.9533\n",
      "Epoch 1809/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 71.8431 - val_loss: 97.6463\n",
      "Epoch 1810/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 78.3835 - val_loss: 90.1878\n",
      "Epoch 1811/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 79.0164 - val_loss: 99.5250\n",
      "Epoch 1812/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 77.6467 - val_loss: 97.7266\n",
      "Epoch 1813/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 75.2992 - val_loss: 104.7071\n",
      "Epoch 1814/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 73.3630 - val_loss: 106.8626\n",
      "Epoch 1815/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 79.1834 - val_loss: 88.8703\n",
      "Epoch 1816/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 74.0627 - val_loss: 92.5883\n",
      "Epoch 1817/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 77.8120 - val_loss: 94.9977\n",
      "Epoch 1818/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 76.0874 - val_loss: 91.8682\n",
      "Epoch 1819/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 77.6933 - val_loss: 110.2418\n",
      "Epoch 1820/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 78.4482 - val_loss: 85.9303\n",
      "Epoch 1821/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 76.1560 - val_loss: 93.0076\n",
      "Epoch 1822/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 77.0461 - val_loss: 94.0394\n",
      "Epoch 1823/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 77.9397 - val_loss: 99.3598\n",
      "Epoch 1824/10000\n",
      "96/96 [==============================] - 0s 487us/step - loss: 78.8614 - val_loss: 82.6810\n",
      "Epoch 1825/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 74.8050 - val_loss: 105.1240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1826/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 76.4514 - val_loss: 102.7363\n",
      "Epoch 1827/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 76.1898 - val_loss: 89.6011\n",
      "Epoch 1828/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 75.6732 - val_loss: 88.0937\n",
      "Epoch 1829/10000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 74.7692 - val_loss: 88.4781\n",
      "Epoch 1830/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 77.4539 - val_loss: 96.8570\n",
      "Epoch 1831/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 76.3672 - val_loss: 90.6471\n",
      "Epoch 1832/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 75.1344 - val_loss: 98.4451\n",
      "Epoch 1833/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 78.5367 - val_loss: 96.8528\n",
      "Epoch 1834/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 72.9840 - val_loss: 86.9129\n",
      "Epoch 1835/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 76.7643 - val_loss: 80.8743\n",
      "Epoch 1836/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 75.1015 - val_loss: 89.8723\n",
      "Epoch 1837/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 82.6735 - val_loss: 101.0026\n",
      "Epoch 1838/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 75.7218 - val_loss: 81.5400\n",
      "Epoch 1839/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 76.3483 - val_loss: 84.0172\n",
      "Epoch 1840/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 78.4540 - val_loss: 103.6164\n",
      "Epoch 1841/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 77.5820 - val_loss: 105.1144\n",
      "Epoch 1842/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 76.0986 - val_loss: 101.4799\n",
      "Epoch 1843/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 75.9745 - val_loss: 97.3564\n",
      "Epoch 1844/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 77.6870 - val_loss: 99.7393\n",
      "Epoch 1845/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 73.7514 - val_loss: 103.9267\n",
      "Epoch 1846/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 73.9529 - val_loss: 90.0617\n",
      "Epoch 1847/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 76.9802 - val_loss: 92.9783\n",
      "Epoch 1848/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 78.4345 - val_loss: 88.2666\n",
      "Epoch 1849/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 75.4282 - val_loss: 91.5337\n",
      "Epoch 1850/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 76.3096 - val_loss: 83.1815\n",
      "Epoch 1851/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 74.7763 - val_loss: 95.7109\n",
      "Epoch 1852/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 70.4927 - val_loss: 101.8554\n",
      "Epoch 1853/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 74.2805 - val_loss: 92.1389\n",
      "Epoch 1854/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 77.9386 - val_loss: 97.7449\n",
      "Epoch 1855/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 70.8720 - val_loss: 92.2941\n",
      "Epoch 1856/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 75.0026 - val_loss: 101.0761\n",
      "Epoch 1857/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 74.0503 - val_loss: 87.4030\n",
      "Epoch 1858/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 74.2482 - val_loss: 78.6734\n",
      "Epoch 1859/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 76.1001 - val_loss: 94.7429\n",
      "Epoch 1860/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 75.5780 - val_loss: 91.0590\n",
      "Epoch 1861/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 74.8912 - val_loss: 87.1469\n",
      "Epoch 1862/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 88.4179 - val_loss: 80.7068\n",
      "Epoch 1863/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 72.6863 - val_loss: 90.1429\n",
      "Epoch 1864/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 76.5216 - val_loss: 91.1635\n",
      "Epoch 1865/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 76.8733 - val_loss: 86.8760\n",
      "Epoch 1866/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 72.7314 - val_loss: 79.8455\n",
      "Epoch 1867/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 76.0978 - val_loss: 93.5304\n",
      "Epoch 1868/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 75.1491 - val_loss: 89.8934\n",
      "Epoch 1869/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 82.4402 - val_loss: 90.3127\n",
      "Epoch 1870/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 75.0297 - val_loss: 94.3837\n",
      "Epoch 1871/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 73.5466 - val_loss: 90.6830\n",
      "Epoch 1872/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 73.0178 - val_loss: 94.6925\n",
      "Epoch 1873/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 75.6683 - val_loss: 92.6466\n",
      "Epoch 1874/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 75.5100 - val_loss: 92.9327\n",
      "Epoch 1875/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 80.3316 - val_loss: 88.7216\n",
      "Epoch 1876/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 72.7099 - val_loss: 97.1971\n",
      "Epoch 1877/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 72.8921 - val_loss: 91.3183\n",
      "Epoch 1878/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 79.1169 - val_loss: 83.4274\n",
      "Epoch 1879/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 77.9648 - val_loss: 91.8533\n",
      "Epoch 1880/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 77.9007 - val_loss: 99.6077\n",
      "Epoch 1881/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 74.2928 - val_loss: 99.2913\n",
      "Epoch 1882/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 81.0499 - val_loss: 97.9770\n",
      "Epoch 1883/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 76.2145 - val_loss: 93.0901\n",
      "Epoch 1884/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 82.4251 - val_loss: 98.2028\n",
      "Epoch 1885/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 77.9016 - val_loss: 88.0153\n",
      "Epoch 1886/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 74.2869 - val_loss: 95.3785\n",
      "Epoch 1887/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 75.3342 - val_loss: 95.5090\n",
      "Epoch 1888/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 74.4452 - val_loss: 94.3187\n",
      "Epoch 1889/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 74.9239 - val_loss: 94.0671\n",
      "Epoch 1890/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 74.5226 - val_loss: 85.8557\n",
      "Epoch 1891/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 83.9172 - val_loss: 91.6133\n",
      "Epoch 1892/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 77.2683 - val_loss: 94.2121\n",
      "Epoch 1893/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 79.9244 - val_loss: 99.9584\n",
      "Epoch 1894/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 73.7999 - val_loss: 98.8086\n",
      "Epoch 1895/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 78.7653 - val_loss: 92.9388\n",
      "Epoch 1896/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 72.3131 - val_loss: 88.2908\n",
      "Epoch 1897/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 74.8072 - val_loss: 95.2672\n",
      "Epoch 1898/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 75.2177 - val_loss: 89.7114\n",
      "Epoch 1899/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 72.8359 - val_loss: 91.2196\n",
      "Epoch 1900/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 72.6561 - val_loss: 95.6533\n",
      "Epoch 1901/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 77.3985 - val_loss: 87.6943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1902/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 72.7252 - val_loss: 89.9963\n",
      "Epoch 1903/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 74.8196 - val_loss: 83.5578\n",
      "Epoch 1904/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 75.3284 - val_loss: 87.2383\n",
      "Epoch 1905/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 74.2262 - val_loss: 94.4248\n",
      "Epoch 1906/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 74.9384 - val_loss: 84.6076\n",
      "Epoch 1907/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 73.0343 - val_loss: 84.2278\n",
      "Epoch 1908/10000\n",
      "96/96 [==============================] - 0s 518us/step - loss: 71.7049 - val_loss: 85.4816\n",
      "Epoch 1909/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 72.5778 - val_loss: 88.9963\n",
      "Epoch 1910/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 70.0134 - val_loss: 85.9095\n",
      "Epoch 1911/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 73.0641 - val_loss: 84.7643\n",
      "Epoch 1912/10000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 70.6706 - val_loss: 96.3213\n",
      "Epoch 1913/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 78.3622 - val_loss: 85.2672\n",
      "Epoch 1914/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 75.0022 - val_loss: 92.2924\n",
      "Epoch 1915/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 69.3871 - val_loss: 90.1381\n",
      "Epoch 1916/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 74.1928 - val_loss: 89.5792\n",
      "Epoch 1917/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 75.7613 - val_loss: 90.3447\n",
      "Epoch 1918/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 78.2097 - val_loss: 97.4794\n",
      "Epoch 1919/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 73.6593 - val_loss: 86.7901\n",
      "Epoch 1920/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 72.4287 - val_loss: 99.2597\n",
      "Epoch 1921/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 71.0076 - val_loss: 95.7255\n",
      "Epoch 1922/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 71.4089 - val_loss: 93.4905\n",
      "Epoch 1923/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 71.9814 - val_loss: 102.0293\n",
      "Epoch 1924/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 72.0100 - val_loss: 90.9104\n",
      "Epoch 1925/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 72.0450 - val_loss: 96.9777\n",
      "Epoch 1926/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 72.7603 - val_loss: 91.1669\n",
      "Epoch 1927/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 72.6846 - val_loss: 98.5689\n",
      "Epoch 1928/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 79.7196 - val_loss: 94.5587\n",
      "Epoch 1929/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 74.0130 - val_loss: 96.1694\n",
      "Epoch 1930/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 74.1939 - val_loss: 95.1018\n",
      "Epoch 1931/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 75.1685 - val_loss: 91.4899\n",
      "Epoch 1932/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 72.8440 - val_loss: 97.9778\n",
      "Epoch 1933/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 73.0721 - val_loss: 103.3943\n",
      "Epoch 1934/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 75.2561 - val_loss: 97.0310\n",
      "Epoch 1935/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 70.9230 - val_loss: 87.5439\n",
      "Epoch 1936/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 75.3427 - val_loss: 92.1326\n",
      "Epoch 1937/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 75.8080 - val_loss: 95.1131\n",
      "Epoch 1938/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 70.6126 - val_loss: 87.1765\n",
      "Epoch 1939/10000\n",
      "96/96 [==============================] - 0s 351us/step - loss: 74.3311 - val_loss: 90.2201\n",
      "Epoch 1940/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 74.9495 - val_loss: 100.8192\n",
      "Epoch 1941/10000\n",
      "96/96 [==============================] - 0s 354us/step - loss: 74.9581 - val_loss: 96.2139\n",
      "Epoch 1942/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 71.8588 - val_loss: 87.4917\n",
      "Epoch 1943/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 71.9604 - val_loss: 86.0916\n",
      "Epoch 1944/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 73.8805 - val_loss: 92.1298\n",
      "Epoch 1945/10000\n",
      "96/96 [==============================] - 0s 349us/step - loss: 71.2215 - val_loss: 78.5444\n",
      "Epoch 1946/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 72.6220 - val_loss: 98.9846\n",
      "Epoch 1947/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 71.3880 - val_loss: 97.9883\n",
      "Epoch 1948/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 73.5061 - val_loss: 87.8936\n",
      "Epoch 1949/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 75.2482 - val_loss: 92.1185\n",
      "Epoch 1950/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 73.7040 - val_loss: 86.4883\n",
      "Epoch 1951/10000\n",
      "96/96 [==============================] - 0s 357us/step - loss: 72.4606 - val_loss: 82.8297\n",
      "Epoch 1952/10000\n",
      "96/96 [==============================] - 0s 348us/step - loss: 71.4696 - val_loss: 96.0794\n",
      "Epoch 1953/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 74.6939 - val_loss: 88.0942\n",
      "Epoch 1954/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 72.4867 - val_loss: 100.5856\n",
      "Epoch 1955/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 73.1538 - val_loss: 89.1284\n",
      "Epoch 1956/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 72.3412 - val_loss: 95.7673\n",
      "Epoch 1957/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 74.0003 - val_loss: 89.7663\n",
      "Epoch 1958/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 74.4546 - val_loss: 89.1460\n",
      "Epoch 1959/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 73.1950 - val_loss: 90.4962\n",
      "Epoch 1960/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 72.0128 - val_loss: 97.1568\n",
      "Epoch 1961/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 74.3148 - val_loss: 89.5019\n",
      "Epoch 1962/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 74.7602 - val_loss: 91.1922\n",
      "Epoch 1963/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 70.7712 - val_loss: 97.4475\n",
      "Epoch 1964/10000\n",
      "96/96 [==============================] - 0s 511us/step - loss: 74.4352 - val_loss: 91.9139\n",
      "Epoch 1965/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 71.5295 - val_loss: 104.4885\n",
      "Epoch 1966/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 74.5158 - val_loss: 92.9354\n",
      "Epoch 1967/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 73.8632 - val_loss: 91.0567\n",
      "Epoch 1968/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 75.1275 - val_loss: 84.6204\n",
      "Epoch 1969/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 72.2337 - val_loss: 92.6000\n",
      "Epoch 1970/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 70.6503 - val_loss: 94.3661\n",
      "Epoch 1971/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 74.1057 - val_loss: 89.1529\n",
      "Epoch 1972/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 72.4063 - val_loss: 91.8416\n",
      "Epoch 1973/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 70.0460 - val_loss: 95.2319\n",
      "Epoch 1974/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 72.3961 - val_loss: 95.6370\n",
      "Epoch 1975/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 73.1720 - val_loss: 95.3620\n",
      "Epoch 1976/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 72.4305 - val_loss: 95.1141\n",
      "Epoch 1977/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 70.0269 - val_loss: 92.1526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1978/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 74.6098 - val_loss: 103.3488\n",
      "Epoch 1979/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 73.2587 - val_loss: 96.9187\n",
      "Epoch 1980/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 71.5986 - val_loss: 99.1983\n",
      "Epoch 1981/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 73.5179 - val_loss: 96.0271\n",
      "Epoch 1982/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 72.1351 - val_loss: 91.2723\n",
      "Epoch 1983/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 71.2279 - val_loss: 95.9501\n",
      "Epoch 1984/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 72.7770 - val_loss: 102.2900\n",
      "Epoch 1985/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 72.9314 - val_loss: 80.6028\n",
      "Epoch 1986/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 72.5244 - val_loss: 91.4470\n",
      "Epoch 1987/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 71.4822 - val_loss: 82.9992\n",
      "Epoch 1988/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 72.4874 - val_loss: 103.5327\n",
      "Epoch 1989/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 71.4054 - val_loss: 95.8786\n",
      "Epoch 1990/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 72.9625 - val_loss: 88.6523\n",
      "Epoch 1991/10000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 77.1504 - val_loss: 83.2797\n",
      "Epoch 1992/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 73.9028 - val_loss: 102.0543\n",
      "Epoch 1993/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 72.1668 - val_loss: 94.4098\n",
      "Epoch 1994/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 72.6846 - val_loss: 94.3391\n",
      "Epoch 1995/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 73.4487 - val_loss: 83.4434\n",
      "Epoch 1996/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 71.3980 - val_loss: 97.6598\n",
      "Epoch 1997/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 72.1749 - val_loss: 99.6181\n",
      "Epoch 1998/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 76.2396 - val_loss: 100.1927\n",
      "Epoch 1999/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 71.5143 - val_loss: 90.7520\n",
      "Epoch 2000/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 80.3668 - val_loss: 95.7306\n",
      "Epoch 2001/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 75.5772 - val_loss: 86.3350\n",
      "Epoch 2002/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 71.4026 - val_loss: 94.7662\n",
      "Epoch 2003/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 72.0902 - val_loss: 101.3179\n",
      "Epoch 2004/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 73.8889 - val_loss: 86.4206\n",
      "Epoch 2005/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 71.5149 - val_loss: 85.8567\n",
      "Epoch 2006/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 71.5880 - val_loss: 87.1235\n",
      "Epoch 2007/10000\n",
      "96/96 [==============================] - 0s 347us/step - loss: 72.3862 - val_loss: 92.4562\n",
      "Epoch 2008/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 73.6252 - val_loss: 98.5281\n",
      "Epoch 2009/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 72.7861 - val_loss: 89.0966\n",
      "Epoch 2010/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 74.3467 - val_loss: 95.2962\n",
      "Epoch 2011/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 73.7252 - val_loss: 98.8732\n",
      "Epoch 2012/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 71.8477 - val_loss: 97.5292\n",
      "Epoch 2013/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 71.4801 - val_loss: 89.3035\n",
      "Epoch 2014/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 76.1824 - val_loss: 88.8491\n",
      "Epoch 2015/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 70.8358 - val_loss: 83.1469\n",
      "Epoch 2016/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 69.8139 - val_loss: 96.0358\n",
      "Epoch 2017/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 72.0818 - val_loss: 96.5137\n",
      "Epoch 2018/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 72.4007 - val_loss: 95.7088\n",
      "Epoch 2019/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 69.8558 - val_loss: 100.3732\n",
      "Epoch 2020/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 74.2317 - val_loss: 93.3850\n",
      "Epoch 2021/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 71.7005 - val_loss: 98.3872\n",
      "Epoch 2022/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 73.9347 - val_loss: 96.2999\n",
      "Epoch 2023/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 72.6638 - val_loss: 89.6474\n",
      "Epoch 2024/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 74.9907 - val_loss: 108.6614\n",
      "Epoch 2025/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 72.6771 - val_loss: 102.2382\n",
      "Epoch 2026/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 72.3799 - val_loss: 94.0260\n",
      "Epoch 2027/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 74.3197 - val_loss: 88.3600\n",
      "Epoch 2028/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 75.0480 - val_loss: 100.1640\n",
      "Epoch 2029/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 77.6547 - val_loss: 83.0799\n",
      "Epoch 2030/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 74.0560 - val_loss: 92.8898\n",
      "Epoch 2031/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 77.1541 - val_loss: 95.8889\n",
      "Epoch 2032/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 72.7338 - val_loss: 84.9313\n",
      "Epoch 2033/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 72.3336 - val_loss: 88.0079\n",
      "Epoch 2034/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 71.6670 - val_loss: 93.2331\n",
      "Epoch 2035/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 70.8921 - val_loss: 87.0054\n",
      "Epoch 2036/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 72.8046 - val_loss: 92.1564\n",
      "Epoch 2037/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 72.8641 - val_loss: 88.9934\n",
      "Epoch 2038/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 73.4601 - val_loss: 92.8255\n",
      "Epoch 2039/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 75.2092 - val_loss: 97.6131\n",
      "Epoch 2040/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 71.0756 - val_loss: 82.1353\n",
      "Epoch 2041/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 69.6692 - val_loss: 98.2053\n",
      "Epoch 2042/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 70.0153 - val_loss: 87.4689\n",
      "Epoch 2043/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 68.9493 - val_loss: 95.1950\n",
      "Epoch 2044/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 71.9320 - val_loss: 93.3485\n",
      "Epoch 2045/10000\n",
      "96/96 [==============================] - 0s 475us/step - loss: 71.1956 - val_loss: 92.9453\n",
      "Epoch 2046/10000\n",
      "96/96 [==============================] - 0s 546us/step - loss: 70.1280 - val_loss: 98.3204\n",
      "Epoch 2047/10000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 70.9089 - val_loss: 97.1825\n",
      "Epoch 2048/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 71.2093 - val_loss: 102.7501\n",
      "Epoch 2049/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 72.9680 - val_loss: 88.6002\n",
      "Epoch 2050/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 69.6256 - val_loss: 92.1908\n",
      "Epoch 2051/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 69.7956 - val_loss: 94.6938\n",
      "Epoch 2052/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 73.6823 - val_loss: 87.9305\n",
      "Epoch 2053/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 70.4502 - val_loss: 81.3211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2054/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 72.6766 - val_loss: 85.8444\n",
      "Epoch 2055/10000\n",
      "96/96 [==============================] - 0s 482us/step - loss: 70.9570 - val_loss: 97.0194\n",
      "Epoch 2056/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 68.5668 - val_loss: 94.6393\n",
      "Epoch 2057/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 71.9304 - val_loss: 104.0797\n",
      "Epoch 2058/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 78.6852 - val_loss: 107.9066\n",
      "Epoch 2059/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 74.9053 - val_loss: 85.6511\n",
      "Epoch 2060/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 72.8491 - val_loss: 87.2889\n",
      "Epoch 2061/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 70.4706 - val_loss: 95.8867\n",
      "Epoch 2062/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 70.5537 - val_loss: 93.0626\n",
      "Epoch 2063/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 74.7655 - val_loss: 88.1799\n",
      "Epoch 2064/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 75.0663 - val_loss: 89.8618\n",
      "Epoch 2065/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 77.1865 - val_loss: 90.7831\n",
      "Epoch 2066/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 80.2114 - val_loss: 89.6697\n",
      "Epoch 2067/10000\n",
      "96/96 [==============================] - 0s 485us/step - loss: 74.7234 - val_loss: 91.7947\n",
      "Epoch 2068/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 73.3496 - val_loss: 97.7982\n",
      "Epoch 2069/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 75.8984 - val_loss: 93.2626\n",
      "Epoch 2070/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 72.2837 - val_loss: 92.4592\n",
      "Epoch 2071/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 71.0517 - val_loss: 96.0221\n",
      "Epoch 2072/10000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 74.4300 - val_loss: 77.5029\n",
      "Epoch 2073/10000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 74.3229 - val_loss: 95.3547\n",
      "Epoch 2074/10000\n",
      "96/96 [==============================] - 0s 512us/step - loss: 65.9909 - val_loss: 91.6987\n",
      "Epoch 2075/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 74.8781 - val_loss: 87.1822\n",
      "Epoch 2076/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 72.0667 - val_loss: 101.0728\n",
      "Epoch 2077/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 72.1846 - val_loss: 97.0858\n",
      "Epoch 2078/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 71.5975 - val_loss: 100.7707\n",
      "Epoch 2079/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 71.1643 - val_loss: 92.7323\n",
      "Epoch 2080/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 72.9753 - val_loss: 83.3433\n",
      "Epoch 2081/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 66.7313 - val_loss: 87.1537\n",
      "Epoch 2082/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 70.2508 - val_loss: 93.7153\n",
      "Epoch 2083/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 68.4244 - val_loss: 89.0022\n",
      "Epoch 2084/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 69.8107 - val_loss: 81.1282\n",
      "Epoch 2085/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 72.8668 - val_loss: 84.3234\n",
      "Epoch 2086/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 68.7473 - val_loss: 100.9566\n",
      "Epoch 2087/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 68.9083 - val_loss: 99.3629\n",
      "Epoch 2088/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 98.2719 - val_loss: 99.4154\n",
      "Epoch 2089/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 79.5596 - val_loss: 103.8370\n",
      "Epoch 2090/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 81.4723 - val_loss: 97.5928\n",
      "Epoch 2091/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 90.9270 - val_loss: 87.7853\n",
      "Epoch 2092/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 78.8167 - val_loss: 95.5224\n",
      "Epoch 2093/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 80.8915 - val_loss: 98.5800\n",
      "Epoch 2094/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 76.5513 - val_loss: 91.2761\n",
      "Epoch 2095/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 77.6108 - val_loss: 83.6209\n",
      "Epoch 2096/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 76.0936 - val_loss: 83.0094\n",
      "Epoch 2097/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 78.2413 - val_loss: 89.5555\n",
      "Epoch 2098/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 77.8453 - val_loss: 98.6823\n",
      "Epoch 2099/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 82.1453 - val_loss: 83.8701\n",
      "Epoch 2100/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 76.6807 - val_loss: 93.5193\n",
      "Epoch 2101/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 74.4990 - val_loss: 91.3055\n",
      "Epoch 2102/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 81.1894 - val_loss: 89.9870\n",
      "Epoch 2103/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 78.4644 - val_loss: 86.6062\n",
      "Epoch 2104/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 77.5862 - val_loss: 83.6973\n",
      "Epoch 2105/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 76.1831 - val_loss: 95.6512\n",
      "Epoch 2106/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 76.4032 - val_loss: 93.8161\n",
      "Epoch 2107/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 77.6859 - val_loss: 82.5677\n",
      "Epoch 2108/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 73.9732 - val_loss: 93.9751\n",
      "Epoch 2109/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 74.0661 - val_loss: 89.7144\n",
      "Epoch 2110/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 73.1410 - val_loss: 79.0235\n",
      "Epoch 2111/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 73.2681 - val_loss: 93.5319\n",
      "Epoch 2112/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 74.1320 - val_loss: 85.5836\n",
      "Epoch 2113/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 75.8858 - val_loss: 93.1149\n",
      "Epoch 2114/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 76.5865 - val_loss: 79.2779\n",
      "Epoch 2115/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 76.9803 - val_loss: 85.5427\n",
      "Epoch 2116/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 72.9869 - val_loss: 90.2352\n",
      "Epoch 2117/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 73.4705 - val_loss: 90.1951\n",
      "Epoch 2118/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 71.7753 - val_loss: 91.8842\n",
      "Epoch 2119/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 77.8917 - val_loss: 86.1446\n",
      "Epoch 2120/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 78.4878 - val_loss: 103.7742\n",
      "Epoch 2121/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 74.3777 - val_loss: 92.4016\n",
      "Epoch 2122/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 73.1978 - val_loss: 84.8143\n",
      "Epoch 2123/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 72.4050 - val_loss: 77.5488\n",
      "Epoch 2124/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 72.5695 - val_loss: 84.3350\n",
      "Epoch 2125/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 73.3355 - val_loss: 93.7150\n",
      "Epoch 2126/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 71.6700 - val_loss: 81.9597\n",
      "Epoch 2127/10000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 72.1520 - val_loss: 82.0900\n",
      "Epoch 2128/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 75.7432 - val_loss: 86.2321\n",
      "Epoch 2129/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 73.4478 - val_loss: 85.9454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2130/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 78.4436 - val_loss: 81.7950\n",
      "Epoch 2131/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 74.0470 - val_loss: 85.9076\n",
      "Epoch 2132/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 73.3172 - val_loss: 91.4535\n",
      "Epoch 2133/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 73.7573 - val_loss: 91.4124\n",
      "Epoch 2134/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 71.6777 - val_loss: 87.2484\n",
      "Epoch 2135/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 74.1766 - val_loss: 83.1272\n",
      "Epoch 2136/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 70.0637 - val_loss: 86.2762\n",
      "Epoch 2137/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 72.5297 - val_loss: 81.8495\n",
      "Epoch 2138/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 72.3035 - val_loss: 84.6923\n",
      "Epoch 2139/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 69.1229 - val_loss: 84.0903\n",
      "Epoch 2140/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 72.2746 - val_loss: 103.5264\n",
      "Epoch 2141/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 70.7187 - val_loss: 94.2140\n",
      "Epoch 2142/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 79.3320 - val_loss: 105.1594\n",
      "Epoch 2143/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 73.3265 - val_loss: 91.3337\n",
      "Epoch 2144/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 73.1260 - val_loss: 99.5766\n",
      "Epoch 2145/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 75.3456 - val_loss: 96.9577\n",
      "Epoch 2146/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 73.8795 - val_loss: 95.5831\n",
      "Epoch 2147/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 72.6006 - val_loss: 87.2372\n",
      "Epoch 2148/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 73.9894 - val_loss: 92.8595\n",
      "Epoch 2149/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 77.6853 - val_loss: 94.8100\n",
      "Epoch 2150/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 70.0044 - val_loss: 86.8624\n",
      "Epoch 2151/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 74.3568 - val_loss: 83.3238\n",
      "Epoch 2152/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 74.8414 - val_loss: 87.8582\n",
      "Epoch 2153/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 69.8874 - val_loss: 88.6346\n",
      "Epoch 2154/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 73.2395 - val_loss: 100.3383\n",
      "Epoch 2155/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 71.6923 - val_loss: 100.4384\n",
      "Epoch 2156/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 72.0814 - val_loss: 91.2339\n",
      "Epoch 2157/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 72.4698 - val_loss: 94.4202\n",
      "Epoch 2158/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 74.6761 - val_loss: 84.7901\n",
      "Epoch 2159/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 71.4954 - val_loss: 80.6971\n",
      "Epoch 2160/10000\n",
      "96/96 [==============================] - 0s 481us/step - loss: 66.5043 - val_loss: 84.4622\n",
      "Epoch 2161/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 73.3209 - val_loss: 92.1946\n",
      "Epoch 2162/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 71.1957 - val_loss: 84.1240\n",
      "Epoch 2163/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 69.1854 - val_loss: 86.0604\n",
      "Epoch 2164/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 68.5981 - val_loss: 96.9383\n",
      "Epoch 2165/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 68.5219 - val_loss: 91.8680\n",
      "Epoch 2166/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 66.5313 - val_loss: 96.6984\n",
      "Epoch 2167/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 78.0723 - val_loss: 96.7728\n",
      "Epoch 2168/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 68.8567 - val_loss: 86.7809\n",
      "Epoch 2169/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 68.8721 - val_loss: 90.7818\n",
      "Epoch 2170/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 68.6538 - val_loss: 91.0415\n",
      "Epoch 2171/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 69.8246 - val_loss: 98.4414\n",
      "Epoch 2172/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 68.3758 - val_loss: 91.5712\n",
      "Epoch 2173/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 70.9931 - val_loss: 96.1051\n",
      "Epoch 2174/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 67.7548 - val_loss: 99.4007\n",
      "Epoch 2175/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 70.2951 - val_loss: 86.1935\n",
      "Epoch 2176/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 72.7452 - val_loss: 88.5289\n",
      "Epoch 2177/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 68.8203 - val_loss: 95.1725\n",
      "Epoch 2178/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 67.3387 - val_loss: 105.9738\n",
      "Epoch 2179/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 69.0663 - val_loss: 90.4498\n",
      "Epoch 2180/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 65.6666 - val_loss: 99.8776\n",
      "Epoch 2181/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 73.3196 - val_loss: 90.4795\n",
      "Epoch 2182/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 74.5371 - val_loss: 86.3187\n",
      "Epoch 2183/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 73.2244 - val_loss: 104.8564\n",
      "Epoch 2184/10000\n",
      "96/96 [==============================] - 0s 475us/step - loss: 69.8252 - val_loss: 92.1405\n",
      "Epoch 2185/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 71.1656 - val_loss: 95.9240\n",
      "Epoch 2186/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 70.0006 - val_loss: 93.0118\n",
      "Epoch 2187/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 70.7146 - val_loss: 90.2619\n",
      "Epoch 2188/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 68.8423 - val_loss: 99.3679\n",
      "Epoch 2189/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 70.3349 - val_loss: 94.4417\n",
      "Epoch 2190/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 71.9567 - val_loss: 92.2053\n",
      "Epoch 2191/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 74.0873 - val_loss: 88.5411\n",
      "Epoch 2192/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 71.4393 - val_loss: 90.1294\n",
      "Epoch 2193/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 71.0599 - val_loss: 93.2437\n",
      "Epoch 2194/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 73.7066 - val_loss: 96.1739\n",
      "Epoch 2195/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 70.3422 - val_loss: 89.2041\n",
      "Epoch 2196/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 69.5286 - val_loss: 92.3163\n",
      "Epoch 2197/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 69.1663 - val_loss: 79.8372\n",
      "Epoch 2198/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 68.4984 - val_loss: 95.1840\n",
      "Epoch 2199/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 65.7253 - val_loss: 94.6325\n",
      "Epoch 2200/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 66.2827 - val_loss: 88.7426\n",
      "Epoch 2201/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 68.3579 - val_loss: 96.2639\n",
      "Epoch 2202/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 76.5291 - val_loss: 93.7194\n",
      "Epoch 2203/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 70.6011 - val_loss: 93.1464\n",
      "Epoch 2204/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 77.3080 - val_loss: 99.6304\n",
      "Epoch 2205/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 74.5753 - val_loss: 89.6092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2206/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 70.5069 - val_loss: 93.4180\n",
      "Epoch 2207/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 71.1815 - val_loss: 91.0320\n",
      "Epoch 2208/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 74.2934 - val_loss: 95.4829\n",
      "Epoch 2209/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 74.3184 - val_loss: 79.3083\n",
      "Epoch 2210/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 77.7537 - val_loss: 103.4648\n",
      "Epoch 2211/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 73.0980 - val_loss: 97.3688\n",
      "Epoch 2212/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 75.1501 - val_loss: 109.4820\n",
      "Epoch 2213/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 72.5763 - val_loss: 90.4262\n",
      "Epoch 2214/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 69.8501 - val_loss: 89.2968\n",
      "Epoch 2215/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 69.5215 - val_loss: 88.3825\n",
      "Epoch 2216/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 69.3002 - val_loss: 89.3192\n",
      "Epoch 2217/10000\n",
      "96/96 [==============================] - 0s 348us/step - loss: 69.3100 - val_loss: 94.6067\n",
      "Epoch 2218/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 70.5598 - val_loss: 89.8873\n",
      "Epoch 2219/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 69.4593 - val_loss: 90.8877\n",
      "Epoch 2220/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 70.8878 - val_loss: 86.3188\n",
      "Epoch 2221/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 69.9565 - val_loss: 90.0695\n",
      "Epoch 2222/10000\n",
      "96/96 [==============================] - 0s 486us/step - loss: 68.1628 - val_loss: 82.9357\n",
      "Epoch 2223/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 72.0244 - val_loss: 96.4829\n",
      "Epoch 2224/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 72.3791 - val_loss: 94.6169\n",
      "Epoch 2225/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 73.1996 - val_loss: 91.1087\n",
      "Epoch 2226/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 71.7503 - val_loss: 85.2939\n",
      "Epoch 2227/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 67.1397 - val_loss: 90.5264\n",
      "Epoch 2228/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 68.7724 - val_loss: 99.4712\n",
      "Epoch 2229/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 70.7262 - val_loss: 92.3181\n",
      "Epoch 2230/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 69.0145 - val_loss: 92.8332\n",
      "Epoch 2231/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 69.1710 - val_loss: 95.0041\n",
      "Epoch 2232/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 70.9310 - val_loss: 90.9650\n",
      "Epoch 2233/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 68.6843 - val_loss: 98.9200\n",
      "Epoch 2234/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 69.4950 - val_loss: 104.5179\n",
      "Epoch 2235/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 66.0093 - val_loss: 84.3150\n",
      "Epoch 2236/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 66.5453 - val_loss: 93.5953\n",
      "Epoch 2237/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 69.4703 - val_loss: 104.9525\n",
      "Epoch 2238/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 71.0039 - val_loss: 96.7154\n",
      "Epoch 2239/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 68.0114 - val_loss: 84.5052\n",
      "Epoch 2240/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 69.4790 - val_loss: 93.3631\n",
      "Epoch 2241/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 68.6786 - val_loss: 93.0825\n",
      "Epoch 2242/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 65.9296 - val_loss: 103.0849\n",
      "Epoch 2243/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 68.2615 - val_loss: 96.8865\n",
      "Epoch 2244/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 69.5660 - val_loss: 85.5039\n",
      "Epoch 2245/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 69.0346 - val_loss: 97.7780\n",
      "Epoch 2246/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 68.4701 - val_loss: 96.5359\n",
      "Epoch 2247/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 68.1730 - val_loss: 88.5719\n",
      "Epoch 2248/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 73.2351 - val_loss: 95.9728\n",
      "Epoch 2249/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 105.5960 - val_loss: 87.0092\n",
      "Epoch 2250/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 74.5002 - val_loss: 96.6038\n",
      "Epoch 2251/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 76.1009 - val_loss: 98.7213\n",
      "Epoch 2252/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 75.8219 - val_loss: 87.0464\n",
      "Epoch 2253/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 73.5091 - val_loss: 87.8149\n",
      "Epoch 2254/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 71.6115 - val_loss: 92.9571\n",
      "Epoch 2255/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 68.3648 - val_loss: 92.1759\n",
      "Epoch 2256/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 75.4821 - val_loss: 93.7357\n",
      "Epoch 2257/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 74.5491 - val_loss: 92.0593\n",
      "Epoch 2258/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 69.1826 - val_loss: 91.6113\n",
      "Epoch 2259/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 74.2985 - val_loss: 95.5502\n",
      "Epoch 2260/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 71.7510 - val_loss: 88.3184\n",
      "Epoch 2261/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 72.8099 - val_loss: 93.2130\n",
      "Epoch 2262/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 74.0179 - val_loss: 99.2591\n",
      "Epoch 2263/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 71.1136 - val_loss: 103.1424\n",
      "Epoch 2264/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 74.1706 - val_loss: 91.4551\n",
      "Epoch 2265/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 72.6834 - val_loss: 92.5775\n",
      "Epoch 2266/10000\n",
      "96/96 [==============================] - 0s 495us/step - loss: 68.2213 - val_loss: 98.4914\n",
      "Epoch 2267/10000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 67.9422 - val_loss: 96.6862\n",
      "Epoch 2268/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 68.5427 - val_loss: 82.9704\n",
      "Epoch 2269/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 68.1686 - val_loss: 91.4270\n",
      "Epoch 2270/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 69.1377 - val_loss: 94.2834\n",
      "Epoch 2271/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 73.0179 - val_loss: 91.1345\n",
      "Epoch 2272/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 69.6119 - val_loss: 94.3119\n",
      "Epoch 2273/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 72.0122 - val_loss: 98.3948\n",
      "Epoch 2274/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 69.5476 - val_loss: 107.2199\n",
      "Epoch 2275/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 76.9777 - val_loss: 90.8407\n",
      "Epoch 2276/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 68.7224 - val_loss: 95.2864\n",
      "Epoch 2277/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 69.3533 - val_loss: 93.2155\n",
      "Epoch 2278/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 70.3712 - val_loss: 97.1826\n",
      "Epoch 2279/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 67.3173 - val_loss: 91.8360\n",
      "Epoch 2280/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 70.1483 - val_loss: 97.0046\n",
      "Epoch 2281/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 70.2013 - val_loss: 85.1959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2282/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 68.4231 - val_loss: 91.1276\n",
      "Epoch 2283/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 68.8801 - val_loss: 96.7654\n",
      "Epoch 2284/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 67.6291 - val_loss: 93.7273\n",
      "Epoch 2285/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 69.2695 - val_loss: 98.0402\n",
      "Epoch 2286/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 66.2607 - val_loss: 92.7312\n",
      "Epoch 2287/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 66.6544 - val_loss: 98.3167\n",
      "Epoch 2288/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 69.3131 - val_loss: 90.9442\n",
      "Epoch 2289/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 67.6141 - val_loss: 94.6020\n",
      "Epoch 2290/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 67.1503 - val_loss: 94.7061\n",
      "Epoch 2291/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 70.2595 - val_loss: 105.1956\n",
      "Epoch 2292/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 66.9490 - val_loss: 106.1600\n",
      "Epoch 2293/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 73.3875 - val_loss: 89.8711\n",
      "Epoch 2294/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 68.3419 - val_loss: 86.1110\n",
      "Epoch 2295/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 69.6536 - val_loss: 92.1367\n",
      "Epoch 2296/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 64.5681 - val_loss: 94.4887\n",
      "Epoch 2297/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 66.7390 - val_loss: 94.5693\n",
      "Epoch 2298/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 71.2955 - val_loss: 91.0849\n",
      "Epoch 2299/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 65.8341 - val_loss: 97.8506\n",
      "Epoch 2300/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 70.4848 - val_loss: 98.4066\n",
      "Epoch 2301/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 68.8493 - val_loss: 86.9550\n",
      "Epoch 2302/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 66.7950 - val_loss: 98.7315\n",
      "Epoch 2303/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 68.5287 - val_loss: 89.8241\n",
      "Epoch 2304/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 68.3867 - val_loss: 101.2431\n",
      "Epoch 2305/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 64.8311 - val_loss: 86.0515\n",
      "Epoch 2306/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 71.7879 - val_loss: 102.3941\n",
      "Epoch 2307/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 67.9454 - val_loss: 89.2123\n",
      "Epoch 2308/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 67.9123 - val_loss: 99.5354\n",
      "Epoch 2309/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 66.5144 - val_loss: 101.5482\n",
      "Epoch 2310/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 72.9291 - val_loss: 80.0628\n",
      "Epoch 2311/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 64.0202 - val_loss: 96.1933\n",
      "Epoch 2312/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 66.1317 - val_loss: 87.0023\n",
      "Epoch 2313/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 68.8098 - val_loss: 95.7624\n",
      "Epoch 2314/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 64.5978 - val_loss: 97.4548\n",
      "Epoch 2315/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 64.3694 - val_loss: 87.0611\n",
      "Epoch 2316/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 66.8652 - val_loss: 95.1225\n",
      "Epoch 2317/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 68.1338 - val_loss: 108.7758\n",
      "Epoch 2318/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 71.6300 - val_loss: 84.4891\n",
      "Epoch 2319/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 68.5278 - val_loss: 106.4712\n",
      "Epoch 2320/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 68.9097 - val_loss: 95.9945\n",
      "Epoch 2321/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 68.8631 - val_loss: 87.9782\n",
      "Epoch 2322/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 66.0659 - val_loss: 98.7561\n",
      "Epoch 2323/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 74.2831 - val_loss: 94.8166\n",
      "Epoch 2324/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 66.1268 - val_loss: 96.5500\n",
      "Epoch 2325/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 63.5358 - val_loss: 102.3684\n",
      "Epoch 2326/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 64.6056 - val_loss: 96.7423\n",
      "Epoch 2327/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 69.6020 - val_loss: 97.1516\n",
      "Epoch 2328/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 65.1829 - val_loss: 92.0279\n",
      "Epoch 2329/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 67.3884 - val_loss: 90.9562\n",
      "Epoch 2330/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 67.5067 - val_loss: 89.1424\n",
      "Epoch 2331/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 69.1838 - val_loss: 94.3853\n",
      "Epoch 2332/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 70.1208 - val_loss: 106.2523\n",
      "Epoch 2333/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 68.9833 - val_loss: 105.8689\n",
      "Epoch 2334/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 64.3190 - val_loss: 118.4774\n",
      "Epoch 2335/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 72.4971 - val_loss: 98.9089\n",
      "Epoch 2336/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 65.5931 - val_loss: 94.6550\n",
      "Epoch 2337/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 66.4414 - val_loss: 94.8309\n",
      "Epoch 2338/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 65.5426 - val_loss: 98.4918\n",
      "Epoch 2339/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 80.5303 - val_loss: 99.4517\n",
      "Epoch 2340/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 69.6354 - val_loss: 96.2354\n",
      "Epoch 2341/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 69.0764 - val_loss: 94.4829\n",
      "Epoch 2342/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 67.4510 - val_loss: 88.8919\n",
      "Epoch 2343/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 67.6135 - val_loss: 102.2250\n",
      "Epoch 2344/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 68.1017 - val_loss: 94.7054\n",
      "Epoch 2345/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 69.1152 - val_loss: 89.2700\n",
      "Epoch 2346/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 71.0010 - val_loss: 89.0504\n",
      "Epoch 2347/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 68.9507 - val_loss: 87.9656\n",
      "Epoch 2348/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 71.1732 - val_loss: 93.6801\n",
      "Epoch 2349/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 68.3764 - val_loss: 83.6568\n",
      "Epoch 2350/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 67.3185 - val_loss: 91.4291\n",
      "Epoch 2351/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 68.6037 - val_loss: 103.5965\n",
      "Epoch 2352/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 66.8194 - val_loss: 108.7758\n",
      "Epoch 2353/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 68.1597 - val_loss: 93.1734\n",
      "Epoch 2354/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 67.2086 - val_loss: 93.3358\n",
      "Epoch 2355/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 66.0329 - val_loss: 89.8841\n",
      "Epoch 2356/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 68.6734 - val_loss: 97.8527\n",
      "Epoch 2357/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 67.3618 - val_loss: 93.4383\n",
      "Epoch 2358/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 65.7205 - val_loss: 103.9324\n",
      "Epoch 2359/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 67.9757 - val_loss: 94.1242\n",
      "Epoch 2360/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 65.7222 - val_loss: 96.7908\n",
      "Epoch 2361/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 68.0792 - val_loss: 101.6246\n",
      "Epoch 2362/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 65.8290 - val_loss: 95.6803\n",
      "Epoch 2363/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 64.4571 - val_loss: 100.9126\n",
      "Epoch 2364/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 73.1091 - val_loss: 90.8685\n",
      "Epoch 2365/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 65.2568 - val_loss: 104.3178\n",
      "Epoch 2366/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 76.5093 - val_loss: 97.0199\n",
      "Epoch 2367/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 71.0219 - val_loss: 92.6319\n",
      "Epoch 2368/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 68.8202 - val_loss: 95.8816\n",
      "Epoch 2369/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 69.6620 - val_loss: 93.5301\n",
      "Epoch 2370/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 69.3782 - val_loss: 88.6666\n",
      "Epoch 2371/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 67.9225 - val_loss: 93.8408\n",
      "Epoch 2372/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 72.8179 - val_loss: 96.2987\n",
      "Epoch 2373/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 66.0887 - val_loss: 103.9537\n",
      "Epoch 2374/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 67.4920 - val_loss: 94.9372\n",
      "Epoch 2375/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 65.6007 - val_loss: 98.6894\n",
      "Epoch 2376/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 68.5018 - val_loss: 101.1643\n",
      "Epoch 2377/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 66.5402 - val_loss: 109.0625\n",
      "Epoch 2378/10000\n",
      "96/96 [==============================] - 0s 487us/step - loss: 64.5046 - val_loss: 104.0676\n",
      "Epoch 2379/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 65.0403 - val_loss: 101.0766\n",
      "Epoch 2380/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 63.9717 - val_loss: 102.3172\n",
      "Epoch 2381/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 65.1800 - val_loss: 90.3503\n",
      "Epoch 2382/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 65.3512 - val_loss: 104.2597\n",
      "Epoch 2383/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 66.8571 - val_loss: 100.1643\n",
      "Epoch 2384/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 66.2166 - val_loss: 104.8268\n",
      "Epoch 2385/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 68.2082 - val_loss: 98.1678\n",
      "Epoch 2386/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 65.5909 - val_loss: 101.2503\n",
      "Epoch 2387/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 66.0419 - val_loss: 97.4383\n",
      "Epoch 2388/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 67.2257 - val_loss: 94.2395\n",
      "Epoch 2389/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 64.4051 - val_loss: 99.6643\n",
      "Epoch 2390/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 66.1733 - val_loss: 99.5846\n",
      "Epoch 2391/10000\n",
      "96/96 [==============================] - 0s 354us/step - loss: 65.9399 - val_loss: 93.6234\n",
      "Epoch 2392/10000\n",
      "96/96 [==============================] - 0s 353us/step - loss: 69.7712 - val_loss: 100.7425\n",
      "Epoch 2393/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 65.0466 - val_loss: 108.9711\n",
      "Epoch 2394/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 63.5050 - val_loss: 91.2823\n",
      "Epoch 2395/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 65.6517 - val_loss: 108.8290\n",
      "Epoch 2396/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 63.4973 - val_loss: 98.7574\n",
      "Epoch 2397/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 65.4211 - val_loss: 101.9862\n",
      "Epoch 2398/10000\n",
      "96/96 [==============================] - 0s 354us/step - loss: 66.0949 - val_loss: 100.9567\n",
      "Epoch 2399/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 67.4903 - val_loss: 105.3784\n",
      "Epoch 2400/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 67.0908 - val_loss: 98.2475\n",
      "Epoch 2401/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 66.7447 - val_loss: 105.4479\n",
      "Epoch 2402/10000\n",
      "96/96 [==============================] - 0s 350us/step - loss: 63.5476 - val_loss: 100.7431\n",
      "Epoch 2403/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 66.7122 - val_loss: 105.4254\n",
      "Epoch 2404/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 67.6700 - val_loss: 91.6593\n",
      "Epoch 2405/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 63.5603 - val_loss: 91.3808\n",
      "Epoch 2406/10000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 64.6761 - val_loss: 92.0484\n",
      "Epoch 2407/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 67.4254 - val_loss: 91.5088\n",
      "Epoch 2408/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 68.9903 - val_loss: 104.9903\n",
      "Epoch 2409/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 61.4259 - val_loss: 96.8940\n",
      "Epoch 2410/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 64.3408 - val_loss: 97.1955\n",
      "Epoch 2411/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 66.7695 - val_loss: 105.5689\n",
      "Epoch 2412/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 63.8395 - val_loss: 97.2723\n",
      "Epoch 2413/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 65.1358 - val_loss: 97.3837\n",
      "Epoch 2414/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 63.8511 - val_loss: 92.7788\n",
      "Epoch 2415/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 62.0520 - val_loss: 109.6704\n",
      "Epoch 2416/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 80.1328 - val_loss: 97.1963\n",
      "Epoch 2417/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 68.0203 - val_loss: 94.4023\n",
      "Epoch 2418/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 71.5068 - val_loss: 95.8964\n",
      "Epoch 2419/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 72.3949 - val_loss: 93.8781\n",
      "Epoch 2420/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 68.4450 - val_loss: 88.3505\n",
      "Epoch 2421/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 66.1653 - val_loss: 98.4952\n",
      "Epoch 2422/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 69.7134 - val_loss: 91.7406\n",
      "Epoch 2423/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 70.8375 - val_loss: 104.8670\n",
      "Epoch 2424/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 76.1116 - val_loss: 96.2045\n",
      "Epoch 2425/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 69.9366 - val_loss: 96.8707\n",
      "Epoch 2426/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 65.2673 - val_loss: 94.3738\n",
      "Epoch 2427/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 66.9169 - val_loss: 85.5640\n",
      "Epoch 2428/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 66.0887 - val_loss: 88.8720\n",
      "Epoch 2429/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 68.1548 - val_loss: 84.4911\n",
      "Epoch 2430/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 68.7665 - val_loss: 86.7252\n",
      "Epoch 2431/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 68.3540 - val_loss: 97.8418\n",
      "Epoch 2432/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 67.2537 - val_loss: 104.0385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2433/10000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 72.8836 - val_loss: 107.8795\n",
      "Epoch 2434/10000\n",
      "96/96 [==============================] - 0s 482us/step - loss: 69.2836 - val_loss: 102.0565\n",
      "Epoch 2435/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 73.3462 - val_loss: 96.1516\n",
      "Epoch 2436/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 65.0595 - val_loss: 101.3419\n",
      "Epoch 2437/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 68.7544 - val_loss: 84.4370\n",
      "Epoch 2438/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 67.5593 - val_loss: 94.7242\n",
      "Epoch 2439/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 68.1368 - val_loss: 92.5431\n",
      "Epoch 2440/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 67.7828 - val_loss: 98.8441\n",
      "Epoch 2441/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 67.7108 - val_loss: 95.5749\n",
      "Epoch 2442/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 67.2136 - val_loss: 99.1824\n",
      "Epoch 2443/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 65.5013 - val_loss: 99.6697\n",
      "Epoch 2444/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 68.3736 - val_loss: 108.2993\n",
      "Epoch 2445/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 67.5607 - val_loss: 101.1867\n",
      "Epoch 2446/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 68.4566 - val_loss: 86.4980\n",
      "Epoch 2447/10000\n",
      "96/96 [==============================] - 0s 558us/step - loss: 66.9573 - val_loss: 95.6581\n",
      "Epoch 2448/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 67.8518 - val_loss: 101.1478\n",
      "Epoch 2449/10000\n",
      "96/96 [==============================] - 0s 488us/step - loss: 70.1561 - val_loss: 93.6648\n",
      "Epoch 2450/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 63.1744 - val_loss: 93.3778\n",
      "Epoch 2451/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 68.7878 - val_loss: 95.1979\n",
      "Epoch 2452/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 70.1214 - val_loss: 93.7348\n",
      "Epoch 2453/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 68.9264 - val_loss: 95.7360\n",
      "Epoch 2454/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 73.5965 - val_loss: 94.4453\n",
      "Epoch 2455/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 69.1893 - val_loss: 91.7285\n",
      "Epoch 2456/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 66.6097 - val_loss: 84.7108\n",
      "Epoch 2457/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 65.4563 - val_loss: 100.1361\n",
      "Epoch 2458/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 65.1640 - val_loss: 99.5941\n",
      "Epoch 2459/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 68.5576 - val_loss: 99.3397\n",
      "Epoch 2460/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 67.1078 - val_loss: 99.9458\n",
      "Epoch 2461/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 65.5254 - val_loss: 97.3651\n",
      "Epoch 2462/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 66.2269 - val_loss: 109.7277\n",
      "Epoch 2463/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 64.8752 - val_loss: 93.9546\n",
      "Epoch 2464/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 68.1162 - val_loss: 96.3011\n",
      "Epoch 2465/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 65.4093 - val_loss: 99.4290\n",
      "Epoch 2466/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 64.9453 - val_loss: 108.7272\n",
      "Epoch 2467/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 65.6934 - val_loss: 111.4316\n",
      "Epoch 2468/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 70.1148 - val_loss: 107.3163\n",
      "Epoch 2469/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 61.3772 - val_loss: 97.7648\n",
      "Epoch 2470/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 63.4692 - val_loss: 100.0247\n",
      "Epoch 2471/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 67.3568 - val_loss: 96.3531\n",
      "Epoch 2472/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 67.1618 - val_loss: 97.7372\n",
      "Epoch 2473/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 61.4117 - val_loss: 96.9673\n",
      "Epoch 2474/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 63.3166 - val_loss: 111.2181\n",
      "Epoch 2475/10000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 62.3859 - val_loss: 112.6695\n",
      "Epoch 2476/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 61.5612 - val_loss: 108.4734\n",
      "Epoch 2477/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 61.6622 - val_loss: 97.2072\n",
      "Epoch 2478/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 62.7708 - val_loss: 98.4877\n",
      "Epoch 2479/10000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 61.3607 - val_loss: 95.7125\n",
      "Epoch 2480/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 62.4187 - val_loss: 90.7414\n",
      "Epoch 2481/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 64.9253 - val_loss: 86.5319\n",
      "Epoch 2482/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 66.8476 - val_loss: 100.4357\n",
      "Epoch 2483/10000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 61.1659 - val_loss: 93.4339\n",
      "Epoch 2484/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 64.2203 - val_loss: 105.2114\n",
      "Epoch 2485/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 65.7371 - val_loss: 102.2936\n",
      "Epoch 2486/10000\n",
      "96/96 [==============================] - 0s 573us/step - loss: 60.7430 - val_loss: 103.9223\n",
      "Epoch 2487/10000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 64.0175 - val_loss: 97.3309\n",
      "Epoch 2488/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 65.6785 - val_loss: 97.9718\n",
      "Epoch 2489/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 64.8170 - val_loss: 89.1044\n",
      "Epoch 2490/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 63.7639 - val_loss: 88.8262\n",
      "Epoch 2491/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 62.7278 - val_loss: 94.1327\n",
      "Epoch 2492/10000\n",
      "96/96 [==============================] - 0s 505us/step - loss: 63.7185 - val_loss: 113.2239\n",
      "Epoch 2493/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 67.2869 - val_loss: 103.2865\n",
      "Epoch 2494/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 62.2743 - val_loss: 92.6265\n",
      "Epoch 2495/10000\n",
      "96/96 [==============================] - 0s 508us/step - loss: 62.3732 - val_loss: 92.2501\n",
      "Epoch 2496/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 63.0813 - val_loss: 103.1655\n",
      "Epoch 2497/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 61.7610 - val_loss: 109.3649\n",
      "Epoch 2498/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 61.9628 - val_loss: 105.5218\n",
      "Epoch 2499/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 60.3772 - val_loss: 103.1577\n",
      "Epoch 2500/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 80.6809 - val_loss: 101.1590\n",
      "Epoch 2501/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 64.1881 - val_loss: 102.6473\n",
      "Epoch 2502/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 69.7197 - val_loss: 99.9960\n",
      "Epoch 2503/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 69.9392 - val_loss: 96.3674\n",
      "Epoch 2504/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 63.7700 - val_loss: 103.0382\n",
      "Epoch 2505/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 66.9536 - val_loss: 99.2233\n",
      "Epoch 2506/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 66.4194 - val_loss: 92.4585\n",
      "Epoch 2507/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 69.1050 - val_loss: 113.2839\n",
      "Epoch 2508/10000\n",
      "96/96 [==============================] - 0s 356us/step - loss: 63.0284 - val_loss: 114.3757\n",
      "Epoch 2509/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 66.1688 - val_loss: 91.8163\n",
      "Epoch 2510/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 67.8157 - val_loss: 105.9948\n",
      "Epoch 2511/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 62.9467 - val_loss: 95.4253\n",
      "Epoch 2512/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 65.4804 - val_loss: 97.6799\n",
      "Epoch 2513/10000\n",
      "96/96 [==============================] - 0s 484us/step - loss: 62.5224 - val_loss: 107.3751\n",
      "Epoch 2514/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 60.3085 - val_loss: 106.4230\n",
      "Epoch 2515/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 61.1686 - val_loss: 102.3634\n",
      "Epoch 2516/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 63.6288 - val_loss: 100.2068\n",
      "Epoch 2517/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 62.6854 - val_loss: 85.6013\n",
      "Epoch 2518/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 65.3384 - val_loss: 102.1179\n",
      "Epoch 2519/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 62.3956 - val_loss: 104.0906\n",
      "Epoch 2520/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 63.9950 - val_loss: 88.4963\n",
      "Epoch 2521/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 62.9981 - val_loss: 95.6072\n",
      "Epoch 2522/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 61.8767 - val_loss: 101.3690\n",
      "Epoch 2523/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 61.3735 - val_loss: 97.4100\n",
      "Epoch 2524/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 94.1673 - val_loss: 93.0264\n",
      "Epoch 2525/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 69.4874 - val_loss: 91.4558\n",
      "Epoch 2526/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 72.9193 - val_loss: 91.6661\n",
      "Epoch 2527/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 69.5172 - val_loss: 94.3136\n",
      "Epoch 2528/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 72.7093 - val_loss: 87.7125\n",
      "Epoch 2529/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 69.3743 - val_loss: 94.5761\n",
      "Epoch 2530/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 64.0195 - val_loss: 88.4795\n",
      "Epoch 2531/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 66.7569 - val_loss: 79.9745\n",
      "Epoch 2532/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 64.5830 - val_loss: 100.1311\n",
      "Epoch 2533/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 63.0611 - val_loss: 94.4493\n",
      "Epoch 2534/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 62.0585 - val_loss: 87.5387\n",
      "Epoch 2535/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 62.0563 - val_loss: 92.9755\n",
      "Epoch 2536/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 63.2532 - val_loss: 101.5006\n",
      "Epoch 2537/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 61.7173 - val_loss: 101.4621\n",
      "Epoch 2538/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 63.5057 - val_loss: 103.5358\n",
      "Epoch 2539/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 66.2740 - val_loss: 90.0688\n",
      "Epoch 2540/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 62.3193 - val_loss: 95.2497\n",
      "Epoch 2541/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 62.6718 - val_loss: 83.7195\n",
      "Epoch 2542/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 63.9363 - val_loss: 99.4788\n",
      "Epoch 2543/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 61.3007 - val_loss: 98.6105\n",
      "Epoch 2544/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 60.4949 - val_loss: 93.5375\n",
      "Epoch 2545/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 60.5443 - val_loss: 101.3975\n",
      "Epoch 2546/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 63.9548 - val_loss: 98.8953\n",
      "Epoch 2547/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 60.4687 - val_loss: 98.9225\n",
      "Epoch 2548/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 64.4676 - val_loss: 104.9617\n",
      "Epoch 2549/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 63.8737 - val_loss: 91.5811\n",
      "Epoch 2550/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 61.7856 - val_loss: 98.3726\n",
      "Epoch 2551/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 62.8988 - val_loss: 93.4916\n",
      "Epoch 2552/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 64.4833 - val_loss: 104.9884\n",
      "Epoch 2553/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 66.7125 - val_loss: 89.5564\n",
      "Epoch 2554/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 67.9324 - val_loss: 100.6584\n",
      "Epoch 2555/10000\n",
      "96/96 [==============================] - 0s 496us/step - loss: 61.4714 - val_loss: 105.1321\n",
      "Epoch 2556/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 62.7300 - val_loss: 91.3749\n",
      "Epoch 2557/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 64.5668 - val_loss: 106.6149\n",
      "Epoch 2558/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 65.0590 - val_loss: 98.6346\n",
      "Epoch 2559/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 63.2729 - val_loss: 86.5273\n",
      "Epoch 2560/10000\n",
      "96/96 [==============================] - 0s 499us/step - loss: 60.5040 - val_loss: 103.3848\n",
      "Epoch 2561/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 63.3311 - val_loss: 101.9731\n",
      "Epoch 2562/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 68.0078 - val_loss: 106.0309\n",
      "Epoch 2563/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 63.6689 - val_loss: 105.1912\n",
      "Epoch 2564/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 60.9305 - val_loss: 105.3318\n",
      "Epoch 2565/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 64.6113 - val_loss: 93.6668\n",
      "Epoch 2566/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 61.9340 - val_loss: 105.8032\n",
      "Epoch 2567/10000\n",
      "96/96 [==============================] - 0s 511us/step - loss: 65.9702 - val_loss: 91.6763\n",
      "Epoch 2568/10000\n",
      "96/96 [==============================] - 0s 506us/step - loss: 63.9108 - val_loss: 101.3554\n",
      "Epoch 2569/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 63.3509 - val_loss: 98.9616\n",
      "Epoch 2570/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 60.7833 - val_loss: 95.6986\n",
      "Epoch 2571/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 61.6298 - val_loss: 104.4591\n",
      "Epoch 2572/10000\n",
      "96/96 [==============================] - 0s 487us/step - loss: 63.4484 - val_loss: 90.4769\n",
      "Epoch 2573/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 63.9413 - val_loss: 90.1479\n",
      "Epoch 2574/10000\n",
      "96/96 [==============================] - 0s 494us/step - loss: 61.6960 - val_loss: 97.4452\n",
      "Epoch 2575/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 65.5573 - val_loss: 103.4504\n",
      "Epoch 2576/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 62.1213 - val_loss: 97.1828\n",
      "Epoch 2577/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 64.7695 - val_loss: 97.3204\n",
      "Epoch 2578/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 66.3245 - val_loss: 97.8383\n",
      "Epoch 2579/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 63.0621 - val_loss: 93.5258\n",
      "Epoch 2580/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 61.7292 - val_loss: 92.8248\n",
      "Epoch 2581/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 62.4602 - val_loss: 90.1254\n",
      "Epoch 2582/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 64.8646 - val_loss: 96.7672\n",
      "Epoch 2583/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 62.8787 - val_loss: 98.7880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2584/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 59.3344 - val_loss: 89.6611\n",
      "Epoch 2585/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 60.0209 - val_loss: 97.2172\n",
      "Epoch 2586/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 61.3335 - val_loss: 100.3550\n",
      "Epoch 2587/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 58.6810 - val_loss: 93.6417\n",
      "Epoch 2588/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 63.6300 - val_loss: 103.8634\n",
      "Epoch 2589/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 57.4758 - val_loss: 97.1029\n",
      "Epoch 2590/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 61.1362 - val_loss: 101.2600\n",
      "Epoch 2591/10000\n",
      "96/96 [==============================] - 0s 355us/step - loss: 60.8885 - val_loss: 105.1613\n",
      "Epoch 2592/10000\n",
      "96/96 [==============================] - 0s 504us/step - loss: 62.4539 - val_loss: 95.4623\n",
      "Epoch 2593/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 62.8847 - val_loss: 99.6803\n",
      "Epoch 2594/10000\n",
      "96/96 [==============================] - 0s 508us/step - loss: 62.0495 - val_loss: 103.1415\n",
      "Epoch 2595/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 60.3232 - val_loss: 94.9625\n",
      "Epoch 2596/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 59.2579 - val_loss: 95.0177\n",
      "Epoch 2597/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 57.8599 - val_loss: 93.1805\n",
      "Epoch 2598/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 60.5776 - val_loss: 101.1598\n",
      "Epoch 2599/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 59.9833 - val_loss: 105.4459\n",
      "Epoch 2600/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 60.4079 - val_loss: 96.2552\n",
      "Epoch 2601/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 60.3612 - val_loss: 96.9848\n",
      "Epoch 2602/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 61.3756 - val_loss: 100.3130\n",
      "Epoch 2603/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 62.4107 - val_loss: 100.4371\n",
      "Epoch 2604/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 59.6680 - val_loss: 96.1286\n",
      "Epoch 2605/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 58.5678 - val_loss: 100.7598\n",
      "Epoch 2606/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 58.9107 - val_loss: 105.5539\n",
      "Epoch 2607/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 63.0782 - val_loss: 87.4548\n",
      "Epoch 2608/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 61.6323 - val_loss: 96.4063\n",
      "Epoch 2609/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 60.9190 - val_loss: 100.9984\n",
      "Epoch 2610/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 61.0611 - val_loss: 92.5191\n",
      "Epoch 2611/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 65.5061 - val_loss: 104.5846\n",
      "Epoch 2612/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 65.0662 - val_loss: 91.6773\n",
      "Epoch 2613/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 59.7448 - val_loss: 92.4918\n",
      "Epoch 2614/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 63.5734 - val_loss: 88.5460\n",
      "Epoch 2615/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 65.4422 - val_loss: 93.9258\n",
      "Epoch 2616/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 64.9307 - val_loss: 99.1635\n",
      "Epoch 2617/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 68.2189 - val_loss: 86.6720\n",
      "Epoch 2618/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 62.3713 - val_loss: 90.5453\n",
      "Epoch 2619/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 62.3263 - val_loss: 87.0798\n",
      "Epoch 2620/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 65.6714 - val_loss: 95.4933\n",
      "Epoch 2621/10000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 62.3282 - val_loss: 91.8684\n",
      "Epoch 2622/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 70.8621 - val_loss: 92.2813\n",
      "Epoch 2623/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 65.7649 - val_loss: 105.5131\n",
      "Epoch 2624/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 66.4194 - val_loss: 102.2680\n",
      "Epoch 2625/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 62.5423 - val_loss: 95.7900\n",
      "Epoch 2626/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 60.7964 - val_loss: 102.8147\n",
      "Epoch 2627/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 79.9509 - val_loss: 97.7418\n",
      "Epoch 2628/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 66.6537 - val_loss: 97.7557\n",
      "Epoch 2629/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 68.7125 - val_loss: 107.7682\n",
      "Epoch 2630/10000\n",
      "96/96 [==============================] - 0s 355us/step - loss: 68.8218 - val_loss: 101.5031\n",
      "Epoch 2631/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 59.8061 - val_loss: 100.4980\n",
      "Epoch 2632/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 62.3373 - val_loss: 104.0388\n",
      "Epoch 2633/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 62.2998 - val_loss: 89.5760\n",
      "Epoch 2634/10000\n",
      "96/96 [==============================] - 0s 349us/step - loss: 64.3485 - val_loss: 93.8433\n",
      "Epoch 2635/10000\n",
      "96/96 [==============================] - 0s 354us/step - loss: 65.1768 - val_loss: 95.4033\n",
      "Epoch 2636/10000\n",
      "96/96 [==============================] - 0s 353us/step - loss: 61.8465 - val_loss: 107.6363\n",
      "Epoch 2637/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 62.9078 - val_loss: 92.8458\n",
      "Epoch 2638/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 62.7345 - val_loss: 96.3107\n",
      "Epoch 2639/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 62.3075 - val_loss: 102.5717\n",
      "Epoch 2640/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 58.0841 - val_loss: 92.7220\n",
      "Epoch 2641/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 68.8541 - val_loss: 90.3547\n",
      "Epoch 2642/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 64.4810 - val_loss: 98.1107\n",
      "Epoch 2643/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 66.0157 - val_loss: 101.6810\n",
      "Epoch 2644/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 61.6736 - val_loss: 92.2246\n",
      "Epoch 2645/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 64.1872 - val_loss: 95.2661\n",
      "Epoch 2646/10000\n",
      "96/96 [==============================] - 0s 355us/step - loss: 64.2968 - val_loss: 82.8619\n",
      "Epoch 2647/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 58.9999 - val_loss: 96.9216\n",
      "Epoch 2648/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 62.4550 - val_loss: 103.6533\n",
      "Epoch 2649/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 62.4425 - val_loss: 106.2707\n",
      "Epoch 2650/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 61.8899 - val_loss: 94.6286\n",
      "Epoch 2651/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 59.8001 - val_loss: 94.1976\n",
      "Epoch 2652/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 60.6624 - val_loss: 82.4646\n",
      "Epoch 2653/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 72.5859 - val_loss: 111.0720\n",
      "Epoch 2654/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 65.1433 - val_loss: 114.8616\n",
      "Epoch 2655/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 64.4180 - val_loss: 94.4550\n",
      "Epoch 2656/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 62.2529 - val_loss: 100.5059\n",
      "Epoch 2657/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 63.6279 - val_loss: 96.6926\n",
      "Epoch 2658/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 68.2814 - val_loss: 98.9682\n",
      "Epoch 2659/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 65.9691 - val_loss: 93.5397\n",
      "Epoch 2660/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 64.7268 - val_loss: 90.9169\n",
      "Epoch 2661/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 62.5524 - val_loss: 97.9639\n",
      "Epoch 2662/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 60.7460 - val_loss: 89.1416\n",
      "Epoch 2663/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 59.3404 - val_loss: 97.1803\n",
      "Epoch 2664/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 62.2064 - val_loss: 90.5805\n",
      "Epoch 2665/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 59.3733 - val_loss: 104.8954\n",
      "Epoch 2666/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 59.5719 - val_loss: 100.1202\n",
      "Epoch 2667/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 61.2213 - val_loss: 107.3903\n",
      "Epoch 2668/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 64.3499 - val_loss: 108.7161\n",
      "Epoch 2669/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 64.5144 - val_loss: 96.5117\n",
      "Epoch 2670/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 60.5119 - val_loss: 95.4243\n",
      "Epoch 2671/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 63.6806 - val_loss: 89.1774\n",
      "Epoch 2672/10000\n",
      "96/96 [==============================] - 0s 497us/step - loss: 65.0001 - val_loss: 97.3121\n",
      "Epoch 2673/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 63.1570 - val_loss: 91.7846\n",
      "Epoch 2674/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 62.9743 - val_loss: 93.4011\n",
      "Epoch 2675/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 64.3016 - val_loss: 103.3648\n",
      "Epoch 2676/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 64.2399 - val_loss: 94.4603\n",
      "Epoch 2677/10000\n",
      "96/96 [==============================] - 0s 506us/step - loss: 61.7773 - val_loss: 92.6424\n",
      "Epoch 2678/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 58.1282 - val_loss: 97.5158\n",
      "Epoch 2679/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 61.7744 - val_loss: 102.7154\n",
      "Epoch 2680/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 67.4921 - val_loss: 101.2762\n",
      "Epoch 2681/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 60.4464 - val_loss: 96.9099\n",
      "Epoch 2682/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 65.8648 - val_loss: 94.0933\n",
      "Epoch 2683/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 65.4416 - val_loss: 95.0406\n",
      "Epoch 2684/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 65.7906 - val_loss: 95.3701\n",
      "Epoch 2685/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 66.3098 - val_loss: 96.4915\n",
      "Epoch 2686/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 69.1157 - val_loss: 103.5043\n",
      "Epoch 2687/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 64.8273 - val_loss: 95.8926\n",
      "Epoch 2688/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 66.9189 - val_loss: 95.7546\n",
      "Epoch 2689/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 65.8759 - val_loss: 90.4245\n",
      "Epoch 2690/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 62.7327 - val_loss: 96.4308\n",
      "Epoch 2691/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 65.6750 - val_loss: 98.7078\n",
      "Epoch 2692/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 61.3765 - val_loss: 94.8662\n",
      "Epoch 2693/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 63.5572 - val_loss: 103.5578\n",
      "Epoch 2694/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 63.9881 - val_loss: 89.9121\n",
      "Epoch 2695/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 58.5901 - val_loss: 92.8651\n",
      "Epoch 2696/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 62.9336 - val_loss: 87.9177\n",
      "Epoch 2697/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 59.2105 - val_loss: 98.1714\n",
      "Epoch 2698/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 59.7543 - val_loss: 96.2908\n",
      "Epoch 2699/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 63.1253 - val_loss: 101.8684\n",
      "Epoch 2700/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 60.1256 - val_loss: 98.7851\n",
      "Epoch 2701/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 57.9745 - val_loss: 102.1615\n",
      "Epoch 2702/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 59.8638 - val_loss: 90.8925\n",
      "Epoch 2703/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 61.6355 - val_loss: 97.4266\n",
      "Epoch 2704/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 61.2102 - val_loss: 95.5557\n",
      "Epoch 2705/10000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 94.2247 - val_loss: 95.5387\n",
      "Epoch 2706/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 73.1503 - val_loss: 96.0195\n",
      "Epoch 2707/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 77.9766 - val_loss: 98.5848\n",
      "Epoch 2708/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 80.1835 - val_loss: 93.0896\n",
      "Epoch 2709/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 77.7315 - val_loss: 91.1444\n",
      "Epoch 2710/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 74.7406 - val_loss: 98.5481\n",
      "Epoch 2711/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 79.5891 - val_loss: 105.5983\n",
      "Epoch 2712/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 72.3844 - val_loss: 86.7672\n",
      "Epoch 2713/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 77.3937 - val_loss: 104.3810\n",
      "Epoch 2714/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 68.8440 - val_loss: 108.1596\n",
      "Epoch 2715/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 72.0172 - val_loss: 114.5860\n",
      "Epoch 2716/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 71.9501 - val_loss: 103.3614\n",
      "Epoch 2717/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 73.0796 - val_loss: 100.2141\n",
      "Epoch 2718/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 77.1089 - val_loss: 92.9655\n",
      "Epoch 2719/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 72.8590 - val_loss: 93.0689\n",
      "Epoch 2720/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 74.7018 - val_loss: 101.8358\n",
      "Epoch 2721/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 73.7178 - val_loss: 92.8112\n",
      "Epoch 2722/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 69.8564 - val_loss: 86.7318\n",
      "Epoch 2723/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 69.7473 - val_loss: 88.9272\n",
      "Epoch 2724/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 70.9510 - val_loss: 107.2780\n",
      "Epoch 2725/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 67.3991 - val_loss: 93.9408\n",
      "Epoch 2726/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 72.7033 - val_loss: 116.4422\n",
      "Epoch 2727/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 75.2910 - val_loss: 105.1816\n",
      "Epoch 2728/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 69.5407 - val_loss: 99.5810\n",
      "Epoch 2729/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 64.6486 - val_loss: 110.5839\n",
      "Epoch 2730/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 73.9682 - val_loss: 105.8098\n",
      "Epoch 2731/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 68.0889 - val_loss: 105.7434\n",
      "Epoch 2732/10000\n",
      "96/96 [==============================] - 0s 492us/step - loss: 65.1391 - val_loss: 96.3543\n",
      "Epoch 2733/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 71.8558 - val_loss: 97.3861\n",
      "Epoch 2734/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 71.6976 - val_loss: 92.6718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2735/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 70.3052 - val_loss: 91.0168\n",
      "Epoch 2736/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 69.2498 - val_loss: 96.8252\n",
      "Epoch 2737/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 68.4186 - val_loss: 103.1551\n",
      "Epoch 2738/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 71.8694 - val_loss: 105.7053\n",
      "Epoch 2739/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 71.6298 - val_loss: 96.2228\n",
      "Epoch 2740/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 70.3746 - val_loss: 86.1779\n",
      "Epoch 2741/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 65.9561 - val_loss: 87.0351\n",
      "Epoch 2742/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 65.6744 - val_loss: 87.0621\n",
      "Epoch 2743/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 69.7978 - val_loss: 92.1404\n",
      "Epoch 2744/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 66.6205 - val_loss: 86.0956\n",
      "Epoch 2745/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 69.0328 - val_loss: 92.4163\n",
      "Epoch 2746/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 63.6259 - val_loss: 98.3959\n",
      "Epoch 2747/10000\n",
      "96/96 [==============================] - 0s 355us/step - loss: 65.6600 - val_loss: 108.6523\n",
      "Epoch 2748/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 65.5409 - val_loss: 103.1088\n",
      "Epoch 2749/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 66.0514 - val_loss: 107.2233\n",
      "Epoch 2750/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 70.1067 - val_loss: 105.4847\n",
      "Epoch 2751/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 66.7567 - val_loss: 98.2493\n",
      "Epoch 2752/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 66.3649 - val_loss: 100.7041\n",
      "Epoch 2753/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 64.8536 - val_loss: 101.1512\n",
      "Epoch 2754/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 62.2510 - val_loss: 99.1563\n",
      "Epoch 2755/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 60.9009 - val_loss: 89.8483\n",
      "Epoch 2756/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 65.3684 - val_loss: 93.2482\n",
      "Epoch 2757/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 61.8331 - val_loss: 97.5240\n",
      "Epoch 2758/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 64.2833 - val_loss: 109.9790\n",
      "Epoch 2759/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 72.0341 - val_loss: 96.0423\n",
      "Epoch 2760/10000\n",
      "96/96 [==============================] - 0s 496us/step - loss: 65.7476 - val_loss: 89.1268\n",
      "Epoch 2761/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 66.9493 - val_loss: 87.2292\n",
      "Epoch 2762/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 67.5685 - val_loss: 98.7586\n",
      "Epoch 2763/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 63.3273 - val_loss: 89.6926\n",
      "Epoch 2764/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 62.7562 - val_loss: 93.2450\n",
      "Epoch 2765/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 64.9260 - val_loss: 85.7135\n",
      "Epoch 2766/10000\n",
      "96/96 [==============================] - 0s 357us/step - loss: 61.9259 - val_loss: 101.9469\n",
      "Epoch 2767/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 62.8212 - val_loss: 97.2099\n",
      "Epoch 2768/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 62.4451 - val_loss: 99.6115\n",
      "Epoch 2769/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 65.4530 - val_loss: 88.8714\n",
      "Epoch 2770/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 65.5233 - val_loss: 100.4027\n",
      "Epoch 2771/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 67.6179 - val_loss: 100.6816\n",
      "Epoch 2772/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 64.1414 - val_loss: 99.6154\n",
      "Epoch 2773/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 61.7165 - val_loss: 109.6529\n",
      "Epoch 2774/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 63.7805 - val_loss: 100.9450\n",
      "Epoch 2775/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 67.0260 - val_loss: 95.7565\n",
      "Epoch 2776/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 66.6633 - val_loss: 96.7745\n",
      "Epoch 2777/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 65.2450 - val_loss: 98.4834\n",
      "Epoch 2778/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 63.7169 - val_loss: 100.9002\n",
      "Epoch 2779/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 64.7877 - val_loss: 95.3892\n",
      "Epoch 2780/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 63.9231 - val_loss: 88.7375\n",
      "Epoch 2781/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 66.9492 - val_loss: 96.9055\n",
      "Epoch 2782/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 62.5573 - val_loss: 91.1288\n",
      "Epoch 2783/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 61.8332 - val_loss: 89.2088\n",
      "Epoch 2784/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 63.7232 - val_loss: 100.3118\n",
      "Epoch 2785/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 61.3102 - val_loss: 107.4264\n",
      "Epoch 2786/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 65.4638 - val_loss: 99.6003\n",
      "Epoch 2787/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 61.1582 - val_loss: 100.1943\n",
      "Epoch 2788/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 61.9089 - val_loss: 108.9153\n",
      "Epoch 2789/10000\n",
      "96/96 [==============================] - 0s 484us/step - loss: 59.6585 - val_loss: 91.1821\n",
      "Epoch 2790/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 68.2508 - val_loss: 99.0934\n",
      "Epoch 2791/10000\n",
      "96/96 [==============================] - 0s 350us/step - loss: 65.8888 - val_loss: 95.3075\n",
      "Epoch 2792/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 62.1993 - val_loss: 98.9284\n",
      "Epoch 2793/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 63.5059 - val_loss: 98.4402\n",
      "Epoch 2794/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 61.1644 - val_loss: 97.9995\n",
      "Epoch 2795/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 67.4490 - val_loss: 80.8156\n",
      "Epoch 2796/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 71.5405 - val_loss: 92.0519\n",
      "Epoch 2797/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 62.8746 - val_loss: 102.1757\n",
      "Epoch 2798/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 66.2458 - val_loss: 101.8020\n",
      "Epoch 2799/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 66.6135 - val_loss: 105.6266\n",
      "Epoch 2800/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 66.5412 - val_loss: 94.4180\n",
      "Epoch 2801/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 59.0865 - val_loss: 96.3382\n",
      "Epoch 2802/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 64.2626 - val_loss: 103.3293\n",
      "Epoch 2803/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 60.7325 - val_loss: 97.5819\n",
      "Epoch 2804/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 61.5746 - val_loss: 93.3795\n",
      "Epoch 2805/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 62.0306 - val_loss: 94.5573\n",
      "Epoch 2806/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 66.3192 - val_loss: 93.9221\n",
      "Epoch 2807/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 63.1855 - val_loss: 100.0225\n",
      "Epoch 2808/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 63.7979 - val_loss: 104.0381\n",
      "Epoch 2809/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 59.5378 - val_loss: 93.0320\n",
      "Epoch 2810/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 58.3905 - val_loss: 105.0534\n",
      "Epoch 2811/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 64.4765 - val_loss: 92.6304\n",
      "Epoch 2812/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 63.0251 - val_loss: 96.6355\n",
      "Epoch 2813/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 64.1235 - val_loss: 107.9201\n",
      "Epoch 2814/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 60.1861 - val_loss: 93.6618\n",
      "Epoch 2815/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 64.0432 - val_loss: 108.7802\n",
      "Epoch 2816/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 61.9031 - val_loss: 93.7900\n",
      "Epoch 2817/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 57.2112 - val_loss: 97.9502\n",
      "Epoch 2818/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 61.7902 - val_loss: 108.9730\n",
      "Epoch 2819/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 62.3467 - val_loss: 100.5793\n",
      "Epoch 2820/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 61.5499 - val_loss: 104.2954\n",
      "Epoch 2821/10000\n",
      "96/96 [==============================] - 0s 516us/step - loss: 61.5567 - val_loss: 109.6575\n",
      "Epoch 2822/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 60.3765 - val_loss: 97.9919\n",
      "Epoch 2823/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 67.9677 - val_loss: 105.5291\n",
      "Epoch 2824/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 60.0426 - val_loss: 95.4903\n",
      "Epoch 2825/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 68.4887 - val_loss: 87.9450\n",
      "Epoch 2826/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 60.8471 - val_loss: 98.3430\n",
      "Epoch 2827/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 62.5464 - val_loss: 88.9689\n",
      "Epoch 2828/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 61.1922 - val_loss: 102.4060\n",
      "Epoch 2829/10000\n",
      "96/96 [==============================] - 0s 576us/step - loss: 63.4152 - val_loss: 101.7151\n",
      "Epoch 2830/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 61.1698 - val_loss: 99.7452\n",
      "Epoch 2831/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 64.3495 - val_loss: 105.9020\n",
      "Epoch 2832/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 64.1115 - val_loss: 94.2215\n",
      "Epoch 2833/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 67.7101 - val_loss: 100.2415\n",
      "Epoch 2834/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 63.1788 - val_loss: 100.3713\n",
      "Epoch 2835/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 57.7222 - val_loss: 89.2315\n",
      "Epoch 2836/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 60.8583 - val_loss: 102.7102\n",
      "Epoch 2837/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 64.6418 - val_loss: 99.2188\n",
      "Epoch 2838/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 63.5392 - val_loss: 99.6702\n",
      "Epoch 2839/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 59.4647 - val_loss: 97.9836\n",
      "Epoch 2840/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 61.6599 - val_loss: 92.7837\n",
      "Epoch 2841/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 60.2450 - val_loss: 99.2234\n",
      "Epoch 2842/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 64.4094 - val_loss: 90.0039\n",
      "Epoch 2843/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 62.7326 - val_loss: 99.9190\n",
      "Epoch 2844/10000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 63.3508 - val_loss: 104.9556\n",
      "Epoch 2845/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 63.2609 - val_loss: 92.7507\n",
      "Epoch 2846/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 66.1175 - val_loss: 97.8607\n",
      "Epoch 2847/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 63.8016 - val_loss: 103.5170\n",
      "Epoch 2848/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 63.1361 - val_loss: 95.7177\n",
      "Epoch 2849/10000\n",
      "96/96 [==============================] - 0s 351us/step - loss: 59.8830 - val_loss: 102.4699\n",
      "Epoch 2850/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 61.6637 - val_loss: 91.9432\n",
      "Epoch 2851/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 71.0742 - val_loss: 89.3735\n",
      "Epoch 2852/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 59.2241 - val_loss: 99.3615\n",
      "Epoch 2853/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 60.1065 - val_loss: 94.0752\n",
      "Epoch 2854/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 61.1866 - val_loss: 103.0331\n",
      "Epoch 2855/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 66.6571 - val_loss: 98.4339\n",
      "Epoch 2856/10000\n",
      "96/96 [==============================] - 0s 354us/step - loss: 61.3575 - val_loss: 85.8283\n",
      "Epoch 2857/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 62.1102 - val_loss: 95.0512\n",
      "Epoch 2858/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 60.9678 - val_loss: 91.9233\n",
      "Epoch 2859/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 59.0896 - val_loss: 106.7827\n",
      "Epoch 2860/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 59.9868 - val_loss: 97.1278\n",
      "Epoch 2861/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 62.5488 - val_loss: 104.8722\n",
      "Epoch 2862/10000\n",
      "96/96 [==============================] - 0s 354us/step - loss: 63.5669 - val_loss: 100.3156\n",
      "Epoch 2863/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 63.0941 - val_loss: 93.8565\n",
      "Epoch 2864/10000\n",
      "96/96 [==============================] - 0s 348us/step - loss: 58.3347 - val_loss: 97.2898\n",
      "Epoch 2865/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 59.0155 - val_loss: 105.3861\n",
      "Epoch 2866/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 61.1467 - val_loss: 86.6490\n",
      "Epoch 2867/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 62.2230 - val_loss: 102.5793\n",
      "Epoch 2868/10000\n",
      "96/96 [==============================] - 0s 351us/step - loss: 61.7671 - val_loss: 108.1203\n",
      "Epoch 2869/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 65.0075 - val_loss: 90.9491\n",
      "Epoch 2870/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 56.2765 - val_loss: 93.2664\n",
      "Epoch 2871/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 56.7935 - val_loss: 89.3644\n",
      "Epoch 2872/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 63.5865 - val_loss: 108.3832\n",
      "Epoch 2873/10000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 61.2442 - val_loss: 100.2951\n",
      "Epoch 2874/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 60.3805 - val_loss: 105.5244\n",
      "Epoch 2875/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 61.4477 - val_loss: 93.1980\n",
      "Epoch 2876/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 63.2954 - val_loss: 95.4993\n",
      "Epoch 2877/10000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 57.8875 - val_loss: 89.8275\n",
      "Epoch 2878/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 59.3348 - val_loss: 95.4844\n",
      "Epoch 2879/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 58.6863 - val_loss: 90.4815\n",
      "Epoch 2880/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 58.2302 - val_loss: 113.5532\n",
      "Epoch 2881/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 59.8119 - val_loss: 93.1051\n",
      "Epoch 2882/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 64.3819 - val_loss: 95.7866\n",
      "Epoch 2883/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 57.9198 - val_loss: 93.7457\n",
      "Epoch 2884/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 58.0732 - val_loss: 95.2429\n",
      "Epoch 2885/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 58.6268 - val_loss: 91.2756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2886/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 58.4485 - val_loss: 100.8437\n",
      "Epoch 2887/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 57.9087 - val_loss: 89.9484\n",
      "Epoch 2888/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 55.6230 - val_loss: 91.9736\n",
      "Epoch 2889/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 57.4809 - val_loss: 102.7631\n",
      "Epoch 2890/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 59.7707 - val_loss: 82.4295\n",
      "Epoch 2891/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 58.7692 - val_loss: 97.6340\n",
      "Epoch 2892/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 59.3025 - val_loss: 110.9840\n",
      "Epoch 2893/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 58.2255 - val_loss: 85.3573\n",
      "Epoch 2894/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 59.6235 - val_loss: 100.7663\n",
      "Epoch 2895/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 60.3862 - val_loss: 95.8999\n",
      "Epoch 2896/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 56.6292 - val_loss: 91.3986\n",
      "Epoch 2897/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 59.2235 - val_loss: 99.5956\n",
      "Epoch 2898/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 59.2438 - val_loss: 105.0110\n",
      "Epoch 2899/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 57.7579 - val_loss: 96.5174\n",
      "Epoch 2900/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 61.8559 - val_loss: 86.9228\n",
      "Epoch 2901/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 58.0385 - val_loss: 96.7734\n",
      "Epoch 2902/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 57.3180 - val_loss: 97.8687\n",
      "Epoch 2903/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 60.1948 - val_loss: 103.4941\n",
      "Epoch 2904/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 59.9146 - val_loss: 104.1927\n",
      "Epoch 2905/10000\n",
      "96/96 [==============================] - 0s 486us/step - loss: 58.2142 - val_loss: 106.8699\n",
      "Epoch 2906/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 57.8879 - val_loss: 95.5397\n",
      "Epoch 2907/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 57.9308 - val_loss: 97.0192\n",
      "Epoch 2908/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 56.9917 - val_loss: 91.8801\n",
      "Epoch 2909/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 59.3041 - val_loss: 90.3200\n",
      "Epoch 2910/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 56.2939 - val_loss: 80.5292\n",
      "Epoch 2911/10000\n",
      "96/96 [==============================] - 0s 346us/step - loss: 56.3190 - val_loss: 90.4378\n",
      "Epoch 2912/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 59.5541 - val_loss: 105.6305\n",
      "Epoch 2913/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 58.1796 - val_loss: 94.2985\n",
      "Epoch 2914/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 56.2703 - val_loss: 98.6485\n",
      "Epoch 2915/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 59.2665 - val_loss: 90.0334\n",
      "Epoch 2916/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 55.2079 - val_loss: 104.5417\n",
      "Epoch 2917/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 54.1999 - val_loss: 103.7399\n",
      "Epoch 2918/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 63.1497 - val_loss: 98.0664\n",
      "Epoch 2919/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 143.9720 - val_loss: 93.7764\n",
      "Epoch 2920/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 63.1829 - val_loss: 92.7733\n",
      "Epoch 2921/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 64.2822 - val_loss: 84.8316\n",
      "Epoch 2922/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 62.5879 - val_loss: 92.6156\n",
      "Epoch 2923/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 58.5851 - val_loss: 98.7927\n",
      "Epoch 2924/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 59.3647 - val_loss: 89.2255\n",
      "Epoch 2925/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 60.3679 - val_loss: 92.0734\n",
      "Epoch 2926/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 58.4448 - val_loss: 88.9226\n",
      "Epoch 2927/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 64.3154 - val_loss: 105.9158\n",
      "Epoch 2928/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 63.9215 - val_loss: 101.4095\n",
      "Epoch 2929/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 59.7708 - val_loss: 96.9962\n",
      "Epoch 2930/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 58.6052 - val_loss: 95.0065\n",
      "Epoch 2931/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 61.4725 - val_loss: 87.2803\n",
      "Epoch 2932/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 61.6326 - val_loss: 90.9989\n",
      "Epoch 2933/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 59.1938 - val_loss: 90.3036\n",
      "Epoch 2934/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 56.5081 - val_loss: 98.4796\n",
      "Epoch 2935/10000\n",
      "96/96 [==============================] - 0s 503us/step - loss: 60.3007 - val_loss: 103.8214\n",
      "Epoch 2936/10000\n",
      "96/96 [==============================] - 0s 488us/step - loss: 57.6951 - val_loss: 96.5004\n",
      "Epoch 2937/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 60.1264 - val_loss: 105.7364\n",
      "Epoch 2938/10000\n",
      "96/96 [==============================] - 0s 481us/step - loss: 61.8859 - val_loss: 90.6852\n",
      "Epoch 2939/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 57.0643 - val_loss: 94.8345\n",
      "Epoch 2940/10000\n",
      "96/96 [==============================] - 0s 496us/step - loss: 55.7244 - val_loss: 96.6877\n",
      "Epoch 2941/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 56.8165 - val_loss: 88.7625\n",
      "Epoch 2942/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 57.2922 - val_loss: 100.2275\n",
      "Epoch 2943/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 57.4706 - val_loss: 104.5536\n",
      "Epoch 2944/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 61.4985 - val_loss: 99.5047\n",
      "Epoch 2945/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 60.2526 - val_loss: 100.3111\n",
      "Epoch 2946/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 56.3174 - val_loss: 96.0403\n",
      "Epoch 2947/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 53.9926 - val_loss: 99.2399\n",
      "Epoch 2948/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 55.4955 - val_loss: 108.0062\n",
      "Epoch 2949/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 56.8463 - val_loss: 91.7894\n",
      "Epoch 2950/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 56.1925 - val_loss: 101.6448\n",
      "Epoch 2951/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 54.9926 - val_loss: 90.4281\n",
      "Epoch 2952/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 56.9800 - val_loss: 91.5291\n",
      "Epoch 2953/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 57.2165 - val_loss: 96.8494\n",
      "Epoch 2954/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 57.6509 - val_loss: 87.6561\n",
      "Epoch 2955/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 58.5756 - val_loss: 97.9050\n",
      "Epoch 2956/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 56.3626 - val_loss: 101.5389\n",
      "Epoch 2957/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 57.9129 - val_loss: 105.7629\n",
      "Epoch 2958/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 56.5610 - val_loss: 106.1489\n",
      "Epoch 2959/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 58.6308 - val_loss: 87.5179\n",
      "Epoch 2960/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 59.1519 - val_loss: 86.9051\n",
      "Epoch 2961/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 57.7936 - val_loss: 87.7155\n",
      "Epoch 2962/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 59.9442 - val_loss: 106.9724\n",
      "Epoch 2963/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 56.3788 - val_loss: 89.3022\n",
      "Epoch 2964/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 56.7871 - val_loss: 94.7428\n",
      "Epoch 2965/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 59.2748 - val_loss: 96.3409\n",
      "Epoch 2966/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 57.9455 - val_loss: 92.2357\n",
      "Epoch 2967/10000\n",
      "96/96 [==============================] - 0s 484us/step - loss: 57.2204 - val_loss: 96.7898\n",
      "Epoch 2968/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 56.6364 - val_loss: 105.0047\n",
      "Epoch 2969/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 55.2670 - val_loss: 98.4919\n",
      "Epoch 2970/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 58.1218 - val_loss: 103.0902\n",
      "Epoch 2971/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 56.3312 - val_loss: 101.0356\n",
      "Epoch 2972/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 59.2862 - val_loss: 106.1657\n",
      "Epoch 2973/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 57.7588 - val_loss: 98.5439\n",
      "Epoch 2974/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 58.5167 - val_loss: 86.0355\n",
      "Epoch 2975/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 58.1374 - val_loss: 100.6452\n",
      "Epoch 2976/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 81.2777 - val_loss: 99.2607\n",
      "Epoch 2977/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 63.5790 - val_loss: 106.5385\n",
      "Epoch 2978/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 64.0634 - val_loss: 94.5382\n",
      "Epoch 2979/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 61.4125 - val_loss: 88.8159\n",
      "Epoch 2980/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 76.2816 - val_loss: 89.1114\n",
      "Epoch 2981/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 65.0937 - val_loss: 81.3556\n",
      "Epoch 2982/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 68.3929 - val_loss: 89.4476\n",
      "Epoch 2983/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 66.8759 - val_loss: 89.1376\n",
      "Epoch 2984/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 59.6982 - val_loss: 96.2640\n",
      "Epoch 2985/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 60.8567 - val_loss: 105.7573\n",
      "Epoch 2986/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 63.7316 - val_loss: 101.2800\n",
      "Epoch 2987/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 62.9884 - val_loss: 95.1065\n",
      "Epoch 2988/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 63.5652 - val_loss: 95.0145\n",
      "Epoch 2989/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 62.5101 - val_loss: 91.2334\n",
      "Epoch 2990/10000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 60.7885 - val_loss: 100.9471\n",
      "Epoch 2991/10000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 63.2168 - val_loss: 109.3567\n",
      "Epoch 2992/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 61.2338 - val_loss: 93.2941\n",
      "Epoch 2993/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 63.3704 - val_loss: 97.7399\n",
      "Epoch 2994/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 59.0950 - val_loss: 97.4849\n",
      "Epoch 2995/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 61.4062 - val_loss: 99.2991\n",
      "Epoch 2996/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 62.7693 - val_loss: 102.6151\n",
      "Epoch 2997/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 55.7991 - val_loss: 87.6407\n",
      "Epoch 2998/10000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 59.7439 - val_loss: 94.4136\n",
      "Epoch 2999/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 58.7953 - val_loss: 90.7247\n",
      "Epoch 3000/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 59.2911 - val_loss: 92.7962\n",
      "Epoch 3001/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 63.2041 - val_loss: 97.3387\n",
      "Epoch 3002/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 59.4192 - val_loss: 91.2803\n",
      "Epoch 3003/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 60.2258 - val_loss: 89.9273\n",
      "Epoch 3004/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 62.6149 - val_loss: 93.7445\n",
      "Epoch 3005/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 58.4299 - val_loss: 98.5357\n",
      "Epoch 3006/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 58.2447 - val_loss: 104.4525\n",
      "Epoch 3007/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 60.5010 - val_loss: 100.0983\n",
      "Epoch 3008/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 58.7838 - val_loss: 102.2658\n",
      "Epoch 3009/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 58.8267 - val_loss: 87.6909\n",
      "Epoch 3010/10000\n",
      "96/96 [==============================] - 0s 493us/step - loss: 57.2321 - val_loss: 94.5888\n",
      "Epoch 3011/10000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 57.2349 - val_loss: 96.2284\n",
      "Epoch 3012/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 58.8469 - val_loss: 100.0537\n",
      "Epoch 3013/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 57.7041 - val_loss: 97.5337\n",
      "Epoch 3014/10000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 57.7123 - val_loss: 102.9355\n",
      "Epoch 3015/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 60.5139 - val_loss: 103.1470\n",
      "Epoch 3016/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 57.7941 - val_loss: 99.7261\n",
      "Epoch 3017/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 56.5111 - val_loss: 93.6791\n",
      "Epoch 3018/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 58.5875 - val_loss: 100.4753\n",
      "Epoch 3019/10000\n",
      "96/96 [==============================] - 0s 507us/step - loss: 57.2652 - val_loss: 93.4903\n",
      "Epoch 3020/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 56.6897 - val_loss: 102.4610\n",
      "Epoch 3021/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 56.3518 - val_loss: 103.8613\n",
      "Epoch 3022/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 57.1841 - val_loss: 97.7619\n",
      "Epoch 3023/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 57.2521 - val_loss: 101.1336\n",
      "Epoch 3024/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 57.5565 - val_loss: 98.3404\n",
      "Epoch 3025/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 56.3844 - val_loss: 102.1029\n",
      "Epoch 3026/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 54.8542 - val_loss: 109.8419\n",
      "Epoch 3027/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 56.9978 - val_loss: 104.9421\n",
      "Epoch 3028/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 68.5099 - val_loss: 108.3401\n",
      "Epoch 3029/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 62.0345 - val_loss: 94.5840\n",
      "Epoch 3030/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 66.2412 - val_loss: 97.2136\n",
      "Epoch 3031/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 61.7669 - val_loss: 90.1139\n",
      "Epoch 3032/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 61.9779 - val_loss: 99.9939\n",
      "Epoch 3033/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 62.8329 - val_loss: 92.1567\n",
      "Epoch 3034/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 56.2441 - val_loss: 87.0212\n",
      "Epoch 3035/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 61.4126 - val_loss: 115.5233\n",
      "Epoch 3036/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 60.3231 - val_loss: 98.0829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3037/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 59.9608 - val_loss: 93.2763\n",
      "Epoch 3038/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 58.1071 - val_loss: 92.9170\n",
      "Epoch 3039/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 59.6935 - val_loss: 87.8883\n",
      "Epoch 3040/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 61.3720 - val_loss: 85.2209\n",
      "Epoch 3041/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 58.1772 - val_loss: 102.2575\n",
      "Epoch 3042/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 58.1957 - val_loss: 107.2076\n",
      "Epoch 3043/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 57.7319 - val_loss: 92.5353\n",
      "Epoch 3044/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 71.1340 - val_loss: 99.2964\n",
      "Epoch 3045/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 57.4833 - val_loss: 97.8323\n",
      "Epoch 3046/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 59.7437 - val_loss: 105.6898\n",
      "Epoch 3047/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 61.0356 - val_loss: 91.6134\n",
      "Epoch 3048/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 61.7139 - val_loss: 98.5626\n",
      "Epoch 3049/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 59.9090 - val_loss: 102.0534\n",
      "Epoch 3050/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 58.9279 - val_loss: 98.4684\n",
      "Epoch 3051/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 59.2142 - val_loss: 104.1909\n",
      "Epoch 3052/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 55.9326 - val_loss: 94.7678\n",
      "Epoch 3053/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 58.8534 - val_loss: 106.4368\n",
      "Epoch 3054/10000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 56.3266 - val_loss: 96.9631\n",
      "Epoch 3055/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 55.4589 - val_loss: 111.1677\n",
      "Epoch 3056/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 55.1108 - val_loss: 105.8893\n",
      "Epoch 3057/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 55.3281 - val_loss: 95.9602\n",
      "Epoch 3058/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 56.3272 - val_loss: 101.8790\n",
      "Epoch 3059/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 56.1626 - val_loss: 90.9425\n",
      "Epoch 3060/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 56.5875 - val_loss: 91.0437\n",
      "Epoch 3061/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 56.3382 - val_loss: 94.7058\n",
      "Epoch 3062/10000\n",
      "96/96 [==============================] - 0s 475us/step - loss: 54.5810 - val_loss: 94.3294\n",
      "Epoch 3063/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 54.6933 - val_loss: 109.0468\n",
      "Epoch 3064/10000\n",
      "96/96 [==============================] - 0s 506us/step - loss: 55.1551 - val_loss: 86.5387\n",
      "Epoch 3065/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 53.6016 - val_loss: 100.9388\n",
      "Epoch 3066/10000\n",
      "96/96 [==============================] - 0s 519us/step - loss: 55.5203 - val_loss: 105.5321\n",
      "Epoch 3067/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 54.9455 - val_loss: 102.3684\n",
      "Epoch 3068/10000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 55.2440 - val_loss: 102.2004\n",
      "Epoch 3069/10000\n",
      "96/96 [==============================] - 0s 495us/step - loss: 56.5444 - val_loss: 96.7133\n",
      "Epoch 3070/10000\n",
      "96/96 [==============================] - 0s 498us/step - loss: 58.8510 - val_loss: 93.9487\n",
      "Epoch 3071/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 57.0140 - val_loss: 104.0259\n",
      "Epoch 3072/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 58.5873 - val_loss: 94.8401\n",
      "Epoch 3073/10000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 58.0912 - val_loss: 95.8024\n",
      "Epoch 3074/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 57.1194 - val_loss: 90.0406\n",
      "Epoch 3075/10000\n",
      "96/96 [==============================] - 0s 493us/step - loss: 56.0103 - val_loss: 94.6154\n",
      "Epoch 3076/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 57.2723 - val_loss: 100.9622\n",
      "Epoch 3077/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 57.2034 - val_loss: 107.1540\n",
      "Epoch 3078/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 56.0037 - val_loss: 95.9910\n",
      "Epoch 3079/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 55.1333 - val_loss: 96.9211\n",
      "Epoch 3080/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 55.3288 - val_loss: 98.6944\n",
      "Epoch 3081/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 53.5789 - val_loss: 106.4617\n",
      "Epoch 3082/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 57.0423 - val_loss: 93.3602\n",
      "Epoch 3083/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 56.8315 - val_loss: 96.6642\n",
      "Epoch 3084/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 51.7070 - val_loss: 110.3038\n",
      "Epoch 3085/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 58.1315 - val_loss: 91.7786\n",
      "Epoch 3086/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 56.2912 - val_loss: 95.1327\n",
      "Epoch 3087/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 54.6387 - val_loss: 102.9815\n",
      "Epoch 3088/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 56.0082 - val_loss: 102.5758\n",
      "Epoch 3089/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 60.0840 - val_loss: 106.1822\n",
      "Epoch 3090/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 55.5598 - val_loss: 98.3080\n",
      "Epoch 3091/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 52.3378 - val_loss: 88.9195\n",
      "Epoch 3092/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 56.9259 - val_loss: 93.0934\n",
      "Epoch 3093/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 56.5077 - val_loss: 92.2366\n",
      "Epoch 3094/10000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 56.2073 - val_loss: 94.2082\n",
      "Epoch 3095/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 54.4798 - val_loss: 94.2411\n",
      "Epoch 3096/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 57.3610 - val_loss: 94.3644\n",
      "Epoch 3097/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 60.6280 - val_loss: 108.4833\n",
      "Epoch 3098/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 58.0944 - val_loss: 100.2668\n",
      "Epoch 3099/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 56.6704 - val_loss: 100.3699\n",
      "Epoch 3100/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 58.4235 - val_loss: 91.5239\n",
      "Epoch 3101/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 55.5570 - val_loss: 94.5369\n",
      "Epoch 3102/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 61.2638 - val_loss: 94.0240\n",
      "Epoch 3103/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 56.9816 - val_loss: 101.4781\n",
      "Epoch 3104/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 54.2808 - val_loss: 114.4797\n",
      "Epoch 3105/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 54.0022 - val_loss: 101.6812\n",
      "Epoch 3106/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 56.1021 - val_loss: 96.1495\n",
      "Epoch 3107/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 56.7480 - val_loss: 106.3390\n",
      "Epoch 3108/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 58.2758 - val_loss: 97.6440\n",
      "Epoch 3109/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 54.6821 - val_loss: 96.9636\n",
      "Epoch 3110/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 55.7914 - val_loss: 95.8590\n",
      "Epoch 3111/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 58.0653 - val_loss: 94.9992\n",
      "Epoch 3112/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 55.6923 - val_loss: 90.7972\n",
      "Epoch 3113/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 61.8306 - val_loss: 98.1413\n",
      "Epoch 3114/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 56.8568 - val_loss: 101.7599\n",
      "Epoch 3115/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 57.2704 - val_loss: 112.1228\n",
      "Epoch 3116/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 57.7868 - val_loss: 104.3848\n",
      "Epoch 3117/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 58.2832 - val_loss: 99.7543\n",
      "Epoch 3118/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 54.2872 - val_loss: 93.7307\n",
      "Epoch 3119/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 56.7115 - val_loss: 104.1153\n",
      "Epoch 3120/10000\n",
      "96/96 [==============================] - 0s 484us/step - loss: 53.8275 - val_loss: 105.5953\n",
      "Epoch 3121/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 57.4633 - val_loss: 99.9212\n",
      "Epoch 3122/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 55.2704 - val_loss: 103.0515\n",
      "Epoch 3123/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 55.9119 - val_loss: 94.5957\n",
      "Epoch 3124/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 57.8409 - val_loss: 106.4728\n",
      "Epoch 3125/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 52.1906 - val_loss: 109.8670\n",
      "Epoch 3126/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 56.5463 - val_loss: 91.6462\n",
      "Epoch 3127/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 55.4963 - val_loss: 111.6427\n",
      "Epoch 3128/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 57.9716 - val_loss: 103.8467\n",
      "Epoch 3129/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 57.4261 - val_loss: 95.0614\n",
      "Epoch 3130/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 57.4475 - val_loss: 97.9552\n",
      "Epoch 3131/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 53.0816 - val_loss: 95.6393\n",
      "Epoch 3132/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 56.2728 - val_loss: 100.3761\n",
      "Epoch 3133/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 65.7414 - val_loss: 92.9589\n",
      "Epoch 3134/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 58.3592 - val_loss: 87.0213\n",
      "Epoch 3135/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 59.0438 - val_loss: 95.1469\n",
      "Epoch 3136/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 55.0940 - val_loss: 98.9821\n",
      "Epoch 3137/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 56.3746 - val_loss: 116.2867\n",
      "Epoch 3138/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 58.9119 - val_loss: 96.9517\n",
      "Epoch 3139/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 54.9548 - val_loss: 96.1056\n",
      "Epoch 3140/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 52.2480 - val_loss: 97.5979\n",
      "Epoch 3141/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 54.4654 - val_loss: 101.4437\n",
      "Epoch 3142/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 107.1370 - val_loss: 80.3309\n",
      "Epoch 3143/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 58.5186 - val_loss: 92.6432\n",
      "Epoch 3144/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 62.6448 - val_loss: 90.3948\n",
      "Epoch 3145/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 63.0022 - val_loss: 91.7068\n",
      "Epoch 3146/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 63.5164 - val_loss: 87.9016\n",
      "Epoch 3147/10000\n",
      "96/96 [==============================] - 0s 475us/step - loss: 63.3018 - val_loss: 92.9014\n",
      "Epoch 3148/10000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 62.9629 - val_loss: 94.1549\n",
      "Epoch 3149/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 60.0360 - val_loss: 98.1234\n",
      "Epoch 3150/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 60.5839 - val_loss: 112.4759\n",
      "Epoch 3151/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 61.2791 - val_loss: 91.8910\n",
      "Epoch 3152/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 64.5254 - val_loss: 103.3661\n",
      "Epoch 3153/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 62.0172 - val_loss: 98.1102\n",
      "Epoch 3154/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 60.9532 - val_loss: 108.9919\n",
      "Epoch 3155/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 65.9455 - val_loss: 105.3284\n",
      "Epoch 3156/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 58.8348 - val_loss: 96.3218\n",
      "Epoch 3157/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 59.0089 - val_loss: 100.6522\n",
      "Epoch 3158/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 59.1875 - val_loss: 100.1811\n",
      "Epoch 3159/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 58.4334 - val_loss: 88.1320\n",
      "Epoch 3160/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 57.5940 - val_loss: 109.9834\n",
      "Epoch 3161/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 60.5796 - val_loss: 104.1491\n",
      "Epoch 3162/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 54.9989 - val_loss: 102.3915\n",
      "Epoch 3163/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 56.7527 - val_loss: 102.5878\n",
      "Epoch 3164/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 56.8529 - val_loss: 103.1691\n",
      "Epoch 3165/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 61.1697 - val_loss: 98.5739\n",
      "Epoch 3166/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 55.9251 - val_loss: 91.2314\n",
      "Epoch 3167/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 56.6341 - val_loss: 94.0956\n",
      "Epoch 3168/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 55.2392 - val_loss: 97.6055\n",
      "Epoch 3169/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 57.6985 - val_loss: 89.3923\n",
      "Epoch 3170/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 57.2638 - val_loss: 98.9783\n",
      "Epoch 3171/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 58.7089 - val_loss: 100.8639\n",
      "Epoch 3172/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 56.4489 - val_loss: 98.4399\n",
      "Epoch 3173/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 57.4952 - val_loss: 107.0501\n",
      "Epoch 3174/10000\n",
      "96/96 [==============================] - 0s 521us/step - loss: 57.7283 - val_loss: 112.3851\n",
      "Epoch 3175/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 56.1031 - val_loss: 102.1886\n",
      "Epoch 3176/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 54.3501 - val_loss: 94.4605\n",
      "Epoch 3177/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 56.5755 - val_loss: 106.7792\n",
      "Epoch 3178/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 58.3097 - val_loss: 106.7831\n",
      "Epoch 3179/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 53.2754 - val_loss: 101.5409\n",
      "Epoch 3180/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 54.7170 - val_loss: 111.4136\n",
      "Epoch 3181/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 54.8654 - val_loss: 95.3723\n",
      "Epoch 3182/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 53.1767 - val_loss: 101.4289\n",
      "Epoch 3183/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 54.0468 - val_loss: 113.5993\n",
      "Epoch 3184/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 56.3002 - val_loss: 90.6891\n",
      "Epoch 3185/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 55.5234 - val_loss: 85.0249\n",
      "Epoch 3186/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 57.9787 - val_loss: 106.8312\n",
      "Epoch 3187/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 384us/step - loss: 55.9468 - val_loss: 99.2304\n",
      "Epoch 3188/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 58.8416 - val_loss: 101.9583\n",
      "Epoch 3189/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 54.4907 - val_loss: 104.1408\n",
      "Epoch 3190/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 57.5500 - val_loss: 97.1947\n",
      "Epoch 3191/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 55.6483 - val_loss: 96.4106\n",
      "Epoch 3192/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 59.3263 - val_loss: 105.4248\n",
      "Epoch 3193/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 58.8370 - val_loss: 99.3367\n",
      "Epoch 3194/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 107.8236 - val_loss: 91.9621\n",
      "Epoch 3195/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 67.9553 - val_loss: 87.6367\n",
      "Epoch 3196/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 72.1903 - val_loss: 85.8814\n",
      "Epoch 3197/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 71.1067 - val_loss: 90.9079\n",
      "Epoch 3198/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 62.7480 - val_loss: 84.5244\n",
      "Epoch 3199/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 68.8886 - val_loss: 97.5311\n",
      "Epoch 3200/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 69.1106 - val_loss: 94.0442\n",
      "Epoch 3201/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 71.6395 - val_loss: 89.3642\n",
      "Epoch 3202/10000\n",
      "96/96 [==============================] - 0s 509us/step - loss: 63.5545 - val_loss: 92.2766\n",
      "Epoch 3203/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 65.2404 - val_loss: 88.3502\n",
      "Epoch 3204/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 62.8226 - val_loss: 101.0259\n",
      "Epoch 3205/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 63.2372 - val_loss: 97.5669\n",
      "Epoch 3206/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 60.2467 - val_loss: 93.7036\n",
      "Epoch 3207/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 61.1889 - val_loss: 97.0291\n",
      "Epoch 3208/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 64.2947 - val_loss: 94.7912\n",
      "Epoch 3209/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 59.2200 - val_loss: 95.0989\n",
      "Epoch 3210/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 59.1230 - val_loss: 95.9313\n",
      "Epoch 3211/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 58.3931 - val_loss: 96.4646\n",
      "Epoch 3212/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 59.9913 - val_loss: 99.4751\n",
      "Epoch 3213/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 58.7985 - val_loss: 104.9895\n",
      "Epoch 3214/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 57.0805 - val_loss: 104.6723\n",
      "Epoch 3215/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 57.3684 - val_loss: 100.6904\n",
      "Epoch 3216/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 60.6742 - val_loss: 98.9082\n",
      "Epoch 3217/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 55.4102 - val_loss: 91.5790\n",
      "Epoch 3218/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 57.0857 - val_loss: 98.9398\n",
      "Epoch 3219/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 59.2878 - val_loss: 104.0407\n",
      "Epoch 3220/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 60.7760 - val_loss: 93.0026\n",
      "Epoch 3221/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 59.2605 - val_loss: 98.1474\n",
      "Epoch 3222/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 57.2421 - val_loss: 96.3086\n",
      "Epoch 3223/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 59.0488 - val_loss: 101.9782\n",
      "Epoch 3224/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 59.1999 - val_loss: 108.5265\n",
      "Epoch 3225/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 55.5411 - val_loss: 99.2727\n",
      "Epoch 3226/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 56.6431 - val_loss: 98.5792\n",
      "Epoch 3227/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 58.3134 - val_loss: 95.7809\n",
      "Epoch 3228/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 54.9179 - val_loss: 105.0054\n",
      "Epoch 3229/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 57.5245 - val_loss: 111.5411\n",
      "Epoch 3230/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 55.7098 - val_loss: 97.6289\n",
      "Epoch 3231/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 56.8457 - val_loss: 98.2365\n",
      "Epoch 3232/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 54.7671 - val_loss: 99.0825\n",
      "Epoch 3233/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 54.3192 - val_loss: 94.6587\n",
      "Epoch 3234/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 57.5152 - val_loss: 91.8351\n",
      "Epoch 3235/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 53.7988 - val_loss: 96.7340\n",
      "Epoch 3236/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 55.3386 - val_loss: 95.2148\n",
      "Epoch 3237/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 55.3391 - val_loss: 102.6698\n",
      "Epoch 3238/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 53.0276 - val_loss: 102.6637\n",
      "Epoch 3239/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 54.7463 - val_loss: 94.2429\n",
      "Epoch 3240/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 53.8807 - val_loss: 107.3295\n",
      "Epoch 3241/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 54.5738 - val_loss: 93.0930\n",
      "Epoch 3242/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 57.0395 - val_loss: 100.7572\n",
      "Epoch 3243/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 54.1068 - val_loss: 98.5893\n",
      "Epoch 3244/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 55.5383 - val_loss: 92.8563\n",
      "Epoch 3245/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 53.1923 - val_loss: 85.9897\n",
      "Epoch 3246/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 73.7207 - val_loss: 92.8155\n",
      "Epoch 3247/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 57.9754 - val_loss: 88.2367\n",
      "Epoch 3248/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 63.8211 - val_loss: 95.1323\n",
      "Epoch 3249/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 63.1418 - val_loss: 97.6016\n",
      "Epoch 3250/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 62.6003 - val_loss: 89.2659\n",
      "Epoch 3251/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 58.9570 - val_loss: 86.2117\n",
      "Epoch 3252/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 61.9628 - val_loss: 97.5309\n",
      "Epoch 3253/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 59.0594 - val_loss: 94.1217\n",
      "Epoch 3254/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 62.2393 - val_loss: 87.3241\n",
      "Epoch 3255/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 58.9827 - val_loss: 89.5377\n",
      "Epoch 3256/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 57.2301 - val_loss: 95.0348\n",
      "Epoch 3257/10000\n",
      "96/96 [==============================] - 0s 505us/step - loss: 62.4175 - val_loss: 95.2725\n",
      "Epoch 3258/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 58.8409 - val_loss: 87.9648\n",
      "Epoch 3259/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 59.4444 - val_loss: 92.2551\n",
      "Epoch 3260/10000\n",
      "96/96 [==============================] - 0s 343us/step - loss: 55.7508 - val_loss: 96.3674\n",
      "Epoch 3261/10000\n",
      "96/96 [==============================] - 0s 324us/step - loss: 58.7533 - val_loss: 95.5075\n",
      "Epoch 3262/10000\n",
      "96/96 [==============================] - 0s 319us/step - loss: 58.2962 - val_loss: 101.2908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3263/10000\n",
      "96/96 [==============================] - 0s 320us/step - loss: 58.4487 - val_loss: 97.2790\n",
      "Epoch 3264/10000\n",
      "96/96 [==============================] - 0s 318us/step - loss: 56.6898 - val_loss: 97.1545\n",
      "Epoch 3265/10000\n",
      "96/96 [==============================] - 0s 346us/step - loss: 55.6089 - val_loss: 95.1316\n",
      "Epoch 3266/10000\n",
      "96/96 [==============================] - 0s 337us/step - loss: 56.5439 - val_loss: 103.4265\n",
      "Epoch 3267/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 57.1674 - val_loss: 93.1036\n",
      "Epoch 3268/10000\n",
      "96/96 [==============================] - 0s 351us/step - loss: 53.6965 - val_loss: 94.5652\n",
      "Epoch 3269/10000\n",
      "96/96 [==============================] - 0s 322us/step - loss: 59.1661 - val_loss: 95.5366\n",
      "Epoch 3270/10000\n",
      "96/96 [==============================] - 0s 349us/step - loss: 56.1171 - val_loss: 93.0366\n",
      "Epoch 3271/10000\n",
      "96/96 [==============================] - 0s 331us/step - loss: 52.2722 - val_loss: 112.3772\n",
      "Epoch 3272/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 57.0273 - val_loss: 115.4334\n",
      "Epoch 3273/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 59.4082 - val_loss: 109.4040\n",
      "Epoch 3274/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 56.8591 - val_loss: 88.3875\n",
      "Epoch 3275/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 58.7534 - val_loss: 103.2178\n",
      "Epoch 3276/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 57.6598 - val_loss: 98.8222\n",
      "Epoch 3277/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 55.7524 - val_loss: 108.6265\n",
      "Epoch 3278/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 55.7703 - val_loss: 106.9471\n",
      "Epoch 3279/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 55.1118 - val_loss: 105.5789\n",
      "Epoch 3280/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 54.7990 - val_loss: 95.3528\n",
      "Epoch 3281/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 54.4928 - val_loss: 105.4671\n",
      "Epoch 3282/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 52.9976 - val_loss: 95.5600\n",
      "Epoch 3283/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 55.5465 - val_loss: 102.7411\n",
      "Epoch 3284/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 53.4357 - val_loss: 106.6425\n",
      "Epoch 3285/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 54.1326 - val_loss: 105.3864\n",
      "Epoch 3286/10000\n",
      "96/96 [==============================] - 0s 512us/step - loss: 53.7309 - val_loss: 96.5088\n",
      "Epoch 3287/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 56.8205 - val_loss: 107.2504\n",
      "Epoch 3288/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 53.1258 - val_loss: 90.3780\n",
      "Epoch 3289/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 56.9782 - val_loss: 113.5675\n",
      "Epoch 3290/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 58.3778 - val_loss: 108.2540\n",
      "Epoch 3291/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 52.1783 - val_loss: 89.4905\n",
      "Epoch 3292/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 54.9132 - val_loss: 109.7772\n",
      "Epoch 3293/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 55.0474 - val_loss: 111.4098\n",
      "Epoch 3294/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 54.8132 - val_loss: 96.4374\n",
      "Epoch 3295/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 55.1548 - val_loss: 106.3516\n",
      "Epoch 3296/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 54.2569 - val_loss: 85.3429\n",
      "Epoch 3297/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 53.7837 - val_loss: 95.6287\n",
      "Epoch 3298/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 52.5528 - val_loss: 95.5303\n",
      "Epoch 3299/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 58.1983 - val_loss: 107.1556\n",
      "Epoch 3300/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 57.3966 - val_loss: 92.3248\n",
      "Epoch 3301/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 54.1224 - val_loss: 91.2569\n",
      "Epoch 3302/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 64.0595 - val_loss: 98.8283\n",
      "Epoch 3303/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 53.0673 - val_loss: 112.1201\n",
      "Epoch 3304/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 55.4635 - val_loss: 98.8567\n",
      "Epoch 3305/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 53.2542 - val_loss: 108.6221\n",
      "Epoch 3306/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 53.5092 - val_loss: 99.4812\n",
      "Epoch 3307/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 52.9233 - val_loss: 100.3944\n",
      "Epoch 3308/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 53.5286 - val_loss: 105.9044\n",
      "Epoch 3309/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 80.9801 - val_loss: 97.4449\n",
      "Epoch 3310/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 57.1863 - val_loss: 93.0255\n",
      "Epoch 3311/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 61.1231 - val_loss: 93.5988\n",
      "Epoch 3312/10000\n",
      "96/96 [==============================] - 0s 512us/step - loss: 62.3856 - val_loss: 95.5511\n",
      "Epoch 3313/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 63.5457 - val_loss: 99.5034\n",
      "Epoch 3314/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 63.3530 - val_loss: 93.8586\n",
      "Epoch 3315/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 63.1634 - val_loss: 93.0114\n",
      "Epoch 3316/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 55.6350 - val_loss: 87.5853\n",
      "Epoch 3317/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 59.6389 - val_loss: 101.5847\n",
      "Epoch 3318/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 56.5957 - val_loss: 84.2636\n",
      "Epoch 3319/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 57.0716 - val_loss: 94.4698\n",
      "Epoch 3320/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 60.5602 - val_loss: 104.5226\n",
      "Epoch 3321/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 54.2399 - val_loss: 86.2746\n",
      "Epoch 3322/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 58.2030 - val_loss: 91.9153\n",
      "Epoch 3323/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 59.7588 - val_loss: 95.2733\n",
      "Epoch 3324/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 80.3500 - val_loss: 90.1574\n",
      "Epoch 3325/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 59.6207 - val_loss: 100.4384\n",
      "Epoch 3326/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 67.8433 - val_loss: 81.6953\n",
      "Epoch 3327/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 67.7867 - val_loss: 93.7355\n",
      "Epoch 3328/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 61.8392 - val_loss: 83.8469\n",
      "Epoch 3329/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 60.3382 - val_loss: 86.1436\n",
      "Epoch 3330/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 61.2694 - val_loss: 96.8511\n",
      "Epoch 3331/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 62.2455 - val_loss: 85.9215\n",
      "Epoch 3332/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 59.9147 - val_loss: 96.8983\n",
      "Epoch 3333/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 61.9250 - val_loss: 88.4169\n",
      "Epoch 3334/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 60.1319 - val_loss: 86.5463\n",
      "Epoch 3335/10000\n",
      "96/96 [==============================] - 0s 475us/step - loss: 57.9900 - val_loss: 89.9558\n",
      "Epoch 3336/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 57.3883 - val_loss: 86.8580\n",
      "Epoch 3337/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 60.0835 - val_loss: 95.7526\n",
      "Epoch 3338/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 58.8921 - val_loss: 99.0965\n",
      "Epoch 3339/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 60.4137 - val_loss: 97.9532\n",
      "Epoch 3340/10000\n",
      "96/96 [==============================] - 0s 492us/step - loss: 60.3079 - val_loss: 95.1177\n",
      "Epoch 3341/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 58.9654 - val_loss: 95.5457\n",
      "Epoch 3342/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 56.4571 - val_loss: 86.9523\n",
      "Epoch 3343/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 60.0478 - val_loss: 90.3525\n",
      "Epoch 3344/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 57.5416 - val_loss: 108.1633\n",
      "Epoch 3345/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 55.6430 - val_loss: 99.9328\n",
      "Epoch 3346/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 53.4129 - val_loss: 102.3990\n",
      "Epoch 3347/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 55.2200 - val_loss: 94.0672\n",
      "Epoch 3348/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 59.5057 - val_loss: 111.4795\n",
      "Epoch 3349/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 56.8294 - val_loss: 92.6799\n",
      "Epoch 3350/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 56.7678 - val_loss: 103.7552\n",
      "Epoch 3351/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 56.8197 - val_loss: 101.9560\n",
      "Epoch 3352/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 55.8366 - val_loss: 98.4278\n",
      "Epoch 3353/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 54.6722 - val_loss: 105.9115\n",
      "Epoch 3354/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 51.0240 - val_loss: 105.4332\n",
      "Epoch 3355/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 56.2677 - val_loss: 101.0286\n",
      "Epoch 3356/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 54.5914 - val_loss: 96.0979\n",
      "Epoch 3357/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 56.8209 - val_loss: 99.2651\n",
      "Epoch 3358/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 52.7367 - val_loss: 86.7916\n",
      "Epoch 3359/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 56.2356 - val_loss: 98.4539\n",
      "Epoch 3360/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 55.3660 - val_loss: 95.0125\n",
      "Epoch 3361/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 59.3557 - val_loss: 111.4014\n",
      "Epoch 3362/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 53.1150 - val_loss: 98.6010\n",
      "Epoch 3363/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 53.5675 - val_loss: 96.8957\n",
      "Epoch 3364/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 54.0301 - val_loss: 98.9763\n",
      "Epoch 3365/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 53.0030 - val_loss: 93.8050\n",
      "Epoch 3366/10000\n",
      "96/96 [==============================] - 0s 521us/step - loss: 53.4211 - val_loss: 101.1652\n",
      "Epoch 3367/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 56.1136 - val_loss: 88.6790\n",
      "Epoch 3368/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 53.4741 - val_loss: 108.0373\n",
      "Epoch 3369/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 54.0849 - val_loss: 104.8350\n",
      "Epoch 3370/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 55.4974 - val_loss: 97.0413\n",
      "Epoch 3371/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 52.4949 - val_loss: 109.5081\n",
      "Epoch 3372/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 53.5053 - val_loss: 90.9389\n",
      "Epoch 3373/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 53.7367 - val_loss: 96.8392\n",
      "Epoch 3374/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 53.6582 - val_loss: 105.4654\n",
      "Epoch 3375/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 51.9018 - val_loss: 94.6110\n",
      "Epoch 3376/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 52.2175 - val_loss: 89.9062\n",
      "Epoch 3377/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 54.3283 - val_loss: 91.6018\n",
      "Epoch 3378/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 51.5685 - val_loss: 100.7437\n",
      "Epoch 3379/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 53.6619 - val_loss: 92.9574\n",
      "Epoch 3380/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 53.2894 - val_loss: 102.5708\n",
      "Epoch 3381/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 54.4035 - val_loss: 98.5325\n",
      "Epoch 3382/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 55.8775 - val_loss: 101.2097\n",
      "Epoch 3383/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 54.4419 - val_loss: 104.4813\n",
      "Epoch 3384/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 53.5734 - val_loss: 95.0310\n",
      "Epoch 3385/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 59.6300 - val_loss: 96.3660\n",
      "Epoch 3386/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 55.6253 - val_loss: 100.6539\n",
      "Epoch 3387/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 52.6329 - val_loss: 92.8741\n",
      "Epoch 3388/10000\n",
      "96/96 [==============================] - 0s 355us/step - loss: 56.0765 - val_loss: 102.2695\n",
      "Epoch 3389/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 54.2611 - val_loss: 107.1719\n",
      "Epoch 3390/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 54.4897 - val_loss: 106.3442\n",
      "Epoch 3391/10000\n",
      "96/96 [==============================] - 0s 355us/step - loss: 58.9554 - val_loss: 77.1442\n",
      "Epoch 3392/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 53.2177 - val_loss: 88.2570\n",
      "Epoch 3393/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 54.3691 - val_loss: 87.5387\n",
      "Epoch 3394/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 54.7922 - val_loss: 103.7360\n",
      "Epoch 3395/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 57.5161 - val_loss: 87.3363\n",
      "Epoch 3396/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 55.2855 - val_loss: 107.7972\n",
      "Epoch 3397/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 61.6931 - val_loss: 94.3226\n",
      "Epoch 3398/10000\n",
      "96/96 [==============================] - 0s 353us/step - loss: 56.9249 - val_loss: 99.9547\n",
      "Epoch 3399/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 60.0825 - val_loss: 99.2317\n",
      "Epoch 3400/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 57.0397 - val_loss: 90.5011\n",
      "Epoch 3401/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 55.5282 - val_loss: 93.8166\n",
      "Epoch 3402/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 56.2250 - val_loss: 104.8317\n",
      "Epoch 3403/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 59.6569 - val_loss: 93.9209\n",
      "Epoch 3404/10000\n",
      "96/96 [==============================] - 0s 355us/step - loss: 56.8043 - val_loss: 101.4987\n",
      "Epoch 3405/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 55.3170 - val_loss: 100.8444\n",
      "Epoch 3406/10000\n",
      "96/96 [==============================] - 0s 353us/step - loss: 56.5650 - val_loss: 97.0971\n",
      "Epoch 3407/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 57.9962 - val_loss: 96.6006\n",
      "Epoch 3408/10000\n",
      "96/96 [==============================] - 0s 352us/step - loss: 54.2800 - val_loss: 97.3444\n",
      "Epoch 3409/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 53.7755 - val_loss: 101.3309\n",
      "Epoch 3410/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 51.8821 - val_loss: 108.9159\n",
      "Epoch 3411/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 55.7077 - val_loss: 100.4105\n",
      "Epoch 3412/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 54.4255 - val_loss: 99.2530\n",
      "Epoch 3413/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 379us/step - loss: 56.0259 - val_loss: 101.8048\n",
      "Epoch 3414/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 52.6938 - val_loss: 114.2909\n",
      "Epoch 3415/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 51.9757 - val_loss: 106.3477\n",
      "Epoch 3416/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 57.1776 - val_loss: 120.5386\n",
      "Epoch 3417/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 55.8648 - val_loss: 104.9421\n",
      "Epoch 3418/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 58.8716 - val_loss: 98.3522\n",
      "Epoch 3419/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 54.1427 - val_loss: 114.5952\n",
      "Epoch 3420/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 53.6806 - val_loss: 114.4893\n",
      "Epoch 3421/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 52.0076 - val_loss: 104.6483\n",
      "Epoch 3422/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 55.2651 - val_loss: 99.7520\n",
      "Epoch 3423/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 51.5323 - val_loss: 98.5593\n",
      "Epoch 3424/10000\n",
      "96/96 [==============================] - 0s 511us/step - loss: 58.6405 - val_loss: 100.6019\n",
      "Epoch 3425/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 55.3029 - val_loss: 103.3960\n",
      "Epoch 3426/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 51.8875 - val_loss: 98.4556\n",
      "Epoch 3427/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 52.5637 - val_loss: 93.9953\n",
      "Epoch 3428/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 56.0058 - val_loss: 98.9870\n",
      "Epoch 3429/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 56.0028 - val_loss: 103.3409\n",
      "Epoch 3430/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 50.9003 - val_loss: 100.2706\n",
      "Epoch 3431/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 52.2003 - val_loss: 99.5338\n",
      "Epoch 3432/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 55.9088 - val_loss: 114.9646\n",
      "Epoch 3433/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 53.9659 - val_loss: 98.3304\n",
      "Epoch 3434/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 56.0756 - val_loss: 102.2581\n",
      "Epoch 3435/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 50.3301 - val_loss: 97.0380\n",
      "Epoch 3436/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 55.9512 - val_loss: 99.1763\n",
      "Epoch 3437/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 51.3132 - val_loss: 101.9303\n",
      "Epoch 3438/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 53.8054 - val_loss: 104.6472\n",
      "Epoch 3439/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 52.8141 - val_loss: 101.0006\n",
      "Epoch 3440/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 52.5496 - val_loss: 96.1857\n",
      "Epoch 3441/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 53.5594 - val_loss: 86.5164\n",
      "Epoch 3442/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 51.7699 - val_loss: 107.6265\n",
      "Epoch 3443/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 59.8736 - val_loss: 97.9727\n",
      "Epoch 3444/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 52.3776 - val_loss: 104.0767\n",
      "Epoch 3445/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 53.3093 - val_loss: 98.7330\n",
      "Epoch 3446/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 53.5344 - val_loss: 94.5298\n",
      "Epoch 3447/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 54.8936 - val_loss: 100.4258\n",
      "Epoch 3448/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 58.5637 - val_loss: 100.0862\n",
      "Epoch 3449/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 52.4395 - val_loss: 96.5946\n",
      "Epoch 3450/10000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 52.5868 - val_loss: 95.5229\n",
      "Epoch 3451/10000\n",
      "96/96 [==============================] - 0s 498us/step - loss: 51.2524 - val_loss: 109.0604\n",
      "Epoch 3452/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 53.9018 - val_loss: 99.0599\n",
      "Epoch 3453/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 59.8345 - val_loss: 93.0555\n",
      "Epoch 3454/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 55.5794 - val_loss: 99.0469\n",
      "Epoch 3455/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 49.0870 - val_loss: 90.9607\n",
      "Epoch 3456/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 50.4868 - val_loss: 94.5933\n",
      "Epoch 3457/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 53.2832 - val_loss: 93.3704\n",
      "Epoch 3458/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 53.6993 - val_loss: 103.8671\n",
      "Epoch 3459/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 52.1051 - val_loss: 109.5065\n",
      "Epoch 3460/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 51.0374 - val_loss: 99.3748\n",
      "Epoch 3461/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 55.4915 - val_loss: 98.3380\n",
      "Epoch 3462/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 55.0339 - val_loss: 114.3262\n",
      "Epoch 3463/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 56.9182 - val_loss: 110.5393\n",
      "Epoch 3464/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 52.7098 - val_loss: 85.9920\n",
      "Epoch 3465/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 52.2977 - val_loss: 107.9046\n",
      "Epoch 3466/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 52.9655 - val_loss: 98.1243\n",
      "Epoch 3467/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 56.2317 - val_loss: 97.8542\n",
      "Epoch 3468/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 51.1891 - val_loss: 103.2268\n",
      "Epoch 3469/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 50.7054 - val_loss: 108.1728\n",
      "Epoch 3470/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 51.9938 - val_loss: 101.2528\n",
      "Epoch 3471/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 52.8311 - val_loss: 99.9919\n",
      "Epoch 3472/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 52.5834 - val_loss: 96.7072\n",
      "Epoch 3473/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 54.1400 - val_loss: 101.0424\n",
      "Epoch 3474/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 50.0471 - val_loss: 94.2388\n",
      "Epoch 3475/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 52.1712 - val_loss: 108.1323\n",
      "Epoch 3476/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 51.5861 - val_loss: 103.2442\n",
      "Epoch 3477/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 49.8333 - val_loss: 110.6885\n",
      "Epoch 3478/10000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 52.6367 - val_loss: 93.2431\n",
      "Epoch 3479/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 51.5968 - val_loss: 110.4114\n",
      "Epoch 3480/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 51.8669 - val_loss: 98.6876\n",
      "Epoch 3481/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 54.5027 - val_loss: 103.0597\n",
      "Epoch 3482/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 51.0672 - val_loss: 87.6660\n",
      "Epoch 3483/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 51.9885 - val_loss: 97.9435\n",
      "Epoch 3484/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 49.1158 - val_loss: 97.1232\n",
      "Epoch 3485/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 49.5845 - val_loss: 100.6172\n",
      "Epoch 3486/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 55.4982 - val_loss: 98.2641\n",
      "Epoch 3487/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 58.0512 - val_loss: 85.5546\n",
      "Epoch 3488/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 54.9120 - val_loss: 94.9909\n",
      "Epoch 3489/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 54.2306 - val_loss: 102.9774\n",
      "Epoch 3490/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 54.0348 - val_loss: 110.9872\n",
      "Epoch 3491/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 55.6966 - val_loss: 93.2929\n",
      "Epoch 3492/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 52.8806 - val_loss: 107.6992\n",
      "Epoch 3493/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 52.6739 - val_loss: 107.2341\n",
      "Epoch 3494/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 53.9946 - val_loss: 91.8491\n",
      "Epoch 3495/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 54.7463 - val_loss: 97.3503\n",
      "Epoch 3496/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 57.7232 - val_loss: 84.1093\n",
      "Epoch 3497/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 54.6662 - val_loss: 88.8740\n",
      "Epoch 3498/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 52.9762 - val_loss: 102.2748\n",
      "Epoch 3499/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 60.2902 - val_loss: 104.3340\n",
      "Epoch 3500/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 56.2394 - val_loss: 102.0035\n",
      "Epoch 3501/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 53.9094 - val_loss: 92.7714\n",
      "Epoch 3502/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 52.6598 - val_loss: 98.6693\n",
      "Epoch 3503/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 53.3151 - val_loss: 105.4609\n",
      "Epoch 3504/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 52.9493 - val_loss: 104.0728\n",
      "Epoch 3505/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 52.9917 - val_loss: 88.1136\n",
      "Epoch 3506/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 58.1176 - val_loss: 105.9363\n",
      "Epoch 3507/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 53.8721 - val_loss: 107.7606\n",
      "Epoch 3508/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 54.6774 - val_loss: 90.8225\n",
      "Epoch 3509/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 54.1632 - val_loss: 106.4994\n",
      "Epoch 3510/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 51.6468 - val_loss: 101.9544\n",
      "Epoch 3511/10000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 55.5735 - val_loss: 99.3293\n",
      "Epoch 3512/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 55.5269 - val_loss: 98.4972\n",
      "Epoch 3513/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 52.4284 - val_loss: 96.3372\n",
      "Epoch 3514/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 50.6319 - val_loss: 106.1728\n",
      "Epoch 3515/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 49.8117 - val_loss: 107.3414\n",
      "Epoch 3516/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 48.9597 - val_loss: 90.9326\n",
      "Epoch 3517/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 53.0309 - val_loss: 101.8344\n",
      "Epoch 3518/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 70.3960 - val_loss: 103.0885\n",
      "Epoch 3519/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 54.1597 - val_loss: 105.4005\n",
      "Epoch 3520/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 57.5549 - val_loss: 91.9570\n",
      "Epoch 3521/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 58.8252 - val_loss: 100.3214\n",
      "Epoch 3522/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 54.7120 - val_loss: 102.7287\n",
      "Epoch 3523/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 56.2459 - val_loss: 99.1076\n",
      "Epoch 3524/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 58.5943 - val_loss: 107.9972\n",
      "Epoch 3525/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 58.4634 - val_loss: 88.7921\n",
      "Epoch 3526/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 57.1137 - val_loss: 104.3996\n",
      "Epoch 3527/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 59.7501 - val_loss: 88.8125\n",
      "Epoch 3528/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 57.5717 - val_loss: 95.0378\n",
      "Epoch 3529/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 60.5184 - val_loss: 94.4573\n",
      "Epoch 3530/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 56.7952 - val_loss: 99.6272\n",
      "Epoch 3531/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 55.6277 - val_loss: 84.6609\n",
      "Epoch 3532/10000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 56.0165 - val_loss: 97.1984\n",
      "Epoch 3533/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 57.6624 - val_loss: 109.9429\n",
      "Epoch 3534/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 57.0008 - val_loss: 100.3832\n",
      "Epoch 3535/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 55.9361 - val_loss: 103.9958\n",
      "Epoch 3536/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 55.7649 - val_loss: 107.0713\n",
      "Epoch 3537/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 55.6698 - val_loss: 100.8269\n",
      "Epoch 3538/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 53.0627 - val_loss: 95.5889\n",
      "Epoch 3539/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 54.5333 - val_loss: 89.3172\n",
      "Epoch 3540/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 54.1368 - val_loss: 108.6500\n",
      "Epoch 3541/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 55.6012 - val_loss: 110.3043\n",
      "Epoch 3542/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 55.2071 - val_loss: 91.4431\n",
      "Epoch 3543/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 53.0967 - val_loss: 96.0508\n",
      "Epoch 3544/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 52.4519 - val_loss: 97.4302\n",
      "Epoch 3545/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 60.0514 - val_loss: 96.5410\n",
      "Epoch 3546/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 58.7699 - val_loss: 95.4262\n",
      "Epoch 3547/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 52.8604 - val_loss: 103.0204\n",
      "Epoch 3548/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 55.1230 - val_loss: 105.8192\n",
      "Epoch 3549/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 54.9707 - val_loss: 96.8279\n",
      "Epoch 3550/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 54.6184 - val_loss: 95.1689\n",
      "Epoch 3551/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 54.6622 - val_loss: 102.7001\n",
      "Epoch 3552/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 50.9134 - val_loss: 101.3783\n",
      "Epoch 3553/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 51.6354 - val_loss: 114.7760\n",
      "Epoch 3554/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 53.6608 - val_loss: 103.3995\n",
      "Epoch 3555/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 51.4637 - val_loss: 113.6534\n",
      "Epoch 3556/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 54.9748 - val_loss: 103.6125\n",
      "Epoch 3557/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 51.3028 - val_loss: 108.9548\n",
      "Epoch 3558/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 50.0260 - val_loss: 101.6776\n",
      "Epoch 3559/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 53.0192 - val_loss: 103.1755\n",
      "Epoch 3560/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 51.3966 - val_loss: 106.3793\n",
      "Epoch 3561/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 50.3484 - val_loss: 104.3817\n",
      "Epoch 3562/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 51.0882 - val_loss: 104.4497\n",
      "Epoch 3563/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 387us/step - loss: 52.0429 - val_loss: 106.8255\n",
      "Epoch 3564/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 49.4966 - val_loss: 114.0120\n",
      "Epoch 3565/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 51.5300 - val_loss: 88.6873\n",
      "Epoch 3566/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 52.1938 - val_loss: 111.2330\n",
      "Epoch 3567/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 54.1647 - val_loss: 96.5041\n",
      "Epoch 3568/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 55.5839 - val_loss: 103.4840\n",
      "Epoch 3569/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 55.5147 - val_loss: 92.6181\n",
      "Epoch 3570/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 54.2477 - val_loss: 102.2088\n",
      "Epoch 3571/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 52.6393 - val_loss: 110.6257\n",
      "Epoch 3572/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 49.4226 - val_loss: 106.6722\n",
      "Epoch 3573/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 51.1774 - val_loss: 106.3281\n",
      "Epoch 3574/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 52.7098 - val_loss: 100.5691\n",
      "Epoch 3575/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 53.8298 - val_loss: 97.1318\n",
      "Epoch 3576/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 55.2154 - val_loss: 97.6868\n",
      "Epoch 3577/10000\n",
      "96/96 [==============================] - 0s 356us/step - loss: 50.0079 - val_loss: 107.8249\n",
      "Epoch 3578/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 51.7105 - val_loss: 99.6491\n",
      "Epoch 3579/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 47.4881 - val_loss: 93.3933\n",
      "Epoch 3580/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 47.7492 - val_loss: 93.5066\n",
      "Epoch 3581/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 47.1317 - val_loss: 112.4965\n",
      "Epoch 3582/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 47.5281 - val_loss: 104.0354\n",
      "Epoch 3583/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 47.5020 - val_loss: 102.8747\n",
      "Epoch 3584/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 46.9854 - val_loss: 93.2718\n",
      "Epoch 3585/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 50.5233 - val_loss: 105.4476\n",
      "Epoch 3586/10000\n",
      "96/96 [==============================] - 0s 481us/step - loss: 53.1852 - val_loss: 92.2892\n",
      "Epoch 3587/10000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 50.8229 - val_loss: 95.8951\n",
      "Epoch 3588/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 46.8330 - val_loss: 100.0980\n",
      "Epoch 3589/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 51.3458 - val_loss: 107.6858\n",
      "Epoch 3590/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 51.7497 - val_loss: 101.5658\n",
      "Epoch 3591/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 50.2048 - val_loss: 94.8611\n",
      "Epoch 3592/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 48.8801 - val_loss: 87.0394\n",
      "Epoch 3593/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 61.6489 - val_loss: 96.7207\n",
      "Epoch 3594/10000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 61.9580 - val_loss: 87.7796\n",
      "Epoch 3595/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 70.5787 - val_loss: 88.1105\n",
      "Epoch 3596/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 60.0662 - val_loss: 89.8645\n",
      "Epoch 3597/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 66.3582 - val_loss: 85.5129\n",
      "Epoch 3598/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 64.6348 - val_loss: 96.3500\n",
      "Epoch 3599/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 60.2893 - val_loss: 87.6694\n",
      "Epoch 3600/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 62.9862 - val_loss: 105.2013\n",
      "Epoch 3601/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 60.1615 - val_loss: 92.3541\n",
      "Epoch 3602/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 57.3713 - val_loss: 89.3946\n",
      "Epoch 3603/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 87.5397 - val_loss: 88.4771\n",
      "Epoch 3604/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 68.0258 - val_loss: 99.4303\n",
      "Epoch 3605/10000\n",
      "96/96 [==============================] - 0s 356us/step - loss: 69.4572 - val_loss: 83.4394\n",
      "Epoch 3606/10000\n",
      "96/96 [==============================] - 0s 353us/step - loss: 70.2056 - val_loss: 86.2884\n",
      "Epoch 3607/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 69.7708 - val_loss: 88.7084\n",
      "Epoch 3608/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 65.6648 - val_loss: 88.5983\n",
      "Epoch 3609/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 64.7373 - val_loss: 86.9865\n",
      "Epoch 3610/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 59.1343 - val_loss: 83.7555\n",
      "Epoch 3611/10000\n",
      "96/96 [==============================] - 0s 351us/step - loss: 63.7242 - val_loss: 95.5061\n",
      "Epoch 3612/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 64.1337 - val_loss: 83.6498\n",
      "Epoch 3613/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 61.6053 - val_loss: 89.9990\n",
      "Epoch 3614/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 60.0790 - val_loss: 89.4090\n",
      "Epoch 3615/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 62.2428 - val_loss: 89.6888\n",
      "Epoch 3616/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 59.2678 - val_loss: 83.6646\n",
      "Epoch 3617/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 58.7178 - val_loss: 91.5256\n",
      "Epoch 3618/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 55.6787 - val_loss: 97.9130\n",
      "Epoch 3619/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 55.4850 - val_loss: 98.9643\n",
      "Epoch 3620/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 58.1345 - val_loss: 91.7856\n",
      "Epoch 3621/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 59.2175 - val_loss: 84.6328\n",
      "Epoch 3622/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 57.0156 - val_loss: 86.9686\n",
      "Epoch 3623/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 59.1802 - val_loss: 91.6672\n",
      "Epoch 3624/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 56.9928 - val_loss: 101.2638\n",
      "Epoch 3625/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 57.4393 - val_loss: 102.3713\n",
      "Epoch 3626/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 53.3361 - val_loss: 95.5440\n",
      "Epoch 3627/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 53.4364 - val_loss: 95.3163\n",
      "Epoch 3628/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 55.7200 - val_loss: 89.8653\n",
      "Epoch 3629/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 53.5441 - val_loss: 93.5698\n",
      "Epoch 3630/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 53.7933 - val_loss: 97.4755\n",
      "Epoch 3631/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 52.1951 - val_loss: 104.8205\n",
      "Epoch 3632/10000\n",
      "96/96 [==============================] - 0s 356us/step - loss: 63.0601 - val_loss: 90.6429\n",
      "Epoch 3633/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 56.0890 - val_loss: 99.3044\n",
      "Epoch 3634/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 51.8425 - val_loss: 110.4033\n",
      "Epoch 3635/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 54.7681 - val_loss: 94.2158\n",
      "Epoch 3636/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 61.2082 - val_loss: 92.0922\n",
      "Epoch 3637/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 52.7123 - val_loss: 91.8078\n",
      "Epoch 3638/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 51.8535 - val_loss: 98.6722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3639/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 53.3920 - val_loss: 97.3726\n",
      "Epoch 3640/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 50.1126 - val_loss: 98.2550\n",
      "Epoch 3641/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 51.0594 - val_loss: 97.5198\n",
      "Epoch 3642/10000\n",
      "96/96 [==============================] - 0s 492us/step - loss: 53.3201 - val_loss: 101.9258\n",
      "Epoch 3643/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 52.3754 - val_loss: 97.6708\n",
      "Epoch 3644/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 51.7330 - val_loss: 103.2973\n",
      "Epoch 3645/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 54.7775 - val_loss: 108.0738\n",
      "Epoch 3646/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 56.2463 - val_loss: 99.4233\n",
      "Epoch 3647/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 69.1788 - val_loss: 86.2816\n",
      "Epoch 3648/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 52.3853 - val_loss: 95.3451\n",
      "Epoch 3649/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 53.5707 - val_loss: 89.7944\n",
      "Epoch 3650/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 58.5917 - val_loss: 85.8725\n",
      "Epoch 3651/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 52.8031 - val_loss: 92.9410\n",
      "Epoch 3652/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 54.4958 - val_loss: 93.5328\n",
      "Epoch 3653/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 56.6756 - val_loss: 96.7454\n",
      "Epoch 3654/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 52.4098 - val_loss: 93.6261\n",
      "Epoch 3655/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 56.4217 - val_loss: 83.5660\n",
      "Epoch 3656/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 53.8407 - val_loss: 90.7074\n",
      "Epoch 3657/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 50.3680 - val_loss: 99.7367\n",
      "Epoch 3658/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 50.7987 - val_loss: 84.5995\n",
      "Epoch 3659/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 57.4270 - val_loss: 90.4469\n",
      "Epoch 3660/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 51.8272 - val_loss: 88.3217\n",
      "Epoch 3661/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 51.5906 - val_loss: 91.1558\n",
      "Epoch 3662/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 51.3922 - val_loss: 103.7953\n",
      "Epoch 3663/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 49.2782 - val_loss: 96.1758\n",
      "Epoch 3664/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 51.5294 - val_loss: 93.9892\n",
      "Epoch 3665/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 50.2414 - val_loss: 97.4448\n",
      "Epoch 3666/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 48.5901 - val_loss: 90.3835\n",
      "Epoch 3667/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 50.7246 - val_loss: 88.6855\n",
      "Epoch 3668/10000\n",
      "96/96 [==============================] - 0s 495us/step - loss: 49.9405 - val_loss: 87.8322\n",
      "Epoch 3669/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 49.7462 - val_loss: 99.9916\n",
      "Epoch 3670/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 48.3863 - val_loss: 99.7339\n",
      "Epoch 3671/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 46.6321 - val_loss: 95.8693\n",
      "Epoch 3672/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 52.2310 - val_loss: 100.5473\n",
      "Epoch 3673/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 51.9383 - val_loss: 95.9401\n",
      "Epoch 3674/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 49.1580 - val_loss: 93.8608\n",
      "Epoch 3675/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 50.6764 - val_loss: 84.9034\n",
      "Epoch 3676/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 53.2173 - val_loss: 99.9250\n",
      "Epoch 3677/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 51.7150 - val_loss: 92.5830\n",
      "Epoch 3678/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 51.1440 - val_loss: 97.6040\n",
      "Epoch 3679/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 51.5484 - val_loss: 91.6672\n",
      "Epoch 3680/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 54.2698 - val_loss: 88.0296\n",
      "Epoch 3681/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 57.4672 - val_loss: 97.8605\n",
      "Epoch 3682/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 64.7427 - val_loss: 107.0155\n",
      "Epoch 3683/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 58.4369 - val_loss: 99.2579\n",
      "Epoch 3684/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 54.5636 - val_loss: 94.7593\n",
      "Epoch 3685/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 55.7491 - val_loss: 93.7825\n",
      "Epoch 3686/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 57.8935 - val_loss: 87.7550\n",
      "Epoch 3687/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 56.4103 - val_loss: 99.7310\n",
      "Epoch 3688/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 55.5207 - val_loss: 85.2324\n",
      "Epoch 3689/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 54.4729 - val_loss: 84.9644\n",
      "Epoch 3690/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 48.5400 - val_loss: 96.8592\n",
      "Epoch 3691/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 53.9870 - val_loss: 109.8541\n",
      "Epoch 3692/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 54.5255 - val_loss: 97.8053\n",
      "Epoch 3693/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 53.0054 - val_loss: 108.8097\n",
      "Epoch 3694/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 48.6817 - val_loss: 101.1640\n",
      "Epoch 3695/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 51.6120 - val_loss: 100.6248\n",
      "Epoch 3696/10000\n",
      "96/96 [==============================] - 0s 487us/step - loss: 51.7582 - val_loss: 95.6534\n",
      "Epoch 3697/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 54.5807 - val_loss: 90.5672\n",
      "Epoch 3698/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 49.5613 - val_loss: 111.6058\n",
      "Epoch 3699/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 48.9896 - val_loss: 98.2720\n",
      "Epoch 3700/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 50.6457 - val_loss: 104.1252\n",
      "Epoch 3701/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 52.9741 - val_loss: 101.2371\n",
      "Epoch 3702/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 47.4002 - val_loss: 91.3430\n",
      "Epoch 3703/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 52.6665 - val_loss: 97.8340\n",
      "Epoch 3704/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 50.0167 - val_loss: 91.6929\n",
      "Epoch 3705/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 44.7637 - val_loss: 104.7411\n",
      "Epoch 3706/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 49.2252 - val_loss: 98.0187\n",
      "Epoch 3707/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 46.6242 - val_loss: 99.8906\n",
      "Epoch 3708/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 50.5630 - val_loss: 109.8372\n",
      "Epoch 3709/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 50.5977 - val_loss: 109.5753\n",
      "Epoch 3710/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 48.7449 - val_loss: 100.0958\n",
      "Epoch 3711/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 51.8352 - val_loss: 110.3849\n",
      "Epoch 3712/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 49.8653 - val_loss: 103.0899\n",
      "Epoch 3713/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 49.0199 - val_loss: 105.1115\n",
      "Epoch 3714/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 48.2273 - val_loss: 91.5980\n",
      "Epoch 3715/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 52.8092 - val_loss: 97.1514\n",
      "Epoch 3716/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 49.5198 - val_loss: 101.5697\n",
      "Epoch 3717/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 48.9081 - val_loss: 102.9141\n",
      "Epoch 3718/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 49.2914 - val_loss: 113.5670\n",
      "Epoch 3719/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 47.6478 - val_loss: 103.5126\n",
      "Epoch 3720/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 47.1601 - val_loss: 96.7657\n",
      "Epoch 3721/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 46.9907 - val_loss: 96.7056\n",
      "Epoch 3722/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 46.6861 - val_loss: 100.3015\n",
      "Epoch 3723/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 47.1037 - val_loss: 95.3078\n",
      "Epoch 3724/10000\n",
      "96/96 [==============================] - 0s 500us/step - loss: 49.2930 - val_loss: 99.3526\n",
      "Epoch 3725/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 45.1506 - val_loss: 100.4065\n",
      "Epoch 3726/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 47.0152 - val_loss: 96.9048\n",
      "Epoch 3727/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 45.2412 - val_loss: 108.9629\n",
      "Epoch 3728/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 46.5396 - val_loss: 87.4526\n",
      "Epoch 3729/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 45.7911 - val_loss: 96.0976\n",
      "Epoch 3730/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 46.4587 - val_loss: 106.6538\n",
      "Epoch 3731/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 45.4601 - val_loss: 99.6596\n",
      "Epoch 3732/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 47.7234 - val_loss: 104.3628\n",
      "Epoch 3733/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 46.7048 - val_loss: 88.9650\n",
      "Epoch 3734/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 44.4349 - val_loss: 92.5424\n",
      "Epoch 3735/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 45.8045 - val_loss: 99.2107\n",
      "Epoch 3736/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 43.3133 - val_loss: 114.6650\n",
      "Epoch 3737/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 44.0941 - val_loss: 100.4317\n",
      "Epoch 3738/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 48.5924 - val_loss: 95.6012\n",
      "Epoch 3739/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 44.3204 - val_loss: 110.6104\n",
      "Epoch 3740/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 48.9081 - val_loss: 93.5344\n",
      "Epoch 3741/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 44.8786 - val_loss: 98.6897\n",
      "Epoch 3742/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 59.8650 - val_loss: 106.5960\n",
      "Epoch 3743/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 43.7170 - val_loss: 96.7116\n",
      "Epoch 3744/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 47.6279 - val_loss: 90.0856\n",
      "Epoch 3745/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 44.3310 - val_loss: 92.9521\n",
      "Epoch 3746/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 48.4806 - val_loss: 99.0482\n",
      "Epoch 3747/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 45.0833 - val_loss: 98.3664\n",
      "Epoch 3748/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 44.6208 - val_loss: 82.5723\n",
      "Epoch 3749/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 45.6615 - val_loss: 93.4332\n",
      "Epoch 3750/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 57.8036 - val_loss: 110.7152\n",
      "Epoch 3751/10000\n",
      "96/96 [==============================] - 0s 535us/step - loss: 48.8165 - val_loss: 100.5275\n",
      "Epoch 3752/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 52.0519 - val_loss: 85.2356\n",
      "Epoch 3753/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 57.3895 - val_loss: 101.8988\n",
      "Epoch 3754/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 57.1279 - val_loss: 86.1586\n",
      "Epoch 3755/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 59.9510 - val_loss: 84.3362\n",
      "Epoch 3756/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 54.5823 - val_loss: 103.7401\n",
      "Epoch 3757/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 58.2560 - val_loss: 102.2807\n",
      "Epoch 3758/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 54.5534 - val_loss: 110.7998\n",
      "Epoch 3759/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 54.1654 - val_loss: 111.3822\n",
      "Epoch 3760/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 59.2632 - val_loss: 107.4921\n",
      "Epoch 3761/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 52.0903 - val_loss: 87.1977\n",
      "Epoch 3762/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 48.1616 - val_loss: 110.7293\n",
      "Epoch 3763/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 55.8208 - val_loss: 95.7366\n",
      "Epoch 3764/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 57.8605 - val_loss: 106.1666\n",
      "Epoch 3765/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 51.4647 - val_loss: 101.5461\n",
      "Epoch 3766/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 49.9953 - val_loss: 104.8699\n",
      "Epoch 3767/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 53.0154 - val_loss: 113.5601\n",
      "Epoch 3768/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 44.8563 - val_loss: 101.3839\n",
      "Epoch 3769/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 48.9420 - val_loss: 109.8508\n",
      "Epoch 3770/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 47.6372 - val_loss: 109.2582\n",
      "Epoch 3771/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 46.9964 - val_loss: 105.7602\n",
      "Epoch 3772/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 53.7277 - val_loss: 98.8348\n",
      "Epoch 3773/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 50.1928 - val_loss: 96.9664\n",
      "Epoch 3774/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 52.3331 - val_loss: 101.4685\n",
      "Epoch 3775/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 47.0530 - val_loss: 103.0301\n",
      "Epoch 3776/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 48.0588 - val_loss: 116.1452\n",
      "Epoch 3777/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 49.6220 - val_loss: 114.5941\n",
      "Epoch 3778/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 48.7001 - val_loss: 111.9928\n",
      "Epoch 3779/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 48.9381 - val_loss: 99.5900\n",
      "Epoch 3780/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 44.8751 - val_loss: 108.0862\n",
      "Epoch 3781/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 51.1956 - val_loss: 106.3670\n",
      "Epoch 3782/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 48.0353 - val_loss: 105.2030\n",
      "Epoch 3783/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 45.9036 - val_loss: 99.1008\n",
      "Epoch 3784/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 41.7020 - val_loss: 111.1628\n",
      "Epoch 3785/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 43.9413 - val_loss: 102.9358\n",
      "Epoch 3786/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 44.2119 - val_loss: 89.9870\n",
      "Epoch 3787/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 52.1874 - val_loss: 106.5114\n",
      "Epoch 3788/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 44.0871 - val_loss: 116.8584\n",
      "Epoch 3789/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 398us/step - loss: 45.4553 - val_loss: 102.5969\n",
      "Epoch 3790/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 48.8235 - val_loss: 105.0766\n",
      "Epoch 3791/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 47.7322 - val_loss: 112.8880\n",
      "Epoch 3792/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 51.0572 - val_loss: 110.3256\n",
      "Epoch 3793/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 48.9054 - val_loss: 112.6874\n",
      "Epoch 3794/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 52.3733 - val_loss: 94.0949\n",
      "Epoch 3795/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 53.4139 - val_loss: 107.3367\n",
      "Epoch 3796/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 52.2397 - val_loss: 96.7320\n",
      "Epoch 3797/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 49.6857 - val_loss: 105.4007\n",
      "Epoch 3798/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 49.5268 - val_loss: 106.2489\n",
      "Epoch 3799/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 44.9836 - val_loss: 97.2428\n",
      "Epoch 3800/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 83.2913 - val_loss: 102.0909\n",
      "Epoch 3801/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 48.3550 - val_loss: 110.2983\n",
      "Epoch 3802/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 42.5968 - val_loss: 97.7873\n",
      "Epoch 3803/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 48.7506 - val_loss: 124.1270\n",
      "Epoch 3804/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 50.6148 - val_loss: 104.5298\n",
      "Epoch 3805/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 45.1893 - val_loss: 108.9664\n",
      "Epoch 3806/10000\n",
      "96/96 [==============================] - 0s 513us/step - loss: 45.9384 - val_loss: 90.1490\n",
      "Epoch 3807/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 43.6844 - val_loss: 93.6760\n",
      "Epoch 3808/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 58.0184 - val_loss: 105.5065\n",
      "Epoch 3809/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 46.0610 - val_loss: 104.9125\n",
      "Epoch 3810/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 49.7796 - val_loss: 103.1031\n",
      "Epoch 3811/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 52.9521 - val_loss: 96.1577\n",
      "Epoch 3812/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 51.9290 - val_loss: 103.9208\n",
      "Epoch 3813/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 54.4144 - val_loss: 93.8456\n",
      "Epoch 3814/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 47.6680 - val_loss: 97.3020\n",
      "Epoch 3815/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 50.2506 - val_loss: 88.3953\n",
      "Epoch 3816/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 47.1207 - val_loss: 100.9599\n",
      "Epoch 3817/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 47.5742 - val_loss: 95.2063\n",
      "Epoch 3818/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 45.7497 - val_loss: 104.4536\n",
      "Epoch 3819/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 45.1206 - val_loss: 99.5019\n",
      "Epoch 3820/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 44.0049 - val_loss: 95.5868\n",
      "Epoch 3821/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 46.2242 - val_loss: 100.1701\n",
      "Epoch 3822/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 45.0115 - val_loss: 114.3025\n",
      "Epoch 3823/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 48.8122 - val_loss: 89.9326\n",
      "Epoch 3824/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 44.1236 - val_loss: 107.4289\n",
      "Epoch 3825/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 47.2322 - val_loss: 102.0357\n",
      "Epoch 3826/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 45.7111 - val_loss: 107.8290\n",
      "Epoch 3827/10000\n",
      "96/96 [==============================] - 0s 357us/step - loss: 44.1105 - val_loss: 95.7867\n",
      "Epoch 3828/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 41.0930 - val_loss: 92.8914\n",
      "Epoch 3829/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 40.9995 - val_loss: 92.6054\n",
      "Epoch 3830/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 44.5907 - val_loss: 90.6007\n",
      "Epoch 3831/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 44.1369 - val_loss: 102.8798\n",
      "Epoch 3832/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 44.5823 - val_loss: 83.3559\n",
      "Epoch 3833/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 41.4817 - val_loss: 99.8184\n",
      "Epoch 3834/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 41.2077 - val_loss: 110.4926\n",
      "Epoch 3835/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 43.8117 - val_loss: 92.8469\n",
      "Epoch 3836/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 46.3567 - val_loss: 104.6751\n",
      "Epoch 3837/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 39.5738 - val_loss: 99.1399\n",
      "Epoch 3838/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 42.8503 - val_loss: 100.6621\n",
      "Epoch 3839/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 39.1215 - val_loss: 101.1533\n",
      "Epoch 3840/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 41.0750 - val_loss: 106.8747\n",
      "Epoch 3841/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 92.5811 - val_loss: 115.4618\n",
      "Epoch 3842/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 41.9622 - val_loss: 92.4014\n",
      "Epoch 3843/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 41.3699 - val_loss: 84.9349\n",
      "Epoch 3844/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 61.4009 - val_loss: 87.1704\n",
      "Epoch 3845/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 48.4926 - val_loss: 95.9844\n",
      "Epoch 3846/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 81.1491 - val_loss: 95.0464\n",
      "Epoch 3847/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 58.9654 - val_loss: 99.8730\n",
      "Epoch 3848/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 56.6391 - val_loss: 95.1698\n",
      "Epoch 3849/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 65.8060 - val_loss: 90.5097\n",
      "Epoch 3850/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 66.7585 - val_loss: 95.1816\n",
      "Epoch 3851/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 58.8346 - val_loss: 83.1001\n",
      "Epoch 3852/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 57.9125 - val_loss: 106.0091\n",
      "Epoch 3853/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 58.6912 - val_loss: 99.7072\n",
      "Epoch 3854/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 63.5504 - val_loss: 90.1704\n",
      "Epoch 3855/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 53.7694 - val_loss: 100.1773\n",
      "Epoch 3856/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 56.9160 - val_loss: 100.7644\n",
      "Epoch 3857/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 53.2715 - val_loss: 101.3117\n",
      "Epoch 3858/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 55.1429 - val_loss: 85.4562\n",
      "Epoch 3859/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 51.4365 - val_loss: 91.5452\n",
      "Epoch 3860/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 50.2685 - val_loss: 99.6653\n",
      "Epoch 3861/10000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 51.6520 - val_loss: 110.9476\n",
      "Epoch 3862/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 51.4508 - val_loss: 99.2218\n",
      "Epoch 3863/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 50.7626 - val_loss: 106.7882\n",
      "Epoch 3864/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 56.2291 - val_loss: 101.3674\n",
      "Epoch 3865/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 51.7724 - val_loss: 111.5497\n",
      "Epoch 3866/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 51.0837 - val_loss: 95.3353\n",
      "Epoch 3867/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 50.2730 - val_loss: 89.6869\n",
      "Epoch 3868/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 53.9291 - val_loss: 109.7010\n",
      "Epoch 3869/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 49.4503 - val_loss: 103.4025\n",
      "Epoch 3870/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 48.2621 - val_loss: 83.9845\n",
      "Epoch 3871/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 51.7462 - val_loss: 109.1040\n",
      "Epoch 3872/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 47.0922 - val_loss: 101.2399\n",
      "Epoch 3873/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 63.4185 - val_loss: 105.3801\n",
      "Epoch 3874/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 46.4612 - val_loss: 87.8823\n",
      "Epoch 3875/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 51.7401 - val_loss: 98.5564\n",
      "Epoch 3876/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 49.7915 - val_loss: 104.5437\n",
      "Epoch 3877/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 47.5028 - val_loss: 105.5300\n",
      "Epoch 3878/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 47.8067 - val_loss: 104.3859\n",
      "Epoch 3879/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 53.8053 - val_loss: 124.7392\n",
      "Epoch 3880/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 121.8056 - val_loss: 106.4634\n",
      "Epoch 3881/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 53.3349 - val_loss: 103.8623\n",
      "Epoch 3882/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 51.5327 - val_loss: 100.1232\n",
      "Epoch 3883/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 54.3980 - val_loss: 97.1366\n",
      "Epoch 3884/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 52.3356 - val_loss: 84.7305\n",
      "Epoch 3885/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 52.3557 - val_loss: 114.2078\n",
      "Epoch 3886/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 45.7488 - val_loss: 92.0445\n",
      "Epoch 3887/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 52.4768 - val_loss: 82.2009\n",
      "Epoch 3888/10000\n",
      "96/96 [==============================] - 0s 484us/step - loss: 52.9151 - val_loss: 90.0114\n",
      "Epoch 3889/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 47.6714 - val_loss: 99.9118\n",
      "Epoch 3890/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 47.0924 - val_loss: 108.7057\n",
      "Epoch 3891/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 45.9456 - val_loss: 98.3859\n",
      "Epoch 3892/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 46.9703 - val_loss: 103.5705\n",
      "Epoch 3893/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 45.8170 - val_loss: 89.6797\n",
      "Epoch 3894/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 46.3134 - val_loss: 92.9056\n",
      "Epoch 3895/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 45.6417 - val_loss: 99.8384\n",
      "Epoch 3896/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 50.8300 - val_loss: 101.0339\n",
      "Epoch 3897/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 44.3975 - val_loss: 99.9911\n",
      "Epoch 3898/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 55.7403 - val_loss: 110.9666\n",
      "Epoch 3899/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 46.9850 - val_loss: 105.3027\n",
      "Epoch 3900/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 45.6985 - val_loss: 108.4196\n",
      "Epoch 3901/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 46.9673 - val_loss: 108.7689\n",
      "Epoch 3902/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 46.7585 - val_loss: 106.0799\n",
      "Epoch 3903/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 45.8455 - val_loss: 94.2799\n",
      "Epoch 3904/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 45.6451 - val_loss: 105.0571\n",
      "Epoch 3905/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 45.1337 - val_loss: 95.9456\n",
      "Epoch 3906/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 46.6756 - val_loss: 87.2593\n",
      "Epoch 3907/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 46.6151 - val_loss: 99.8743\n",
      "Epoch 3908/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 41.9665 - val_loss: 119.4182\n",
      "Epoch 3909/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 52.6000 - val_loss: 118.6199\n",
      "Epoch 3910/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 46.1653 - val_loss: 112.4052\n",
      "Epoch 3911/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 45.6865 - val_loss: 102.7405\n",
      "Epoch 3912/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 48.3597 - val_loss: 96.5578\n",
      "Epoch 3913/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 44.7689 - val_loss: 96.1422\n",
      "Epoch 3914/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 42.0105 - val_loss: 113.0940\n",
      "Epoch 3915/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 45.0855 - val_loss: 109.1538\n",
      "Epoch 3916/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 41.6260 - val_loss: 104.1068\n",
      "Epoch 3917/10000\n",
      "96/96 [==============================] - 0s 492us/step - loss: 41.4175 - val_loss: 104.0113\n",
      "Epoch 3918/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 39.9382 - val_loss: 97.5337\n",
      "Epoch 3919/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 38.9327 - val_loss: 101.8269\n",
      "Epoch 3920/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 43.1597 - val_loss: 117.0425\n",
      "Epoch 3921/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 42.8736 - val_loss: 97.8074\n",
      "Epoch 3922/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 42.9711 - val_loss: 103.8202\n",
      "Epoch 3923/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 41.1940 - val_loss: 106.5194\n",
      "Epoch 3924/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 45.9532 - val_loss: 104.2864\n",
      "Epoch 3925/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 41.8987 - val_loss: 102.4747\n",
      "Epoch 3926/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 44.7359 - val_loss: 91.5967\n",
      "Epoch 3927/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 42.4838 - val_loss: 94.3311\n",
      "Epoch 3928/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 40.9718 - val_loss: 98.3181\n",
      "Epoch 3929/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 38.9817 - val_loss: 108.7334\n",
      "Epoch 3930/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 40.1997 - val_loss: 95.1788\n",
      "Epoch 3931/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 40.9756 - val_loss: 99.3532\n",
      "Epoch 3932/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 63.1888 - val_loss: 103.9504\n",
      "Epoch 3933/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 50.8872 - val_loss: 113.1854\n",
      "Epoch 3934/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 54.6108 - val_loss: 98.6518\n",
      "Epoch 3935/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 51.0745 - val_loss: 89.5173\n",
      "Epoch 3936/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 62.2041 - val_loss: 107.4732\n",
      "Epoch 3937/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 49.3817 - val_loss: 114.0708\n",
      "Epoch 3938/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 52.2932 - val_loss: 102.3575\n",
      "Epoch 3939/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 388us/step - loss: 46.8681 - val_loss: 102.9541\n",
      "Epoch 3940/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 49.7696 - val_loss: 88.4655\n",
      "Epoch 3941/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 53.0557 - val_loss: 91.0883\n",
      "Epoch 3942/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 49.5824 - val_loss: 101.7910\n",
      "Epoch 3943/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 55.9729 - val_loss: 88.6633\n",
      "Epoch 3944/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 53.3341 - val_loss: 101.1291\n",
      "Epoch 3945/10000\n",
      "96/96 [==============================] - 0s 480us/step - loss: 50.6492 - val_loss: 104.3090\n",
      "Epoch 3946/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 49.5113 - val_loss: 90.0925\n",
      "Epoch 3947/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 50.4577 - val_loss: 95.8460\n",
      "Epoch 3948/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 43.0124 - val_loss: 90.9592\n",
      "Epoch 3949/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 47.3847 - val_loss: 113.0552\n",
      "Epoch 3950/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 40.6608 - val_loss: 90.3774\n",
      "Epoch 3951/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 43.9605 - val_loss: 95.9671\n",
      "Epoch 3952/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 42.5804 - val_loss: 105.3551\n",
      "Epoch 3953/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 44.0560 - val_loss: 96.8345\n",
      "Epoch 3954/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 38.6138 - val_loss: 106.5460\n",
      "Epoch 3955/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 48.1797 - val_loss: 114.6646\n",
      "Epoch 3956/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 47.2971 - val_loss: 97.4553\n",
      "Epoch 3957/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 51.7748 - val_loss: 105.8969\n",
      "Epoch 3958/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 52.9677 - val_loss: 104.1981\n",
      "Epoch 3959/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 43.7648 - val_loss: 87.1957\n",
      "Epoch 3960/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 43.9427 - val_loss: 111.8985\n",
      "Epoch 3961/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 42.8686 - val_loss: 104.9520\n",
      "Epoch 3962/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 42.0110 - val_loss: 103.8157\n",
      "Epoch 3963/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 46.2067 - val_loss: 114.1163\n",
      "Epoch 3964/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 43.0871 - val_loss: 112.5873\n",
      "Epoch 3965/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 46.8552 - val_loss: 109.0109\n",
      "Epoch 3966/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 42.2078 - val_loss: 110.0096\n",
      "Epoch 3967/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 48.7874 - val_loss: 110.0807\n",
      "Epoch 3968/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 37.5211 - val_loss: 104.5659\n",
      "Epoch 3969/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 40.9686 - val_loss: 116.6177\n",
      "Epoch 3970/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 38.7366 - val_loss: 89.6902\n",
      "Epoch 3971/10000\n",
      "96/96 [==============================] - 0s 495us/step - loss: 36.8959 - val_loss: 104.4962\n",
      "Epoch 3972/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 40.9568 - val_loss: 102.0925\n",
      "Epoch 3973/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 38.9005 - val_loss: 95.6272\n",
      "Epoch 3974/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 38.2217 - val_loss: 103.3842\n",
      "Epoch 3975/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 34.4763 - val_loss: 105.2637\n",
      "Epoch 3976/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 41.6778 - val_loss: 110.8001\n",
      "Epoch 3977/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 39.3963 - val_loss: 100.5523\n",
      "Epoch 3978/10000\n",
      "96/96 [==============================] - 0s 353us/step - loss: 41.6538 - val_loss: 97.2882\n",
      "Epoch 3979/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 34.5377 - val_loss: 103.2319\n",
      "Epoch 3980/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 50.2989 - val_loss: 93.7779\n",
      "Epoch 3981/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 42.5152 - val_loss: 110.0595\n",
      "Epoch 3982/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 47.9798 - val_loss: 105.8385\n",
      "Epoch 3983/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 45.2245 - val_loss: 87.8009\n",
      "Epoch 3984/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 48.8670 - val_loss: 97.6296\n",
      "Epoch 3985/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 44.6151 - val_loss: 99.5754\n",
      "Epoch 3986/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 40.8605 - val_loss: 102.0389\n",
      "Epoch 3987/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 45.6606 - val_loss: 93.5859\n",
      "Epoch 3988/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 58.0936 - val_loss: 92.8904\n",
      "Epoch 3989/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 48.6858 - val_loss: 86.4464\n",
      "Epoch 3990/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 52.0871 - val_loss: 85.0865\n",
      "Epoch 3991/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 52.2365 - val_loss: 90.2128\n",
      "Epoch 3992/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 57.5149 - val_loss: 80.9182\n",
      "Epoch 3993/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 50.7639 - val_loss: 93.8055\n",
      "Epoch 3994/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 52.6070 - val_loss: 91.8997\n",
      "Epoch 3995/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 48.4751 - val_loss: 84.8289\n",
      "Epoch 3996/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 50.3679 - val_loss: 82.1792\n",
      "Epoch 3997/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 50.8865 - val_loss: 87.3342\n",
      "Epoch 3998/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 46.3885 - val_loss: 81.3334\n",
      "Epoch 3999/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 49.8668 - val_loss: 86.6365\n",
      "Epoch 4000/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 49.1567 - val_loss: 84.7582\n",
      "Epoch 4001/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 44.5099 - val_loss: 79.8579\n",
      "Epoch 4002/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 43.1955 - val_loss: 81.5763\n",
      "Epoch 4003/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 46.7942 - val_loss: 86.7463\n",
      "Epoch 4004/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 47.9158 - val_loss: 80.5039\n",
      "Epoch 4005/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 44.6493 - val_loss: 85.4210\n",
      "Epoch 4006/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 47.9850 - val_loss: 90.3376\n",
      "Epoch 4007/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 47.3768 - val_loss: 87.7902\n",
      "Epoch 4008/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 42.7213 - val_loss: 98.8949\n",
      "Epoch 4009/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 41.9161 - val_loss: 86.3815\n",
      "Epoch 4010/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 40.0266 - val_loss: 87.4289\n",
      "Epoch 4011/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 44.6723 - val_loss: 84.7565\n",
      "Epoch 4012/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 40.8772 - val_loss: 95.4121\n",
      "Epoch 4013/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 43.6724 - val_loss: 93.1670\n",
      "Epoch 4014/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 40.5223 - val_loss: 74.4853\n",
      "Epoch 4015/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 35.6088 - val_loss: 96.9015\n",
      "Epoch 4016/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 38.4149 - val_loss: 107.9123\n",
      "Epoch 4017/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 41.5355 - val_loss: 97.1990\n",
      "Epoch 4018/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 38.8648 - val_loss: 96.1297\n",
      "Epoch 4019/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 37.9046 - val_loss: 85.4174\n",
      "Epoch 4020/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 34.3963 - val_loss: 107.9456\n",
      "Epoch 4021/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 36.5612 - val_loss: 91.2667\n",
      "Epoch 4022/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 39.1137 - val_loss: 112.0500\n",
      "Epoch 4023/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 34.3811 - val_loss: 106.8105\n",
      "Epoch 4024/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 37.1923 - val_loss: 93.3229\n",
      "Epoch 4025/10000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 36.4470 - val_loss: 111.1354\n",
      "Epoch 4026/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 38.3532 - val_loss: 92.2305\n",
      "Epoch 4027/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 37.2690 - val_loss: 104.1184\n",
      "Epoch 4028/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 36.8103 - val_loss: 105.6466\n",
      "Epoch 4029/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 43.4251 - val_loss: 107.6395\n",
      "Epoch 4030/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 38.2087 - val_loss: 100.8272\n",
      "Epoch 4031/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 38.0944 - val_loss: 91.3347\n",
      "Epoch 4032/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 35.6991 - val_loss: 106.5134\n",
      "Epoch 4033/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 41.3515 - val_loss: 98.1791\n",
      "Epoch 4034/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 42.9655 - val_loss: 107.8192\n",
      "Epoch 4035/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 50.1456 - val_loss: 97.4595\n",
      "Epoch 4036/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 41.3118 - val_loss: 95.7924\n",
      "Epoch 4037/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 42.0573 - val_loss: 100.3051\n",
      "Epoch 4038/10000\n",
      "96/96 [==============================] - 0s 506us/step - loss: 45.5704 - val_loss: 93.4119\n",
      "Epoch 4039/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 36.6180 - val_loss: 104.2313\n",
      "Epoch 4040/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 38.3881 - val_loss: 84.3215\n",
      "Epoch 4041/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 37.2445 - val_loss: 91.6143\n",
      "Epoch 4042/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 37.3280 - val_loss: 84.8854\n",
      "Epoch 4043/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 37.8362 - val_loss: 101.1412\n",
      "Epoch 4044/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 39.3512 - val_loss: 107.3289\n",
      "Epoch 4045/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 36.9787 - val_loss: 94.5884\n",
      "Epoch 4046/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 36.4110 - val_loss: 97.2953\n",
      "Epoch 4047/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 34.2181 - val_loss: 99.0132\n",
      "Epoch 4048/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 38.6639 - val_loss: 95.2061\n",
      "Epoch 4049/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 32.6123 - val_loss: 93.9038\n",
      "Epoch 4050/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 39.9745 - val_loss: 88.4588\n",
      "Epoch 4051/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 34.6162 - val_loss: 94.0468\n",
      "Epoch 4052/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 32.2687 - val_loss: 94.5264\n",
      "Epoch 4053/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 34.2526 - val_loss: 90.0926\n",
      "Epoch 4054/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 41.8918 - val_loss: 94.2911\n",
      "Epoch 4055/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 33.7982 - val_loss: 88.5168\n",
      "Epoch 4056/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 34.6937 - val_loss: 96.7155\n",
      "Epoch 4057/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 39.8976 - val_loss: 94.3362\n",
      "Epoch 4058/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 35.6855 - val_loss: 93.6133\n",
      "Epoch 4059/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 36.0528 - val_loss: 97.9746\n",
      "Epoch 4060/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 39.5718 - val_loss: 82.8430\n",
      "Epoch 4061/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 39.0119 - val_loss: 85.5605\n",
      "Epoch 4062/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 36.8624 - val_loss: 106.3034\n",
      "Epoch 4063/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 36.7405 - val_loss: 100.6594\n",
      "Epoch 4064/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 37.5129 - val_loss: 103.2402\n",
      "Epoch 4065/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 39.6401 - val_loss: 97.6266\n",
      "Epoch 4066/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 39.6221 - val_loss: 95.2051\n",
      "Epoch 4067/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 34.2592 - val_loss: 93.1938\n",
      "Epoch 4068/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 34.5030 - val_loss: 86.6480\n",
      "Epoch 4069/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 38.8871 - val_loss: 103.0490\n",
      "Epoch 4070/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 36.7828 - val_loss: 103.9191\n",
      "Epoch 4071/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 36.4158 - val_loss: 87.1286\n",
      "Epoch 4072/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 30.5755 - val_loss: 98.1036\n",
      "Epoch 4073/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 30.3877 - val_loss: 101.4959\n",
      "Epoch 4074/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 34.1633 - val_loss: 102.2073\n",
      "Epoch 4075/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 34.2990 - val_loss: 88.6662\n",
      "Epoch 4076/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 35.8907 - val_loss: 94.5329\n",
      "Epoch 4077/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 36.7630 - val_loss: 112.6869\n",
      "Epoch 4078/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 36.1816 - val_loss: 96.5689\n",
      "Epoch 4079/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 37.1878 - val_loss: 85.7123\n",
      "Epoch 4080/10000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 34.3275 - val_loss: 100.9947\n",
      "Epoch 4081/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 33.7976 - val_loss: 82.0755\n",
      "Epoch 4082/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 36.0569 - val_loss: 95.1181\n",
      "Epoch 4083/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 40.1615 - val_loss: 80.9494\n",
      "Epoch 4084/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 37.1183 - val_loss: 90.7474\n",
      "Epoch 4085/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 34.5851 - val_loss: 99.7358\n",
      "Epoch 4086/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 36.9546 - val_loss: 82.1154\n",
      "Epoch 4087/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 36.5234 - val_loss: 90.7900\n",
      "Epoch 4088/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 35.7343 - val_loss: 88.6106\n",
      "Epoch 4089/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 34.8136 - val_loss: 103.3849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4090/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 36.2301 - val_loss: 97.6299\n",
      "Epoch 4091/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 34.2350 - val_loss: 99.0745\n",
      "Epoch 4092/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 34.6163 - val_loss: 94.5202\n",
      "Epoch 4093/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 34.9492 - val_loss: 86.6302\n",
      "Epoch 4094/10000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 35.4667 - val_loss: 92.3833\n",
      "Epoch 4095/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 33.1790 - val_loss: 89.9824\n",
      "Epoch 4096/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 37.6328 - val_loss: 98.9946\n",
      "Epoch 4097/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 34.4448 - val_loss: 105.0106\n",
      "Epoch 4098/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 72.5364 - val_loss: 86.7869\n",
      "Epoch 4099/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 34.9111 - val_loss: 93.8782\n",
      "Epoch 4100/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 37.1045 - val_loss: 88.3374\n",
      "Epoch 4101/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 61.1347 - val_loss: 92.4783\n",
      "Epoch 4102/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 55.2330 - val_loss: 95.2459\n",
      "Epoch 4103/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 65.8700 - val_loss: 79.2096\n",
      "Epoch 4104/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 56.7861 - val_loss: 91.5120\n",
      "Epoch 4105/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 55.2659 - val_loss: 111.6184\n",
      "Epoch 4106/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 65.7675 - val_loss: 110.8592\n",
      "Epoch 4107/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 53.3196 - val_loss: 89.9770\n",
      "Epoch 4108/10000\n",
      "96/96 [==============================] - 0s 498us/step - loss: 54.5384 - val_loss: 86.8051\n",
      "Epoch 4109/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 53.0566 - val_loss: 90.9927\n",
      "Epoch 4110/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 54.6097 - val_loss: 94.8274\n",
      "Epoch 4111/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 48.4340 - val_loss: 94.3615\n",
      "Epoch 4112/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 49.7338 - val_loss: 96.8674\n",
      "Epoch 4113/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 47.4433 - val_loss: 88.9959\n",
      "Epoch 4114/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 46.3844 - val_loss: 107.3860\n",
      "Epoch 4115/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 45.6722 - val_loss: 104.6986\n",
      "Epoch 4116/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 40.1581 - val_loss: 95.5539\n",
      "Epoch 4117/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 50.6202 - val_loss: 91.0147\n",
      "Epoch 4118/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 42.6627 - val_loss: 104.9482\n",
      "Epoch 4119/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 38.8574 - val_loss: 86.4314\n",
      "Epoch 4120/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 37.2146 - val_loss: 95.2731\n",
      "Epoch 4121/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 107.2770 - val_loss: 102.7248\n",
      "Epoch 4122/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 42.4495 - val_loss: 94.6319\n",
      "Epoch 4123/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 75.9423 - val_loss: 90.2810\n",
      "Epoch 4124/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 43.4307 - val_loss: 96.1248\n",
      "Epoch 4125/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 51.5004 - val_loss: 89.4574\n",
      "Epoch 4126/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 48.3264 - val_loss: 87.2447\n",
      "Epoch 4127/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 50.7545 - val_loss: 90.2131\n",
      "Epoch 4128/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 54.6670 - val_loss: 82.9877\n",
      "Epoch 4129/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 47.3632 - val_loss: 94.1717\n",
      "Epoch 4130/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 50.1352 - val_loss: 82.9163\n",
      "Epoch 4131/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 51.1998 - val_loss: 81.0382\n",
      "Epoch 4132/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 51.4892 - val_loss: 89.5792\n",
      "Epoch 4133/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 48.7522 - val_loss: 83.9523\n",
      "Epoch 4134/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 50.0000 - val_loss: 96.8263\n",
      "Epoch 4135/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 47.0418 - val_loss: 92.8103\n",
      "Epoch 4136/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 52.0398 - val_loss: 92.6270\n",
      "Epoch 4137/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 46.0166 - val_loss: 87.1229\n",
      "Epoch 4138/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 47.2951 - val_loss: 101.6124\n",
      "Epoch 4139/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 49.5627 - val_loss: 87.9160\n",
      "Epoch 4140/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 45.0258 - val_loss: 112.5470\n",
      "Epoch 4141/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 45.5570 - val_loss: 86.4147\n",
      "Epoch 4142/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 44.8241 - val_loss: 79.8799\n",
      "Epoch 4143/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 42.1827 - val_loss: 90.8471\n",
      "Epoch 4144/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 44.3897 - val_loss: 97.9692\n",
      "Epoch 4145/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 43.1502 - val_loss: 90.7779\n",
      "Epoch 4146/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 43.1613 - val_loss: 107.4671\n",
      "Epoch 4147/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 39.9120 - val_loss: 87.7668\n",
      "Epoch 4148/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 35.7211 - val_loss: 89.0206\n",
      "Epoch 4149/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 36.9916 - val_loss: 92.8374\n",
      "Epoch 4150/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 36.2163 - val_loss: 90.2003\n",
      "Epoch 4151/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 40.0324 - val_loss: 80.0354\n",
      "Epoch 4152/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 40.1833 - val_loss: 100.4334\n",
      "Epoch 4153/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 42.4846 - val_loss: 81.4102\n",
      "Epoch 4154/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 38.8549 - val_loss: 101.0346\n",
      "Epoch 4155/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 42.6968 - val_loss: 95.7701\n",
      "Epoch 4156/10000\n",
      "96/96 [==============================] - 0s 354us/step - loss: 43.6716 - val_loss: 87.1779\n",
      "Epoch 4157/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 41.9728 - val_loss: 107.3860\n",
      "Epoch 4158/10000\n",
      "96/96 [==============================] - 0s 355us/step - loss: 37.5232 - val_loss: 89.6647\n",
      "Epoch 4159/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 42.6585 - val_loss: 93.0568\n",
      "Epoch 4160/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 45.3771 - val_loss: 97.6385\n",
      "Epoch 4161/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 40.7492 - val_loss: 81.7967\n",
      "Epoch 4162/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 45.1875 - val_loss: 86.0420\n",
      "Epoch 4163/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 40.6196 - val_loss: 88.2240\n",
      "Epoch 4164/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 39.1199 - val_loss: 76.5031\n",
      "Epoch 4165/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 34.6440 - val_loss: 95.4434\n",
      "Epoch 4166/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 42.5217 - val_loss: 87.4527\n",
      "Epoch 4167/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 42.8083 - val_loss: 84.8564\n",
      "Epoch 4168/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 41.4855 - val_loss: 86.5654\n",
      "Epoch 4169/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 45.4502 - val_loss: 89.5734\n",
      "Epoch 4170/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 37.9733 - val_loss: 108.3541\n",
      "Epoch 4171/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 40.3555 - val_loss: 83.5994\n",
      "Epoch 4172/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 39.0567 - val_loss: 90.9503\n",
      "Epoch 4173/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 36.2665 - val_loss: 86.0815\n",
      "Epoch 4174/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 38.0341 - val_loss: 91.3395\n",
      "Epoch 4175/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 38.3413 - val_loss: 96.6624\n",
      "Epoch 4176/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 38.9510 - val_loss: 98.1530\n",
      "Epoch 4177/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 42.5499 - val_loss: 85.3735\n",
      "Epoch 4178/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 38.3703 - val_loss: 89.8737\n",
      "Epoch 4179/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 34.7366 - val_loss: 88.1655\n",
      "Epoch 4180/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 37.2086 - val_loss: 86.9473\n",
      "Epoch 4181/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 37.0705 - val_loss: 81.0396\n",
      "Epoch 4182/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 34.6369 - val_loss: 105.5547\n",
      "Epoch 4183/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 35.6872 - val_loss: 94.9316\n",
      "Epoch 4184/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 38.0634 - val_loss: 95.2350\n",
      "Epoch 4185/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 35.2360 - val_loss: 94.1031\n",
      "Epoch 4186/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 38.3677 - val_loss: 94.0077\n",
      "Epoch 4187/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 39.2356 - val_loss: 77.7188\n",
      "Epoch 4188/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 36.6914 - val_loss: 97.2665\n",
      "Epoch 4189/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 37.3306 - val_loss: 87.4969\n",
      "Epoch 4190/10000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 34.3164 - val_loss: 92.0990\n",
      "Epoch 4191/10000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 37.1165 - val_loss: 88.3035\n",
      "Epoch 4192/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 35.3601 - val_loss: 89.3340\n",
      "Epoch 4193/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 35.2805 - val_loss: 99.4072\n",
      "Epoch 4194/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 31.8875 - val_loss: 98.6279\n",
      "Epoch 4195/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 35.4865 - val_loss: 105.2596\n",
      "Epoch 4196/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 32.1958 - val_loss: 92.5906\n",
      "Epoch 4197/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 36.4721 - val_loss: 97.8654\n",
      "Epoch 4198/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 39.6531 - val_loss: 111.7188\n",
      "Epoch 4199/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 35.3725 - val_loss: 99.1522\n",
      "Epoch 4200/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 34.5847 - val_loss: 96.4073\n",
      "Epoch 4201/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 35.8487 - val_loss: 94.7887\n",
      "Epoch 4202/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 36.4127 - val_loss: 97.3132\n",
      "Epoch 4203/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 31.3361 - val_loss: 86.6371\n",
      "Epoch 4204/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 30.8544 - val_loss: 83.1906\n",
      "Epoch 4205/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 32.9008 - val_loss: 100.4212\n",
      "Epoch 4206/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 31.3872 - val_loss: 93.5413\n",
      "Epoch 4207/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 29.6492 - val_loss: 102.2199\n",
      "Epoch 4208/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 33.5409 - val_loss: 96.9302\n",
      "Epoch 4209/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 34.2823 - val_loss: 89.1872\n",
      "Epoch 4210/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 36.3849 - val_loss: 97.6889\n",
      "Epoch 4211/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 35.0903 - val_loss: 90.9466\n",
      "Epoch 4212/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 35.8026 - val_loss: 95.4325\n",
      "Epoch 4213/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 35.1094 - val_loss: 96.3880\n",
      "Epoch 4214/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 35.9141 - val_loss: 93.9253\n",
      "Epoch 4215/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 34.8955 - val_loss: 95.1322\n",
      "Epoch 4216/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 30.7599 - val_loss: 102.9735\n",
      "Epoch 4217/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 34.7261 - val_loss: 107.2368\n",
      "Epoch 4218/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 35.4201 - val_loss: 102.2517\n",
      "Epoch 4219/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 38.0154 - val_loss: 99.0783\n",
      "Epoch 4220/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 102.7200 - val_loss: 85.9323\n",
      "Epoch 4221/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 32.3083 - val_loss: 95.6735\n",
      "Epoch 4222/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 37.4982 - val_loss: 84.6748\n",
      "Epoch 4223/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 35.6630 - val_loss: 88.5702\n",
      "Epoch 4224/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 37.2578 - val_loss: 84.3695\n",
      "Epoch 4225/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 35.4186 - val_loss: 96.0924\n",
      "Epoch 4226/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 35.7734 - val_loss: 86.6223\n",
      "Epoch 4227/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 35.5990 - val_loss: 91.7168\n",
      "Epoch 4228/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 38.0942 - val_loss: 87.3301\n",
      "Epoch 4229/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 35.8232 - val_loss: 98.0218\n",
      "Epoch 4230/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 37.4155 - val_loss: 88.9999\n",
      "Epoch 4231/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 33.3949 - val_loss: 90.6407\n",
      "Epoch 4232/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 33.2576 - val_loss: 107.0512\n",
      "Epoch 4233/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 35.8583 - val_loss: 95.4750\n",
      "Epoch 4234/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 32.5149 - val_loss: 107.8436\n",
      "Epoch 4235/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 36.8627 - val_loss: 85.5183\n",
      "Epoch 4236/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 34.2148 - val_loss: 100.3845\n",
      "Epoch 4237/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 34.3651 - val_loss: 101.2583\n",
      "Epoch 4238/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 36.2442 - val_loss: 99.3329\n",
      "Epoch 4239/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 43.1916 - val_loss: 97.1911\n",
      "Epoch 4240/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 35.8324 - val_loss: 88.2953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4241/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 42.6423 - val_loss: 95.3239\n",
      "Epoch 4242/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 42.6333 - val_loss: 94.2029\n",
      "Epoch 4243/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 48.8708 - val_loss: 99.9300\n",
      "Epoch 4244/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 47.2056 - val_loss: 85.5540\n",
      "Epoch 4245/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 42.0223 - val_loss: 93.1873\n",
      "Epoch 4246/10000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 50.6556 - val_loss: 85.2336\n",
      "Epoch 4247/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 52.0055 - val_loss: 94.5623\n",
      "Epoch 4248/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 39.1910 - val_loss: 96.0318\n",
      "Epoch 4249/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 43.3115 - val_loss: 103.1254\n",
      "Epoch 4250/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 43.8678 - val_loss: 102.7038\n",
      "Epoch 4251/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 40.7352 - val_loss: 95.6898\n",
      "Epoch 4252/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 40.9146 - val_loss: 115.3435\n",
      "Epoch 4253/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 42.5138 - val_loss: 101.7852\n",
      "Epoch 4254/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 36.8450 - val_loss: 89.3531\n",
      "Epoch 4255/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 37.0698 - val_loss: 90.5374\n",
      "Epoch 4256/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 32.9801 - val_loss: 97.2061\n",
      "Epoch 4257/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 33.9423 - val_loss: 99.3311\n",
      "Epoch 4258/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 36.0782 - val_loss: 101.3146\n",
      "Epoch 4259/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 36.5999 - val_loss: 88.3891\n",
      "Epoch 4260/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 39.4770 - val_loss: 82.2848\n",
      "Epoch 4261/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 35.9714 - val_loss: 99.8883\n",
      "Epoch 4262/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 44.2352 - val_loss: 78.8962\n",
      "Epoch 4263/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 37.1848 - val_loss: 91.3301\n",
      "Epoch 4264/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 35.0789 - val_loss: 105.2700\n",
      "Epoch 4265/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 34.3650 - val_loss: 103.6372\n",
      "Epoch 4266/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 34.9424 - val_loss: 104.7177\n",
      "Epoch 4267/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 41.4139 - val_loss: 97.2350\n",
      "Epoch 4268/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 35.0392 - val_loss: 100.7033\n",
      "Epoch 4269/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 39.2052 - val_loss: 89.6114\n",
      "Epoch 4270/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 33.9283 - val_loss: 98.3422\n",
      "Epoch 4271/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 33.3982 - val_loss: 106.7361\n",
      "Epoch 4272/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 34.0936 - val_loss: 105.2121\n",
      "Epoch 4273/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 35.8952 - val_loss: 103.0948\n",
      "Epoch 4274/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 33.0398 - val_loss: 118.2045\n",
      "Epoch 4275/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 35.4105 - val_loss: 99.7889\n",
      "Epoch 4276/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 34.6757 - val_loss: 108.4531\n",
      "Epoch 4277/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 32.1135 - val_loss: 106.3686\n",
      "Epoch 4278/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 34.3802 - val_loss: 99.6782\n",
      "Epoch 4279/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 33.7646 - val_loss: 85.4342\n",
      "Epoch 4280/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 45.8748 - val_loss: 101.7563\n",
      "Epoch 4281/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 39.5148 - val_loss: 104.5618\n",
      "Epoch 4282/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 39.8379 - val_loss: 99.8665\n",
      "Epoch 4283/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 39.9992 - val_loss: 106.5693\n",
      "Epoch 4284/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 41.0431 - val_loss: 89.2780\n",
      "Epoch 4285/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 39.9497 - val_loss: 97.5509\n",
      "Epoch 4286/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 40.8950 - val_loss: 90.4559\n",
      "Epoch 4287/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 38.0776 - val_loss: 94.4932\n",
      "Epoch 4288/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 36.7577 - val_loss: 103.3929\n",
      "Epoch 4289/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 35.3003 - val_loss: 99.7136\n",
      "Epoch 4290/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 35.9236 - val_loss: 110.9438\n",
      "Epoch 4291/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 39.2065 - val_loss: 103.2879\n",
      "Epoch 4292/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 39.6761 - val_loss: 79.1466\n",
      "Epoch 4293/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 33.2582 - val_loss: 101.5855\n",
      "Epoch 4294/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 34.6281 - val_loss: 81.3766\n",
      "Epoch 4295/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 33.6139 - val_loss: 106.1409\n",
      "Epoch 4296/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 38.6909 - val_loss: 108.9540\n",
      "Epoch 4297/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 34.3971 - val_loss: 90.4900\n",
      "Epoch 4298/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 34.3115 - val_loss: 90.2196\n",
      "Epoch 4299/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 35.6960 - val_loss: 98.4881\n",
      "Epoch 4300/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 35.4425 - val_loss: 94.1924\n",
      "Epoch 4301/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 36.4973 - val_loss: 106.7745\n",
      "Epoch 4302/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 30.3452 - val_loss: 97.9353\n",
      "Epoch 4303/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 31.4635 - val_loss: 80.6178\n",
      "Epoch 4304/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 31.2656 - val_loss: 96.8160\n",
      "Epoch 4305/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 33.0175 - val_loss: 99.9215\n",
      "Epoch 4306/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 30.6085 - val_loss: 92.1176\n",
      "Epoch 4307/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 33.5920 - val_loss: 94.4384\n",
      "Epoch 4308/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 34.8149 - val_loss: 94.1413\n",
      "Epoch 4309/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 32.0252 - val_loss: 100.4031\n",
      "Epoch 4310/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 30.2553 - val_loss: 108.3351\n",
      "Epoch 4311/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 32.5691 - val_loss: 100.1514\n",
      "Epoch 4312/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 34.9222 - val_loss: 96.4600\n",
      "Epoch 4313/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 33.7135 - val_loss: 100.9046\n",
      "Epoch 4314/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 31.1228 - val_loss: 88.9713\n",
      "Epoch 4315/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 34.0498 - val_loss: 91.3183\n",
      "Epoch 4316/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 34.9073 - val_loss: 78.5886\n",
      "Epoch 4317/10000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 38.6671 - val_loss: 99.2598\n",
      "Epoch 4318/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 36.4743 - val_loss: 89.0019\n",
      "Epoch 4319/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 34.7267 - val_loss: 90.1520\n",
      "Epoch 4320/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 36.9796 - val_loss: 89.5369\n",
      "Epoch 4321/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 34.7498 - val_loss: 78.4671\n",
      "Epoch 4322/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 35.5172 - val_loss: 72.6809\n",
      "Epoch 4323/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 34.9372 - val_loss: 84.7114\n",
      "Epoch 4324/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 33.9751 - val_loss: 96.2427\n",
      "Epoch 4325/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 35.8514 - val_loss: 101.4873\n",
      "Epoch 4326/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 35.1272 - val_loss: 95.1239\n",
      "Epoch 4327/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 35.6117 - val_loss: 95.4680\n",
      "Epoch 4328/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 34.3476 - val_loss: 106.4069\n",
      "Epoch 4329/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 33.3386 - val_loss: 86.9960\n",
      "Epoch 4330/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 33.4171 - val_loss: 109.5868\n",
      "Epoch 4331/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 33.3739 - val_loss: 82.5493\n",
      "Epoch 4332/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 32.4450 - val_loss: 90.2917\n",
      "Epoch 4333/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 30.7292 - val_loss: 92.6371\n",
      "Epoch 4334/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 29.4686 - val_loss: 75.9356\n",
      "Epoch 4335/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 30.3343 - val_loss: 89.9025\n",
      "Epoch 4336/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 33.5321 - val_loss: 91.0358\n",
      "Epoch 4337/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 32.5265 - val_loss: 83.4480\n",
      "Epoch 4338/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 30.1789 - val_loss: 101.7697\n",
      "Epoch 4339/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 31.4997 - val_loss: 104.8729\n",
      "Epoch 4340/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 31.8785 - val_loss: 91.6405\n",
      "Epoch 4341/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 33.2886 - val_loss: 94.2375\n",
      "Epoch 4342/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 31.4001 - val_loss: 91.7579\n",
      "Epoch 4343/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 31.5577 - val_loss: 94.7927\n",
      "Epoch 4344/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 38.7521 - val_loss: 94.1297\n",
      "Epoch 4345/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 36.5363 - val_loss: 95.2391\n",
      "Epoch 4346/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 38.3791 - val_loss: 92.4165\n",
      "Epoch 4347/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 37.8835 - val_loss: 92.1357\n",
      "Epoch 4348/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 33.0296 - val_loss: 105.4137\n",
      "Epoch 4349/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 34.0705 - val_loss: 87.7913\n",
      "Epoch 4350/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 35.3200 - val_loss: 90.3643\n",
      "Epoch 4351/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 31.7923 - val_loss: 98.2739\n",
      "Epoch 4352/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 34.1254 - val_loss: 87.6501\n",
      "Epoch 4353/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 40.0366 - val_loss: 95.2721\n",
      "Epoch 4354/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 35.5319 - val_loss: 101.8608\n",
      "Epoch 4355/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 41.4272 - val_loss: 97.2701\n",
      "Epoch 4356/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 36.1196 - val_loss: 103.7053\n",
      "Epoch 4357/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 34.1234 - val_loss: 100.2075\n",
      "Epoch 4358/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 135.7376 - val_loss: 91.8405\n",
      "Epoch 4359/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 35.9557 - val_loss: 91.9149\n",
      "Epoch 4360/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 37.8590 - val_loss: 105.4264\n",
      "Epoch 4361/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 38.0650 - val_loss: 89.6985\n",
      "Epoch 4362/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 31.1929 - val_loss: 80.7647\n",
      "Epoch 4363/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 31.4560 - val_loss: 87.2484\n",
      "Epoch 4364/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 32.3720 - val_loss: 91.6733\n",
      "Epoch 4365/10000\n",
      "96/96 [==============================] - 0s 354us/step - loss: 36.4483 - val_loss: 93.8427\n",
      "Epoch 4366/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 30.4092 - val_loss: 105.8951\n",
      "Epoch 4367/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 30.8837 - val_loss: 90.5056\n",
      "Epoch 4368/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 31.6774 - val_loss: 99.3933\n",
      "Epoch 4369/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 34.4125 - val_loss: 88.6543\n",
      "Epoch 4370/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 32.0643 - val_loss: 84.4948\n",
      "Epoch 4371/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 29.5251 - val_loss: 94.1707\n",
      "Epoch 4372/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 31.6604 - val_loss: 86.7418\n",
      "Epoch 4373/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 29.6044 - val_loss: 86.4641\n",
      "Epoch 4374/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 34.4583 - val_loss: 98.4956\n",
      "Epoch 4375/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 33.7436 - val_loss: 81.8723\n",
      "Epoch 4376/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 27.6188 - val_loss: 99.6472\n",
      "Epoch 4377/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 32.5732 - val_loss: 87.5469\n",
      "Epoch 4378/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 34.9419 - val_loss: 96.8690\n",
      "Epoch 4379/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 30.9548 - val_loss: 71.8959\n",
      "Epoch 4380/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 32.4171 - val_loss: 83.6143\n",
      "Epoch 4381/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 27.2330 - val_loss: 94.2503\n",
      "Epoch 4382/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 27.8310 - val_loss: 94.6961\n",
      "Epoch 4383/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 32.0987 - val_loss: 97.4317\n",
      "Epoch 4384/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 33.1034 - val_loss: 91.0149\n",
      "Epoch 4385/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 29.1732 - val_loss: 85.3132\n",
      "Epoch 4386/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 29.4276 - val_loss: 89.4764\n",
      "Epoch 4387/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 33.9539 - val_loss: 80.8708\n",
      "Epoch 4388/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 32.8300 - val_loss: 97.8373\n",
      "Epoch 4389/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 31.6875 - val_loss: 87.1991\n",
      "Epoch 4390/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 32.1269 - val_loss: 93.0755\n",
      "Epoch 4391/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 32.2250 - val_loss: 87.1898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4392/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 32.8923 - val_loss: 95.0062\n",
      "Epoch 4393/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 33.1371 - val_loss: 99.6386\n",
      "Epoch 4394/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 32.6606 - val_loss: 83.6236\n",
      "Epoch 4395/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 31.9334 - val_loss: 79.3864\n",
      "Epoch 4396/10000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 30.2550 - val_loss: 85.8354\n",
      "Epoch 4397/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 31.6278 - val_loss: 94.8117\n",
      "Epoch 4398/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 27.7271 - val_loss: 96.9546\n",
      "Epoch 4399/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 29.7743 - val_loss: 85.2901\n",
      "Epoch 4400/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 28.7812 - val_loss: 83.7980\n",
      "Epoch 4401/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 30.5476 - val_loss: 95.5442\n",
      "Epoch 4402/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 27.7574 - val_loss: 93.8973\n",
      "Epoch 4403/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 31.6216 - val_loss: 107.8372\n",
      "Epoch 4404/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 30.3362 - val_loss: 93.9259\n",
      "Epoch 4405/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 35.6079 - val_loss: 94.6696\n",
      "Epoch 4406/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 34.5560 - val_loss: 83.4683\n",
      "Epoch 4407/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 28.3978 - val_loss: 104.1084\n",
      "Epoch 4408/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 29.2232 - val_loss: 98.8809\n",
      "Epoch 4409/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 35.6566 - val_loss: 87.4105\n",
      "Epoch 4410/10000\n",
      "96/96 [==============================] - 0s 504us/step - loss: 30.7079 - val_loss: 103.8307\n",
      "Epoch 4411/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 29.9098 - val_loss: 97.3725\n",
      "Epoch 4412/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 30.4184 - val_loss: 95.0223\n",
      "Epoch 4413/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 30.2908 - val_loss: 101.0716\n",
      "Epoch 4414/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 30.2670 - val_loss: 79.7283\n",
      "Epoch 4415/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 29.5062 - val_loss: 81.7060\n",
      "Epoch 4416/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 34.2458 - val_loss: 91.7124\n",
      "Epoch 4417/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 27.5981 - val_loss: 90.4327\n",
      "Epoch 4418/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 27.0634 - val_loss: 91.8853\n",
      "Epoch 4419/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 27.0687 - val_loss: 90.2368\n",
      "Epoch 4420/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 30.3344 - val_loss: 92.4213\n",
      "Epoch 4421/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 27.9132 - val_loss: 83.0172\n",
      "Epoch 4422/10000\n",
      "96/96 [==============================] - 0s 348us/step - loss: 30.4018 - val_loss: 107.4739\n",
      "Epoch 4423/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 35.0337 - val_loss: 84.0393\n",
      "Epoch 4424/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 30.3877 - val_loss: 94.5785\n",
      "Epoch 4425/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 34.3854 - val_loss: 90.1410\n",
      "Epoch 4426/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 38.9160 - val_loss: 87.2405\n",
      "Epoch 4427/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 37.0578 - val_loss: 89.0046\n",
      "Epoch 4428/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 40.6531 - val_loss: 89.1577\n",
      "Epoch 4429/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 34.1448 - val_loss: 92.2311\n",
      "Epoch 4430/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 32.8783 - val_loss: 89.3652\n",
      "Epoch 4431/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 38.3975 - val_loss: 79.7881\n",
      "Epoch 4432/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 33.0178 - val_loss: 87.2366\n",
      "Epoch 4433/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 31.7595 - val_loss: 83.1504\n",
      "Epoch 4434/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 36.4781 - val_loss: 99.0304\n",
      "Epoch 4435/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 40.0823 - val_loss: 89.1438\n",
      "Epoch 4436/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 35.1997 - val_loss: 85.5746\n",
      "Epoch 4437/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 33.9603 - val_loss: 88.9795\n",
      "Epoch 4438/10000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 34.1120 - val_loss: 89.7185\n",
      "Epoch 4439/10000\n",
      "96/96 [==============================] - 0s 490us/step - loss: 35.4829 - val_loss: 85.4856\n",
      "Epoch 4440/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 29.6483 - val_loss: 86.1143\n",
      "Epoch 4441/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 27.7597 - val_loss: 93.6365\n",
      "Epoch 4442/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 30.3185 - val_loss: 86.0448\n",
      "Epoch 4443/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 31.3684 - val_loss: 98.3857\n",
      "Epoch 4444/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 31.5914 - val_loss: 101.2389\n",
      "Epoch 4445/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 29.8519 - val_loss: 93.6264\n",
      "Epoch 4446/10000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 27.7181 - val_loss: 86.5251\n",
      "Epoch 4447/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 29.4210 - val_loss: 90.4701\n",
      "Epoch 4448/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 33.8368 - val_loss: 99.5630\n",
      "Epoch 4449/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 31.0784 - val_loss: 86.3541\n",
      "Epoch 4450/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 31.1480 - val_loss: 91.5021\n",
      "Epoch 4451/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 30.8819 - val_loss: 95.7323\n",
      "Epoch 4452/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 28.6986 - val_loss: 96.8023\n",
      "Epoch 4453/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 28.9806 - val_loss: 106.0540\n",
      "Epoch 4454/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 29.7060 - val_loss: 95.6799\n",
      "Epoch 4455/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 29.3384 - val_loss: 82.5433\n",
      "Epoch 4456/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 34.7269 - val_loss: 86.4598\n",
      "Epoch 4457/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 28.8227 - val_loss: 93.7631\n",
      "Epoch 4458/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 33.6725 - val_loss: 101.7954\n",
      "Epoch 4459/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 30.7316 - val_loss: 83.1737\n",
      "Epoch 4460/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 31.2239 - val_loss: 88.1515\n",
      "Epoch 4461/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 32.3444 - val_loss: 98.6259\n",
      "Epoch 4462/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 33.8384 - val_loss: 99.2143\n",
      "Epoch 4463/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 32.0622 - val_loss: 91.2576\n",
      "Epoch 4464/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 28.9899 - val_loss: 78.1661\n",
      "Epoch 4465/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 27.9497 - val_loss: 89.6571\n",
      "Epoch 4466/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 28.7708 - val_loss: 91.2791\n",
      "Epoch 4467/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 26.6922 - val_loss: 93.5822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4468/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 31.2001 - val_loss: 94.5279\n",
      "Epoch 4469/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 27.5720 - val_loss: 95.5207\n",
      "Epoch 4470/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 27.8547 - val_loss: 98.3713\n",
      "Epoch 4471/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 30.5777 - val_loss: 98.4126\n",
      "Epoch 4472/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 31.9768 - val_loss: 94.9339\n",
      "Epoch 4473/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 31.6158 - val_loss: 88.8377\n",
      "Epoch 4474/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 28.8062 - val_loss: 84.6159\n",
      "Epoch 4475/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 30.2688 - val_loss: 86.9416\n",
      "Epoch 4476/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 31.6163 - val_loss: 84.5095\n",
      "Epoch 4477/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 77.0433 - val_loss: 96.8596\n",
      "Epoch 4478/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 41.0347 - val_loss: 94.1516\n",
      "Epoch 4479/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 54.1893 - val_loss: 95.5039\n",
      "Epoch 4480/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 112.2942 - val_loss: 83.6093\n",
      "Epoch 4481/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 56.4399 - val_loss: 84.7427\n",
      "Epoch 4482/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 54.5063 - val_loss: 83.0053\n",
      "Epoch 4483/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 57.5764 - val_loss: 95.6810\n",
      "Epoch 4484/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 53.6908 - val_loss: 80.6499\n",
      "Epoch 4485/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 58.7293 - val_loss: 84.6564\n",
      "Epoch 4486/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 49.1687 - val_loss: 88.8591\n",
      "Epoch 4487/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 51.9879 - val_loss: 85.3375\n",
      "Epoch 4488/10000\n",
      "96/96 [==============================] - 0s 484us/step - loss: 51.5686 - val_loss: 77.6668\n",
      "Epoch 4489/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 51.8074 - val_loss: 78.4040\n",
      "Epoch 4490/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 48.6480 - val_loss: 84.9859\n",
      "Epoch 4491/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 53.9684 - val_loss: 91.3131\n",
      "Epoch 4492/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 52.3953 - val_loss: 87.3641\n",
      "Epoch 4493/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 44.9317 - val_loss: 101.3098\n",
      "Epoch 4494/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 41.3318 - val_loss: 73.6982\n",
      "Epoch 4495/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 45.2860 - val_loss: 76.1943\n",
      "Epoch 4496/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 42.1069 - val_loss: 83.3100\n",
      "Epoch 4497/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 47.2086 - val_loss: 94.1833\n",
      "Epoch 4498/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 45.3465 - val_loss: 97.2726\n",
      "Epoch 4499/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 41.7079 - val_loss: 106.9069\n",
      "Epoch 4500/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 45.4773 - val_loss: 89.0635\n",
      "Epoch 4501/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 38.8210 - val_loss: 92.5232\n",
      "Epoch 4502/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 39.9016 - val_loss: 85.9509\n",
      "Epoch 4503/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 41.7582 - val_loss: 92.5765\n",
      "Epoch 4504/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 38.9315 - val_loss: 85.7277\n",
      "Epoch 4505/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 38.3178 - val_loss: 110.1047\n",
      "Epoch 4506/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 40.6900 - val_loss: 98.2280\n",
      "Epoch 4507/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 36.5173 - val_loss: 79.1168\n",
      "Epoch 4508/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 40.7860 - val_loss: 101.3639\n",
      "Epoch 4509/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 37.0909 - val_loss: 98.2215\n",
      "Epoch 4510/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 36.9276 - val_loss: 97.1564\n",
      "Epoch 4511/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 34.3019 - val_loss: 103.4857\n",
      "Epoch 4512/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 42.4851 - val_loss: 103.1758\n",
      "Epoch 4513/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 36.8591 - val_loss: 92.1613\n",
      "Epoch 4514/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 38.7559 - val_loss: 95.7139\n",
      "Epoch 4515/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 33.4878 - val_loss: 104.8707\n",
      "Epoch 4516/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 39.3820 - val_loss: 99.5228\n",
      "Epoch 4517/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 34.9476 - val_loss: 104.3142\n",
      "Epoch 4518/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 32.2160 - val_loss: 84.6917\n",
      "Epoch 4519/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 50.6659 - val_loss: 89.7591\n",
      "Epoch 4520/10000\n",
      "96/96 [==============================] - 0s 481us/step - loss: 45.6054 - val_loss: 99.4372\n",
      "Epoch 4521/10000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 48.6939 - val_loss: 91.7867\n",
      "Epoch 4522/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 39.5141 - val_loss: 98.8670\n",
      "Epoch 4523/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 41.7892 - val_loss: 98.6276\n",
      "Epoch 4524/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 40.1424 - val_loss: 86.9500\n",
      "Epoch 4525/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 41.6348 - val_loss: 91.1104\n",
      "Epoch 4526/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 37.1914 - val_loss: 85.4691\n",
      "Epoch 4527/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 37.7062 - val_loss: 93.8535\n",
      "Epoch 4528/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 46.5973 - val_loss: 108.9585\n",
      "Epoch 4529/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 42.0433 - val_loss: 98.2748\n",
      "Epoch 4530/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 40.2554 - val_loss: 97.8725\n",
      "Epoch 4531/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 37.0761 - val_loss: 96.7462\n",
      "Epoch 4532/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 35.7099 - val_loss: 97.6595\n",
      "Epoch 4533/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 36.1876 - val_loss: 100.3613\n",
      "Epoch 4534/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 38.6192 - val_loss: 104.8319\n",
      "Epoch 4535/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 36.5076 - val_loss: 81.5696\n",
      "Epoch 4536/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 35.4174 - val_loss: 102.7657\n",
      "Epoch 4537/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 39.0282 - val_loss: 94.4047\n",
      "Epoch 4538/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 33.2551 - val_loss: 100.7988\n",
      "Epoch 4539/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 34.3628 - val_loss: 96.1149\n",
      "Epoch 4540/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 38.3609 - val_loss: 90.9579\n",
      "Epoch 4541/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 31.6311 - val_loss: 98.8902\n",
      "Epoch 4542/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 32.9327 - val_loss: 89.9123\n",
      "Epoch 4543/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 32.0565 - val_loss: 95.6523\n",
      "Epoch 4544/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 30.4460 - val_loss: 90.8372\n",
      "Epoch 4545/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 36.4453 - val_loss: 95.1445\n",
      "Epoch 4546/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 29.6795 - val_loss: 85.0668\n",
      "Epoch 4547/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 34.2012 - val_loss: 100.5735\n",
      "Epoch 4548/10000\n",
      "96/96 [==============================] - 0s 492us/step - loss: 30.2386 - val_loss: 80.5791\n",
      "Epoch 4549/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 32.1879 - val_loss: 104.6267\n",
      "Epoch 4550/10000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 31.0309 - val_loss: 82.1978\n",
      "Epoch 4551/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 33.7473 - val_loss: 88.0502\n",
      "Epoch 4552/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 36.3167 - val_loss: 92.9728\n",
      "Epoch 4553/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 35.3540 - val_loss: 108.0461\n",
      "Epoch 4554/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 36.3000 - val_loss: 90.0265\n",
      "Epoch 4555/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 35.4168 - val_loss: 83.5231\n",
      "Epoch 4556/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 32.9274 - val_loss: 84.6001\n",
      "Epoch 4557/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 31.7802 - val_loss: 95.7883\n",
      "Epoch 4558/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 31.9601 - val_loss: 94.2337\n",
      "Epoch 4559/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 31.3388 - val_loss: 93.2004\n",
      "Epoch 4560/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 35.7343 - val_loss: 84.8631\n",
      "Epoch 4561/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 34.0729 - val_loss: 86.0484\n",
      "Epoch 4562/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 38.1615 - val_loss: 90.4083\n",
      "Epoch 4563/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 31.0813 - val_loss: 87.6559\n",
      "Epoch 4564/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 28.9083 - val_loss: 75.6656\n",
      "Epoch 4565/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 29.6061 - val_loss: 75.5037\n",
      "Epoch 4566/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 32.3194 - val_loss: 98.7680\n",
      "Epoch 4567/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 28.4082 - val_loss: 96.7012\n",
      "Epoch 4568/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 30.8504 - val_loss: 70.4846\n",
      "Epoch 4569/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 33.2697 - val_loss: 91.8295\n",
      "Epoch 4570/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 29.4895 - val_loss: 86.2285\n",
      "Epoch 4571/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 29.0834 - val_loss: 98.2450\n",
      "Epoch 4572/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 29.9640 - val_loss: 97.2915\n",
      "Epoch 4573/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 30.1221 - val_loss: 94.9321\n",
      "Epoch 4574/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 30.2667 - val_loss: 85.8177\n",
      "Epoch 4575/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 30.5292 - val_loss: 102.7971\n",
      "Epoch 4576/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 30.9785 - val_loss: 92.5608\n",
      "Epoch 4577/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 28.1628 - val_loss: 87.0583\n",
      "Epoch 4578/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 28.6769 - val_loss: 91.6653\n",
      "Epoch 4579/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 30.8309 - val_loss: 93.1371\n",
      "Epoch 4580/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 27.6202 - val_loss: 84.2723\n",
      "Epoch 4581/10000\n",
      "96/96 [==============================] - 0s 357us/step - loss: 25.5564 - val_loss: 86.7937\n",
      "Epoch 4582/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 29.7245 - val_loss: 86.4383\n",
      "Epoch 4583/10000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 28.4799 - val_loss: 89.3799\n",
      "Epoch 4584/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 30.3746 - val_loss: 92.9118\n",
      "Epoch 4585/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 28.9884 - val_loss: 97.8145\n",
      "Epoch 4586/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 29.5574 - val_loss: 88.0528\n",
      "Epoch 4587/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 27.2389 - val_loss: 100.5397\n",
      "Epoch 4588/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 27.1089 - val_loss: 83.1595\n",
      "Epoch 4589/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 30.8790 - val_loss: 83.8690\n",
      "Epoch 4590/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 29.2957 - val_loss: 94.6448\n",
      "Epoch 4591/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 26.4828 - val_loss: 91.0446\n",
      "Epoch 4592/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 28.5881 - val_loss: 101.6272\n",
      "Epoch 4593/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 28.4083 - val_loss: 91.9388\n",
      "Epoch 4594/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 30.2403 - val_loss: 101.4592\n",
      "Epoch 4595/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 26.8607 - val_loss: 86.5133\n",
      "Epoch 4596/10000\n",
      "96/96 [==============================] - 0s 350us/step - loss: 27.4221 - val_loss: 103.0324\n",
      "Epoch 4597/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 25.9211 - val_loss: 90.3845\n",
      "Epoch 4598/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 29.6986 - val_loss: 91.6983\n",
      "Epoch 4599/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 29.5511 - val_loss: 90.1879\n",
      "Epoch 4600/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 27.3193 - val_loss: 84.6772\n",
      "Epoch 4601/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 27.4101 - val_loss: 100.9513\n",
      "Epoch 4602/10000\n",
      "96/96 [==============================] - 0s 502us/step - loss: 32.0851 - val_loss: 98.3863\n",
      "Epoch 4603/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 29.9302 - val_loss: 101.9957\n",
      "Epoch 4604/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 28.7901 - val_loss: 82.2306\n",
      "Epoch 4605/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 30.3404 - val_loss: 101.6130\n",
      "Epoch 4606/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 28.5619 - val_loss: 102.2020\n",
      "Epoch 4607/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 28.6864 - val_loss: 102.4335\n",
      "Epoch 4608/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 29.1665 - val_loss: 96.3202\n",
      "Epoch 4609/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 29.6684 - val_loss: 95.5091\n",
      "Epoch 4610/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 29.6509 - val_loss: 80.9830\n",
      "Epoch 4611/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 29.0109 - val_loss: 100.3562\n",
      "Epoch 4612/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 26.6732 - val_loss: 79.1364\n",
      "Epoch 4613/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 28.3740 - val_loss: 88.1208\n",
      "Epoch 4614/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 29.2483 - val_loss: 89.5564\n",
      "Epoch 4615/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 28.9018 - val_loss: 93.9228\n",
      "Epoch 4616/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 28.9076 - val_loss: 79.4033\n",
      "Epoch 4617/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 29.6675 - val_loss: 88.0065\n",
      "Epoch 4618/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 30.5736 - val_loss: 76.5723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4619/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 26.5031 - val_loss: 85.6979\n",
      "Epoch 4620/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 29.1852 - val_loss: 81.1500\n",
      "Epoch 4621/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 36.2875 - val_loss: 87.0278\n",
      "Epoch 4622/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 30.4034 - val_loss: 102.4690\n",
      "Epoch 4623/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 35.9478 - val_loss: 91.1361\n",
      "Epoch 4624/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 34.5738 - val_loss: 95.2987\n",
      "Epoch 4625/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 31.9534 - val_loss: 88.9029\n",
      "Epoch 4626/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 31.9790 - val_loss: 94.9585\n",
      "Epoch 4627/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 29.9318 - val_loss: 93.1168\n",
      "Epoch 4628/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 34.2935 - val_loss: 95.5870\n",
      "Epoch 4629/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 34.3486 - val_loss: 99.1307\n",
      "Epoch 4630/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 32.0462 - val_loss: 100.9859\n",
      "Epoch 4631/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 33.0822 - val_loss: 92.3642\n",
      "Epoch 4632/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 27.3111 - val_loss: 111.9425\n",
      "Epoch 4633/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 27.6942 - val_loss: 100.2773\n",
      "Epoch 4634/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 28.3212 - val_loss: 90.5981\n",
      "Epoch 4635/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 30.9810 - val_loss: 94.4942\n",
      "Epoch 4636/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 29.5268 - val_loss: 94.3704\n",
      "Epoch 4637/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 26.8160 - val_loss: 96.8204\n",
      "Epoch 4638/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 30.2464 - val_loss: 89.6716\n",
      "Epoch 4639/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 29.8598 - val_loss: 85.2554\n",
      "Epoch 4640/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 27.1625 - val_loss: 91.8627\n",
      "Epoch 4641/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 28.7438 - val_loss: 95.1012\n",
      "Epoch 4642/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 27.2565 - val_loss: 102.6301\n",
      "Epoch 4643/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 30.4138 - val_loss: 106.3483\n",
      "Epoch 4644/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 29.2571 - val_loss: 94.0768\n",
      "Epoch 4645/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 28.0966 - val_loss: 86.8918\n",
      "Epoch 4646/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 30.0115 - val_loss: 89.0654\n",
      "Epoch 4647/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 31.6770 - val_loss: 87.9837\n",
      "Epoch 4648/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 30.7088 - val_loss: 91.5985\n",
      "Epoch 4649/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 31.9958 - val_loss: 96.7352\n",
      "Epoch 4650/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 27.1915 - val_loss: 87.7933\n",
      "Epoch 4651/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 30.4076 - val_loss: 91.6949\n",
      "Epoch 4652/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 32.8286 - val_loss: 83.9906\n",
      "Epoch 4653/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 24.7199 - val_loss: 88.7855\n",
      "Epoch 4654/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 28.0966 - val_loss: 90.8118\n",
      "Epoch 4655/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 26.7454 - val_loss: 100.4751\n",
      "Epoch 4656/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 28.5298 - val_loss: 81.8660\n",
      "Epoch 4657/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 28.7969 - val_loss: 85.6870\n",
      "Epoch 4658/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 25.8745 - val_loss: 91.0364\n",
      "Epoch 4659/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 26.4485 - val_loss: 96.3607\n",
      "Epoch 4660/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 30.4530 - val_loss: 77.9294\n",
      "Epoch 4661/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 29.6632 - val_loss: 81.0020\n",
      "Epoch 4662/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 28.1467 - val_loss: 81.2604\n",
      "Epoch 4663/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 29.8073 - val_loss: 78.8203\n",
      "Epoch 4664/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 28.1613 - val_loss: 91.2480\n",
      "Epoch 4665/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 28.8746 - val_loss: 89.0766\n",
      "Epoch 4666/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 30.5372 - val_loss: 85.4774\n",
      "Epoch 4667/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 27.8878 - val_loss: 84.6594\n",
      "Epoch 4668/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 28.6468 - val_loss: 82.6685\n",
      "Epoch 4669/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 33.2879 - val_loss: 78.2731\n",
      "Epoch 4670/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 27.6331 - val_loss: 80.3138\n",
      "Epoch 4671/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 30.3456 - val_loss: 81.9766\n",
      "Epoch 4672/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 28.2481 - val_loss: 89.2606\n",
      "Epoch 4673/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 26.6620 - val_loss: 79.2165\n",
      "Epoch 4674/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 28.1206 - val_loss: 96.9656\n",
      "Epoch 4675/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 30.1383 - val_loss: 82.4510\n",
      "Epoch 4676/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 28.0997 - val_loss: 93.3341\n",
      "Epoch 4677/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 28.3172 - val_loss: 96.9296\n",
      "Epoch 4678/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 27.8067 - val_loss: 86.0418\n",
      "Epoch 4679/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 28.6691 - val_loss: 91.7918\n",
      "Epoch 4680/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 28.6433 - val_loss: 91.8988\n",
      "Epoch 4681/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 27.5443 - val_loss: 71.6854\n",
      "Epoch 4682/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 27.5311 - val_loss: 81.2479\n",
      "Epoch 4683/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 27.8463 - val_loss: 77.9820\n",
      "Epoch 4684/10000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 26.0595 - val_loss: 83.0588\n",
      "Epoch 4685/10000\n",
      "96/96 [==============================] - 0s 508us/step - loss: 30.5208 - val_loss: 85.3323\n",
      "Epoch 4686/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 28.9228 - val_loss: 84.6320\n",
      "Epoch 4687/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 32.3285 - val_loss: 80.6292\n",
      "Epoch 4688/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 27.4854 - val_loss: 85.7858\n",
      "Epoch 4689/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 28.7990 - val_loss: 91.0811\n",
      "Epoch 4690/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 30.4253 - val_loss: 85.7769\n",
      "Epoch 4691/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 25.1367 - val_loss: 83.0264\n",
      "Epoch 4692/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 33.5520 - val_loss: 84.8843\n",
      "Epoch 4693/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 27.8966 - val_loss: 83.7209\n",
      "Epoch 4694/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 26.0378 - val_loss: 96.0688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4695/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 26.5120 - val_loss: 83.4286\n",
      "Epoch 4696/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 25.7281 - val_loss: 85.4192\n",
      "Epoch 4697/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 26.2035 - val_loss: 91.8338\n",
      "Epoch 4698/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 23.8987 - val_loss: 88.9243\n",
      "Epoch 4699/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 39.6992 - val_loss: 89.8570\n",
      "Epoch 4700/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 27.0985 - val_loss: 101.8813\n",
      "Epoch 4701/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 32.0385 - val_loss: 99.0116\n",
      "Epoch 4702/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 34.4650 - val_loss: 94.0683\n",
      "Epoch 4703/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 40.8399 - val_loss: 91.6309\n",
      "Epoch 4704/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 33.9239 - val_loss: 118.9068\n",
      "Epoch 4705/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 35.4691 - val_loss: 100.8806\n",
      "Epoch 4706/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 37.2734 - val_loss: 99.9170\n",
      "Epoch 4707/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 90.9486 - val_loss: 101.1254\n",
      "Epoch 4708/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 40.2148 - val_loss: 105.4171\n",
      "Epoch 4709/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 45.4078 - val_loss: 89.4390\n",
      "Epoch 4710/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 43.1880 - val_loss: 92.2488\n",
      "Epoch 4711/10000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 42.9790 - val_loss: 83.3508\n",
      "Epoch 4712/10000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 43.4945 - val_loss: 88.3093\n",
      "Epoch 4713/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 36.4597 - val_loss: 93.6282\n",
      "Epoch 4714/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 37.8792 - val_loss: 89.0000\n",
      "Epoch 4715/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 35.1383 - val_loss: 87.8989\n",
      "Epoch 4716/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 35.9659 - val_loss: 94.6810\n",
      "Epoch 4717/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 39.1242 - val_loss: 80.6811\n",
      "Epoch 4718/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 38.0883 - val_loss: 83.8758\n",
      "Epoch 4719/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 37.4075 - val_loss: 94.4459\n",
      "Epoch 4720/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 40.5307 - val_loss: 96.2484\n",
      "Epoch 4721/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 35.7587 - val_loss: 93.1949\n",
      "Epoch 4722/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 35.8824 - val_loss: 81.7247\n",
      "Epoch 4723/10000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 33.5031 - val_loss: 93.3678\n",
      "Epoch 4724/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 34.1556 - val_loss: 93.5312\n",
      "Epoch 4725/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 36.0212 - val_loss: 87.8893\n",
      "Epoch 4726/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 38.6736 - val_loss: 73.5827\n",
      "Epoch 4727/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 41.3405 - val_loss: 92.5875\n",
      "Epoch 4728/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 37.6405 - val_loss: 83.5027\n",
      "Epoch 4729/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 38.2801 - val_loss: 75.9419\n",
      "Epoch 4730/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 42.1205 - val_loss: 95.2702\n",
      "Epoch 4731/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 43.2263 - val_loss: 97.7565\n",
      "Epoch 4732/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 37.4115 - val_loss: 105.7587\n",
      "Epoch 4733/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 39.4676 - val_loss: 87.9106\n",
      "Epoch 4734/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 41.2210 - val_loss: 100.5189\n",
      "Epoch 4735/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 33.4435 - val_loss: 79.1728\n",
      "Epoch 4736/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 33.6583 - val_loss: 95.6209\n",
      "Epoch 4737/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 38.0290 - val_loss: 101.5620\n",
      "Epoch 4738/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 31.5356 - val_loss: 89.4769\n",
      "Epoch 4739/10000\n",
      "96/96 [==============================] - 0s 490us/step - loss: 33.5402 - val_loss: 86.8902\n",
      "Epoch 4740/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 32.5216 - val_loss: 85.6200\n",
      "Epoch 4741/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 29.0594 - val_loss: 83.5967\n",
      "Epoch 4742/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 32.2274 - val_loss: 95.3230\n",
      "Epoch 4743/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 79.4988 - val_loss: 101.3703\n",
      "Epoch 4744/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 35.4396 - val_loss: 112.2418\n",
      "Epoch 4745/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 38.6224 - val_loss: 107.0922\n",
      "Epoch 4746/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 43.4232 - val_loss: 106.5053\n",
      "Epoch 4747/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 40.1907 - val_loss: 89.8248\n",
      "Epoch 4748/10000\n",
      "96/96 [==============================] - 0s 501us/step - loss: 57.7118 - val_loss: 92.7556\n",
      "Epoch 4749/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 48.3031 - val_loss: 92.6955\n",
      "Epoch 4750/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 41.1915 - val_loss: 81.9842\n",
      "Epoch 4751/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 36.6324 - val_loss: 94.0653\n",
      "Epoch 4752/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 38.7428 - val_loss: 96.4609\n",
      "Epoch 4753/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 42.6036 - val_loss: 83.9259\n",
      "Epoch 4754/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 39.5802 - val_loss: 103.7114\n",
      "Epoch 4755/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 39.2892 - val_loss: 101.6002\n",
      "Epoch 4756/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 35.6750 - val_loss: 96.2355\n",
      "Epoch 4757/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 42.4060 - val_loss: 105.8701\n",
      "Epoch 4758/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 37.3077 - val_loss: 94.5875\n",
      "Epoch 4759/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 41.7543 - val_loss: 102.1144\n",
      "Epoch 4760/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 42.6150 - val_loss: 85.7372\n",
      "Epoch 4761/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 38.2619 - val_loss: 97.3416\n",
      "Epoch 4762/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 37.1213 - val_loss: 90.2207\n",
      "Epoch 4763/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 37.6994 - val_loss: 85.9904\n",
      "Epoch 4764/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 36.9700 - val_loss: 103.9539\n",
      "Epoch 4765/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 36.3306 - val_loss: 103.8370\n",
      "Epoch 4766/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 35.1742 - val_loss: 97.8545\n",
      "Epoch 4767/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 34.2108 - val_loss: 104.8484\n",
      "Epoch 4768/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 36.8239 - val_loss: 98.3183\n",
      "Epoch 4769/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 33.6484 - val_loss: 96.6607\n",
      "Epoch 4770/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 34.4615 - val_loss: 90.1062\n",
      "Epoch 4771/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 35.4741 - val_loss: 91.5432\n",
      "Epoch 4772/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 38.4710 - val_loss: 90.9656\n",
      "Epoch 4773/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 34.6236 - val_loss: 100.7321\n",
      "Epoch 4774/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 36.3573 - val_loss: 93.1819\n",
      "Epoch 4775/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 34.9153 - val_loss: 102.5690\n",
      "Epoch 4776/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 35.6721 - val_loss: 94.8317\n",
      "Epoch 4777/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 31.2186 - val_loss: 80.8243\n",
      "Epoch 4778/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 33.8174 - val_loss: 88.5033\n",
      "Epoch 4779/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 32.2081 - val_loss: 89.0260\n",
      "Epoch 4780/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 37.5209 - val_loss: 83.7006\n",
      "Epoch 4781/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 30.5673 - val_loss: 102.2647\n",
      "Epoch 4782/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 37.0579 - val_loss: 95.5240\n",
      "Epoch 4783/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 36.2137 - val_loss: 98.9835\n",
      "Epoch 4784/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 35.2855 - val_loss: 95.3386\n",
      "Epoch 4785/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 31.1803 - val_loss: 101.8811\n",
      "Epoch 4786/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 31.5045 - val_loss: 89.7056\n",
      "Epoch 4787/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 31.9641 - val_loss: 91.1983\n",
      "Epoch 4788/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 32.5319 - val_loss: 89.7729\n",
      "Epoch 4789/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 35.7593 - val_loss: 90.2325\n",
      "Epoch 4790/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 26.7102 - val_loss: 92.3783\n",
      "Epoch 4791/10000\n",
      "96/96 [==============================] - 0s 352us/step - loss: 28.5829 - val_loss: 93.5437\n",
      "Epoch 4792/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 27.3798 - val_loss: 90.7701\n",
      "Epoch 4793/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 31.9135 - val_loss: 88.5985\n",
      "Epoch 4794/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 30.3501 - val_loss: 88.8573\n",
      "Epoch 4795/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 27.9818 - val_loss: 84.3639\n",
      "Epoch 4796/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 28.9677 - val_loss: 94.2314\n",
      "Epoch 4797/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 30.2456 - val_loss: 97.6635\n",
      "Epoch 4798/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 33.4025 - val_loss: 80.9550\n",
      "Epoch 4799/10000\n",
      "96/96 [==============================] - 0s 327us/step - loss: 28.5109 - val_loss: 94.6524\n",
      "Epoch 4800/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 30.8240 - val_loss: 87.3430\n",
      "Epoch 4801/10000\n",
      "96/96 [==============================] - 0s 320us/step - loss: 27.5441 - val_loss: 100.5498\n",
      "Epoch 4802/10000\n",
      "96/96 [==============================] - 0s 339us/step - loss: 26.1091 - val_loss: 95.6387\n",
      "Epoch 4803/10000\n",
      "96/96 [==============================] - 0s 329us/step - loss: 31.6305 - val_loss: 102.1692\n",
      "Epoch 4804/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 30.6519 - val_loss: 95.3194\n",
      "Epoch 4805/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 29.1959 - val_loss: 89.9725\n",
      "Epoch 4806/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 31.0589 - val_loss: 90.0533\n",
      "Epoch 4807/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 30.2640 - val_loss: 87.3386\n",
      "Epoch 4808/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 29.6869 - val_loss: 83.3362\n",
      "Epoch 4809/10000\n",
      "96/96 [==============================] - 0s 325us/step - loss: 31.6523 - val_loss: 83.0902\n",
      "Epoch 4810/10000\n",
      "96/96 [==============================] - 0s 329us/step - loss: 28.1091 - val_loss: 82.2985\n",
      "Epoch 4811/10000\n",
      "96/96 [==============================] - 0s 333us/step - loss: 26.7534 - val_loss: 93.2733\n",
      "Epoch 4812/10000\n",
      "96/96 [==============================] - 0s 324us/step - loss: 23.9452 - val_loss: 81.0785\n",
      "Epoch 4813/10000\n",
      "96/96 [==============================] - 0s 341us/step - loss: 31.0718 - val_loss: 86.2682\n",
      "Epoch 4814/10000\n",
      "96/96 [==============================] - 0s 339us/step - loss: 28.8931 - val_loss: 87.5833\n",
      "Epoch 4815/10000\n",
      "96/96 [==============================] - 0s 339us/step - loss: 28.3587 - val_loss: 91.6040\n",
      "Epoch 4816/10000\n",
      "96/96 [==============================] - 0s 338us/step - loss: 29.1252 - val_loss: 99.8173\n",
      "Epoch 4817/10000\n",
      "96/96 [==============================] - 0s 354us/step - loss: 39.8984 - val_loss: 104.2087\n",
      "Epoch 4818/10000\n",
      "96/96 [==============================] - 0s 322us/step - loss: 28.8742 - val_loss: 99.1045\n",
      "Epoch 4819/10000\n",
      "96/96 [==============================] - 0s 328us/step - loss: 32.4671 - val_loss: 94.1160\n",
      "Epoch 4820/10000\n",
      "96/96 [==============================] - 0s 326us/step - loss: 31.5255 - val_loss: 92.0155\n",
      "Epoch 4821/10000\n",
      "96/96 [==============================] - 0s 323us/step - loss: 35.0900 - val_loss: 90.6870\n",
      "Epoch 4822/10000\n",
      "96/96 [==============================] - 0s 335us/step - loss: 31.3717 - val_loss: 92.0098\n",
      "Epoch 4823/10000\n",
      "96/96 [==============================] - 0s 340us/step - loss: 30.0607 - val_loss: 98.7660\n",
      "Epoch 4824/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 33.1546 - val_loss: 100.1285\n",
      "Epoch 4825/10000\n",
      "96/96 [==============================] - 0s 323us/step - loss: 35.6532 - val_loss: 103.3166\n",
      "Epoch 4826/10000\n",
      "96/96 [==============================] - 0s 322us/step - loss: 30.3441 - val_loss: 75.5724\n",
      "Epoch 4827/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 32.7843 - val_loss: 91.1934\n",
      "Epoch 4828/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 28.9307 - val_loss: 89.3955\n",
      "Epoch 4829/10000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 31.3108 - val_loss: 87.0427\n",
      "Epoch 4830/10000\n",
      "96/96 [==============================] - 0s 352us/step - loss: 30.4251 - val_loss: 99.7977\n",
      "Epoch 4831/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 27.8171 - val_loss: 93.1587\n",
      "Epoch 4832/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 29.2914 - val_loss: 99.2067\n",
      "Epoch 4833/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 26.1357 - val_loss: 95.5185\n",
      "Epoch 4834/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 30.4266 - val_loss: 86.7713\n",
      "Epoch 4835/10000\n",
      "96/96 [==============================] - 0s 495us/step - loss: 28.3175 - val_loss: 84.5728\n",
      "Epoch 4836/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 32.5887 - val_loss: 99.2243\n",
      "Epoch 4837/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 32.0010 - val_loss: 98.0707\n",
      "Epoch 4838/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 25.6999 - val_loss: 96.0549\n",
      "Epoch 4839/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 26.8410 - val_loss: 110.0525\n",
      "Epoch 4840/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 28.9160 - val_loss: 98.0618\n",
      "Epoch 4841/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 28.7710 - val_loss: 102.9774\n",
      "Epoch 4842/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 28.3626 - val_loss: 93.6116\n",
      "Epoch 4843/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 27.9661 - val_loss: 93.3163\n",
      "Epoch 4844/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 30.4142 - val_loss: 96.3390\n",
      "Epoch 4845/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 29.3731 - val_loss: 92.5491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4846/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 25.3392 - val_loss: 93.8649\n",
      "Epoch 4847/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 26.6026 - val_loss: 108.6710\n",
      "Epoch 4848/10000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 28.9857 - val_loss: 94.5327\n",
      "Epoch 4849/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 23.1133 - val_loss: 89.7354\n",
      "Epoch 4850/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 26.4165 - val_loss: 95.6701\n",
      "Epoch 4851/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 28.9574 - val_loss: 92.5408\n",
      "Epoch 4852/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 25.7564 - val_loss: 92.8920\n",
      "Epoch 4853/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 26.9350 - val_loss: 100.0870\n",
      "Epoch 4854/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 28.6133 - val_loss: 89.6710\n",
      "Epoch 4855/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 28.0029 - val_loss: 89.7058\n",
      "Epoch 4856/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 26.6011 - val_loss: 89.2399\n",
      "Epoch 4857/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 27.9936 - val_loss: 94.5899\n",
      "Epoch 4858/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 27.0793 - val_loss: 84.7902\n",
      "Epoch 4859/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 24.4274 - val_loss: 103.8564\n",
      "Epoch 4860/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 29.9970 - val_loss: 96.8288\n",
      "Epoch 4861/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 26.4253 - val_loss: 109.0984\n",
      "Epoch 4862/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 28.4174 - val_loss: 98.7760\n",
      "Epoch 4863/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 28.6284 - val_loss: 98.6082\n",
      "Epoch 4864/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 24.6754 - val_loss: 100.2788\n",
      "Epoch 4865/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 27.3331 - val_loss: 88.8455\n",
      "Epoch 4866/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 25.7511 - val_loss: 100.1997\n",
      "Epoch 4867/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 28.3148 - val_loss: 90.3790\n",
      "Epoch 4868/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 26.7002 - val_loss: 95.0488\n",
      "Epoch 4869/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 26.7875 - val_loss: 97.2004\n",
      "Epoch 4870/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 43.3730 - val_loss: 95.6700\n",
      "Epoch 4871/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 39.0643 - val_loss: 96.7475\n",
      "Epoch 4872/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 52.1423 - val_loss: 83.9486\n",
      "Epoch 4873/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 82.6688 - val_loss: 97.9837\n",
      "Epoch 4874/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 59.9805 - val_loss: 86.6939\n",
      "Epoch 4875/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 112.2043 - val_loss: 67.5152\n",
      "Epoch 4876/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 46.5187 - val_loss: 80.0905\n",
      "Epoch 4877/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 45.5630 - val_loss: 79.7693\n",
      "Epoch 4878/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 47.9929 - val_loss: 83.6625\n",
      "Epoch 4879/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 48.8227 - val_loss: 83.7356\n",
      "Epoch 4880/10000\n",
      "96/96 [==============================] - 0s 355us/step - loss: 50.1976 - val_loss: 80.5774\n",
      "Epoch 4881/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 40.0441 - val_loss: 83.5564\n",
      "Epoch 4882/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 42.9231 - val_loss: 94.1290\n",
      "Epoch 4883/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 37.6929 - val_loss: 84.4007\n",
      "Epoch 4884/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 41.6493 - val_loss: 95.7108\n",
      "Epoch 4885/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 41.0884 - val_loss: 110.5310\n",
      "Epoch 4886/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 43.7914 - val_loss: 95.3770\n",
      "Epoch 4887/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 38.7701 - val_loss: 102.8167\n",
      "Epoch 4888/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 37.1934 - val_loss: 97.9841\n",
      "Epoch 4889/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 32.8882 - val_loss: 89.4595\n",
      "Epoch 4890/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 36.0792 - val_loss: 97.5893\n",
      "Epoch 4891/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 32.9433 - val_loss: 102.9266\n",
      "Epoch 4892/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 33.8238 - val_loss: 96.9885\n",
      "Epoch 4893/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 31.4398 - val_loss: 97.9006\n",
      "Epoch 4894/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 31.3548 - val_loss: 91.9048\n",
      "Epoch 4895/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 30.5687 - val_loss: 105.8099\n",
      "Epoch 4896/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 34.9104 - val_loss: 79.1321\n",
      "Epoch 4897/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 30.4542 - val_loss: 85.8460\n",
      "Epoch 4898/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 30.6094 - val_loss: 94.6491\n",
      "Epoch 4899/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 31.8067 - val_loss: 96.6362\n",
      "Epoch 4900/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 30.9307 - val_loss: 97.2475\n",
      "Epoch 4901/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 31.1473 - val_loss: 99.4690\n",
      "Epoch 4902/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 32.3111 - val_loss: 89.6961\n",
      "Epoch 4903/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 31.7570 - val_loss: 94.7927\n",
      "Epoch 4904/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 28.2166 - val_loss: 74.8415\n",
      "Epoch 4905/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 32.5092 - val_loss: 96.3138\n",
      "Epoch 4906/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 29.6919 - val_loss: 108.3391\n",
      "Epoch 4907/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 30.3190 - val_loss: 88.4301\n",
      "Epoch 4908/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 27.7997 - val_loss: 88.1468\n",
      "Epoch 4909/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 27.1340 - val_loss: 104.3649\n",
      "Epoch 4910/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 32.3489 - val_loss: 109.3315\n",
      "Epoch 4911/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 30.3623 - val_loss: 84.0789\n",
      "Epoch 4912/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 31.6625 - val_loss: 77.5908\n",
      "Epoch 4913/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 30.5441 - val_loss: 95.1613\n",
      "Epoch 4914/10000\n",
      "96/96 [==============================] - 0s 353us/step - loss: 29.1069 - val_loss: 101.7490\n",
      "Epoch 4915/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 28.0251 - val_loss: 96.8335\n",
      "Epoch 4916/10000\n",
      "96/96 [==============================] - 0s 354us/step - loss: 27.3671 - val_loss: 103.9188\n",
      "Epoch 4917/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 27.1113 - val_loss: 113.2471\n",
      "Epoch 4918/10000\n",
      "96/96 [==============================] - 0s 499us/step - loss: 26.9967 - val_loss: 101.1203\n",
      "Epoch 4919/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 30.1446 - val_loss: 104.2641\n",
      "Epoch 4920/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 25.0720 - val_loss: 102.5350\n",
      "Epoch 4921/10000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 31.0433 - val_loss: 107.7388\n",
      "Epoch 4922/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 29.4161 - val_loss: 102.1224\n",
      "Epoch 4923/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 33.7058 - val_loss: 92.0092\n",
      "Epoch 4924/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 31.8322 - val_loss: 88.5769\n",
      "Epoch 4925/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 27.9419 - val_loss: 96.1676\n",
      "Epoch 4926/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 29.9469 - val_loss: 107.0016\n",
      "Epoch 4927/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 31.1601 - val_loss: 101.1203\n",
      "Epoch 4928/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 29.3966 - val_loss: 94.1514\n",
      "Epoch 4929/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 25.4241 - val_loss: 104.3482\n",
      "Epoch 4930/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 28.4995 - val_loss: 87.7777\n",
      "Epoch 4931/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 27.0612 - val_loss: 93.6202\n",
      "Epoch 4932/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 26.1425 - val_loss: 76.0955\n",
      "Epoch 4933/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 23.6605 - val_loss: 75.6623\n",
      "Epoch 4934/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 24.8366 - val_loss: 99.4844\n",
      "Epoch 4935/10000\n",
      "96/96 [==============================] - 0s 504us/step - loss: 27.1694 - val_loss: 101.6026\n",
      "Epoch 4936/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 30.8254 - val_loss: 95.3531\n",
      "Epoch 4937/10000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 23.7903 - val_loss: 105.3126\n",
      "Epoch 4938/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 29.2189 - val_loss: 89.6698\n",
      "Epoch 4939/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 29.9760 - val_loss: 87.9526\n",
      "Epoch 4940/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 28.9861 - val_loss: 98.9632\n",
      "Epoch 4941/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 25.1236 - val_loss: 102.1935\n",
      "Epoch 4942/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 41.9521 - val_loss: 103.6326\n",
      "Epoch 4943/10000\n",
      "96/96 [==============================] - 0s 569us/step - loss: 59.1705 - val_loss: 87.2821\n",
      "Epoch 4944/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 41.9799 - val_loss: 102.6511\n",
      "Epoch 4945/10000\n",
      "96/96 [==============================] - 0s 508us/step - loss: 40.1747 - val_loss: 88.9717\n",
      "Epoch 4946/10000\n",
      "96/96 [==============================] - 0s 646us/step - loss: 32.9823 - val_loss: 100.4358\n",
      "Epoch 4947/10000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 38.5468 - val_loss: 92.2303\n",
      "Epoch 4948/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 37.8744 - val_loss: 82.1319\n",
      "Epoch 4949/10000\n",
      "96/96 [==============================] - 0s 488us/step - loss: 36.8361 - val_loss: 91.5096\n",
      "Epoch 4950/10000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 37.9247 - val_loss: 92.3133\n",
      "Epoch 4951/10000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 32.5171 - val_loss: 102.4122\n",
      "Epoch 4952/10000\n",
      "96/96 [==============================] - 0s 512us/step - loss: 36.7884 - val_loss: 86.0190\n",
      "Epoch 4953/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 37.1893 - val_loss: 106.7598\n",
      "Epoch 4954/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 44.9340 - val_loss: 105.9150\n",
      "Epoch 4955/10000\n",
      "96/96 [==============================] - 0s 497us/step - loss: 40.7441 - val_loss: 90.8253\n",
      "Epoch 4956/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 33.5586 - val_loss: 98.2895\n",
      "Epoch 4957/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 35.0031 - val_loss: 97.3333\n",
      "Epoch 4958/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 31.2362 - val_loss: 76.1139\n",
      "Epoch 4959/10000\n",
      "96/96 [==============================] - 0s 505us/step - loss: 33.4306 - val_loss: 98.8514\n",
      "Epoch 4960/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 30.8074 - val_loss: 92.4452\n",
      "Epoch 4961/10000\n",
      "96/96 [==============================] - 0s 356us/step - loss: 35.9487 - val_loss: 99.7944\n",
      "Epoch 4962/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 33.3310 - val_loss: 86.6782\n",
      "Epoch 4963/10000\n",
      "96/96 [==============================] - 0s 523us/step - loss: 30.9556 - val_loss: 96.9482\n",
      "Epoch 4964/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 32.5351 - val_loss: 89.2493\n",
      "Epoch 4965/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 35.4774 - val_loss: 87.3438\n",
      "Epoch 4966/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 31.2591 - val_loss: 97.2394\n",
      "Epoch 4967/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 31.7069 - val_loss: 93.6002\n",
      "Epoch 4968/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 32.1020 - val_loss: 95.4324\n",
      "Epoch 4969/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 32.3444 - val_loss: 91.8682\n",
      "Epoch 4970/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 29.5401 - val_loss: 83.2906\n",
      "Epoch 4971/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 29.8188 - val_loss: 95.9394\n",
      "Epoch 4972/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 29.2707 - val_loss: 90.5433\n",
      "Epoch 4973/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 37.4831 - val_loss: 88.6288\n",
      "Epoch 4974/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 34.5350 - val_loss: 105.5598\n",
      "Epoch 4975/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 36.4179 - val_loss: 82.1044\n",
      "Epoch 4976/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 32.7691 - val_loss: 92.6115\n",
      "Epoch 4977/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 33.2085 - val_loss: 90.9739\n",
      "Epoch 4978/10000\n",
      "96/96 [==============================] - 0s 501us/step - loss: 34.4530 - val_loss: 76.5601\n",
      "Epoch 4979/10000\n",
      "96/96 [==============================] - 0s 568us/step - loss: 35.3581 - val_loss: 91.9711\n",
      "Epoch 4980/10000\n",
      "96/96 [==============================] - 0s 599us/step - loss: 32.7165 - val_loss: 73.5298\n",
      "Epoch 4981/10000\n",
      "96/96 [==============================] - 0s 562us/step - loss: 34.6751 - val_loss: 103.1820\n",
      "Epoch 4982/10000\n",
      "96/96 [==============================] - 0s 504us/step - loss: 34.5464 - val_loss: 90.5707\n",
      "Epoch 4983/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 33.3918 - val_loss: 108.5209\n",
      "Epoch 4984/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 29.5821 - val_loss: 90.0667\n",
      "Epoch 4985/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 31.2203 - val_loss: 78.2038\n",
      "Epoch 4986/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 28.8181 - val_loss: 106.0839\n",
      "Epoch 4987/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 32.0145 - val_loss: 85.4748\n",
      "Epoch 4988/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 33.5868 - val_loss: 79.8847\n",
      "Epoch 4989/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 30.7811 - val_loss: 101.4543\n",
      "Epoch 4990/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 28.1972 - val_loss: 79.4098\n",
      "Epoch 4991/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 31.9338 - val_loss: 94.2255\n",
      "Epoch 4992/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 31.9898 - val_loss: 74.2096\n",
      "Epoch 4993/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 27.8853 - val_loss: 95.2588\n",
      "Epoch 4994/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 31.6446 - val_loss: 81.7146\n",
      "Epoch 4995/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 27.8060 - val_loss: 80.1384\n",
      "Epoch 4996/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 24.9785 - val_loss: 93.8764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4997/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 25.8167 - val_loss: 79.3314\n",
      "Epoch 4998/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 26.5217 - val_loss: 88.0233\n",
      "Epoch 4999/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 24.6289 - val_loss: 101.0372\n",
      "Epoch 5000/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 26.1951 - val_loss: 89.8168\n",
      "Epoch 5001/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 26.5585 - val_loss: 94.4457\n",
      "Epoch 5002/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 28.3362 - val_loss: 93.7117\n",
      "Epoch 5003/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 27.0329 - val_loss: 98.0288\n",
      "Epoch 5004/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 28.8144 - val_loss: 99.4343\n",
      "Epoch 5005/10000\n",
      "96/96 [==============================] - 0s 356us/step - loss: 24.0859 - val_loss: 100.8459\n",
      "Epoch 5006/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 29.7585 - val_loss: 101.9958\n",
      "Epoch 5007/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 24.4095 - val_loss: 83.2039\n",
      "Epoch 5008/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 30.2052 - val_loss: 91.3783\n",
      "Epoch 5009/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 27.8876 - val_loss: 103.6041\n",
      "Epoch 5010/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 29.6705 - val_loss: 87.7606\n",
      "Epoch 5011/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 25.6175 - val_loss: 90.6528\n",
      "Epoch 5012/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 33.2660 - val_loss: 102.2659\n",
      "Epoch 5013/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 27.6060 - val_loss: 107.6417\n",
      "Epoch 5014/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 30.8711 - val_loss: 84.9897\n",
      "Epoch 5015/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 26.4730 - val_loss: 89.4837\n",
      "Epoch 5016/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 27.3573 - val_loss: 87.8038\n",
      "Epoch 5017/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 26.7603 - val_loss: 94.5361\n",
      "Epoch 5018/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 23.1387 - val_loss: 92.6648\n",
      "Epoch 5019/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 27.9287 - val_loss: 100.9592\n",
      "Epoch 5020/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 24.7702 - val_loss: 102.5524\n",
      "Epoch 5021/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 23.8619 - val_loss: 104.0244\n",
      "Epoch 5022/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 26.2225 - val_loss: 107.5507\n",
      "Epoch 5023/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 26.5462 - val_loss: 83.8737\n",
      "Epoch 5024/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 27.8706 - val_loss: 77.8326\n",
      "Epoch 5025/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 29.2645 - val_loss: 69.6554\n",
      "Epoch 5026/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 26.9359 - val_loss: 108.0066\n",
      "Epoch 5027/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 29.5230 - val_loss: 97.9181\n",
      "Epoch 5028/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 27.4530 - val_loss: 78.4267\n",
      "Epoch 5029/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 29.2387 - val_loss: 98.2382\n",
      "Epoch 5030/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 24.9823 - val_loss: 83.8027\n",
      "Epoch 5031/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 24.6178 - val_loss: 96.2617\n",
      "Epoch 5032/10000\n",
      "96/96 [==============================] - 0s 594us/step - loss: 27.8434 - val_loss: 101.6536\n",
      "Epoch 5033/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 25.1739 - val_loss: 102.6567\n",
      "Epoch 5034/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 27.7819 - val_loss: 88.9101\n",
      "Epoch 5035/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 32.2091 - val_loss: 76.6449\n",
      "Epoch 5036/10000\n",
      "96/96 [==============================] - 0s 607us/step - loss: 33.9561 - val_loss: 91.6973\n",
      "Epoch 5037/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 30.0221 - val_loss: 85.0756\n",
      "Epoch 5038/10000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 26.2575 - val_loss: 73.4223\n",
      "Epoch 5039/10000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 26.4273 - val_loss: 88.8941\n",
      "Epoch 5040/10000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 27.1657 - val_loss: 83.3295\n",
      "Epoch 5041/10000\n",
      "96/96 [==============================] - 0s 486us/step - loss: 60.7866 - val_loss: 87.7211\n",
      "Epoch 5042/10000\n",
      "96/96 [==============================] - 0s 502us/step - loss: 41.6955 - val_loss: 87.9610\n",
      "Epoch 5043/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 40.9047 - val_loss: 75.7477\n",
      "Epoch 5044/10000\n",
      "96/96 [==============================] - 0s 501us/step - loss: 42.3148 - val_loss: 85.4103\n",
      "Epoch 5045/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 37.9629 - val_loss: 72.7612\n",
      "Epoch 5046/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 39.0879 - val_loss: 73.6932\n",
      "Epoch 5047/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 35.0542 - val_loss: 86.9609\n",
      "Epoch 5048/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 40.9076 - val_loss: 82.7020\n",
      "Epoch 5049/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 40.3983 - val_loss: 91.0063\n",
      "Epoch 5050/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 37.6745 - val_loss: 83.6789\n",
      "Epoch 5051/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 37.3423 - val_loss: 92.1850\n",
      "Epoch 5052/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 34.3447 - val_loss: 93.7692\n",
      "Epoch 5053/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 33.7716 - val_loss: 98.7000\n",
      "Epoch 5054/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 29.0892 - val_loss: 76.6939\n",
      "Epoch 5055/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 31.0752 - val_loss: 91.8917\n",
      "Epoch 5056/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 30.1578 - val_loss: 97.9246\n",
      "Epoch 5057/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 30.1668 - val_loss: 90.7399\n",
      "Epoch 5058/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 28.3704 - val_loss: 99.2123\n",
      "Epoch 5059/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 31.3328 - val_loss: 92.7701\n",
      "Epoch 5060/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 29.9781 - val_loss: 84.7121\n",
      "Epoch 5061/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 28.5159 - val_loss: 92.6365\n",
      "Epoch 5062/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 29.3289 - val_loss: 96.1463\n",
      "Epoch 5063/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 28.8358 - val_loss: 97.9828\n",
      "Epoch 5064/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 34.9118 - val_loss: 108.4857\n",
      "Epoch 5065/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 31.3101 - val_loss: 96.1983\n",
      "Epoch 5066/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 29.7683 - val_loss: 99.8280\n",
      "Epoch 5067/10000\n",
      "96/96 [==============================] - 0s 571us/step - loss: 30.3347 - val_loss: 89.4399\n",
      "Epoch 5068/10000\n",
      "96/96 [==============================] - 0s 642us/step - loss: 33.1202 - val_loss: 101.2267\n",
      "Epoch 5069/10000\n",
      "96/96 [==============================] - 0s 592us/step - loss: 30.3713 - val_loss: 93.7250\n",
      "Epoch 5070/10000\n",
      "96/96 [==============================] - 0s 591us/step - loss: 28.4480 - val_loss: 87.4719\n",
      "Epoch 5071/10000\n",
      "96/96 [==============================] - 0s 509us/step - loss: 28.6119 - val_loss: 98.7803\n",
      "Epoch 5072/10000\n",
      "96/96 [==============================] - 0s 513us/step - loss: 26.4961 - val_loss: 98.6059\n",
      "Epoch 5073/10000\n",
      "96/96 [==============================] - 0s 487us/step - loss: 29.4859 - val_loss: 91.8268\n",
      "Epoch 5074/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 29.1800 - val_loss: 110.5565\n",
      "Epoch 5075/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 31.9272 - val_loss: 97.5005\n",
      "Epoch 5076/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 27.5769 - val_loss: 100.5177\n",
      "Epoch 5077/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 28.6171 - val_loss: 92.3257\n",
      "Epoch 5078/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 26.5351 - val_loss: 98.7299\n",
      "Epoch 5079/10000\n",
      "96/96 [==============================] - 0s 497us/step - loss: 28.4323 - val_loss: 108.7652\n",
      "Epoch 5080/10000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 27.3569 - val_loss: 105.1469\n",
      "Epoch 5081/10000\n",
      "96/96 [==============================] - 0s 350us/step - loss: 27.6092 - val_loss: 98.2966\n",
      "Epoch 5082/10000\n",
      "96/96 [==============================] - 0s 351us/step - loss: 23.2601 - val_loss: 107.2943\n",
      "Epoch 5083/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 29.8261 - val_loss: 100.7551\n",
      "Epoch 5084/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 24.9633 - val_loss: 85.7817\n",
      "Epoch 5085/10000\n",
      "96/96 [==============================] - 0s 482us/step - loss: 23.0174 - val_loss: 96.0433\n",
      "Epoch 5086/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 29.7494 - val_loss: 103.4839\n",
      "Epoch 5087/10000\n",
      "96/96 [==============================] - 0s 492us/step - loss: 28.4837 - val_loss: 102.3072\n",
      "Epoch 5088/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 25.8695 - val_loss: 81.3180\n",
      "Epoch 5089/10000\n",
      "96/96 [==============================] - 0s 335us/step - loss: 27.6853 - val_loss: 98.7701\n",
      "Epoch 5090/10000\n",
      "96/96 [==============================] - 0s 338us/step - loss: 27.1209 - val_loss: 100.0787\n",
      "Epoch 5091/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 29.1036 - val_loss: 85.0747\n",
      "Epoch 5092/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 23.0620 - val_loss: 88.3203\n",
      "Epoch 5093/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 24.9317 - val_loss: 96.1196\n",
      "Epoch 5094/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 26.9843 - val_loss: 95.7877\n",
      "Epoch 5095/10000\n",
      "96/96 [==============================] - 0s 487us/step - loss: 26.0866 - val_loss: 98.1235\n",
      "Epoch 5096/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 28.3484 - val_loss: 112.7293\n",
      "Epoch 5097/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 24.0458 - val_loss: 101.4065\n",
      "Epoch 5098/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 23.3694 - val_loss: 85.1493\n",
      "Epoch 5099/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 67.9108 - val_loss: 97.6820\n",
      "Epoch 5100/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 28.6766 - val_loss: 82.5355\n",
      "Epoch 5101/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 35.5381 - val_loss: 96.5284\n",
      "Epoch 5102/10000\n",
      "96/96 [==============================] - 0s 573us/step - loss: 37.5296 - val_loss: 86.5750\n",
      "Epoch 5103/10000\n",
      "96/96 [==============================] - 0s 524us/step - loss: 61.2448 - val_loss: 92.1360\n",
      "Epoch 5104/10000\n",
      "96/96 [==============================] - 0s 499us/step - loss: 40.5770 - val_loss: 89.3325\n",
      "Epoch 5105/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 41.7587 - val_loss: 98.1654\n",
      "Epoch 5106/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 38.0477 - val_loss: 89.1537\n",
      "Epoch 5107/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 42.3928 - val_loss: 76.7737\n",
      "Epoch 5108/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 42.5362 - val_loss: 89.1231\n",
      "Epoch 5109/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 39.4541 - val_loss: 77.6764\n",
      "Epoch 5110/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 37.6290 - val_loss: 95.5471\n",
      "Epoch 5111/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 41.4341 - val_loss: 89.1207\n",
      "Epoch 5112/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 39.0190 - val_loss: 79.5880\n",
      "Epoch 5113/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 38.6704 - val_loss: 74.9762\n",
      "Epoch 5114/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 37.4506 - val_loss: 86.6058\n",
      "Epoch 5115/10000\n",
      "96/96 [==============================] - 0s 500us/step - loss: 33.4335 - val_loss: 85.8878\n",
      "Epoch 5116/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 36.7597 - val_loss: 107.2144\n",
      "Epoch 5117/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 35.9963 - val_loss: 91.8026\n",
      "Epoch 5118/10000\n",
      "96/96 [==============================] - 0s 485us/step - loss: 33.7920 - val_loss: 89.4462\n",
      "Epoch 5119/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 31.4968 - val_loss: 97.6553\n",
      "Epoch 5120/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 31.3427 - val_loss: 93.1305\n",
      "Epoch 5121/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 35.1962 - val_loss: 99.4871\n",
      "Epoch 5122/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 30.2160 - val_loss: 93.1917\n",
      "Epoch 5123/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 28.6029 - val_loss: 94.1218\n",
      "Epoch 5124/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 26.3444 - val_loss: 86.9533\n",
      "Epoch 5125/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 28.0453 - val_loss: 78.4475\n",
      "Epoch 5126/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 27.7672 - val_loss: 97.5683\n",
      "Epoch 5127/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 31.2326 - val_loss: 90.8991\n",
      "Epoch 5128/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 27.3387 - val_loss: 107.7987\n",
      "Epoch 5129/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 28.9748 - val_loss: 72.8022\n",
      "Epoch 5130/10000\n",
      "96/96 [==============================] - 0s 581us/step - loss: 28.0241 - val_loss: 94.8548\n",
      "Epoch 5131/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 29.2465 - val_loss: 110.9291\n",
      "Epoch 5132/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 29.4061 - val_loss: 104.1414\n",
      "Epoch 5133/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 25.6106 - val_loss: 102.8582\n",
      "Epoch 5134/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 28.2509 - val_loss: 97.1778\n",
      "Epoch 5135/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 33.9453 - val_loss: 113.3310\n",
      "Epoch 5136/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 32.7452 - val_loss: 98.2987\n",
      "Epoch 5137/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 32.1829 - val_loss: 103.4999\n",
      "Epoch 5138/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 35.9987 - val_loss: 104.8615\n",
      "Epoch 5139/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 29.3333 - val_loss: 98.3828\n",
      "Epoch 5140/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 37.4188 - val_loss: 94.6195\n",
      "Epoch 5141/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 38.9390 - val_loss: 97.5781\n",
      "Epoch 5142/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 31.4329 - val_loss: 96.9421\n",
      "Epoch 5143/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 32.8127 - val_loss: 87.4053\n",
      "Epoch 5144/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 33.7722 - val_loss: 88.5936\n",
      "Epoch 5145/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 35.6652 - val_loss: 100.2350\n",
      "Epoch 5146/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 32.4353 - val_loss: 104.9079\n",
      "Epoch 5147/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 34.6027 - val_loss: 80.8962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5148/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 36.5697 - val_loss: 86.5023\n",
      "Epoch 5149/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 32.0997 - val_loss: 96.9538\n",
      "Epoch 5150/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 30.0010 - val_loss: 101.6298\n",
      "Epoch 5151/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 27.9984 - val_loss: 102.8599\n",
      "Epoch 5152/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 34.0344 - val_loss: 101.7624\n",
      "Epoch 5153/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 33.9700 - val_loss: 92.4268\n",
      "Epoch 5154/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 29.9300 - val_loss: 90.7067\n",
      "Epoch 5155/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 28.8026 - val_loss: 92.8664\n",
      "Epoch 5156/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 29.6925 - val_loss: 105.0540\n",
      "Epoch 5157/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 27.1966 - val_loss: 112.2333\n",
      "Epoch 5158/10000\n",
      "96/96 [==============================] - 0s 521us/step - loss: 34.4462 - val_loss: 100.8067\n",
      "Epoch 5159/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 30.5499 - val_loss: 103.4453\n",
      "Epoch 5160/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 32.3257 - val_loss: 73.3060\n",
      "Epoch 5161/10000\n",
      "96/96 [==============================] - 0s 505us/step - loss: 27.0584 - val_loss: 84.5696\n",
      "Epoch 5162/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 29.9061 - val_loss: 101.3323\n",
      "Epoch 5163/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 32.3573 - val_loss: 80.2753\n",
      "Epoch 5164/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 40.6187 - val_loss: 111.2253\n",
      "Epoch 5165/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 30.2576 - val_loss: 96.7827\n",
      "Epoch 5166/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 27.7869 - val_loss: 89.4832\n",
      "Epoch 5167/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 31.3254 - val_loss: 85.8092\n",
      "Epoch 5168/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 28.6882 - val_loss: 81.6552\n",
      "Epoch 5169/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 25.8489 - val_loss: 102.4368\n",
      "Epoch 5170/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 29.2346 - val_loss: 87.5620\n",
      "Epoch 5171/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 30.4766 - val_loss: 87.4872\n",
      "Epoch 5172/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 25.3664 - val_loss: 92.8015\n",
      "Epoch 5173/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 31.6831 - val_loss: 97.2256\n",
      "Epoch 5174/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 27.1775 - val_loss: 111.7274\n",
      "Epoch 5175/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 29.0807 - val_loss: 101.1472\n",
      "Epoch 5176/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 28.3316 - val_loss: 91.2028\n",
      "Epoch 5177/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 25.5047 - val_loss: 101.1287\n",
      "Epoch 5178/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 25.6299 - val_loss: 106.7138\n",
      "Epoch 5179/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 24.4415 - val_loss: 105.7003\n",
      "Epoch 5180/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 26.7128 - val_loss: 101.3963\n",
      "Epoch 5181/10000\n",
      "96/96 [==============================] - 0s 485us/step - loss: 21.1226 - val_loss: 83.5556\n",
      "Epoch 5182/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 25.0121 - val_loss: 100.2504\n",
      "Epoch 5183/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 25.8268 - val_loss: 90.0543\n",
      "Epoch 5184/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 25.6014 - val_loss: 98.3442\n",
      "Epoch 5185/10000\n",
      "96/96 [==============================] - 0s 606us/step - loss: 20.5003 - val_loss: 100.8091\n",
      "Epoch 5186/10000\n",
      "96/96 [==============================] - 0s 567us/step - loss: 25.6759 - val_loss: 87.0448\n",
      "Epoch 5187/10000\n",
      "96/96 [==============================] - 0s 497us/step - loss: 21.6173 - val_loss: 84.2121\n",
      "Epoch 5188/10000\n",
      "96/96 [==============================] - 0s 576us/step - loss: 26.3826 - val_loss: 81.9505\n",
      "Epoch 5189/10000\n",
      "96/96 [==============================] - 0s 497us/step - loss: 21.3124 - val_loss: 88.7143\n",
      "Epoch 5190/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 23.5861 - val_loss: 97.7480\n",
      "Epoch 5191/10000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 26.5012 - val_loss: 103.0288\n",
      "Epoch 5192/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 23.5726 - val_loss: 84.4560\n",
      "Epoch 5193/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 25.3683 - val_loss: 86.8175\n",
      "Epoch 5194/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 29.7462 - val_loss: 97.6438\n",
      "Epoch 5195/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 26.3519 - val_loss: 110.2018\n",
      "Epoch 5196/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 23.9215 - val_loss: 105.8843\n",
      "Epoch 5197/10000\n",
      "96/96 [==============================] - 0s 353us/step - loss: 23.5936 - val_loss: 99.5361\n",
      "Epoch 5198/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 23.7984 - val_loss: 96.2784\n",
      "Epoch 5199/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 22.3508 - val_loss: 103.9004\n",
      "Epoch 5200/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 27.0993 - val_loss: 101.6460\n",
      "Epoch 5201/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 23.5426 - val_loss: 97.0633\n",
      "Epoch 5202/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 24.0134 - val_loss: 93.8676\n",
      "Epoch 5203/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 26.7473 - val_loss: 96.5443\n",
      "Epoch 5204/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 25.0839 - val_loss: 80.2218\n",
      "Epoch 5205/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 23.6022 - val_loss: 100.2050\n",
      "Epoch 5206/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 25.6886 - val_loss: 99.1219\n",
      "Epoch 5207/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 24.8537 - val_loss: 90.5872\n",
      "Epoch 5208/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 28.3209 - val_loss: 91.3251\n",
      "Epoch 5209/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 25.7282 - val_loss: 109.7926\n",
      "Epoch 5210/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 25.5803 - val_loss: 92.0055\n",
      "Epoch 5211/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 24.7283 - val_loss: 108.4048\n",
      "Epoch 5212/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 24.4683 - val_loss: 102.7704\n",
      "Epoch 5213/10000\n",
      "96/96 [==============================] - 0s 518us/step - loss: 25.2359 - val_loss: 106.9754\n",
      "Epoch 5214/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 30.1744 - val_loss: 101.4384\n",
      "Epoch 5215/10000\n",
      "96/96 [==============================] - 0s 328us/step - loss: 25.0043 - val_loss: 101.6021\n",
      "Epoch 5216/10000\n",
      "96/96 [==============================] - 0s 487us/step - loss: 23.6984 - val_loss: 81.1357\n",
      "Epoch 5217/10000\n",
      "96/96 [==============================] - 0s 628us/step - loss: 28.2031 - val_loss: 97.7479\n",
      "Epoch 5218/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 22.8019 - val_loss: 107.3512\n",
      "Epoch 5219/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 24.7365 - val_loss: 100.5779\n",
      "Epoch 5220/10000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 29.8698 - val_loss: 106.9902\n",
      "Epoch 5221/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 24.1310 - val_loss: 91.2302\n",
      "Epoch 5222/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 23.0877 - val_loss: 93.0054\n",
      "Epoch 5223/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 22.4531 - val_loss: 102.6280\n",
      "Epoch 5224/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 22.1763 - val_loss: 98.4976\n",
      "Epoch 5225/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 29.0291 - val_loss: 104.9866\n",
      "Epoch 5226/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 20.7143 - val_loss: 98.3021\n",
      "Epoch 5227/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 20.0199 - val_loss: 102.7629\n",
      "Epoch 5228/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 28.0487 - val_loss: 93.8800\n",
      "Epoch 5229/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 22.0299 - val_loss: 92.2489\n",
      "Epoch 5230/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 21.2536 - val_loss: 98.3473\n",
      "Epoch 5231/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 21.3944 - val_loss: 86.9133\n",
      "Epoch 5232/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 21.6889 - val_loss: 105.9252\n",
      "Epoch 5233/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 21.3790 - val_loss: 92.5674\n",
      "Epoch 5234/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 21.2081 - val_loss: 96.7111\n",
      "Epoch 5235/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 22.1285 - val_loss: 79.8844\n",
      "Epoch 5236/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 24.3306 - val_loss: 107.6354\n",
      "Epoch 5237/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 27.2679 - val_loss: 91.1238\n",
      "Epoch 5238/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 18.8532 - val_loss: 91.7088\n",
      "Epoch 5239/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 26.8801 - val_loss: 89.9880\n",
      "Epoch 5240/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 21.1060 - val_loss: 87.2550\n",
      "Epoch 5241/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 21.7279 - val_loss: 94.7009\n",
      "Epoch 5242/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 22.4883 - val_loss: 105.4981\n",
      "Epoch 5243/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 25.9598 - val_loss: 103.4967\n",
      "Epoch 5244/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 21.3566 - val_loss: 114.3858\n",
      "Epoch 5245/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 23.5829 - val_loss: 89.1427\n",
      "Epoch 5246/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 21.0746 - val_loss: 79.6318\n",
      "Epoch 5247/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 27.7550 - val_loss: 107.8601\n",
      "Epoch 5248/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 22.9790 - val_loss: 84.4829\n",
      "Epoch 5249/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 25.5233 - val_loss: 98.7757\n",
      "Epoch 5250/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 20.2330 - val_loss: 92.3796\n",
      "Epoch 5251/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 21.0886 - val_loss: 88.1745\n",
      "Epoch 5252/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 25.5355 - val_loss: 101.2502\n",
      "Epoch 5253/10000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 22.5264 - val_loss: 97.8976\n",
      "Epoch 5254/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 24.9398 - val_loss: 87.4887\n",
      "Epoch 5255/10000\n",
      "96/96 [==============================] - 0s 499us/step - loss: 23.1373 - val_loss: 93.2184\n",
      "Epoch 5256/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 19.5981 - val_loss: 90.2657\n",
      "Epoch 5257/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 22.3118 - val_loss: 101.0057\n",
      "Epoch 5258/10000\n",
      "96/96 [==============================] - 0s 589us/step - loss: 26.1335 - val_loss: 103.9060\n",
      "Epoch 5259/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 21.3308 - val_loss: 89.1414\n",
      "Epoch 5260/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 24.2232 - val_loss: 104.7828\n",
      "Epoch 5261/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 21.3537 - val_loss: 102.2817\n",
      "Epoch 5262/10000\n",
      "96/96 [==============================] - 0s 475us/step - loss: 21.4593 - val_loss: 91.2547\n",
      "Epoch 5263/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 26.7501 - val_loss: 96.6788\n",
      "Epoch 5264/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 24.0428 - val_loss: 96.9410\n",
      "Epoch 5265/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 25.0206 - val_loss: 114.7919\n",
      "Epoch 5266/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 21.5080 - val_loss: 100.5919\n",
      "Epoch 5267/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 18.9491 - val_loss: 92.2402\n",
      "Epoch 5268/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 23.1996 - val_loss: 93.3634\n",
      "Epoch 5269/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 62.2734 - val_loss: 85.4268\n",
      "Epoch 5270/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 27.0566 - val_loss: 107.3736\n",
      "Epoch 5271/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 41.7458 - val_loss: 101.8967\n",
      "Epoch 5272/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 49.2101 - val_loss: 94.5801\n",
      "Epoch 5273/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 35.7451 - val_loss: 88.2635\n",
      "Epoch 5274/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 37.0317 - val_loss: 99.3591\n",
      "Epoch 5275/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 105.9082 - val_loss: 89.4065\n",
      "Epoch 5276/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 46.0077 - val_loss: 83.8781\n",
      "Epoch 5277/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 49.6036 - val_loss: 89.9844\n",
      "Epoch 5278/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 43.8529 - val_loss: 85.4465\n",
      "Epoch 5279/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 70.2794 - val_loss: 99.1198\n",
      "Epoch 5280/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 43.6924 - val_loss: 85.8146\n",
      "Epoch 5281/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 42.9687 - val_loss: 91.4143\n",
      "Epoch 5282/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 43.7286 - val_loss: 96.8318\n",
      "Epoch 5283/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 42.5024 - val_loss: 79.9194\n",
      "Epoch 5284/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 40.9040 - val_loss: 85.5712\n",
      "Epoch 5285/10000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 92.8976 - val_loss: 78.4315\n",
      "Epoch 5286/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 43.4000 - val_loss: 98.0739\n",
      "Epoch 5287/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 35.8099 - val_loss: 99.3977\n",
      "Epoch 5288/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 41.0880 - val_loss: 90.2343\n",
      "Epoch 5289/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 36.6117 - val_loss: 91.6200\n",
      "Epoch 5290/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 41.3136 - val_loss: 96.4142\n",
      "Epoch 5291/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 37.8378 - val_loss: 100.0112\n",
      "Epoch 5292/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 38.6439 - val_loss: 99.4315\n",
      "Epoch 5293/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 36.0004 - val_loss: 97.9536\n",
      "Epoch 5294/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 40.7967 - val_loss: 95.1013\n",
      "Epoch 5295/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 30.3216 - val_loss: 87.2131\n",
      "Epoch 5296/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 35.3331 - val_loss: 80.0938\n",
      "Epoch 5297/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 33.8954 - val_loss: 95.8965\n",
      "Epoch 5298/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 34.0073 - val_loss: 99.4839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5299/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 30.6393 - val_loss: 101.7684\n",
      "Epoch 5300/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 31.5424 - val_loss: 91.8675\n",
      "Epoch 5301/10000\n",
      "96/96 [==============================] - 0s 542us/step - loss: 32.1428 - val_loss: 88.3251\n",
      "Epoch 5302/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 31.4816 - val_loss: 101.8499\n",
      "Epoch 5303/10000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 26.7585 - val_loss: 99.4971\n",
      "Epoch 5304/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 28.1780 - val_loss: 85.2805\n",
      "Epoch 5305/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 31.9149 - val_loss: 98.1116\n",
      "Epoch 5306/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 92.5115 - val_loss: 95.9666\n",
      "Epoch 5307/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 33.8629 - val_loss: 102.8991\n",
      "Epoch 5308/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 40.9809 - val_loss: 87.2807\n",
      "Epoch 5309/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 37.5618 - val_loss: 88.9317\n",
      "Epoch 5310/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 37.1715 - val_loss: 85.1224\n",
      "Epoch 5311/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 44.7140 - val_loss: 88.5864\n",
      "Epoch 5312/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 41.0956 - val_loss: 74.0685\n",
      "Epoch 5313/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 33.7494 - val_loss: 88.2029\n",
      "Epoch 5314/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 37.3712 - val_loss: 92.6973\n",
      "Epoch 5315/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 36.8274 - val_loss: 94.0550\n",
      "Epoch 5316/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 38.5176 - val_loss: 91.2051\n",
      "Epoch 5317/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 35.8830 - val_loss: 83.6361\n",
      "Epoch 5318/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 32.1097 - val_loss: 94.0817\n",
      "Epoch 5319/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 35.4991 - val_loss: 72.8552\n",
      "Epoch 5320/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 30.3731 - val_loss: 95.6441\n",
      "Epoch 5321/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 30.6306 - val_loss: 102.1490\n",
      "Epoch 5322/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 32.4835 - val_loss: 81.9074\n",
      "Epoch 5323/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 33.1289 - val_loss: 83.0437\n",
      "Epoch 5324/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 33.7483 - val_loss: 89.2241\n",
      "Epoch 5325/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 27.1763 - val_loss: 84.5737\n",
      "Epoch 5326/10000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 29.8667 - val_loss: 97.4974\n",
      "Epoch 5327/10000\n",
      "96/96 [==============================] - 0s 499us/step - loss: 28.6399 - val_loss: 88.9120\n",
      "Epoch 5328/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 26.7065 - val_loss: 89.2504\n",
      "Epoch 5329/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 25.4223 - val_loss: 88.2049\n",
      "Epoch 5330/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 23.2531 - val_loss: 86.8497\n",
      "Epoch 5331/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 25.7853 - val_loss: 99.4752\n",
      "Epoch 5332/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 30.5551 - val_loss: 105.0182\n",
      "Epoch 5333/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 29.0660 - val_loss: 90.3755\n",
      "Epoch 5334/10000\n",
      "96/96 [==============================] - 0s 495us/step - loss: 25.1078 - val_loss: 81.5071\n",
      "Epoch 5335/10000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 30.7106 - val_loss: 103.2826\n",
      "Epoch 5336/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 23.2591 - val_loss: 91.8515\n",
      "Epoch 5337/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 26.1926 - val_loss: 97.8222\n",
      "Epoch 5338/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 26.7917 - val_loss: 105.8830\n",
      "Epoch 5339/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 25.3198 - val_loss: 110.3683\n",
      "Epoch 5340/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 27.2037 - val_loss: 85.9142\n",
      "Epoch 5341/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 30.3848 - val_loss: 97.6775\n",
      "Epoch 5342/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 28.0707 - val_loss: 100.1458\n",
      "Epoch 5343/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 28.3436 - val_loss: 103.3503\n",
      "Epoch 5344/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 26.2090 - val_loss: 96.1806\n",
      "Epoch 5345/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 24.7197 - val_loss: 107.7175\n",
      "Epoch 5346/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 27.9787 - val_loss: 99.1872\n",
      "Epoch 5347/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 27.9018 - val_loss: 85.6537\n",
      "Epoch 5348/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 26.5106 - val_loss: 88.7776\n",
      "Epoch 5349/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 24.4374 - val_loss: 76.2157\n",
      "Epoch 5350/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 28.7046 - val_loss: 94.6264\n",
      "Epoch 5351/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 28.3070 - val_loss: 91.3823\n",
      "Epoch 5352/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 28.3924 - val_loss: 97.9352\n",
      "Epoch 5353/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 27.9277 - val_loss: 105.9020\n",
      "Epoch 5354/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 29.0195 - val_loss: 103.2104\n",
      "Epoch 5355/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 25.2323 - val_loss: 100.3508\n",
      "Epoch 5356/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 22.2372 - val_loss: 100.9226\n",
      "Epoch 5357/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 25.2234 - val_loss: 92.9918\n",
      "Epoch 5358/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 23.8499 - val_loss: 93.2551\n",
      "Epoch 5359/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 26.6671 - val_loss: 79.3383\n",
      "Epoch 5360/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 29.2710 - val_loss: 79.2831\n",
      "Epoch 5361/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 23.9975 - val_loss: 97.6631\n",
      "Epoch 5362/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 26.9413 - val_loss: 91.4146\n",
      "Epoch 5363/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 24.6974 - val_loss: 75.7591\n",
      "Epoch 5364/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 22.6342 - val_loss: 85.1939\n",
      "Epoch 5365/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 21.4950 - val_loss: 92.8259\n",
      "Epoch 5366/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 24.2428 - val_loss: 102.0760\n",
      "Epoch 5367/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 23.0704 - val_loss: 81.1768\n",
      "Epoch 5368/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 23.1249 - val_loss: 93.3379\n",
      "Epoch 5369/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 21.6480 - val_loss: 97.9851\n",
      "Epoch 5370/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 22.7717 - val_loss: 90.8174\n",
      "Epoch 5371/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 21.0377 - val_loss: 102.1833\n",
      "Epoch 5372/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 19.7618 - val_loss: 101.8349\n",
      "Epoch 5373/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 22.6738 - val_loss: 104.9694\n",
      "Epoch 5374/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 22.1144 - val_loss: 96.8262\n",
      "Epoch 5375/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 19.4365 - val_loss: 89.4275\n",
      "Epoch 5376/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 23.4772 - val_loss: 83.3401\n",
      "Epoch 5377/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 21.7547 - val_loss: 105.1300\n",
      "Epoch 5378/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 28.5785 - val_loss: 103.3351\n",
      "Epoch 5379/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 25.7416 - val_loss: 89.7684\n",
      "Epoch 5380/10000\n",
      "96/96 [==============================] - 0s 481us/step - loss: 22.0399 - val_loss: 107.7010\n",
      "Epoch 5381/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 18.2898 - val_loss: 109.8793\n",
      "Epoch 5382/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 19.2934 - val_loss: 104.3067\n",
      "Epoch 5383/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 22.2569 - val_loss: 81.8537\n",
      "Epoch 5384/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 21.6284 - val_loss: 88.6669\n",
      "Epoch 5385/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 21.7719 - val_loss: 92.8641\n",
      "Epoch 5386/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 19.1523 - val_loss: 79.9803\n",
      "Epoch 5387/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 21.6845 - val_loss: 102.4608\n",
      "Epoch 5388/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 21.1923 - val_loss: 107.3554\n",
      "Epoch 5389/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 24.0971 - val_loss: 92.6575\n",
      "Epoch 5390/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 22.9301 - val_loss: 88.0610\n",
      "Epoch 5391/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 21.6141 - val_loss: 85.1193\n",
      "Epoch 5392/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 19.0589 - val_loss: 98.6606\n",
      "Epoch 5393/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 21.6810 - val_loss: 89.1960\n",
      "Epoch 5394/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 19.5328 - val_loss: 99.8000\n",
      "Epoch 5395/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 20.0110 - val_loss: 95.7809\n",
      "Epoch 5396/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 21.8726 - val_loss: 94.2078\n",
      "Epoch 5397/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 19.9728 - val_loss: 97.1711\n",
      "Epoch 5398/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 21.9289 - val_loss: 111.1554\n",
      "Epoch 5399/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 18.5602 - val_loss: 102.8619\n",
      "Epoch 5400/10000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 20.8532 - val_loss: 97.3311\n",
      "Epoch 5401/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 23.5793 - val_loss: 95.8669\n",
      "Epoch 5402/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 18.3744 - val_loss: 85.5656\n",
      "Epoch 5403/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 18.3590 - val_loss: 96.1437\n",
      "Epoch 5404/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 15.5294 - val_loss: 106.7661\n",
      "Epoch 5405/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 18.3634 - val_loss: 103.6845\n",
      "Epoch 5406/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 26.1243 - val_loss: 122.5140\n",
      "Epoch 5407/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 23.5509 - val_loss: 94.8208\n",
      "Epoch 5408/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 22.3043 - val_loss: 100.1660\n",
      "Epoch 5409/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 46.8812 - val_loss: 97.7206\n",
      "Epoch 5410/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 25.6143 - val_loss: 118.7561\n",
      "Epoch 5411/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 23.8635 - val_loss: 105.8245\n",
      "Epoch 5412/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 28.9526 - val_loss: 89.9319\n",
      "Epoch 5413/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 29.1289 - val_loss: 93.9482\n",
      "Epoch 5414/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 24.5373 - val_loss: 104.2276\n",
      "Epoch 5415/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 29.1938 - val_loss: 87.1553\n",
      "Epoch 5416/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 29.1099 - val_loss: 96.7336\n",
      "Epoch 5417/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 26.3675 - val_loss: 100.8861\n",
      "Epoch 5418/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 28.3403 - val_loss: 99.0051\n",
      "Epoch 5419/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 30.9291 - val_loss: 106.4027\n",
      "Epoch 5420/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 29.0047 - val_loss: 103.4313\n",
      "Epoch 5421/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 27.1423 - val_loss: 107.5589\n",
      "Epoch 5422/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 24.7147 - val_loss: 98.1677\n",
      "Epoch 5423/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 22.5864 - val_loss: 110.1172\n",
      "Epoch 5424/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 24.5491 - val_loss: 93.3689\n",
      "Epoch 5425/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 29.8254 - val_loss: 108.5733\n",
      "Epoch 5426/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 24.3112 - val_loss: 108.5888\n",
      "Epoch 5427/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 20.6200 - val_loss: 112.1772\n",
      "Epoch 5428/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 24.4399 - val_loss: 88.6064\n",
      "Epoch 5429/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 19.1724 - val_loss: 105.1547\n",
      "Epoch 5430/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 23.5382 - val_loss: 103.4125\n",
      "Epoch 5431/10000\n",
      "96/96 [==============================] - 0s 501us/step - loss: 23.1266 - val_loss: 83.1729\n",
      "Epoch 5432/10000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 23.2051 - val_loss: 105.9136\n",
      "Epoch 5433/10000\n",
      "96/96 [==============================] - 0s 602us/step - loss: 22.8572 - val_loss: 107.2633\n",
      "Epoch 5434/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 18.1615 - val_loss: 115.6093\n",
      "Epoch 5435/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 25.4042 - val_loss: 88.5158\n",
      "Epoch 5436/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 21.4269 - val_loss: 90.9954\n",
      "Epoch 5437/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 40.5971 - val_loss: 112.6183\n",
      "Epoch 5438/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 28.4672 - val_loss: 85.1332\n",
      "Epoch 5439/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 32.0452 - val_loss: 102.5356\n",
      "Epoch 5440/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 28.3532 - val_loss: 101.3935\n",
      "Epoch 5441/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 24.4787 - val_loss: 86.3182\n",
      "Epoch 5442/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 28.7461 - val_loss: 91.9193\n",
      "Epoch 5443/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 24.3530 - val_loss: 93.8521\n",
      "Epoch 5444/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 26.1595 - val_loss: 103.5120\n",
      "Epoch 5445/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 25.1400 - val_loss: 89.6810\n",
      "Epoch 5446/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 25.1045 - val_loss: 97.6096\n",
      "Epoch 5447/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 24.8634 - val_loss: 105.9335\n",
      "Epoch 5448/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 24.3674 - val_loss: 104.6213\n",
      "Epoch 5449/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 416us/step - loss: 21.6111 - val_loss: 102.9738\n",
      "Epoch 5450/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 25.0543 - val_loss: 108.9760\n",
      "Epoch 5451/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 24.2921 - val_loss: 110.0880\n",
      "Epoch 5452/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 20.4799 - val_loss: 111.2654\n",
      "Epoch 5453/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 24.3500 - val_loss: 85.2593\n",
      "Epoch 5454/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 22.5606 - val_loss: 89.3711\n",
      "Epoch 5455/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 22.4350 - val_loss: 105.6493\n",
      "Epoch 5456/10000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 23.9845 - val_loss: 91.5946\n",
      "Epoch 5457/10000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 22.5228 - val_loss: 114.6919\n",
      "Epoch 5458/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 19.8530 - val_loss: 101.1721\n",
      "Epoch 5459/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 22.2635 - val_loss: 99.5038\n",
      "Epoch 5460/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 20.9094 - val_loss: 85.9479\n",
      "Epoch 5461/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 23.7069 - val_loss: 103.1099\n",
      "Epoch 5462/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 18.2694 - val_loss: 106.2182\n",
      "Epoch 5463/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 20.4041 - val_loss: 102.1286\n",
      "Epoch 5464/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 24.3525 - val_loss: 98.0070\n",
      "Epoch 5465/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 19.9320 - val_loss: 99.8966\n",
      "Epoch 5466/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 25.4467 - val_loss: 85.7209\n",
      "Epoch 5467/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 23.7363 - val_loss: 83.5397\n",
      "Epoch 5468/10000\n",
      "96/96 [==============================] - 0s 349us/step - loss: 20.8074 - val_loss: 89.8416\n",
      "Epoch 5469/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 23.8501 - val_loss: 101.3283\n",
      "Epoch 5470/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 24.6027 - val_loss: 98.5582\n",
      "Epoch 5471/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 23.1201 - val_loss: 99.9969\n",
      "Epoch 5472/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 20.4214 - val_loss: 87.3850\n",
      "Epoch 5473/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 18.4564 - val_loss: 101.5707\n",
      "Epoch 5474/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 20.4425 - val_loss: 99.8409\n",
      "Epoch 5475/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 21.8614 - val_loss: 91.2306\n",
      "Epoch 5476/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 21.8992 - val_loss: 93.3809\n",
      "Epoch 5477/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 20.3337 - val_loss: 103.6209\n",
      "Epoch 5478/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 22.8642 - val_loss: 98.0676\n",
      "Epoch 5479/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 24.5547 - val_loss: 108.1154\n",
      "Epoch 5480/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 19.9526 - val_loss: 98.8537\n",
      "Epoch 5481/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 22.0709 - val_loss: 102.9381\n",
      "Epoch 5482/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 21.3658 - val_loss: 101.5150\n",
      "Epoch 5483/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 20.6966 - val_loss: 101.4073\n",
      "Epoch 5484/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 19.8055 - val_loss: 98.2642\n",
      "Epoch 5485/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 20.0867 - val_loss: 109.5790\n",
      "Epoch 5486/10000\n",
      "96/96 [==============================] - 0s 515us/step - loss: 20.8305 - val_loss: 91.0818\n",
      "Epoch 5487/10000\n",
      "96/96 [==============================] - 0s 482us/step - loss: 21.4636 - val_loss: 99.4918\n",
      "Epoch 5488/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 20.4380 - val_loss: 101.0470\n",
      "Epoch 5489/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 20.7426 - val_loss: 90.5214\n",
      "Epoch 5490/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 20.3671 - val_loss: 102.7020\n",
      "Epoch 5491/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 19.2980 - val_loss: 99.6025\n",
      "Epoch 5492/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 22.1749 - val_loss: 95.9097\n",
      "Epoch 5493/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 20.0060 - val_loss: 112.8069\n",
      "Epoch 5494/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 21.8860 - val_loss: 102.8862\n",
      "Epoch 5495/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 19.8759 - val_loss: 93.2485\n",
      "Epoch 5496/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 19.7034 - val_loss: 102.5626\n",
      "Epoch 5497/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 21.8892 - val_loss: 93.8990\n",
      "Epoch 5498/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 20.8343 - val_loss: 99.8464\n",
      "Epoch 5499/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 21.2619 - val_loss: 91.1343\n",
      "Epoch 5500/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 17.9391 - val_loss: 109.9761\n",
      "Epoch 5501/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 18.9179 - val_loss: 94.3490\n",
      "Epoch 5502/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 20.6454 - val_loss: 99.6316\n",
      "Epoch 5503/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 19.3909 - val_loss: 88.0819\n",
      "Epoch 5504/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 19.3386 - val_loss: 104.3778\n",
      "Epoch 5505/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 18.9633 - val_loss: 101.6545\n",
      "Epoch 5506/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 23.4152 - val_loss: 91.6532\n",
      "Epoch 5507/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 21.3129 - val_loss: 101.2527\n",
      "Epoch 5508/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 22.7242 - val_loss: 101.1831\n",
      "Epoch 5509/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 24.1029 - val_loss: 95.0828\n",
      "Epoch 5510/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 24.1117 - val_loss: 97.7094\n",
      "Epoch 5511/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 19.8942 - val_loss: 101.8912\n",
      "Epoch 5512/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 24.2501 - val_loss: 104.8505\n",
      "Epoch 5513/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 17.2959 - val_loss: 109.2391\n",
      "Epoch 5514/10000\n",
      "96/96 [==============================] - 0s 525us/step - loss: 23.1225 - val_loss: 110.1393\n",
      "Epoch 5515/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 17.4958 - val_loss: 111.8567\n",
      "Epoch 5516/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 17.5067 - val_loss: 93.2636\n",
      "Epoch 5517/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 23.9185 - val_loss: 83.7508\n",
      "Epoch 5518/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 18.2681 - val_loss: 107.7968\n",
      "Epoch 5519/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 20.9399 - val_loss: 107.0650\n",
      "Epoch 5520/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 19.9443 - val_loss: 91.5117\n",
      "Epoch 5521/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 21.2623 - val_loss: 103.0462\n",
      "Epoch 5522/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 20.0798 - val_loss: 99.6381\n",
      "Epoch 5523/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 22.7863 - val_loss: 103.2668\n",
      "Epoch 5524/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 20.4673 - val_loss: 94.7770\n",
      "Epoch 5525/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 18.7484 - val_loss: 91.4422\n",
      "Epoch 5526/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 19.0164 - val_loss: 98.3025\n",
      "Epoch 5527/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 21.7878 - val_loss: 94.3316\n",
      "Epoch 5528/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 18.9433 - val_loss: 96.6021\n",
      "Epoch 5529/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 17.5345 - val_loss: 106.4353\n",
      "Epoch 5530/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 19.6701 - val_loss: 101.9457\n",
      "Epoch 5531/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 18.4049 - val_loss: 83.8302\n",
      "Epoch 5532/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 18.4013 - val_loss: 97.2592\n",
      "Epoch 5533/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 22.2802 - val_loss: 104.1918\n",
      "Epoch 5534/10000\n",
      "96/96 [==============================] - 0s 356us/step - loss: 18.8249 - val_loss: 102.3774\n",
      "Epoch 5535/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 19.9455 - val_loss: 100.8212\n",
      "Epoch 5536/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 19.7967 - val_loss: 96.4981\n",
      "Epoch 5537/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 19.5063 - val_loss: 98.4301\n",
      "Epoch 5538/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 18.2649 - val_loss: 89.1610\n",
      "Epoch 5539/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 17.4345 - val_loss: 100.8958\n",
      "Epoch 5540/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 22.2644 - val_loss: 95.1079\n",
      "Epoch 5541/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 20.4509 - val_loss: 90.8644\n",
      "Epoch 5542/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 19.6273 - val_loss: 94.0659\n",
      "Epoch 5543/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 17.9985 - val_loss: 112.0873\n",
      "Epoch 5544/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 20.3908 - val_loss: 99.2620\n",
      "Epoch 5545/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 20.9604 - val_loss: 104.9070\n",
      "Epoch 5546/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 20.9479 - val_loss: 102.5102\n",
      "Epoch 5547/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 20.1589 - val_loss: 103.5506\n",
      "Epoch 5548/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 19.9968 - val_loss: 105.0839\n",
      "Epoch 5549/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 17.1685 - val_loss: 88.5152\n",
      "Epoch 5550/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 22.4680 - val_loss: 103.0843\n",
      "Epoch 5551/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 18.0114 - val_loss: 114.2577\n",
      "Epoch 5552/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 20.6352 - val_loss: 117.6026\n",
      "Epoch 5553/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 23.6050 - val_loss: 101.2476\n",
      "Epoch 5554/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 19.5704 - val_loss: 103.2944\n",
      "Epoch 5555/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 19.6092 - val_loss: 107.6983\n",
      "Epoch 5556/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 24.6479 - val_loss: 112.1202\n",
      "Epoch 5557/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 24.9261 - val_loss: 94.9671\n",
      "Epoch 5558/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 19.2081 - val_loss: 79.6198\n",
      "Epoch 5559/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 21.6808 - val_loss: 81.6603\n",
      "Epoch 5560/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 23.0119 - val_loss: 105.0668\n",
      "Epoch 5561/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 22.0768 - val_loss: 103.4682\n",
      "Epoch 5562/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 22.5277 - val_loss: 108.4644\n",
      "Epoch 5563/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 20.6981 - val_loss: 93.8319\n",
      "Epoch 5564/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 24.3924 - val_loss: 94.6978\n",
      "Epoch 5565/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 20.4653 - val_loss: 101.7152\n",
      "Epoch 5566/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 18.6582 - val_loss: 97.0418\n",
      "Epoch 5567/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 28.8025 - val_loss: 89.5021\n",
      "Epoch 5568/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 23.0703 - val_loss: 104.5681\n",
      "Epoch 5569/10000\n",
      "96/96 [==============================] - 0s 519us/step - loss: 27.3561 - val_loss: 105.1161\n",
      "Epoch 5570/10000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 24.4712 - val_loss: 108.3257\n",
      "Epoch 5571/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 24.4662 - val_loss: 95.5202\n",
      "Epoch 5572/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 23.6017 - val_loss: 84.1282\n",
      "Epoch 5573/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 25.9359 - val_loss: 104.0137\n",
      "Epoch 5574/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 22.4307 - val_loss: 96.5147\n",
      "Epoch 5575/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 21.3819 - val_loss: 90.2847\n",
      "Epoch 5576/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 23.5026 - val_loss: 91.8771\n",
      "Epoch 5577/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 21.6810 - val_loss: 102.5487\n",
      "Epoch 5578/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 23.2920 - val_loss: 81.0686\n",
      "Epoch 5579/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 23.3880 - val_loss: 104.8759\n",
      "Epoch 5580/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 46.1906 - val_loss: 90.6572\n",
      "Epoch 5581/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 30.1846 - val_loss: 105.8182\n",
      "Epoch 5582/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 65.5644 - val_loss: 85.5033\n",
      "Epoch 5583/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 51.2909 - val_loss: 106.7232\n",
      "Epoch 5584/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 112.3776 - val_loss: 83.3994\n",
      "Epoch 5585/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 49.0061 - val_loss: 91.0053\n",
      "Epoch 5586/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 111.6697 - val_loss: 95.1534\n",
      "Epoch 5587/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 90.0155 - val_loss: 93.3015\n",
      "Epoch 5588/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 40.7010 - val_loss: 99.7896\n",
      "Epoch 5589/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 42.8094 - val_loss: 100.8941\n",
      "Epoch 5590/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 40.1462 - val_loss: 106.1438\n",
      "Epoch 5591/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 44.6151 - val_loss: 93.6382\n",
      "Epoch 5592/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 37.7795 - val_loss: 82.0773\n",
      "Epoch 5593/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 39.6347 - val_loss: 95.0203\n",
      "Epoch 5594/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 39.6748 - val_loss: 96.9058\n",
      "Epoch 5595/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 35.5013 - val_loss: 97.7418\n",
      "Epoch 5596/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 34.4680 - val_loss: 90.4484\n",
      "Epoch 5597/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 31.6942 - val_loss: 97.1353\n",
      "Epoch 5598/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 33.1806 - val_loss: 75.6578\n",
      "Epoch 5599/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 413us/step - loss: 30.0419 - val_loss: 89.4620\n",
      "Epoch 5600/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 29.8378 - val_loss: 100.1671\n",
      "Epoch 5601/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 28.8973 - val_loss: 98.7294\n",
      "Epoch 5602/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 24.7511 - val_loss: 101.8775\n",
      "Epoch 5603/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 30.7312 - val_loss: 93.1362\n",
      "Epoch 5604/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 23.9119 - val_loss: 99.6286\n",
      "Epoch 5605/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 22.0375 - val_loss: 92.8381\n",
      "Epoch 5606/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 22.8813 - val_loss: 100.1571\n",
      "Epoch 5607/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 21.8894 - val_loss: 102.7182\n",
      "Epoch 5608/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 22.0693 - val_loss: 92.7992\n",
      "Epoch 5609/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 25.6245 - val_loss: 83.4435\n",
      "Epoch 5610/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 22.4970 - val_loss: 100.9308\n",
      "Epoch 5611/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 20.4678 - val_loss: 85.8853\n",
      "Epoch 5612/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 19.6037 - val_loss: 91.7295\n",
      "Epoch 5613/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 26.2517 - val_loss: 98.4472\n",
      "Epoch 5614/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 20.8954 - val_loss: 116.5767\n",
      "Epoch 5615/10000\n",
      "96/96 [==============================] - 0s 493us/step - loss: 23.8227 - val_loss: 94.9605\n",
      "Epoch 5616/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 17.7885 - val_loss: 95.8478\n",
      "Epoch 5617/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 19.4867 - val_loss: 87.5334\n",
      "Epoch 5618/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 20.1424 - val_loss: 92.7036\n",
      "Epoch 5619/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 21.0544 - val_loss: 88.6190\n",
      "Epoch 5620/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 20.0186 - val_loss: 98.4250\n",
      "Epoch 5621/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 19.6848 - val_loss: 92.7529\n",
      "Epoch 5622/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 21.6972 - val_loss: 98.9499\n",
      "Epoch 5623/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 20.2870 - val_loss: 98.9773\n",
      "Epoch 5624/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 20.0338 - val_loss: 78.8585\n",
      "Epoch 5625/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 22.8439 - val_loss: 96.5708\n",
      "Epoch 5626/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 21.1588 - val_loss: 76.5805\n",
      "Epoch 5627/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 20.1852 - val_loss: 99.8660\n",
      "Epoch 5628/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 19.6398 - val_loss: 96.1045\n",
      "Epoch 5629/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 19.4228 - val_loss: 104.7471\n",
      "Epoch 5630/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 22.2136 - val_loss: 81.4716\n",
      "Epoch 5631/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 18.0238 - val_loss: 95.0771\n",
      "Epoch 5632/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 23.7811 - val_loss: 96.9209\n",
      "Epoch 5633/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 22.3130 - val_loss: 94.5199\n",
      "Epoch 5634/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 22.3133 - val_loss: 87.8940\n",
      "Epoch 5635/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 25.4712 - val_loss: 102.5108\n",
      "Epoch 5636/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 23.7019 - val_loss: 99.7268\n",
      "Epoch 5637/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 17.1702 - val_loss: 96.1711\n",
      "Epoch 5638/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 20.3316 - val_loss: 82.5638\n",
      "Epoch 5639/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 19.3832 - val_loss: 74.3068\n",
      "Epoch 5640/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 16.8367 - val_loss: 96.1682\n",
      "Epoch 5641/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 19.0504 - val_loss: 90.6855\n",
      "Epoch 5642/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 21.1143 - val_loss: 98.9284\n",
      "Epoch 5643/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 18.5438 - val_loss: 89.7420\n",
      "Epoch 5644/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 17.7077 - val_loss: 95.7260\n",
      "Epoch 5645/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 21.4102 - val_loss: 90.3086\n",
      "Epoch 5646/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 18.2906 - val_loss: 93.8317\n",
      "Epoch 5647/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 18.7336 - val_loss: 100.0674\n",
      "Epoch 5648/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 20.2072 - val_loss: 97.7775\n",
      "Epoch 5649/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 20.9768 - val_loss: 99.6124\n",
      "Epoch 5650/10000\n",
      "96/96 [==============================] - 0s 504us/step - loss: 18.9795 - val_loss: 92.5895\n",
      "Epoch 5651/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 21.2430 - val_loss: 105.3895\n",
      "Epoch 5652/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 18.6821 - val_loss: 90.4077\n",
      "Epoch 5653/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 18.6292 - val_loss: 93.6652\n",
      "Epoch 5654/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 20.4332 - val_loss: 95.7209\n",
      "Epoch 5655/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 19.3498 - val_loss: 100.9369\n",
      "Epoch 5656/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 19.6561 - val_loss: 98.6923\n",
      "Epoch 5657/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 20.3993 - val_loss: 79.4091\n",
      "Epoch 5658/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 21.1669 - val_loss: 111.5018\n",
      "Epoch 5659/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 18.6897 - val_loss: 91.2460\n",
      "Epoch 5660/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 22.2432 - val_loss: 101.4751\n",
      "Epoch 5661/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 19.9846 - val_loss: 95.9193\n",
      "Epoch 5662/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 20.3423 - val_loss: 83.1963\n",
      "Epoch 5663/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 36.6714 - val_loss: 95.7815\n",
      "Epoch 5664/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 24.4348 - val_loss: 92.1684\n",
      "Epoch 5665/10000\n",
      "96/96 [==============================] - 0s 347us/step - loss: 23.4386 - val_loss: 103.1909\n",
      "Epoch 5666/10000\n",
      "96/96 [==============================] - 0s 357us/step - loss: 23.3874 - val_loss: 90.7836\n",
      "Epoch 5667/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 26.8602 - val_loss: 103.5612\n",
      "Epoch 5668/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 24.8649 - val_loss: 98.3974\n",
      "Epoch 5669/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 20.5463 - val_loss: 98.9735\n",
      "Epoch 5670/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 21.5829 - val_loss: 96.9724\n",
      "Epoch 5671/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 21.7479 - val_loss: 100.2210\n",
      "Epoch 5672/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 19.9441 - val_loss: 100.8393\n",
      "Epoch 5673/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 18.5930 - val_loss: 84.5161\n",
      "Epoch 5674/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 19.0800 - val_loss: 90.7124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5675/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 18.4927 - val_loss: 90.8423\n",
      "Epoch 5676/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 19.3171 - val_loss: 79.9419\n",
      "Epoch 5677/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 18.5236 - val_loss: 102.5487\n",
      "Epoch 5678/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 18.8718 - val_loss: 87.3279\n",
      "Epoch 5679/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 17.5540 - val_loss: 115.9720\n",
      "Epoch 5680/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 21.0599 - val_loss: 85.9246\n",
      "Epoch 5681/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 18.2785 - val_loss: 104.6714\n",
      "Epoch 5682/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 21.0565 - val_loss: 100.6270\n",
      "Epoch 5683/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 19.8226 - val_loss: 100.9457\n",
      "Epoch 5684/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 19.0629 - val_loss: 101.2748\n",
      "Epoch 5685/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 18.7464 - val_loss: 99.7128\n",
      "Epoch 5686/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 25.2605 - val_loss: 97.2911\n",
      "Epoch 5687/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 18.5775 - val_loss: 104.3234\n",
      "Epoch 5688/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 19.7551 - val_loss: 100.9418\n",
      "Epoch 5689/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 16.6928 - val_loss: 98.9479\n",
      "Epoch 5690/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 20.1705 - val_loss: 100.9258\n",
      "Epoch 5691/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 20.3875 - val_loss: 95.2001\n",
      "Epoch 5692/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 20.2391 - val_loss: 103.4157\n",
      "Epoch 5693/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 19.8793 - val_loss: 85.2806\n",
      "Epoch 5694/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 16.8390 - val_loss: 99.5512\n",
      "Epoch 5695/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 17.5725 - val_loss: 109.3888\n",
      "Epoch 5696/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 20.7642 - val_loss: 107.1572\n",
      "Epoch 5697/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 17.4628 - val_loss: 113.3205\n",
      "Epoch 5698/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 16.9672 - val_loss: 101.3717\n",
      "Epoch 5699/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 17.4334 - val_loss: 96.8075\n",
      "Epoch 5700/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 18.3844 - val_loss: 95.9301\n",
      "Epoch 5701/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 19.1857 - val_loss: 105.3277\n",
      "Epoch 5702/10000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 21.1426 - val_loss: 95.2633\n",
      "Epoch 5703/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 19.6686 - val_loss: 92.3721\n",
      "Epoch 5704/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 17.8047 - val_loss: 91.8871\n",
      "Epoch 5705/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 15.8771 - val_loss: 105.7979\n",
      "Epoch 5706/10000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 20.0279 - val_loss: 80.8919\n",
      "Epoch 5707/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 19.1897 - val_loss: 89.2883\n",
      "Epoch 5708/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 17.9321 - val_loss: 82.1377\n",
      "Epoch 5709/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 18.0771 - val_loss: 96.2173\n",
      "Epoch 5710/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 21.1556 - val_loss: 99.1135\n",
      "Epoch 5711/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 22.1149 - val_loss: 107.9528\n",
      "Epoch 5712/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 18.7861 - val_loss: 90.3516\n",
      "Epoch 5713/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 20.7290 - val_loss: 113.0705\n",
      "Epoch 5714/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 19.2239 - val_loss: 99.4904\n",
      "Epoch 5715/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 16.9480 - val_loss: 99.4035\n",
      "Epoch 5716/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 16.9838 - val_loss: 106.4278\n",
      "Epoch 5717/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 18.3160 - val_loss: 95.7252\n",
      "Epoch 5718/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 21.5551 - val_loss: 96.0377\n",
      "Epoch 5719/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 21.2892 - val_loss: 104.6641\n",
      "Epoch 5720/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 15.4175 - val_loss: 93.0907\n",
      "Epoch 5721/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 15.6223 - val_loss: 100.9883\n",
      "Epoch 5722/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 18.3580 - val_loss: 88.7404\n",
      "Epoch 5723/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 17.8511 - val_loss: 90.4849\n",
      "Epoch 5724/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 23.4074 - val_loss: 104.2503\n",
      "Epoch 5725/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 19.7910 - val_loss: 101.0939\n",
      "Epoch 5726/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 22.8676 - val_loss: 108.6958\n",
      "Epoch 5727/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 21.9080 - val_loss: 94.6382\n",
      "Epoch 5728/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 20.2121 - val_loss: 101.5278\n",
      "Epoch 5729/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 24.7268 - val_loss: 95.7437\n",
      "Epoch 5730/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 28.3695 - val_loss: 96.0639\n",
      "Epoch 5731/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 22.0048 - val_loss: 103.5047\n",
      "Epoch 5732/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 21.7800 - val_loss: 102.2239\n",
      "Epoch 5733/10000\n",
      "96/96 [==============================] - 0s 490us/step - loss: 23.7338 - val_loss: 108.0928\n",
      "Epoch 5734/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 24.9012 - val_loss: 90.9190\n",
      "Epoch 5735/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 21.1098 - val_loss: 95.6234\n",
      "Epoch 5736/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 20.7796 - val_loss: 97.1525\n",
      "Epoch 5737/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 20.7408 - val_loss: 98.0479\n",
      "Epoch 5738/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 20.3217 - val_loss: 93.4879\n",
      "Epoch 5739/10000\n",
      "96/96 [==============================] - 0s 516us/step - loss: 19.2332 - val_loss: 101.2725\n",
      "Epoch 5740/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 21.3953 - val_loss: 95.6315\n",
      "Epoch 5741/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 20.5876 - val_loss: 104.3182\n",
      "Epoch 5742/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 18.1581 - val_loss: 100.9043\n",
      "Epoch 5743/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 21.5650 - val_loss: 84.4483\n",
      "Epoch 5744/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 15.2626 - val_loss: 94.8264\n",
      "Epoch 5745/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 17.5216 - val_loss: 98.4920\n",
      "Epoch 5746/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 14.5740 - val_loss: 94.9505\n",
      "Epoch 5747/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 18.1358 - val_loss: 91.8781\n",
      "Epoch 5748/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 17.5063 - val_loss: 91.2632\n",
      "Epoch 5749/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 18.6143 - val_loss: 98.6345\n",
      "Epoch 5750/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 17.8910 - val_loss: 91.3477\n",
      "Epoch 5751/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 19.7374 - val_loss: 81.9583\n",
      "Epoch 5752/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 16.4085 - val_loss: 102.8388\n",
      "Epoch 5753/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 18.5539 - val_loss: 79.3277\n",
      "Epoch 5754/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 17.5110 - val_loss: 87.4835\n",
      "Epoch 5755/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 17.7932 - val_loss: 102.4871\n",
      "Epoch 5756/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 16.3680 - val_loss: 91.6399\n",
      "Epoch 5757/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 18.8830 - val_loss: 96.4903\n",
      "Epoch 5758/10000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 18.0872 - val_loss: 100.8397\n",
      "Epoch 5759/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 16.7014 - val_loss: 93.4403\n",
      "Epoch 5760/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 17.2762 - val_loss: 90.3263\n",
      "Epoch 5761/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 17.6813 - val_loss: 102.7126\n",
      "Epoch 5762/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 20.3700 - val_loss: 93.7999\n",
      "Epoch 5763/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 21.0161 - val_loss: 95.5086\n",
      "Epoch 5764/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 19.0116 - val_loss: 93.1344\n",
      "Epoch 5765/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 20.2767 - val_loss: 101.6323\n",
      "Epoch 5766/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 24.4911 - val_loss: 78.0629\n",
      "Epoch 5767/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 19.0495 - val_loss: 112.9633\n",
      "Epoch 5768/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 23.0800 - val_loss: 108.6987\n",
      "Epoch 5769/10000\n",
      "96/96 [==============================] - 0s 356us/step - loss: 26.5189 - val_loss: 91.0139\n",
      "Epoch 5770/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 27.0541 - val_loss: 103.5462\n",
      "Epoch 5771/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 19.3782 - val_loss: 99.6994\n",
      "Epoch 5772/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 26.7826 - val_loss: 74.8396\n",
      "Epoch 5773/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 24.5931 - val_loss: 83.7405\n",
      "Epoch 5774/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 25.4828 - val_loss: 85.1721\n",
      "Epoch 5775/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 18.5118 - val_loss: 104.9680\n",
      "Epoch 5776/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 19.0312 - val_loss: 98.8159\n",
      "Epoch 5777/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 25.7533 - val_loss: 106.8454\n",
      "Epoch 5778/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 21.0848 - val_loss: 108.7855\n",
      "Epoch 5779/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 20.1776 - val_loss: 101.8407\n",
      "Epoch 5780/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 20.7511 - val_loss: 97.4412\n",
      "Epoch 5781/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 19.1617 - val_loss: 105.4219\n",
      "Epoch 5782/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 20.3755 - val_loss: 101.7750\n",
      "Epoch 5783/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 18.8320 - val_loss: 85.7049\n",
      "Epoch 5784/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 15.0904 - val_loss: 84.1826\n",
      "Epoch 5785/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 17.1424 - val_loss: 97.6237\n",
      "Epoch 5786/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 18.6824 - val_loss: 84.3764\n",
      "Epoch 5787/10000\n",
      "96/96 [==============================] - 0s 487us/step - loss: 20.2711 - val_loss: 95.8459\n",
      "Epoch 5788/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 15.7676 - val_loss: 89.0183\n",
      "Epoch 5789/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 17.2626 - val_loss: 88.0041\n",
      "Epoch 5790/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 19.9164 - val_loss: 95.9184\n",
      "Epoch 5791/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 16.5784 - val_loss: 111.2828\n",
      "Epoch 5792/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 17.9669 - val_loss: 100.8153\n",
      "Epoch 5793/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 17.8177 - val_loss: 96.5060\n",
      "Epoch 5794/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 16.3428 - val_loss: 98.6851\n",
      "Epoch 5795/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 16.5181 - val_loss: 94.5842\n",
      "Epoch 5796/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 16.3041 - val_loss: 102.7258\n",
      "Epoch 5797/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 21.5857 - val_loss: 89.2829\n",
      "Epoch 5798/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 15.6771 - val_loss: 111.3674\n",
      "Epoch 5799/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 87.5592 - val_loss: 102.2146\n",
      "Epoch 5800/10000\n",
      "96/96 [==============================] - 0s 511us/step - loss: 19.4930 - val_loss: 89.8031\n",
      "Epoch 5801/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 19.9111 - val_loss: 106.1360\n",
      "Epoch 5802/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 19.5191 - val_loss: 100.5400\n",
      "Epoch 5803/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 17.6950 - val_loss: 87.5275\n",
      "Epoch 5804/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 16.4262 - val_loss: 108.6478\n",
      "Epoch 5805/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 16.4743 - val_loss: 83.9742\n",
      "Epoch 5806/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 21.0505 - val_loss: 104.6003\n",
      "Epoch 5807/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 19.9213 - val_loss: 100.2256\n",
      "Epoch 5808/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 17.7159 - val_loss: 109.2897\n",
      "Epoch 5809/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 16.2638 - val_loss: 109.9791\n",
      "Epoch 5810/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 18.9328 - val_loss: 102.4134\n",
      "Epoch 5811/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 16.5603 - val_loss: 103.3706\n",
      "Epoch 5812/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 17.1356 - val_loss: 109.0238\n",
      "Epoch 5813/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 17.4070 - val_loss: 97.5775\n",
      "Epoch 5814/10000\n",
      "96/96 [==============================] - 0s 556us/step - loss: 15.2933 - val_loss: 101.4939\n",
      "Epoch 5815/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 17.0478 - val_loss: 109.3953\n",
      "Epoch 5816/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 16.0616 - val_loss: 97.2667\n",
      "Epoch 5817/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 15.9110 - val_loss: 106.3026\n",
      "Epoch 5818/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 19.4355 - val_loss: 104.2905\n",
      "Epoch 5819/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 14.4545 - val_loss: 96.3113\n",
      "Epoch 5820/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 16.5686 - val_loss: 99.3293\n",
      "Epoch 5821/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 17.4133 - val_loss: 94.7956\n",
      "Epoch 5822/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 17.0896 - val_loss: 89.1146\n",
      "Epoch 5823/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 15.4212 - val_loss: 100.5292\n",
      "Epoch 5824/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 16.8423 - val_loss: 102.5547\n",
      "Epoch 5825/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 397us/step - loss: 18.4656 - val_loss: 96.5524\n",
      "Epoch 5826/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 19.2512 - val_loss: 113.4279\n",
      "Epoch 5827/10000\n",
      "96/96 [==============================] - 0s 494us/step - loss: 19.4791 - val_loss: 103.5965\n",
      "Epoch 5828/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 15.9506 - val_loss: 80.4985\n",
      "Epoch 5829/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 17.8133 - val_loss: 99.7782\n",
      "Epoch 5830/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 17.9328 - val_loss: 94.8596\n",
      "Epoch 5831/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 19.8151 - val_loss: 87.0099\n",
      "Epoch 5832/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 17.7262 - val_loss: 93.0797\n",
      "Epoch 5833/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 20.9455 - val_loss: 92.9381\n",
      "Epoch 5834/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 14.1423 - val_loss: 106.7350\n",
      "Epoch 5835/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 16.9765 - val_loss: 94.0064\n",
      "Epoch 5836/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 15.3934 - val_loss: 106.1640\n",
      "Epoch 5837/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 17.3028 - val_loss: 94.0401\n",
      "Epoch 5838/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 18.8079 - val_loss: 107.7007\n",
      "Epoch 5839/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 40.2904 - val_loss: 99.8554\n",
      "Epoch 5840/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 32.1160 - val_loss: 113.6604\n",
      "Epoch 5841/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 33.3887 - val_loss: 108.6086\n",
      "Epoch 5842/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 32.1158 - val_loss: 97.2567\n",
      "Epoch 5843/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 25.8599 - val_loss: 96.7357\n",
      "Epoch 5844/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 27.6668 - val_loss: 92.1669\n",
      "Epoch 5845/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 29.7666 - val_loss: 73.7788\n",
      "Epoch 5846/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 26.0425 - val_loss: 92.9831\n",
      "Epoch 5847/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 27.8055 - val_loss: 89.1703\n",
      "Epoch 5848/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 28.4190 - val_loss: 104.6346\n",
      "Epoch 5849/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 26.0283 - val_loss: 90.2057\n",
      "Epoch 5850/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 26.2259 - val_loss: 90.7912\n",
      "Epoch 5851/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 25.5264 - val_loss: 105.5318\n",
      "Epoch 5852/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 25.8932 - val_loss: 102.0048\n",
      "Epoch 5853/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 22.9480 - val_loss: 106.3844\n",
      "Epoch 5854/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 19.8987 - val_loss: 106.7926\n",
      "Epoch 5855/10000\n",
      "96/96 [==============================] - 0s 349us/step - loss: 21.1381 - val_loss: 104.5851\n",
      "Epoch 5856/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 22.3371 - val_loss: 103.2553\n",
      "Epoch 5857/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 22.4281 - val_loss: 102.8666\n",
      "Epoch 5858/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 18.5866 - val_loss: 99.7550\n",
      "Epoch 5859/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 21.5185 - val_loss: 105.7307\n",
      "Epoch 5860/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 20.2131 - val_loss: 100.2555\n",
      "Epoch 5861/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 18.0316 - val_loss: 106.1573\n",
      "Epoch 5862/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 19.3631 - val_loss: 86.4888\n",
      "Epoch 5863/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 22.6659 - val_loss: 102.5962\n",
      "Epoch 5864/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 17.3863 - val_loss: 94.6907\n",
      "Epoch 5865/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 20.7717 - val_loss: 102.8208\n",
      "Epoch 5866/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 23.0960 - val_loss: 86.8536\n",
      "Epoch 5867/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 21.0021 - val_loss: 113.8768\n",
      "Epoch 5868/10000\n",
      "96/96 [==============================] - 0s 490us/step - loss: 18.3547 - val_loss: 83.1084\n",
      "Epoch 5869/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 22.4709 - val_loss: 106.9384\n",
      "Epoch 5870/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 25.1803 - val_loss: 105.4761\n",
      "Epoch 5871/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 22.7854 - val_loss: 95.1521\n",
      "Epoch 5872/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 18.8101 - val_loss: 80.1403\n",
      "Epoch 5873/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 19.6005 - val_loss: 88.4335\n",
      "Epoch 5874/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 17.6744 - val_loss: 106.8522\n",
      "Epoch 5875/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 17.7004 - val_loss: 104.8670\n",
      "Epoch 5876/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 17.6161 - val_loss: 97.0689\n",
      "Epoch 5877/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 18.6953 - val_loss: 92.2229\n",
      "Epoch 5878/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 17.2098 - val_loss: 94.3326\n",
      "Epoch 5879/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 16.8472 - val_loss: 106.6395\n",
      "Epoch 5880/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 18.5841 - val_loss: 94.0970\n",
      "Epoch 5881/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 17.9471 - val_loss: 103.6566\n",
      "Epoch 5882/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 15.2246 - val_loss: 105.1049\n",
      "Epoch 5883/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 15.6016 - val_loss: 80.6255\n",
      "Epoch 5884/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 17.0394 - val_loss: 96.5972\n",
      "Epoch 5885/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 17.7853 - val_loss: 97.4605\n",
      "Epoch 5886/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 16.6143 - val_loss: 109.8466\n",
      "Epoch 5887/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 17.7248 - val_loss: 97.4748\n",
      "Epoch 5888/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 16.7208 - val_loss: 80.8131\n",
      "Epoch 5889/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 17.0130 - val_loss: 93.7127\n",
      "Epoch 5890/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 21.1547 - val_loss: 102.2962\n",
      "Epoch 5891/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 16.3630 - val_loss: 109.0520\n",
      "Epoch 5892/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 18.1445 - val_loss: 108.3632\n",
      "Epoch 5893/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 15.8932 - val_loss: 101.6835\n",
      "Epoch 5894/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 15.1117 - val_loss: 102.3348\n",
      "Epoch 5895/10000\n",
      "96/96 [==============================] - 0s 506us/step - loss: 18.2553 - val_loss: 103.5298\n",
      "Epoch 5896/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 17.0647 - val_loss: 105.3304\n",
      "Epoch 5897/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 17.0973 - val_loss: 95.8831\n",
      "Epoch 5898/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 111.5668 - val_loss: 107.1266\n",
      "Epoch 5899/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 20.5088 - val_loss: 104.3402\n",
      "Epoch 5900/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 18.3735 - val_loss: 95.9095\n",
      "Epoch 5901/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 19.2248 - val_loss: 98.3211\n",
      "Epoch 5902/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 17.0790 - val_loss: 106.7772\n",
      "Epoch 5903/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 18.7640 - val_loss: 98.5557\n",
      "Epoch 5904/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 19.6289 - val_loss: 83.5027\n",
      "Epoch 5905/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 16.1540 - val_loss: 98.3285\n",
      "Epoch 5906/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 15.2663 - val_loss: 102.9766\n",
      "Epoch 5907/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 18.4849 - val_loss: 99.0395\n",
      "Epoch 5908/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 15.5732 - val_loss: 96.6970\n",
      "Epoch 5909/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 17.2977 - val_loss: 87.5462\n",
      "Epoch 5910/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 16.6880 - val_loss: 93.0899\n",
      "Epoch 5911/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 17.1732 - val_loss: 104.2253\n",
      "Epoch 5912/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 13.4101 - val_loss: 101.2329\n",
      "Epoch 5913/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 38.8971 - val_loss: 101.8971\n",
      "Epoch 5914/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 14.7955 - val_loss: 95.1898\n",
      "Epoch 5915/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 17.7973 - val_loss: 81.4642\n",
      "Epoch 5916/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 23.9296 - val_loss: 103.4527\n",
      "Epoch 5917/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 21.3043 - val_loss: 102.5497\n",
      "Epoch 5918/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 22.8564 - val_loss: 95.4841\n",
      "Epoch 5919/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 16.7638 - val_loss: 105.4034\n",
      "Epoch 5920/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 21.1319 - val_loss: 103.8107\n",
      "Epoch 5921/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 19.9035 - val_loss: 103.6662\n",
      "Epoch 5922/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 23.1398 - val_loss: 102.0037\n",
      "Epoch 5923/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 19.7960 - val_loss: 82.0399\n",
      "Epoch 5924/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 20.3652 - val_loss: 102.3294\n",
      "Epoch 5925/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 17.6133 - val_loss: 104.3522\n",
      "Epoch 5926/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 17.4709 - val_loss: 109.8062\n",
      "Epoch 5927/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 16.4388 - val_loss: 95.0897\n",
      "Epoch 5928/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 20.9427 - val_loss: 101.1599\n",
      "Epoch 5929/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 20.3672 - val_loss: 105.4741\n",
      "Epoch 5930/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 19.5285 - val_loss: 89.6297\n",
      "Epoch 5931/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 20.1189 - val_loss: 93.0112\n",
      "Epoch 5932/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 21.6566 - val_loss: 86.8790\n",
      "Epoch 5933/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 20.7831 - val_loss: 107.3352\n",
      "Epoch 5934/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 19.3395 - val_loss: 93.5805\n",
      "Epoch 5935/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 16.7123 - val_loss: 103.3166\n",
      "Epoch 5936/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 20.1286 - val_loss: 107.4201\n",
      "Epoch 5937/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 19.4462 - val_loss: 110.0831\n",
      "Epoch 5938/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 18.3513 - val_loss: 94.8587\n",
      "Epoch 5939/10000\n",
      "96/96 [==============================] - 0s 353us/step - loss: 18.5273 - val_loss: 95.7225\n",
      "Epoch 5940/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 15.2857 - val_loss: 91.6541\n",
      "Epoch 5941/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 19.2737 - val_loss: 90.7672\n",
      "Epoch 5942/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 17.4265 - val_loss: 102.4389\n",
      "Epoch 5943/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 17.4290 - val_loss: 108.5739\n",
      "Epoch 5944/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 24.9545 - val_loss: 101.1198\n",
      "Epoch 5945/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 17.9774 - val_loss: 89.4522\n",
      "Epoch 5946/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 18.9546 - val_loss: 103.9648\n",
      "Epoch 5947/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 18.9778 - val_loss: 97.9636\n",
      "Epoch 5948/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 20.7090 - val_loss: 96.1110\n",
      "Epoch 5949/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 20.1172 - val_loss: 103.1320\n",
      "Epoch 5950/10000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 16.8045 - val_loss: 86.8525\n",
      "Epoch 5951/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 17.2489 - val_loss: 93.0132\n",
      "Epoch 5952/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 17.6486 - val_loss: 100.6126\n",
      "Epoch 5953/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 15.9732 - val_loss: 105.7177\n",
      "Epoch 5954/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 16.4660 - val_loss: 112.5133\n",
      "Epoch 5955/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 14.9638 - val_loss: 103.8872\n",
      "Epoch 5956/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 16.8076 - val_loss: 102.5809\n",
      "Epoch 5957/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 14.5189 - val_loss: 107.8312\n",
      "Epoch 5958/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 16.8900 - val_loss: 107.8276\n",
      "Epoch 5959/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 17.9079 - val_loss: 96.3359\n",
      "Epoch 5960/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 15.1689 - val_loss: 101.0662\n",
      "Epoch 5961/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 17.8668 - val_loss: 107.9137\n",
      "Epoch 5962/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 19.9805 - val_loss: 93.3803\n",
      "Epoch 5963/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 92.8462 - val_loss: 104.5525\n",
      "Epoch 5964/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 17.5788 - val_loss: 87.2529\n",
      "Epoch 5965/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 21.8391 - val_loss: 103.9982\n",
      "Epoch 5966/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 20.8329 - val_loss: 100.5486\n",
      "Epoch 5967/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 21.9170 - val_loss: 100.2544\n",
      "Epoch 5968/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 20.4694 - val_loss: 109.5560\n",
      "Epoch 5969/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 17.5588 - val_loss: 89.6292\n",
      "Epoch 5970/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 21.7625 - val_loss: 96.9710\n",
      "Epoch 5971/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 21.6223 - val_loss: 96.5972\n",
      "Epoch 5972/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 18.7900 - val_loss: 84.4621\n",
      "Epoch 5973/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 22.3872 - val_loss: 92.1657\n",
      "Epoch 5974/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 22.2403 - val_loss: 102.4075\n",
      "Epoch 5975/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 467us/step - loss: 18.5657 - val_loss: 98.3290\n",
      "Epoch 5976/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 18.8005 - val_loss: 92.5935\n",
      "Epoch 5977/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 18.6453 - val_loss: 103.7060\n",
      "Epoch 5978/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 20.2406 - val_loss: 101.1721\n",
      "Epoch 5979/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 21.7305 - val_loss: 102.2622\n",
      "Epoch 5980/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 17.0126 - val_loss: 88.7920\n",
      "Epoch 5981/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 14.2678 - val_loss: 92.5272\n",
      "Epoch 5982/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 16.4923 - val_loss: 92.3970\n",
      "Epoch 5983/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 17.2875 - val_loss: 97.6815\n",
      "Epoch 5984/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 16.3483 - val_loss: 101.5667\n",
      "Epoch 5985/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 17.9110 - val_loss: 102.7771\n",
      "Epoch 5986/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 16.7411 - val_loss: 99.4072\n",
      "Epoch 5987/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 16.1559 - val_loss: 107.4923\n",
      "Epoch 5988/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 19.3849 - val_loss: 100.0480\n",
      "Epoch 5989/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 17.4121 - val_loss: 103.9074\n",
      "Epoch 5990/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 16.5571 - val_loss: 99.9642\n",
      "Epoch 5991/10000\n",
      "96/96 [==============================] - 0s 356us/step - loss: 17.4381 - val_loss: 101.5289\n",
      "Epoch 5992/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 15.6232 - val_loss: 95.8576\n",
      "Epoch 5993/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 13.5781 - val_loss: 99.1384\n",
      "Epoch 5994/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 14.0945 - val_loss: 91.1368\n",
      "Epoch 5995/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 15.3748 - val_loss: 111.7320\n",
      "Epoch 5996/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 17.1977 - val_loss: 88.7811\n",
      "Epoch 5997/10000\n",
      "96/96 [==============================] - 0s 504us/step - loss: 19.4694 - val_loss: 95.7464\n",
      "Epoch 5998/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 18.0013 - val_loss: 98.6551\n",
      "Epoch 5999/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 14.8163 - val_loss: 84.7240\n",
      "Epoch 6000/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 61.6943 - val_loss: 101.6453\n",
      "Epoch 6001/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 25.5535 - val_loss: 114.8712\n",
      "Epoch 6002/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 23.7255 - val_loss: 103.6497\n",
      "Epoch 6003/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 22.8898 - val_loss: 99.6480\n",
      "Epoch 6004/10000\n",
      "96/96 [==============================] - 0s 492us/step - loss: 28.3396 - val_loss: 85.6282\n",
      "Epoch 6005/10000\n",
      "96/96 [==============================] - 0s 528us/step - loss: 33.3242 - val_loss: 90.1399\n",
      "Epoch 6006/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 34.8255 - val_loss: 77.9574\n",
      "Epoch 6007/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 26.4973 - val_loss: 85.6473\n",
      "Epoch 6008/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 32.2538 - val_loss: 103.1570\n",
      "Epoch 6009/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 28.8577 - val_loss: 106.2409\n",
      "Epoch 6010/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 24.4595 - val_loss: 95.0698\n",
      "Epoch 6011/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 27.3852 - val_loss: 96.4331\n",
      "Epoch 6012/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 28.3871 - val_loss: 107.0927\n",
      "Epoch 6013/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 22.2344 - val_loss: 100.6748\n",
      "Epoch 6014/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 23.7960 - val_loss: 86.1103\n",
      "Epoch 6015/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 26.1033 - val_loss: 97.9881\n",
      "Epoch 6016/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 23.6684 - val_loss: 119.1420\n",
      "Epoch 6017/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 21.8078 - val_loss: 106.4538\n",
      "Epoch 6018/10000\n",
      "96/96 [==============================] - 0s 357us/step - loss: 22.7554 - val_loss: 105.1822\n",
      "Epoch 6019/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 26.3277 - val_loss: 101.0777\n",
      "Epoch 6020/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 24.6836 - val_loss: 104.0923\n",
      "Epoch 6021/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 23.8334 - val_loss: 108.8867\n",
      "Epoch 6022/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 24.3661 - val_loss: 109.7816\n",
      "Epoch 6023/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 25.6086 - val_loss: 100.0265\n",
      "Epoch 6024/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 21.4587 - val_loss: 76.5138\n",
      "Epoch 6025/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 19.3993 - val_loss: 96.7346\n",
      "Epoch 6026/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 22.3788 - val_loss: 102.9502\n",
      "Epoch 6027/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 20.8542 - val_loss: 84.1648\n",
      "Epoch 6028/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 20.8084 - val_loss: 106.7038\n",
      "Epoch 6029/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 18.8020 - val_loss: 99.3827\n",
      "Epoch 6030/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 21.5413 - val_loss: 82.9680\n",
      "Epoch 6031/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 21.6985 - val_loss: 109.3196\n",
      "Epoch 6032/10000\n",
      "96/96 [==============================] - 0s 488us/step - loss: 22.2487 - val_loss: 100.2489\n",
      "Epoch 6033/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 29.9622 - val_loss: 100.4886\n",
      "Epoch 6034/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 34.1217 - val_loss: 81.9324\n",
      "Epoch 6035/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 25.1677 - val_loss: 100.9552\n",
      "Epoch 6036/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 27.0296 - val_loss: 109.6899\n",
      "Epoch 6037/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 23.4102 - val_loss: 92.9794\n",
      "Epoch 6038/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 24.4547 - val_loss: 106.5923\n",
      "Epoch 6039/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 27.7993 - val_loss: 88.9798\n",
      "Epoch 6040/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 23.9553 - val_loss: 111.6918\n",
      "Epoch 6041/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 21.5283 - val_loss: 103.0519\n",
      "Epoch 6042/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 20.4687 - val_loss: 105.0910\n",
      "Epoch 6043/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 21.2948 - val_loss: 86.2446\n",
      "Epoch 6044/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 21.6482 - val_loss: 89.6625\n",
      "Epoch 6045/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 20.4442 - val_loss: 109.7046\n",
      "Epoch 6046/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 20.8141 - val_loss: 94.0365\n",
      "Epoch 6047/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 18.2739 - val_loss: 81.9337\n",
      "Epoch 6048/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 19.5032 - val_loss: 104.0721\n",
      "Epoch 6049/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 21.7943 - val_loss: 102.3815\n",
      "Epoch 6050/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 21.2370 - val_loss: 97.7367\n",
      "Epoch 6051/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 16.5868 - val_loss: 108.1228\n",
      "Epoch 6052/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 17.0162 - val_loss: 90.3030\n",
      "Epoch 6053/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 20.3898 - val_loss: 106.8844\n",
      "Epoch 6054/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 21.8740 - val_loss: 103.6060\n",
      "Epoch 6055/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 20.2445 - val_loss: 102.2179\n",
      "Epoch 6056/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 22.6098 - val_loss: 100.8787\n",
      "Epoch 6057/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 20.8387 - val_loss: 99.9868\n",
      "Epoch 6058/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 18.1395 - val_loss: 98.9606\n",
      "Epoch 6059/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 17.1925 - val_loss: 94.0582\n",
      "Epoch 6060/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 18.2578 - val_loss: 112.5801\n",
      "Epoch 6061/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 17.6612 - val_loss: 94.9081\n",
      "Epoch 6062/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 19.2644 - val_loss: 105.5706\n",
      "Epoch 6063/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 17.1586 - val_loss: 93.7682\n",
      "Epoch 6064/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 17.7278 - val_loss: 107.1448\n",
      "Epoch 6065/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 18.3598 - val_loss: 81.7940\n",
      "Epoch 6066/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 14.4232 - val_loss: 102.2478\n",
      "Epoch 6067/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 15.5159 - val_loss: 102.6670\n",
      "Epoch 6068/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 18.3153 - val_loss: 96.9256\n",
      "Epoch 6069/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 18.7329 - val_loss: 100.9473\n",
      "Epoch 6070/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 14.3566 - val_loss: 100.9943\n",
      "Epoch 6071/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 21.6185 - val_loss: 101.0089\n",
      "Epoch 6072/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 17.7510 - val_loss: 81.4467\n",
      "Epoch 6073/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 17.2236 - val_loss: 86.2409\n",
      "Epoch 6074/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 20.5923 - val_loss: 99.2520\n",
      "Epoch 6075/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 18.8951 - val_loss: 98.6786\n",
      "Epoch 6076/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 18.0003 - val_loss: 106.7063\n",
      "Epoch 6077/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 15.2250 - val_loss: 95.1561\n",
      "Epoch 6078/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 18.2705 - val_loss: 93.8700\n",
      "Epoch 6079/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 15.9363 - val_loss: 86.9144\n",
      "Epoch 6080/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 17.9048 - val_loss: 106.9498\n",
      "Epoch 6081/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 19.6518 - val_loss: 99.6718\n",
      "Epoch 6082/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 18.1602 - val_loss: 92.2705\n",
      "Epoch 6083/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 21.9301 - val_loss: 101.9766\n",
      "Epoch 6084/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 19.6847 - val_loss: 86.0417\n",
      "Epoch 6085/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 18.8490 - val_loss: 97.4850\n",
      "Epoch 6086/10000\n",
      "96/96 [==============================] - 0s 356us/step - loss: 17.0085 - val_loss: 96.2963\n",
      "Epoch 6087/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 14.4524 - val_loss: 104.5892\n",
      "Epoch 6088/10000\n",
      "96/96 [==============================] - 0s 481us/step - loss: 20.6033 - val_loss: 87.0646\n",
      "Epoch 6089/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 19.6189 - val_loss: 95.1155\n",
      "Epoch 6090/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 13.9127 - val_loss: 112.0099\n",
      "Epoch 6091/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 15.9333 - val_loss: 96.1872\n",
      "Epoch 6092/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 16.8118 - val_loss: 82.6313\n",
      "Epoch 6093/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 20.9221 - val_loss: 85.9269\n",
      "Epoch 6094/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 15.9054 - val_loss: 104.0264\n",
      "Epoch 6095/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 16.6054 - val_loss: 100.8875\n",
      "Epoch 6096/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 18.1807 - val_loss: 103.9859\n",
      "Epoch 6097/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 16.6665 - val_loss: 93.3429\n",
      "Epoch 6098/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 18.2526 - val_loss: 104.4157\n",
      "Epoch 6099/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 18.1045 - val_loss: 98.7269\n",
      "Epoch 6100/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 17.3509 - val_loss: 95.5751\n",
      "Epoch 6101/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 18.0226 - val_loss: 89.7773\n",
      "Epoch 6102/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 15.5335 - val_loss: 97.3487\n",
      "Epoch 6103/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 14.7682 - val_loss: 98.6173\n",
      "Epoch 6104/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 20.2856 - val_loss: 93.5453\n",
      "Epoch 6105/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 17.4752 - val_loss: 89.1878\n",
      "Epoch 6106/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 16.4501 - val_loss: 98.3334\n",
      "Epoch 6107/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 22.6073 - val_loss: 96.6003\n",
      "Epoch 6108/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 17.1435 - val_loss: 106.9874\n",
      "Epoch 6109/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 14.2402 - val_loss: 72.9817\n",
      "Epoch 6110/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 16.6737 - val_loss: 93.7887\n",
      "Epoch 6111/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 20.4095 - val_loss: 90.1607\n",
      "Epoch 6112/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 15.4507 - val_loss: 99.0880\n",
      "Epoch 6113/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 17.5954 - val_loss: 84.7713\n",
      "Epoch 6114/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 22.8123 - val_loss: 87.2052\n",
      "Epoch 6115/10000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 18.1564 - val_loss: 90.3242\n",
      "Epoch 6116/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 17.0185 - val_loss: 94.6790\n",
      "Epoch 6117/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 19.9494 - val_loss: 95.8090\n",
      "Epoch 6118/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 18.4946 - val_loss: 94.5893\n",
      "Epoch 6119/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 16.3270 - val_loss: 107.3407\n",
      "Epoch 6120/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 18.4605 - val_loss: 97.7385\n",
      "Epoch 6121/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 15.7150 - val_loss: 103.3072\n",
      "Epoch 6122/10000\n",
      "96/96 [==============================] - 0s 352us/step - loss: 18.3492 - val_loss: 97.1639\n",
      "Epoch 6123/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 17.7258 - val_loss: 105.3600\n",
      "Epoch 6124/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 19.5107 - val_loss: 109.9049\n",
      "Epoch 6125/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 17.3075 - val_loss: 102.2488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6126/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 17.3977 - val_loss: 96.8781\n",
      "Epoch 6127/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 16.1853 - val_loss: 89.8336\n",
      "Epoch 6128/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 17.2342 - val_loss: 99.9651\n",
      "Epoch 6129/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 15.3812 - val_loss: 103.1573\n",
      "Epoch 6130/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 15.7758 - val_loss: 108.9458\n",
      "Epoch 6131/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 18.3549 - val_loss: 110.4214\n",
      "Epoch 6132/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 17.1724 - val_loss: 100.4325\n",
      "Epoch 6133/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 15.0266 - val_loss: 110.9072\n",
      "Epoch 6134/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 17.1726 - val_loss: 99.0036\n",
      "Epoch 6135/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 14.3860 - val_loss: 85.3009\n",
      "Epoch 6136/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 17.5611 - val_loss: 98.7999\n",
      "Epoch 6137/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 19.6710 - val_loss: 103.9303\n",
      "Epoch 6138/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 16.1839 - val_loss: 90.5462\n",
      "Epoch 6139/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 16.5885 - val_loss: 99.3847\n",
      "Epoch 6140/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 17.7059 - val_loss: 113.3725\n",
      "Epoch 6141/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 14.8456 - val_loss: 112.6286\n",
      "Epoch 6142/10000\n",
      "96/96 [==============================] - 0s 499us/step - loss: 18.2743 - val_loss: 107.0088\n",
      "Epoch 6143/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 16.5285 - val_loss: 100.9215\n",
      "Epoch 6144/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 16.9393 - val_loss: 100.2777\n",
      "Epoch 6145/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 15.1454 - val_loss: 95.6202\n",
      "Epoch 6146/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 18.6111 - val_loss: 98.7845\n",
      "Epoch 6147/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 17.2141 - val_loss: 97.6718\n",
      "Epoch 6148/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 15.1309 - val_loss: 99.5012\n",
      "Epoch 6149/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 17.2760 - val_loss: 105.0862\n",
      "Epoch 6150/10000\n",
      "96/96 [==============================] - 0s 350us/step - loss: 16.3690 - val_loss: 100.7907\n",
      "Epoch 6151/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 15.8603 - val_loss: 104.0012\n",
      "Epoch 6152/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 18.8746 - val_loss: 98.3524\n",
      "Epoch 6153/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 17.1674 - val_loss: 103.1621\n",
      "Epoch 6154/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 15.2849 - val_loss: 119.3618\n",
      "Epoch 6155/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 15.2147 - val_loss: 102.1900\n",
      "Epoch 6156/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 16.2326 - val_loss: 110.8336\n",
      "Epoch 6157/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 15.0888 - val_loss: 109.3636\n",
      "Epoch 6158/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 19.8445 - val_loss: 95.9966\n",
      "Epoch 6159/10000\n",
      "96/96 [==============================] - 0s 352us/step - loss: 16.4831 - val_loss: 102.4715\n",
      "Epoch 6160/10000\n",
      "96/96 [==============================] - 0s 355us/step - loss: 14.9330 - val_loss: 111.5083\n",
      "Epoch 6161/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 15.2838 - val_loss: 93.4579\n",
      "Epoch 6162/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 13.8386 - val_loss: 98.5276\n",
      "Epoch 6163/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 16.1467 - val_loss: 78.6327\n",
      "Epoch 6164/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 16.1451 - val_loss: 108.3733\n",
      "Epoch 6165/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 15.8789 - val_loss: 102.5357\n",
      "Epoch 6166/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 16.2756 - val_loss: 107.9917\n",
      "Epoch 6167/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 15.7745 - val_loss: 88.1849\n",
      "Epoch 6168/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 17.1648 - val_loss: 97.6393\n",
      "Epoch 6169/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 18.5899 - val_loss: 97.3428\n",
      "Epoch 6170/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 16.7945 - val_loss: 99.9911\n",
      "Epoch 6171/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 19.5437 - val_loss: 104.6353\n",
      "Epoch 6172/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 21.1929 - val_loss: 103.7642\n",
      "Epoch 6173/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 16.4244 - val_loss: 106.2717\n",
      "Epoch 6174/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 15.6598 - val_loss: 97.5564\n",
      "Epoch 6175/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 15.1763 - val_loss: 99.7240\n",
      "Epoch 6176/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 15.4288 - val_loss: 86.2212\n",
      "Epoch 6177/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 14.3416 - val_loss: 95.5999\n",
      "Epoch 6178/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 20.6668 - val_loss: 110.7119\n",
      "Epoch 6179/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 15.4608 - val_loss: 99.2908\n",
      "Epoch 6180/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 14.2911 - val_loss: 101.3588\n",
      "Epoch 6181/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 14.7553 - val_loss: 102.3763\n",
      "Epoch 6182/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 22.0005 - val_loss: 93.4278\n",
      "Epoch 6183/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 65.4750 - val_loss: 97.3914\n",
      "Epoch 6184/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 15.6762 - val_loss: 108.1009\n",
      "Epoch 6185/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 15.7551 - val_loss: 104.9571\n",
      "Epoch 6186/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 14.6174 - val_loss: 76.5285\n",
      "Epoch 6187/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 20.1749 - val_loss: 96.2037\n",
      "Epoch 6188/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 19.7498 - val_loss: 77.7047\n",
      "Epoch 6189/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 21.8572 - val_loss: 104.8913\n",
      "Epoch 6190/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 20.7914 - val_loss: 106.7908\n",
      "Epoch 6191/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 20.1736 - val_loss: 107.3833\n",
      "Epoch 6192/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 17.7978 - val_loss: 98.8907\n",
      "Epoch 6193/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 17.1663 - val_loss: 94.6940\n",
      "Epoch 6194/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 18.7844 - val_loss: 94.8352\n",
      "Epoch 6195/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 18.0515 - val_loss: 101.2619\n",
      "Epoch 6196/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 15.5790 - val_loss: 88.4123\n",
      "Epoch 6197/10000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 15.9661 - val_loss: 94.9376\n",
      "Epoch 6198/10000\n",
      "96/96 [==============================] - 0s 497us/step - loss: 20.3628 - val_loss: 98.9147\n",
      "Epoch 6199/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 20.9102 - val_loss: 109.4836\n",
      "Epoch 6200/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 20.4478 - val_loss: 99.6318\n",
      "Epoch 6201/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 14.3194 - val_loss: 98.8751\n",
      "Epoch 6202/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 14.1288 - val_loss: 99.2112\n",
      "Epoch 6203/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 17.5828 - val_loss: 91.3544\n",
      "Epoch 6204/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 14.3103 - val_loss: 99.0966\n",
      "Epoch 6205/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 17.6147 - val_loss: 109.2686\n",
      "Epoch 6206/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 17.3716 - val_loss: 102.8276\n",
      "Epoch 6207/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 16.2325 - val_loss: 99.7950\n",
      "Epoch 6208/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 15.5743 - val_loss: 112.4963\n",
      "Epoch 6209/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 17.5663 - val_loss: 96.3070\n",
      "Epoch 6210/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 17.9070 - val_loss: 111.8686\n",
      "Epoch 6211/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 14.9066 - val_loss: 100.2952\n",
      "Epoch 6212/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 16.7421 - val_loss: 112.4939\n",
      "Epoch 6213/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 14.6964 - val_loss: 103.7465\n",
      "Epoch 6214/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 17.6381 - val_loss: 101.4791\n",
      "Epoch 6215/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 27.0114 - val_loss: 116.1293\n",
      "Epoch 6216/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 25.9402 - val_loss: 99.9568\n",
      "Epoch 6217/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 42.1301 - val_loss: 88.6046\n",
      "Epoch 6218/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 37.2545 - val_loss: 107.6825\n",
      "Epoch 6219/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 33.5691 - val_loss: 105.4419\n",
      "Epoch 6220/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 29.8829 - val_loss: 104.3822\n",
      "Epoch 6221/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 23.4008 - val_loss: 83.1166\n",
      "Epoch 6222/10000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 23.6112 - val_loss: 108.2805\n",
      "Epoch 6223/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 19.0040 - val_loss: 113.4233\n",
      "Epoch 6224/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 20.7599 - val_loss: 85.3964\n",
      "Epoch 6225/10000\n",
      "96/96 [==============================] - 0s 569us/step - loss: 19.4105 - val_loss: 116.2366\n",
      "Epoch 6226/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 17.1418 - val_loss: 111.1251\n",
      "Epoch 6227/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 18.3331 - val_loss: 94.6809\n",
      "Epoch 6228/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 17.3705 - val_loss: 114.2571\n",
      "Epoch 6229/10000\n",
      "96/96 [==============================] - 0s 515us/step - loss: 17.9540 - val_loss: 116.1278\n",
      "Epoch 6230/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 16.5391 - val_loss: 101.4201\n",
      "Epoch 6231/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 16.4731 - val_loss: 97.9335\n",
      "Epoch 6232/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 15.6161 - val_loss: 112.8698\n",
      "Epoch 6233/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 16.9563 - val_loss: 109.8943\n",
      "Epoch 6234/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 19.9248 - val_loss: 113.4422\n",
      "Epoch 6235/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 18.1961 - val_loss: 108.9209\n",
      "Epoch 6236/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 22.6959 - val_loss: 117.6209\n",
      "Epoch 6237/10000\n",
      "96/96 [==============================] - 0s 521us/step - loss: 20.1047 - val_loss: 96.0523\n",
      "Epoch 6238/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 17.4191 - val_loss: 111.6714\n",
      "Epoch 6239/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 16.4164 - val_loss: 110.6829\n",
      "Epoch 6240/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 17.4866 - val_loss: 110.4947\n",
      "Epoch 6241/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 19.1434 - val_loss: 101.2952\n",
      "Epoch 6242/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 15.8326 - val_loss: 96.2565\n",
      "Epoch 6243/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 15.8634 - val_loss: 107.9581\n",
      "Epoch 6244/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 20.2792 - val_loss: 117.8141\n",
      "Epoch 6245/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 16.2812 - val_loss: 102.2189\n",
      "Epoch 6246/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 12.3872 - val_loss: 109.7071\n",
      "Epoch 6247/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 17.1796 - val_loss: 105.4154\n",
      "Epoch 6248/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 15.6531 - val_loss: 108.2245\n",
      "Epoch 6249/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 16.2198 - val_loss: 105.3791\n",
      "Epoch 6250/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 14.5337 - val_loss: 102.8030\n",
      "Epoch 6251/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 16.3036 - val_loss: 104.6284\n",
      "Epoch 6252/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 17.4731 - val_loss: 109.2173\n",
      "Epoch 6253/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 14.8436 - val_loss: 113.8117\n",
      "Epoch 6254/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 15.0570 - val_loss: 103.3154\n",
      "Epoch 6255/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 19.2536 - val_loss: 83.6962\n",
      "Epoch 6256/10000\n",
      "96/96 [==============================] - 0s 513us/step - loss: 14.1047 - val_loss: 85.3893\n",
      "Epoch 6257/10000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 15.1049 - val_loss: 100.2942\n",
      "Epoch 6258/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 14.9755 - val_loss: 103.8842\n",
      "Epoch 6259/10000\n",
      "96/96 [==============================] - 0s 331us/step - loss: 14.9430 - val_loss: 103.7779\n",
      "Epoch 6260/10000\n",
      "96/96 [==============================] - 0s 312us/step - loss: 17.0467 - val_loss: 106.0294\n",
      "Epoch 6261/10000\n",
      "96/96 [==============================] - 0s 321us/step - loss: 13.7968 - val_loss: 104.2939\n",
      "Epoch 6262/10000\n",
      "96/96 [==============================] - 0s 323us/step - loss: 15.1358 - val_loss: 115.7346\n",
      "Epoch 6263/10000\n",
      "96/96 [==============================] - 0s 328us/step - loss: 15.8574 - val_loss: 109.4540\n",
      "Epoch 6264/10000\n",
      "96/96 [==============================] - 0s 330us/step - loss: 17.0067 - val_loss: 119.8534\n",
      "Epoch 6265/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 13.9377 - val_loss: 105.9062\n",
      "Epoch 6266/10000\n",
      "96/96 [==============================] - 0s 352us/step - loss: 14.6616 - val_loss: 112.9358\n",
      "Epoch 6267/10000\n",
      "96/96 [==============================] - 0s 335us/step - loss: 13.6441 - val_loss: 113.6796\n",
      "Epoch 6268/10000\n",
      "96/96 [==============================] - 0s 339us/step - loss: 17.9632 - val_loss: 98.2736\n",
      "Epoch 6269/10000\n",
      "96/96 [==============================] - 0s 335us/step - loss: 17.9844 - val_loss: 101.6682\n",
      "Epoch 6270/10000\n",
      "96/96 [==============================] - 0s 353us/step - loss: 18.8003 - val_loss: 103.1963\n",
      "Epoch 6271/10000\n",
      "96/96 [==============================] - 0s 328us/step - loss: 19.7151 - val_loss: 97.2749\n",
      "Epoch 6272/10000\n",
      "96/96 [==============================] - 0s 323us/step - loss: 19.9607 - val_loss: 113.9164\n",
      "Epoch 6273/10000\n",
      "96/96 [==============================] - 0s 339us/step - loss: 20.0573 - val_loss: 105.1006\n",
      "Epoch 6274/10000\n",
      "96/96 [==============================] - 0s 336us/step - loss: 18.6214 - val_loss: 97.6677\n",
      "Epoch 6275/10000\n",
      "96/96 [==============================] - 0s 342us/step - loss: 14.5866 - val_loss: 121.2694\n",
      "Epoch 6276/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 436us/step - loss: 16.8853 - val_loss: 110.0549\n",
      "Epoch 6277/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 17.8914 - val_loss: 109.5364\n",
      "Epoch 6278/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 17.1978 - val_loss: 90.5610\n",
      "Epoch 6279/10000\n",
      "96/96 [==============================] - 0s 520us/step - loss: 18.2148 - val_loss: 91.8970\n",
      "Epoch 6280/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 18.5938 - val_loss: 96.6111\n",
      "Epoch 6281/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 23.0521 - val_loss: 91.4396\n",
      "Epoch 6282/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 17.3564 - val_loss: 103.3966\n",
      "Epoch 6283/10000\n",
      "96/96 [==============================] - 0s 353us/step - loss: 31.3909 - val_loss: 113.6962\n",
      "Epoch 6284/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 50.3503 - val_loss: 116.7425\n",
      "Epoch 6285/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 62.9581 - val_loss: 105.0252\n",
      "Epoch 6286/10000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 37.9285 - val_loss: 77.4949\n",
      "Epoch 6287/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 40.8613 - val_loss: 108.4905\n",
      "Epoch 6288/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 53.2705 - val_loss: 105.4400\n",
      "Epoch 6289/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 59.0864 - val_loss: 88.8967\n",
      "Epoch 6290/10000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 61.2644 - val_loss: 93.3679\n",
      "Epoch 6291/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 51.3662 - val_loss: 87.6490\n",
      "Epoch 6292/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 44.6779 - val_loss: 95.0975\n",
      "Epoch 6293/10000\n",
      "96/96 [==============================] - 0s 493us/step - loss: 42.2430 - val_loss: 88.5934\n",
      "Epoch 6294/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 42.2889 - val_loss: 77.9832\n",
      "Epoch 6295/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 44.4192 - val_loss: 107.9605\n",
      "Epoch 6296/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 39.7499 - val_loss: 98.5923\n",
      "Epoch 6297/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 36.3089 - val_loss: 101.5980\n",
      "Epoch 6298/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 34.5817 - val_loss: 98.8435\n",
      "Epoch 6299/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 39.1695 - val_loss: 78.0252\n",
      "Epoch 6300/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 37.4596 - val_loss: 74.7950\n",
      "Epoch 6301/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 32.7462 - val_loss: 85.7177\n",
      "Epoch 6302/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 30.0671 - val_loss: 97.4306\n",
      "Epoch 6303/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 30.9734 - val_loss: 100.8943\n",
      "Epoch 6304/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 35.9299 - val_loss: 112.4490\n",
      "Epoch 6305/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 29.2857 - val_loss: 98.0641\n",
      "Epoch 6306/10000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 25.0515 - val_loss: 89.0785\n",
      "Epoch 6307/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 26.3990 - val_loss: 92.6221\n",
      "Epoch 6308/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 39.9487 - val_loss: 93.2909\n",
      "Epoch 6309/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 43.2702 - val_loss: 108.0785\n",
      "Epoch 6310/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 43.2091 - val_loss: 91.9969\n",
      "Epoch 6311/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 40.0996 - val_loss: 109.8485\n",
      "Epoch 6312/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 41.8573 - val_loss: 89.2348\n",
      "Epoch 6313/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 52.2614 - val_loss: 88.6610\n",
      "Epoch 6314/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 42.2736 - val_loss: 103.5763\n",
      "Epoch 6315/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 45.0762 - val_loss: 99.1017\n",
      "Epoch 6316/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 43.3100 - val_loss: 107.2093\n",
      "Epoch 6317/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 42.6784 - val_loss: 91.3697\n",
      "Epoch 6318/10000\n",
      "96/96 [==============================] - 0s 520us/step - loss: 36.3017 - val_loss: 89.5881\n",
      "Epoch 6319/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 34.5928 - val_loss: 95.7473\n",
      "Epoch 6320/10000\n",
      "96/96 [==============================] - 0s 502us/step - loss: 32.9073 - val_loss: 85.6970\n",
      "Epoch 6321/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 35.6270 - val_loss: 90.7587\n",
      "Epoch 6322/10000\n",
      "96/96 [==============================] - 0s 523us/step - loss: 36.4679 - val_loss: 93.6320\n",
      "Epoch 6323/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 37.4486 - val_loss: 80.4967\n",
      "Epoch 6324/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 28.4240 - val_loss: 80.4437\n",
      "Epoch 6325/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 31.5212 - val_loss: 114.9688\n",
      "Epoch 6326/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 28.7699 - val_loss: 109.2078\n",
      "Epoch 6327/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 30.4203 - val_loss: 95.9428\n",
      "Epoch 6328/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 28.0620 - val_loss: 101.0099\n",
      "Epoch 6329/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 27.6829 - val_loss: 93.1371\n",
      "Epoch 6330/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 29.7710 - val_loss: 104.0205\n",
      "Epoch 6331/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 26.4084 - val_loss: 104.7272\n",
      "Epoch 6332/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 31.5768 - val_loss: 108.8528\n",
      "Epoch 6333/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 26.7725 - val_loss: 94.4231\n",
      "Epoch 6334/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 24.9600 - val_loss: 100.9605\n",
      "Epoch 6335/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 26.1432 - val_loss: 115.2586\n",
      "Epoch 6336/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 26.1120 - val_loss: 82.4683\n",
      "Epoch 6337/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 23.1809 - val_loss: 85.0122\n",
      "Epoch 6338/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 24.2917 - val_loss: 97.7577\n",
      "Epoch 6339/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 23.8917 - val_loss: 93.4048\n",
      "Epoch 6340/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 34.8478 - val_loss: 93.1718\n",
      "Epoch 6341/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 33.0508 - val_loss: 108.8585\n",
      "Epoch 6342/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 29.2479 - val_loss: 101.9301\n",
      "Epoch 6343/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 35.6910 - val_loss: 83.0508\n",
      "Epoch 6344/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 24.0670 - val_loss: 99.3154\n",
      "Epoch 6345/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 23.7830 - val_loss: 105.9534\n",
      "Epoch 6346/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 27.1011 - val_loss: 106.7109\n",
      "Epoch 6347/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 26.3506 - val_loss: 97.5049\n",
      "Epoch 6348/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 22.3522 - val_loss: 91.6889\n",
      "Epoch 6349/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 24.3761 - val_loss: 97.7276\n",
      "Epoch 6350/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 21.0575 - val_loss: 107.0487\n",
      "Epoch 6351/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 25.9655 - val_loss: 102.4429\n",
      "Epoch 6352/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 21.9544 - val_loss: 104.9687\n",
      "Epoch 6353/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 19.0107 - val_loss: 107.0660\n",
      "Epoch 6354/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 22.7076 - val_loss: 108.2807\n",
      "Epoch 6355/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 21.8015 - val_loss: 108.1330\n",
      "Epoch 6356/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 26.3553 - val_loss: 88.2537\n",
      "Epoch 6357/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 21.2709 - val_loss: 105.2841\n",
      "Epoch 6358/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 20.1744 - val_loss: 100.3283\n",
      "Epoch 6359/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 23.5305 - val_loss: 95.2433\n",
      "Epoch 6360/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 19.9662 - val_loss: 100.3970\n",
      "Epoch 6361/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 20.7200 - val_loss: 101.9116\n",
      "Epoch 6362/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 19.2479 - val_loss: 99.0726\n",
      "Epoch 6363/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 16.7055 - val_loss: 104.5025\n",
      "Epoch 6364/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 23.6622 - val_loss: 89.8465\n",
      "Epoch 6365/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 19.9659 - val_loss: 101.8415\n",
      "Epoch 6366/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 15.4924 - val_loss: 101.0413\n",
      "Epoch 6367/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 23.6021 - val_loss: 99.7822\n",
      "Epoch 6368/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 22.0618 - val_loss: 101.4618\n",
      "Epoch 6369/10000\n",
      "96/96 [==============================] - 0s 500us/step - loss: 17.3245 - val_loss: 100.9208\n",
      "Epoch 6370/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 15.5912 - val_loss: 110.3692\n",
      "Epoch 6371/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 21.6187 - val_loss: 83.5043\n",
      "Epoch 6372/10000\n",
      "96/96 [==============================] - 0s 509us/step - loss: 17.7849 - val_loss: 100.3457\n",
      "Epoch 6373/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 19.5301 - val_loss: 109.0633\n",
      "Epoch 6374/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 17.8217 - val_loss: 89.5479\n",
      "Epoch 6375/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 16.7051 - val_loss: 104.8565\n",
      "Epoch 6376/10000\n",
      "96/96 [==============================] - 0s 496us/step - loss: 18.5625 - val_loss: 101.6600\n",
      "Epoch 6377/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 19.2639 - val_loss: 84.5641\n",
      "Epoch 6378/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 17.0263 - val_loss: 99.1904\n",
      "Epoch 6379/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 18.4336 - val_loss: 103.7032\n",
      "Epoch 6380/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 15.5606 - val_loss: 111.6962\n",
      "Epoch 6381/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 16.9110 - val_loss: 106.1095\n",
      "Epoch 6382/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 13.5521 - val_loss: 82.8779\n",
      "Epoch 6383/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 17.2926 - val_loss: 92.9611\n",
      "Epoch 6384/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 20.2480 - val_loss: 115.1579\n",
      "Epoch 6385/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 18.7033 - val_loss: 101.0729\n",
      "Epoch 6386/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 15.4918 - val_loss: 108.4641\n",
      "Epoch 6387/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 17.4592 - val_loss: 88.2785\n",
      "Epoch 6388/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 15.0039 - val_loss: 113.1013\n",
      "Epoch 6389/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 16.1916 - val_loss: 112.0806\n",
      "Epoch 6390/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 15.4611 - val_loss: 103.0802\n",
      "Epoch 6391/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 19.4454 - val_loss: 108.2422\n",
      "Epoch 6392/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 16.3725 - val_loss: 97.2037\n",
      "Epoch 6393/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 16.4508 - val_loss: 91.8667\n",
      "Epoch 6394/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 16.7671 - val_loss: 86.1472\n",
      "Epoch 6395/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 17.7291 - val_loss: 94.4255\n",
      "Epoch 6396/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 20.1824 - val_loss: 97.0325\n",
      "Epoch 6397/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 17.8526 - val_loss: 105.8072\n",
      "Epoch 6398/10000\n",
      "96/96 [==============================] - 0s 484us/step - loss: 20.4111 - val_loss: 106.8636\n",
      "Epoch 6399/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 15.7284 - val_loss: 112.6145\n",
      "Epoch 6400/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 14.0769 - val_loss: 109.3919\n",
      "Epoch 6401/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 18.2086 - val_loss: 106.1709\n",
      "Epoch 6402/10000\n",
      "96/96 [==============================] - 0s 512us/step - loss: 16.6765 - val_loss: 112.9142\n",
      "Epoch 6403/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 16.0253 - val_loss: 105.3048\n",
      "Epoch 6404/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 16.9967 - val_loss: 92.5996\n",
      "Epoch 6405/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 16.7470 - val_loss: 110.7273\n",
      "Epoch 6406/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 14.1962 - val_loss: 106.3946\n",
      "Epoch 6407/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 16.1115 - val_loss: 93.6666\n",
      "Epoch 6408/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 17.3194 - val_loss: 107.4002\n",
      "Epoch 6409/10000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 14.5757 - val_loss: 115.2408\n",
      "Epoch 6410/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 13.3522 - val_loss: 113.6531\n",
      "Epoch 6411/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 18.2979 - val_loss: 81.2506\n",
      "Epoch 6412/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 16.0553 - val_loss: 93.0201\n",
      "Epoch 6413/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 13.9802 - val_loss: 102.9697\n",
      "Epoch 6414/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 17.7636 - val_loss: 105.2882\n",
      "Epoch 6415/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 14.6154 - val_loss: 109.5561\n",
      "Epoch 6416/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 17.8730 - val_loss: 104.7813\n",
      "Epoch 6417/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 16.1280 - val_loss: 101.6882\n",
      "Epoch 6418/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 15.1546 - val_loss: 105.1469\n",
      "Epoch 6419/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 14.4076 - val_loss: 98.5435\n",
      "Epoch 6420/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 15.8808 - val_loss: 112.1812\n",
      "Epoch 6421/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 14.8909 - val_loss: 109.9208\n",
      "Epoch 6422/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 17.8611 - val_loss: 110.2255\n",
      "Epoch 6423/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 14.5205 - val_loss: 114.5445\n",
      "Epoch 6424/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 18.7134 - val_loss: 110.4665\n",
      "Epoch 6425/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 19.3690 - val_loss: 110.3962\n",
      "Epoch 6426/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 407us/step - loss: 19.0324 - val_loss: 112.7396\n",
      "Epoch 6427/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 22.6829 - val_loss: 93.0545\n",
      "Epoch 6428/10000\n",
      "96/96 [==============================] - 0s 606us/step - loss: 37.4694 - val_loss: 101.6800\n",
      "Epoch 6429/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 31.4119 - val_loss: 101.0718\n",
      "Epoch 6430/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 34.3245 - val_loss: 96.4686\n",
      "Epoch 6431/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 36.7940 - val_loss: 91.3665\n",
      "Epoch 6432/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 40.0704 - val_loss: 96.9998\n",
      "Epoch 6433/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 25.9499 - val_loss: 97.6309\n",
      "Epoch 6434/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 37.3773 - val_loss: 79.7901\n",
      "Epoch 6435/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 30.0239 - val_loss: 101.5860\n",
      "Epoch 6436/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 30.0868 - val_loss: 114.4308\n",
      "Epoch 6437/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 28.1969 - val_loss: 112.0560\n",
      "Epoch 6438/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 27.6160 - val_loss: 108.0127\n",
      "Epoch 6439/10000\n",
      "96/96 [==============================] - 0s 482us/step - loss: 28.6452 - val_loss: 104.4622\n",
      "Epoch 6440/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 29.3191 - val_loss: 107.5627\n",
      "Epoch 6441/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 23.2799 - val_loss: 107.2154\n",
      "Epoch 6442/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 25.4857 - val_loss: 107.8914\n",
      "Epoch 6443/10000\n",
      "96/96 [==============================] - 0s 502us/step - loss: 24.9331 - val_loss: 95.5828\n",
      "Epoch 6444/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 24.5162 - val_loss: 104.1798\n",
      "Epoch 6445/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 23.3028 - val_loss: 96.2145\n",
      "Epoch 6446/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 24.2624 - val_loss: 106.8410\n",
      "Epoch 6447/10000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 23.5361 - val_loss: 98.6430\n",
      "Epoch 6448/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 23.9450 - val_loss: 107.6070\n",
      "Epoch 6449/10000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 23.3951 - val_loss: 105.5615\n",
      "Epoch 6450/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 20.1918 - val_loss: 88.8641\n",
      "Epoch 6451/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 18.1686 - val_loss: 111.7782\n",
      "Epoch 6452/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 23.8621 - val_loss: 96.4614\n",
      "Epoch 6453/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 22.6889 - val_loss: 104.2927\n",
      "Epoch 6454/10000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 20.2628 - val_loss: 111.5325\n",
      "Epoch 6455/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 20.4152 - val_loss: 106.9261\n",
      "Epoch 6456/10000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 20.6468 - val_loss: 107.7583\n",
      "Epoch 6457/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 19.0525 - val_loss: 101.8153\n",
      "Epoch 6458/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 20.1654 - val_loss: 105.4059\n",
      "Epoch 6459/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 20.3946 - val_loss: 111.1518\n",
      "Epoch 6460/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 14.3291 - val_loss: 101.4698\n",
      "Epoch 6461/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 18.5796 - val_loss: 110.0496\n",
      "Epoch 6462/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 19.3611 - val_loss: 109.2920\n",
      "Epoch 6463/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 18.6515 - val_loss: 111.9187\n",
      "Epoch 6464/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 19.2329 - val_loss: 121.8226\n",
      "Epoch 6465/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 21.7551 - val_loss: 117.8588\n",
      "Epoch 6466/10000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 16.8714 - val_loss: 103.1399\n",
      "Epoch 6467/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 17.8147 - val_loss: 108.1174\n",
      "Epoch 6468/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 17.9604 - val_loss: 99.8641\n",
      "Epoch 6469/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 18.2200 - val_loss: 113.8426\n",
      "Epoch 6470/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 16.5099 - val_loss: 106.7151\n",
      "Epoch 6471/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 17.5955 - val_loss: 115.4712\n",
      "Epoch 6472/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 15.7549 - val_loss: 115.8100\n",
      "Epoch 6473/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 17.0428 - val_loss: 112.2720\n",
      "Epoch 6474/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 17.2419 - val_loss: 114.5973\n",
      "Epoch 6475/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 16.9994 - val_loss: 112.0331\n",
      "Epoch 6476/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 24.3153 - val_loss: 104.4760\n",
      "Epoch 6477/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 21.9825 - val_loss: 102.1087\n",
      "Epoch 6478/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 29.6588 - val_loss: 108.5986\n",
      "Epoch 6479/10000\n",
      "96/96 [==============================] - 0s 502us/step - loss: 19.8235 - val_loss: 108.7047\n",
      "Epoch 6480/10000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 44.0720 - val_loss: 105.1241\n",
      "Epoch 6481/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 27.8017 - val_loss: 79.0756\n",
      "Epoch 6482/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 38.4375 - val_loss: 90.6971\n",
      "Epoch 6483/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 36.0025 - val_loss: 89.5295\n",
      "Epoch 6484/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 116.5750 - val_loss: 94.3478\n",
      "Epoch 6485/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 34.7793 - val_loss: 96.4128\n",
      "Epoch 6486/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 34.6689 - val_loss: 103.3440\n",
      "Epoch 6487/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 34.0885 - val_loss: 100.7066\n",
      "Epoch 6488/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 35.3293 - val_loss: 107.7877\n",
      "Epoch 6489/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 30.7466 - val_loss: 104.6970\n",
      "Epoch 6490/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 33.1023 - val_loss: 109.2044\n",
      "Epoch 6491/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 31.1034 - val_loss: 88.4058\n",
      "Epoch 6492/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 29.8447 - val_loss: 95.9712\n",
      "Epoch 6493/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 27.4066 - val_loss: 104.0756\n",
      "Epoch 6494/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 25.4745 - val_loss: 95.7282\n",
      "Epoch 6495/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 25.3781 - val_loss: 111.9175\n",
      "Epoch 6496/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 23.5280 - val_loss: 90.4882\n",
      "Epoch 6497/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 22.9061 - val_loss: 101.4587\n",
      "Epoch 6498/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 19.8465 - val_loss: 111.0826\n",
      "Epoch 6499/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 22.6268 - val_loss: 115.3140\n",
      "Epoch 6500/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 20.6029 - val_loss: 98.7361\n",
      "Epoch 6501/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 21.4823 - val_loss: 95.8115\n",
      "Epoch 6502/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 20.7859 - val_loss: 101.0900\n",
      "Epoch 6503/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 17.7310 - val_loss: 100.4113\n",
      "Epoch 6504/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 23.5605 - val_loss: 102.5641\n",
      "Epoch 6505/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 18.0472 - val_loss: 105.5235\n",
      "Epoch 6506/10000\n",
      "96/96 [==============================] - 0s 511us/step - loss: 19.5208 - val_loss: 101.1350\n",
      "Epoch 6507/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 20.2591 - val_loss: 107.3099\n",
      "Epoch 6508/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 20.8797 - val_loss: 94.7373\n",
      "Epoch 6509/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 17.8572 - val_loss: 105.5236\n",
      "Epoch 6510/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 20.7993 - val_loss: 111.2639\n",
      "Epoch 6511/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 26.6168 - val_loss: 111.4992\n",
      "Epoch 6512/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 19.4636 - val_loss: 91.0232\n",
      "Epoch 6513/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 19.6172 - val_loss: 100.8102\n",
      "Epoch 6514/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 18.4323 - val_loss: 124.1824\n",
      "Epoch 6515/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 20.2549 - val_loss: 101.0410\n",
      "Epoch 6516/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 15.5770 - val_loss: 99.8018\n",
      "Epoch 6517/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 17.1229 - val_loss: 121.1831\n",
      "Epoch 6518/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 17.6083 - val_loss: 112.5374\n",
      "Epoch 6519/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 16.7139 - val_loss: 95.7559\n",
      "Epoch 6520/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 20.7473 - val_loss: 81.1894\n",
      "Epoch 6521/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 18.6835 - val_loss: 83.3991\n",
      "Epoch 6522/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 17.9156 - val_loss: 114.8646\n",
      "Epoch 6523/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 18.0391 - val_loss: 116.6076\n",
      "Epoch 6524/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 17.1055 - val_loss: 117.7077\n",
      "Epoch 6525/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 19.3688 - val_loss: 105.2360\n",
      "Epoch 6526/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 14.8524 - val_loss: 100.4812\n",
      "Epoch 6527/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 17.6331 - val_loss: 112.9277\n",
      "Epoch 6528/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 13.3978 - val_loss: 105.7489\n",
      "Epoch 6529/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 19.4840 - val_loss: 104.8720\n",
      "Epoch 6530/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 13.8916 - val_loss: 104.8166\n",
      "Epoch 6531/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 17.6278 - val_loss: 90.5340\n",
      "Epoch 6532/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 15.3562 - val_loss: 92.0343\n",
      "Epoch 6533/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 12.6329 - val_loss: 118.4771\n",
      "Epoch 6534/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 21.8111 - val_loss: 113.6622\n",
      "Epoch 6535/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 15.7716 - val_loss: 97.3892\n",
      "Epoch 6536/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 13.1809 - val_loss: 106.6906\n",
      "Epoch 6537/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 17.9540 - val_loss: 107.3864\n",
      "Epoch 6538/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 19.0566 - val_loss: 109.8368\n",
      "Epoch 6539/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 16.6458 - val_loss: 102.9056\n",
      "Epoch 6540/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 15.3100 - val_loss: 118.4554\n",
      "Epoch 6541/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 14.2651 - val_loss: 101.5976\n",
      "Epoch 6542/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 13.8904 - val_loss: 97.5435\n",
      "Epoch 6543/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 15.8415 - val_loss: 118.6974\n",
      "Epoch 6544/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 15.3309 - val_loss: 108.8768\n",
      "Epoch 6545/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 17.5724 - val_loss: 110.5533\n",
      "Epoch 6546/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 16.2687 - val_loss: 125.1590\n",
      "Epoch 6547/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 19.4254 - val_loss: 102.7929\n",
      "Epoch 6548/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 13.6328 - val_loss: 100.6047\n",
      "Epoch 6549/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 18.2380 - val_loss: 106.9582\n",
      "Epoch 6550/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 14.3486 - val_loss: 99.2185\n",
      "Epoch 6551/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 14.4983 - val_loss: 107.4675\n",
      "Epoch 6552/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 17.0162 - val_loss: 103.9920\n",
      "Epoch 6553/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 13.0847 - val_loss: 107.4882\n",
      "Epoch 6554/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 14.4980 - val_loss: 109.8494\n",
      "Epoch 6555/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 17.3927 - val_loss: 107.9602\n",
      "Epoch 6556/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 13.5314 - val_loss: 104.1186\n",
      "Epoch 6557/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 13.7468 - val_loss: 115.1366\n",
      "Epoch 6558/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 13.1659 - val_loss: 103.0996\n",
      "Epoch 6559/10000\n",
      "96/96 [==============================] - 0s 498us/step - loss: 14.8542 - val_loss: 98.9509\n",
      "Epoch 6560/10000\n",
      "96/96 [==============================] - 0s 502us/step - loss: 14.2164 - val_loss: 99.2986\n",
      "Epoch 6561/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 13.9569 - val_loss: 104.4752\n",
      "Epoch 6562/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 16.3289 - val_loss: 108.1262\n",
      "Epoch 6563/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 17.6796 - val_loss: 96.7475\n",
      "Epoch 6564/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 14.2968 - val_loss: 112.2573\n",
      "Epoch 6565/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 12.6131 - val_loss: 95.7509\n",
      "Epoch 6566/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 15.4486 - val_loss: 103.8157\n",
      "Epoch 6567/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 11.1309 - val_loss: 90.9413\n",
      "Epoch 6568/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 14.6513 - val_loss: 104.0554\n",
      "Epoch 6569/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 11.6434 - val_loss: 119.5045\n",
      "Epoch 6570/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 13.8789 - val_loss: 108.7904\n",
      "Epoch 6571/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 12.7212 - val_loss: 116.2559\n",
      "Epoch 6572/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 15.6103 - val_loss: 119.4198\n",
      "Epoch 6573/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 14.2066 - val_loss: 102.2122\n",
      "Epoch 6574/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 14.0454 - val_loss: 119.3724\n",
      "Epoch 6575/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 13.2171 - val_loss: 101.3535\n",
      "Epoch 6576/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 422us/step - loss: 11.9072 - val_loss: 111.0692\n",
      "Epoch 6577/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 13.7959 - val_loss: 95.6376\n",
      "Epoch 6578/10000\n",
      "96/96 [==============================] - 0s 528us/step - loss: 12.8212 - val_loss: 113.1215\n",
      "Epoch 6579/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 15.0651 - val_loss: 109.6653\n",
      "Epoch 6580/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 15.0268 - val_loss: 116.6274\n",
      "Epoch 6581/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 15.0724 - val_loss: 121.0309\n",
      "Epoch 6582/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 12.0964 - val_loss: 98.8399\n",
      "Epoch 6583/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 15.3947 - val_loss: 107.6277\n",
      "Epoch 6584/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 12.7076 - val_loss: 113.6738\n",
      "Epoch 6585/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 13.3748 - val_loss: 93.2669\n",
      "Epoch 6586/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 15.7657 - val_loss: 98.0605\n",
      "Epoch 6587/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 12.5641 - val_loss: 101.3741\n",
      "Epoch 6588/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 12.6857 - val_loss: 102.9103\n",
      "Epoch 6589/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 11.6834 - val_loss: 123.0233\n",
      "Epoch 6590/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 12.3596 - val_loss: 106.0390\n",
      "Epoch 6591/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 13.3840 - val_loss: 99.4479\n",
      "Epoch 6592/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 11.7255 - val_loss: 104.8976\n",
      "Epoch 6593/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 13.0278 - val_loss: 106.7726\n",
      "Epoch 6594/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 13.3608 - val_loss: 91.4243\n",
      "Epoch 6595/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 15.5558 - val_loss: 107.0759\n",
      "Epoch 6596/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 12.1973 - val_loss: 101.9988\n",
      "Epoch 6597/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 15.0798 - val_loss: 109.4462\n",
      "Epoch 6598/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 11.7787 - val_loss: 101.4263\n",
      "Epoch 6599/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 12.6223 - val_loss: 101.0473\n",
      "Epoch 6600/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 12.7642 - val_loss: 116.6994\n",
      "Epoch 6601/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 13.4668 - val_loss: 111.5953\n",
      "Epoch 6602/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 13.8474 - val_loss: 88.8595\n",
      "Epoch 6603/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 12.7332 - val_loss: 110.3918\n",
      "Epoch 6604/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 13.3273 - val_loss: 89.6935\n",
      "Epoch 6605/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 14.4487 - val_loss: 98.4392\n",
      "Epoch 6606/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 13.4580 - val_loss: 94.0783\n",
      "Epoch 6607/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 13.1131 - val_loss: 109.5443\n",
      "Epoch 6608/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 11.9197 - val_loss: 103.6163\n",
      "Epoch 6609/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 13.0725 - val_loss: 107.0907\n",
      "Epoch 6610/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 11.7613 - val_loss: 107.2052\n",
      "Epoch 6611/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 9.4317 - val_loss: 116.1225\n",
      "Epoch 6612/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 11.7559 - val_loss: 105.4741\n",
      "Epoch 6613/10000\n",
      "96/96 [==============================] - 0s 492us/step - loss: 12.9705 - val_loss: 106.7665\n",
      "Epoch 6614/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 14.3856 - val_loss: 117.6426\n",
      "Epoch 6615/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 12.0005 - val_loss: 113.5990\n",
      "Epoch 6616/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 14.7993 - val_loss: 103.6328\n",
      "Epoch 6617/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 16.1537 - val_loss: 96.3971\n",
      "Epoch 6618/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 15.9271 - val_loss: 118.2601\n",
      "Epoch 6619/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 13.8527 - val_loss: 104.0122\n",
      "Epoch 6620/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 13.1691 - val_loss: 117.4330\n",
      "Epoch 6621/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 13.4091 - val_loss: 100.5174\n",
      "Epoch 6622/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 14.5890 - val_loss: 100.2622\n",
      "Epoch 6623/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 14.7606 - val_loss: 106.8614\n",
      "Epoch 6624/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 68.5811 - val_loss: 106.3356\n",
      "Epoch 6625/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 53.6391 - val_loss: 109.3045\n",
      "Epoch 6626/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 46.7481 - val_loss: 121.3678\n",
      "Epoch 6627/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 47.1882 - val_loss: 88.5455\n",
      "Epoch 6628/10000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 147.9854 - val_loss: 86.0336\n",
      "Epoch 6629/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 122.3344 - val_loss: 85.6067\n",
      "Epoch 6630/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 43.1316 - val_loss: 76.3979\n",
      "Epoch 6631/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 92.8012 - val_loss: 67.6273\n",
      "Epoch 6632/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 57.5536 - val_loss: 84.8065\n",
      "Epoch 6633/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 48.4642 - val_loss: 81.5898\n",
      "Epoch 6634/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 85.1018 - val_loss: 89.6742\n",
      "Epoch 6635/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 82.5183 - val_loss: 91.8701\n",
      "Epoch 6636/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 45.4702 - val_loss: 79.5740\n",
      "Epoch 6637/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 42.4628 - val_loss: 102.3186\n",
      "Epoch 6638/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 45.2901 - val_loss: 95.3156\n",
      "Epoch 6639/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 55.7764 - val_loss: 91.7653\n",
      "Epoch 6640/10000\n",
      "96/96 [==============================] - 0s 523us/step - loss: 61.0344 - val_loss: 73.5351\n",
      "Epoch 6641/10000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 71.8031 - val_loss: 80.3728\n",
      "Epoch 6642/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 53.8349 - val_loss: 72.7307\n",
      "Epoch 6643/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 68.8006 - val_loss: 84.5899\n",
      "Epoch 6644/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 114.2455 - val_loss: 72.8845\n",
      "Epoch 6645/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 62.8657 - val_loss: 77.0017\n",
      "Epoch 6646/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 46.2806 - val_loss: 105.7905\n",
      "Epoch 6647/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 51.6430 - val_loss: 104.3309\n",
      "Epoch 6648/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 56.2686 - val_loss: 108.2097\n",
      "Epoch 6649/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 82.1376 - val_loss: 100.3491\n",
      "Epoch 6650/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 68.4666 - val_loss: 97.8968\n",
      "Epoch 6651/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 42.4591 - val_loss: 94.5803\n",
      "Epoch 6652/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 41.4854 - val_loss: 97.3371\n",
      "Epoch 6653/10000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 61.6422 - val_loss: 83.0248\n",
      "Epoch 6654/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 43.5560 - val_loss: 109.3974\n",
      "Epoch 6655/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 40.8264 - val_loss: 94.3992\n",
      "Epoch 6656/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 38.4580 - val_loss: 97.9789\n",
      "Epoch 6657/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 36.5195 - val_loss: 96.2501\n",
      "Epoch 6658/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 50.4841 - val_loss: 97.7051\n",
      "Epoch 6659/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 47.4715 - val_loss: 83.8009\n",
      "Epoch 6660/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 37.7718 - val_loss: 79.1319\n",
      "Epoch 6661/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 33.6298 - val_loss: 111.6325\n",
      "Epoch 6662/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 35.7531 - val_loss: 102.5815\n",
      "Epoch 6663/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 38.6701 - val_loss: 100.9835\n",
      "Epoch 6664/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 61.4190 - val_loss: 88.2541\n",
      "Epoch 6665/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 43.0938 - val_loss: 102.1863\n",
      "Epoch 6666/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 34.5926 - val_loss: 105.2910\n",
      "Epoch 6667/10000\n",
      "96/96 [==============================] - 0s 500us/step - loss: 33.8069 - val_loss: 100.2148\n",
      "Epoch 6668/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 42.3729 - val_loss: 95.4115\n",
      "Epoch 6669/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 56.0342 - val_loss: 103.4893\n",
      "Epoch 6670/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 39.8888 - val_loss: 107.4863\n",
      "Epoch 6671/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 26.3761 - val_loss: 94.8851\n",
      "Epoch 6672/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 29.7552 - val_loss: 100.1442\n",
      "Epoch 6673/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 40.5722 - val_loss: 91.0321\n",
      "Epoch 6674/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 41.3708 - val_loss: 92.6811\n",
      "Epoch 6675/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 32.2571 - val_loss: 110.2438\n",
      "Epoch 6676/10000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 27.6486 - val_loss: 88.7937\n",
      "Epoch 6677/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 29.6650 - val_loss: 87.8213\n",
      "Epoch 6678/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 44.3339 - val_loss: 88.2246\n",
      "Epoch 6679/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 51.9183 - val_loss: 110.7007\n",
      "Epoch 6680/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 25.3890 - val_loss: 110.0634\n",
      "Epoch 6681/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 27.4075 - val_loss: 114.3749\n",
      "Epoch 6682/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 47.5494 - val_loss: 106.6914\n",
      "Epoch 6683/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 39.1671 - val_loss: 105.3639\n",
      "Epoch 6684/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 50.0197 - val_loss: 98.6881\n",
      "Epoch 6685/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 29.9101 - val_loss: 90.3575\n",
      "Epoch 6686/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 38.1154 - val_loss: 90.8472\n",
      "Epoch 6687/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 33.5386 - val_loss: 86.7649\n",
      "Epoch 6688/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 38.4534 - val_loss: 95.0640\n",
      "Epoch 6689/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 28.5042 - val_loss: 105.8824\n",
      "Epoch 6690/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 32.3071 - val_loss: 100.5458\n",
      "Epoch 6691/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 26.8154 - val_loss: 101.9533\n",
      "Epoch 6692/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 32.9506 - val_loss: 108.4252\n",
      "Epoch 6693/10000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 32.8448 - val_loss: 95.6359\n",
      "Epoch 6694/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 29.8132 - val_loss: 107.5831\n",
      "Epoch 6695/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 27.6277 - val_loss: 106.5379\n",
      "Epoch 6696/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 30.1096 - val_loss: 118.8290\n",
      "Epoch 6697/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 25.5927 - val_loss: 101.0679\n",
      "Epoch 6698/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 46.1790 - val_loss: 104.0298\n",
      "Epoch 6699/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 51.9428 - val_loss: 100.5272\n",
      "Epoch 6700/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 29.4679 - val_loss: 106.6602\n",
      "Epoch 6701/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 32.1163 - val_loss: 116.7490\n",
      "Epoch 6702/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 28.4007 - val_loss: 89.1239\n",
      "Epoch 6703/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 40.1513 - val_loss: 111.4554\n",
      "Epoch 6704/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 49.9523 - val_loss: 102.4373\n",
      "Epoch 6705/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 29.7516 - val_loss: 102.9046\n",
      "Epoch 6706/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 28.5047 - val_loss: 98.0000\n",
      "Epoch 6707/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 32.1889 - val_loss: 107.6281\n",
      "Epoch 6708/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 30.7183 - val_loss: 86.6202\n",
      "Epoch 6709/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 32.5257 - val_loss: 104.0253\n",
      "Epoch 6710/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 41.5352 - val_loss: 101.3246\n",
      "Epoch 6711/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 33.6972 - val_loss: 103.1477\n",
      "Epoch 6712/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 37.8422 - val_loss: 103.8319\n",
      "Epoch 6713/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 34.5855 - val_loss: 98.8291\n",
      "Epoch 6714/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 33.5926 - val_loss: 105.9305\n",
      "Epoch 6715/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 32.3921 - val_loss: 101.5540\n",
      "Epoch 6716/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 30.7330 - val_loss: 90.0679\n",
      "Epoch 6717/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 36.7495 - val_loss: 90.0628\n",
      "Epoch 6718/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 29.3341 - val_loss: 105.4312\n",
      "Epoch 6719/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 22.6483 - val_loss: 100.1553\n",
      "Epoch 6720/10000\n",
      "96/96 [==============================] - 0s 519us/step - loss: 24.4006 - val_loss: 113.8241\n",
      "Epoch 6721/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 27.2457 - val_loss: 100.9231\n",
      "Epoch 6722/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 27.9157 - val_loss: 112.4119\n",
      "Epoch 6723/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 38.2256 - val_loss: 110.8095\n",
      "Epoch 6724/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 34.1671 - val_loss: 108.5168\n",
      "Epoch 6725/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 44.8282 - val_loss: 105.7220\n",
      "Epoch 6726/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 446us/step - loss: 24.5354 - val_loss: 95.3490\n",
      "Epoch 6727/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 26.2381 - val_loss: 97.9521\n",
      "Epoch 6728/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 37.9449 - val_loss: 102.0427\n",
      "Epoch 6729/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 35.4653 - val_loss: 106.8160\n",
      "Epoch 6730/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 30.2943 - val_loss: 127.7698\n",
      "Epoch 6731/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 23.6787 - val_loss: 104.8471\n",
      "Epoch 6732/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 31.3085 - val_loss: 106.2526\n",
      "Epoch 6733/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 32.3810 - val_loss: 112.3556\n",
      "Epoch 6734/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 28.5474 - val_loss: 99.2107\n",
      "Epoch 6735/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 39.3410 - val_loss: 104.5163\n",
      "Epoch 6736/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 27.4689 - val_loss: 105.4374\n",
      "Epoch 6737/10000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 40.1859 - val_loss: 113.3427\n",
      "Epoch 6738/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 25.9154 - val_loss: 104.1487\n",
      "Epoch 6739/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 24.5462 - val_loss: 101.0869\n",
      "Epoch 6740/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 27.1545 - val_loss: 113.5106\n",
      "Epoch 6741/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 25.4863 - val_loss: 99.0519\n",
      "Epoch 6742/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 20.7488 - val_loss: 97.2989\n",
      "Epoch 6743/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 26.0080 - val_loss: 108.5251\n",
      "Epoch 6744/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 25.1362 - val_loss: 97.5792\n",
      "Epoch 6745/10000\n",
      "96/96 [==============================] - 0s 590us/step - loss: 22.4696 - val_loss: 115.2566\n",
      "Epoch 6746/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 22.8659 - val_loss: 85.5259\n",
      "Epoch 6747/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 24.7549 - val_loss: 116.2997\n",
      "Epoch 6748/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 20.9409 - val_loss: 104.5299\n",
      "Epoch 6749/10000\n",
      "96/96 [==============================] - 0s 493us/step - loss: 39.8988 - val_loss: 106.9194\n",
      "Epoch 6750/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 27.5569 - val_loss: 99.4188\n",
      "Epoch 6751/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 54.4448 - val_loss: 86.4110\n",
      "Epoch 6752/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 20.7892 - val_loss: 106.7072\n",
      "Epoch 6753/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 36.7811 - val_loss: 85.1425\n",
      "Epoch 6754/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 26.5656 - val_loss: 101.4730\n",
      "Epoch 6755/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 27.2550 - val_loss: 94.5171\n",
      "Epoch 6756/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 56.5997 - val_loss: 98.7262\n",
      "Epoch 6757/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 59.3436 - val_loss: 114.5392\n",
      "Epoch 6758/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 29.8032 - val_loss: 107.8913\n",
      "Epoch 6759/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 47.3079 - val_loss: 106.2507\n",
      "Epoch 6760/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 30.3066 - val_loss: 99.8393\n",
      "Epoch 6761/10000\n",
      "96/96 [==============================] - 0s 498us/step - loss: 31.0834 - val_loss: 99.4122\n",
      "Epoch 6762/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 33.4445 - val_loss: 109.7654\n",
      "Epoch 6763/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 27.8370 - val_loss: 94.5909\n",
      "Epoch 6764/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 27.0329 - val_loss: 94.2483\n",
      "Epoch 6765/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 26.5755 - val_loss: 109.3082\n",
      "Epoch 6766/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 78.1766 - val_loss: 117.7658\n",
      "Epoch 6767/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 46.5355 - val_loss: 102.4012\n",
      "Epoch 6768/10000\n",
      "96/96 [==============================] - 0s 490us/step - loss: 25.5561 - val_loss: 106.2328\n",
      "Epoch 6769/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 29.8898 - val_loss: 110.8772\n",
      "Epoch 6770/10000\n",
      "96/96 [==============================] - 0s 593us/step - loss: 34.4995 - val_loss: 109.6174\n",
      "Epoch 6771/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 26.8426 - val_loss: 103.2902\n",
      "Epoch 6772/10000\n",
      "96/96 [==============================] - 0s 485us/step - loss: 24.5882 - val_loss: 100.0259\n",
      "Epoch 6773/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 26.9771 - val_loss: 114.2128\n",
      "Epoch 6774/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 26.7163 - val_loss: 91.0480\n",
      "Epoch 6775/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 32.3703 - val_loss: 114.6828\n",
      "Epoch 6776/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 26.0178 - val_loss: 105.9429\n",
      "Epoch 6777/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 25.5678 - val_loss: 97.0900\n",
      "Epoch 6778/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 28.5404 - val_loss: 97.1312\n",
      "Epoch 6779/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 24.3078 - val_loss: 115.3047\n",
      "Epoch 6780/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 33.3754 - val_loss: 102.2620\n",
      "Epoch 6781/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 23.1031 - val_loss: 108.0941\n",
      "Epoch 6782/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 23.1859 - val_loss: 103.0813\n",
      "Epoch 6783/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 21.9368 - val_loss: 116.4736\n",
      "Epoch 6784/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 27.8242 - val_loss: 103.0130\n",
      "Epoch 6785/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 26.2556 - val_loss: 100.8930\n",
      "Epoch 6786/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 30.9386 - val_loss: 117.0666\n",
      "Epoch 6787/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 29.0059 - val_loss: 104.1665\n",
      "Epoch 6788/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 28.7252 - val_loss: 109.4453\n",
      "Epoch 6789/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 28.0706 - val_loss: 101.8740\n",
      "Epoch 6790/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 28.5455 - val_loss: 87.3513\n",
      "Epoch 6791/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 21.9846 - val_loss: 89.7343\n",
      "Epoch 6792/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 19.3852 - val_loss: 92.6283\n",
      "Epoch 6793/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 27.0037 - val_loss: 100.7034\n",
      "Epoch 6794/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 26.1323 - val_loss: 111.5414\n",
      "Epoch 6795/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 23.0119 - val_loss: 110.2056\n",
      "Epoch 6796/10000\n",
      "96/96 [==============================] - 0s 507us/step - loss: 28.4115 - val_loss: 102.8023\n",
      "Epoch 6797/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 30.1782 - val_loss: 106.5180\n",
      "Epoch 6798/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 32.0271 - val_loss: 111.9815\n",
      "Epoch 6799/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 32.8671 - val_loss: 93.3040\n",
      "Epoch 6800/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 20.7054 - val_loss: 109.4688\n",
      "Epoch 6801/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 18.1033 - val_loss: 106.2969\n",
      "Epoch 6802/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 37.2802 - val_loss: 96.8416\n",
      "Epoch 6803/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 22.3319 - val_loss: 87.1328\n",
      "Epoch 6804/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 26.1917 - val_loss: 113.3872\n",
      "Epoch 6805/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 32.3269 - val_loss: 111.1431\n",
      "Epoch 6806/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 33.1877 - val_loss: 98.6372\n",
      "Epoch 6807/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 26.8951 - val_loss: 95.7932\n",
      "Epoch 6808/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 22.0565 - val_loss: 103.3224\n",
      "Epoch 6809/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 29.3263 - val_loss: 99.4414\n",
      "Epoch 6810/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 29.0218 - val_loss: 106.2516\n",
      "Epoch 6811/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 22.5650 - val_loss: 107.5654\n",
      "Epoch 6812/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 23.4919 - val_loss: 116.7997\n",
      "Epoch 6813/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 21.7287 - val_loss: 100.6723\n",
      "Epoch 6814/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 21.0721 - val_loss: 110.1741\n",
      "Epoch 6815/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 30.9447 - val_loss: 110.0725\n",
      "Epoch 6816/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 22.9950 - val_loss: 100.7416\n",
      "Epoch 6817/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 43.8870 - val_loss: 106.9096\n",
      "Epoch 6818/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 20.7235 - val_loss: 94.5630\n",
      "Epoch 6819/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 55.4714 - val_loss: 97.2948\n",
      "Epoch 6820/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 19.5877 - val_loss: 106.0306\n",
      "Epoch 6821/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 36.2990 - val_loss: 104.3954\n",
      "Epoch 6822/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 26.2747 - val_loss: 99.3972\n",
      "Epoch 6823/10000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 29.0451 - val_loss: 107.4814\n",
      "Epoch 6824/10000\n",
      "96/96 [==============================] - 0s 356us/step - loss: 30.2566 - val_loss: 92.0263\n",
      "Epoch 6825/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 20.3353 - val_loss: 78.8858\n",
      "Epoch 6826/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 23.4022 - val_loss: 103.7178\n",
      "Epoch 6827/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 23.6354 - val_loss: 107.0744\n",
      "Epoch 6828/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 25.6992 - val_loss: 105.6534\n",
      "Epoch 6829/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 30.3835 - val_loss: 81.8591\n",
      "Epoch 6830/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 21.6313 - val_loss: 76.2838\n",
      "Epoch 6831/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 20.0151 - val_loss: 111.1511\n",
      "Epoch 6832/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 22.6076 - val_loss: 101.9216\n",
      "Epoch 6833/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 21.7840 - val_loss: 93.9018\n",
      "Epoch 6834/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 24.2820 - val_loss: 108.4163\n",
      "Epoch 6835/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 21.7171 - val_loss: 106.4097\n",
      "Epoch 6836/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 21.4014 - val_loss: 109.2430\n",
      "Epoch 6837/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 27.1508 - val_loss: 75.9752\n",
      "Epoch 6838/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 21.6751 - val_loss: 83.9988\n",
      "Epoch 6839/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 54.5487 - val_loss: 103.9387\n",
      "Epoch 6840/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 23.9471 - val_loss: 106.7090\n",
      "Epoch 6841/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 19.7711 - val_loss: 102.8438\n",
      "Epoch 6842/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 29.5877 - val_loss: 106.8110\n",
      "Epoch 6843/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 26.0888 - val_loss: 106.9010\n",
      "Epoch 6844/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 22.9926 - val_loss: 101.1937\n",
      "Epoch 6845/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 26.3560 - val_loss: 103.2701\n",
      "Epoch 6846/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 28.7124 - val_loss: 93.8862\n",
      "Epoch 6847/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 23.9677 - val_loss: 89.5849\n",
      "Epoch 6848/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 25.0630 - val_loss: 87.3375\n",
      "Epoch 6849/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 20.6711 - val_loss: 98.6088\n",
      "Epoch 6850/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 23.4315 - val_loss: 118.5356\n",
      "Epoch 6851/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 22.6422 - val_loss: 115.6658\n",
      "Epoch 6852/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 28.6082 - val_loss: 108.5095\n",
      "Epoch 6853/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 21.2614 - val_loss: 113.4931\n",
      "Epoch 6854/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 29.1914 - val_loss: 101.5053\n",
      "Epoch 6855/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 22.1189 - val_loss: 110.3462\n",
      "Epoch 6856/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 27.8047 - val_loss: 114.6448\n",
      "Epoch 6857/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 23.0314 - val_loss: 105.1750\n",
      "Epoch 6858/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 20.6520 - val_loss: 120.3583\n",
      "Epoch 6859/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 17.6120 - val_loss: 111.5273\n",
      "Epoch 6860/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 22.5918 - val_loss: 106.4706\n",
      "Epoch 6861/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 20.6812 - val_loss: 113.6388\n",
      "Epoch 6862/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 18.5665 - val_loss: 104.2030\n",
      "Epoch 6863/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 22.5902 - val_loss: 114.7137\n",
      "Epoch 6864/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 20.2539 - val_loss: 116.9220\n",
      "Epoch 6865/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 20.1854 - val_loss: 111.4818\n",
      "Epoch 6866/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 22.4459 - val_loss: 114.9066\n",
      "Epoch 6867/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 24.3904 - val_loss: 95.4727\n",
      "Epoch 6868/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 23.6500 - val_loss: 96.9111\n",
      "Epoch 6869/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 30.7871 - val_loss: 111.1980\n",
      "Epoch 6870/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 36.0489 - val_loss: 94.6736\n",
      "Epoch 6871/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 35.4116 - val_loss: 102.5313\n",
      "Epoch 6872/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 28.3806 - val_loss: 111.0589\n",
      "Epoch 6873/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 26.3107 - val_loss: 72.0502\n",
      "Epoch 6874/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 28.4752 - val_loss: 94.6554\n",
      "Epoch 6875/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 26.5170 - val_loss: 111.1474\n",
      "Epoch 6876/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 413us/step - loss: 24.3119 - val_loss: 115.8510\n",
      "Epoch 6877/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 30.5021 - val_loss: 128.7104\n",
      "Epoch 6878/10000\n",
      "96/96 [==============================] - 0s 589us/step - loss: 20.8761 - val_loss: 122.9807\n",
      "Epoch 6879/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 28.2085 - val_loss: 107.9034\n",
      "Epoch 6880/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 25.2136 - val_loss: 101.0722\n",
      "Epoch 6881/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 22.6897 - val_loss: 108.2691\n",
      "Epoch 6882/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 31.9923 - val_loss: 112.4661\n",
      "Epoch 6883/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 19.9771 - val_loss: 106.7606\n",
      "Epoch 6884/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 20.6079 - val_loss: 95.4730\n",
      "Epoch 6885/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 42.3471 - val_loss: 103.6456\n",
      "Epoch 6886/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 19.7283 - val_loss: 99.2365\n",
      "Epoch 6887/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 31.1436 - val_loss: 83.2471\n",
      "Epoch 6888/10000\n",
      "96/96 [==============================] - 0s 485us/step - loss: 20.2185 - val_loss: 103.6205\n",
      "Epoch 6889/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 21.5846 - val_loss: 112.2929\n",
      "Epoch 6890/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 20.9421 - val_loss: 95.1468\n",
      "Epoch 6891/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 21.7236 - val_loss: 77.6092\n",
      "Epoch 6892/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 18.9038 - val_loss: 87.1671\n",
      "Epoch 6893/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 22.8408 - val_loss: 112.5089\n",
      "Epoch 6894/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 41.9747 - val_loss: 107.9431\n",
      "Epoch 6895/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 27.1211 - val_loss: 103.4570\n",
      "Epoch 6896/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 26.1514 - val_loss: 96.5381\n",
      "Epoch 6897/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 27.8083 - val_loss: 117.4417\n",
      "Epoch 6898/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 29.8406 - val_loss: 122.0029\n",
      "Epoch 6899/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 25.9934 - val_loss: 105.0207\n",
      "Epoch 6900/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 32.2199 - val_loss: 111.0685\n",
      "Epoch 6901/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 24.2831 - val_loss: 92.2927\n",
      "Epoch 6902/10000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 24.7814 - val_loss: 97.8839\n",
      "Epoch 6903/10000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 30.2078 - val_loss: 101.8602\n",
      "Epoch 6904/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 28.5753 - val_loss: 122.4597\n",
      "Epoch 6905/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 25.1422 - val_loss: 89.8876\n",
      "Epoch 6906/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 31.1993 - val_loss: 101.8462\n",
      "Epoch 6907/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 21.7709 - val_loss: 97.8028\n",
      "Epoch 6908/10000\n",
      "96/96 [==============================] - 0s 480us/step - loss: 23.3940 - val_loss: 101.3556\n",
      "Epoch 6909/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 23.6018 - val_loss: 89.4883\n",
      "Epoch 6910/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 18.6929 - val_loss: 116.2774\n",
      "Epoch 6911/10000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 24.5837 - val_loss: 112.3792\n",
      "Epoch 6912/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 19.8141 - val_loss: 87.9479\n",
      "Epoch 6913/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 21.8845 - val_loss: 109.5316\n",
      "Epoch 6914/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 22.3083 - val_loss: 109.4328\n",
      "Epoch 6915/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 23.1449 - val_loss: 87.2988\n",
      "Epoch 6916/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 16.7774 - val_loss: 107.4916\n",
      "Epoch 6917/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 20.6676 - val_loss: 87.4930\n",
      "Epoch 6918/10000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 18.3557 - val_loss: 114.7980\n",
      "Epoch 6919/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 28.7338 - val_loss: 101.1783\n",
      "Epoch 6920/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 23.9698 - val_loss: 109.9528\n",
      "Epoch 6921/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 30.9830 - val_loss: 80.8727\n",
      "Epoch 6922/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 21.8654 - val_loss: 98.7921\n",
      "Epoch 6923/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 25.8535 - val_loss: 107.2823\n",
      "Epoch 6924/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 17.3578 - val_loss: 91.3271\n",
      "Epoch 6925/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 46.2106 - val_loss: 115.4807\n",
      "Epoch 6926/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 22.3480 - val_loss: 118.3147\n",
      "Epoch 6927/10000\n",
      "96/96 [==============================] - 0s 484us/step - loss: 19.7784 - val_loss: 97.8742\n",
      "Epoch 6928/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 24.7366 - val_loss: 105.0635\n",
      "Epoch 6929/10000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 27.8687 - val_loss: 93.8315\n",
      "Epoch 6930/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 22.2780 - val_loss: 98.2889\n",
      "Epoch 6931/10000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 21.4703 - val_loss: 93.8324\n",
      "Epoch 6932/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 19.8263 - val_loss: 89.5284\n",
      "Epoch 6933/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 22.8993 - val_loss: 99.8891\n",
      "Epoch 6934/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 25.9176 - val_loss: 100.6824\n",
      "Epoch 6935/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 65.9878 - val_loss: 92.6845\n",
      "Epoch 6936/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 49.9025 - val_loss: 99.0049\n",
      "Epoch 6937/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 63.6854 - val_loss: 96.2055\n",
      "Epoch 6938/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 40.1900 - val_loss: 82.8529\n",
      "Epoch 6939/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 36.0642 - val_loss: 98.8507\n",
      "Epoch 6940/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 38.0794 - val_loss: 88.1551\n",
      "Epoch 6941/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 31.5854 - val_loss: 81.7382\n",
      "Epoch 6942/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 30.9293 - val_loss: 100.6049\n",
      "Epoch 6943/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 27.7433 - val_loss: 104.1976\n",
      "Epoch 6944/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 25.1636 - val_loss: 96.9558\n",
      "Epoch 6945/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 25.6531 - val_loss: 96.4719\n",
      "Epoch 6946/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 27.0102 - val_loss: 103.9458\n",
      "Epoch 6947/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 22.3967 - val_loss: 109.2185\n",
      "Epoch 6948/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 30.4631 - val_loss: 113.0511\n",
      "Epoch 6949/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 25.0640 - val_loss: 109.0269\n",
      "Epoch 6950/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 22.4266 - val_loss: 101.0468\n",
      "Epoch 6951/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 19.2768 - val_loss: 110.5866\n",
      "Epoch 6952/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 19.1100 - val_loss: 92.8493\n",
      "Epoch 6953/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 19.4497 - val_loss: 90.8025\n",
      "Epoch 6954/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 25.8613 - val_loss: 85.0244\n",
      "Epoch 6955/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 18.4535 - val_loss: 98.3299\n",
      "Epoch 6956/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 52.4006 - val_loss: 85.3742\n",
      "Epoch 6957/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 28.7274 - val_loss: 97.9856\n",
      "Epoch 6958/10000\n",
      "96/96 [==============================] - 0s 525us/step - loss: 38.0248 - val_loss: 86.3412\n",
      "Epoch 6959/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 35.4926 - val_loss: 91.4697\n",
      "Epoch 6960/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 26.4528 - val_loss: 93.2221\n",
      "Epoch 6961/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 27.0410 - val_loss: 87.1837\n",
      "Epoch 6962/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 28.7580 - val_loss: 93.0840\n",
      "Epoch 6963/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 24.0964 - val_loss: 86.7178\n",
      "Epoch 6964/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 23.5777 - val_loss: 88.6493\n",
      "Epoch 6965/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 22.3675 - val_loss: 94.7295\n",
      "Epoch 6966/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 29.2987 - val_loss: 87.4492\n",
      "Epoch 6967/10000\n",
      "96/96 [==============================] - 0s 492us/step - loss: 25.3037 - val_loss: 75.2646\n",
      "Epoch 6968/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 21.8654 - val_loss: 94.0568\n",
      "Epoch 6969/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 19.1261 - val_loss: 105.6417\n",
      "Epoch 6970/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 31.0993 - val_loss: 120.8535\n",
      "Epoch 6971/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 56.9821 - val_loss: 119.4529\n",
      "Epoch 6972/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 45.3443 - val_loss: 95.6257\n",
      "Epoch 6973/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 36.8892 - val_loss: 104.2994\n",
      "Epoch 6974/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 33.0072 - val_loss: 109.7901\n",
      "Epoch 6975/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 40.1582 - val_loss: 83.5932\n",
      "Epoch 6976/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 39.5210 - val_loss: 91.0111\n",
      "Epoch 6977/10000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 48.8489 - val_loss: 75.7920\n",
      "Epoch 6978/10000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 33.4374 - val_loss: 84.4100\n",
      "Epoch 6979/10000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 30.1031 - val_loss: 104.0672\n",
      "Epoch 6980/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 29.2476 - val_loss: 98.0706\n",
      "Epoch 6981/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 27.3125 - val_loss: 93.0489\n",
      "Epoch 6982/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 21.8910 - val_loss: 106.7020\n",
      "Epoch 6983/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 27.2295 - val_loss: 100.4887\n",
      "Epoch 6984/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 26.2654 - val_loss: 110.1886\n",
      "Epoch 6985/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 25.7697 - val_loss: 94.2236\n",
      "Epoch 6986/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 29.5369 - val_loss: 86.7381\n",
      "Epoch 6987/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 32.6200 - val_loss: 85.4955\n",
      "Epoch 6988/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 37.9847 - val_loss: 88.6629\n",
      "Epoch 6989/10000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 28.5888 - val_loss: 90.3874\n",
      "Epoch 6990/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 23.3801 - val_loss: 98.4540\n",
      "Epoch 6991/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 25.8457 - val_loss: 113.2823\n",
      "Epoch 6992/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 25.2026 - val_loss: 89.0537\n",
      "Epoch 6993/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 27.2205 - val_loss: 86.9615\n",
      "Epoch 6994/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 30.2247 - val_loss: 90.5598\n",
      "Epoch 6995/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 27.4041 - val_loss: 86.0230\n",
      "Epoch 6996/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 21.0475 - val_loss: 102.8777\n",
      "Epoch 6997/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 22.5814 - val_loss: 103.1039\n",
      "Epoch 6998/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 19.5230 - val_loss: 116.8698\n",
      "Epoch 6999/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 26.4106 - val_loss: 96.5964\n",
      "Epoch 7000/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 28.9273 - val_loss: 105.4327\n",
      "Epoch 7001/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 34.6474 - val_loss: 113.8493\n",
      "Epoch 7002/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 22.6693 - val_loss: 103.7993\n",
      "Epoch 7003/10000\n",
      "96/96 [==============================] - 0s 512us/step - loss: 20.6908 - val_loss: 94.0986\n",
      "Epoch 7004/10000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 24.1950 - val_loss: 107.5523\n",
      "Epoch 7005/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 23.6341 - val_loss: 101.5916\n",
      "Epoch 7006/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 27.2925 - val_loss: 85.8674\n",
      "Epoch 7007/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 23.3492 - val_loss: 93.9602\n",
      "Epoch 7008/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 21.6683 - val_loss: 96.4960\n",
      "Epoch 7009/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 21.7145 - val_loss: 121.2064\n",
      "Epoch 7010/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 22.7354 - val_loss: 102.0646\n",
      "Epoch 7011/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 27.4191 - val_loss: 107.4086\n",
      "Epoch 7012/10000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 24.0104 - val_loss: 95.7896\n",
      "Epoch 7013/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 22.3231 - val_loss: 102.7293\n",
      "Epoch 7014/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 26.0193 - val_loss: 72.4246\n",
      "Epoch 7015/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 18.1379 - val_loss: 101.7480\n",
      "Epoch 7016/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 27.4170 - val_loss: 99.5991\n",
      "Epoch 7017/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 25.1987 - val_loss: 103.4820\n",
      "Epoch 7018/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 19.2365 - val_loss: 98.5285\n",
      "Epoch 7019/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 18.5636 - val_loss: 101.3296\n",
      "Epoch 7020/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 34.3135 - val_loss: 91.8141\n",
      "Epoch 7021/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 27.3621 - val_loss: 118.5796\n",
      "Epoch 7022/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 21.2427 - val_loss: 97.8686\n",
      "Epoch 7023/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 24.4817 - val_loss: 96.0127\n",
      "Epoch 7024/10000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 23.7889 - val_loss: 100.0225\n",
      "Epoch 7025/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 17.7110 - val_loss: 111.7282\n",
      "Epoch 7026/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 430us/step - loss: 22.7857 - val_loss: 93.6760\n",
      "Epoch 7027/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 20.7096 - val_loss: 100.7069\n",
      "Epoch 7028/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 29.6224 - val_loss: 96.0978\n",
      "Epoch 7029/10000\n",
      "96/96 [==============================] - 0s 660us/step - loss: 29.6596 - val_loss: 107.7876\n",
      "Epoch 7030/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 18.6952 - val_loss: 101.7071\n",
      "Epoch 7031/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 17.7942 - val_loss: 99.2173\n",
      "Epoch 7032/10000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 20.7862 - val_loss: 116.4954\n",
      "Epoch 7033/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 20.5459 - val_loss: 103.1286\n",
      "Epoch 7034/10000\n",
      "96/96 [==============================] - 0s 480us/step - loss: 23.3007 - val_loss: 112.0496\n",
      "Epoch 7035/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 18.4786 - val_loss: 92.7415\n",
      "Epoch 7036/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 23.8371 - val_loss: 92.6586\n",
      "Epoch 7037/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 21.2518 - val_loss: 103.8948\n",
      "Epoch 7038/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 19.3789 - val_loss: 103.9682\n",
      "Epoch 7039/10000\n",
      "96/96 [==============================] - 0s 524us/step - loss: 18.3746 - val_loss: 109.7035\n",
      "Epoch 7040/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 19.7154 - val_loss: 104.0336\n",
      "Epoch 7041/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 16.6893 - val_loss: 97.3360\n",
      "Epoch 7042/10000\n",
      "96/96 [==============================] - 0s 521us/step - loss: 18.4983 - val_loss: 99.5705\n",
      "Epoch 7043/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 23.1550 - val_loss: 84.3773\n",
      "Epoch 7044/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 19.5922 - val_loss: 101.3335\n",
      "Epoch 7045/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 17.4065 - val_loss: 98.5430\n",
      "Epoch 7046/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 20.3478 - val_loss: 106.6134\n",
      "Epoch 7047/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 43.3404 - val_loss: 102.6896\n",
      "Epoch 7048/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 19.7482 - val_loss: 80.8967\n",
      "Epoch 7049/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 25.1070 - val_loss: 96.5010\n",
      "Epoch 7050/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 21.2498 - val_loss: 89.9140\n",
      "Epoch 7051/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 32.5081 - val_loss: 102.0102\n",
      "Epoch 7052/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 28.7163 - val_loss: 104.6996\n",
      "Epoch 7053/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 27.4261 - val_loss: 103.6336\n",
      "Epoch 7054/10000\n",
      "96/96 [==============================] - 0s 534us/step - loss: 18.5897 - val_loss: 108.1591\n",
      "Epoch 7055/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 22.2663 - val_loss: 97.3903\n",
      "Epoch 7056/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 19.4737 - val_loss: 109.4083\n",
      "Epoch 7057/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 22.3307 - val_loss: 96.0946\n",
      "Epoch 7058/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 21.2489 - val_loss: 102.0596\n",
      "Epoch 7059/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 21.3362 - val_loss: 92.9978\n",
      "Epoch 7060/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 21.3158 - val_loss: 100.6319\n",
      "Epoch 7061/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 22.3048 - val_loss: 104.2493\n",
      "Epoch 7062/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 24.7617 - val_loss: 80.1100\n",
      "Epoch 7063/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 18.2312 - val_loss: 87.2782\n",
      "Epoch 7064/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 22.2826 - val_loss: 91.2211\n",
      "Epoch 7065/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 28.7612 - val_loss: 101.8011\n",
      "Epoch 7066/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 16.9446 - val_loss: 102.3883\n",
      "Epoch 7067/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 16.3810 - val_loss: 100.3910\n",
      "Epoch 7068/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 17.9080 - val_loss: 110.7490\n",
      "Epoch 7069/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 15.5100 - val_loss: 91.9890\n",
      "Epoch 7070/10000\n",
      "96/96 [==============================] - 0s 480us/step - loss: 27.0895 - val_loss: 111.9891\n",
      "Epoch 7071/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 25.4282 - val_loss: 85.8426\n",
      "Epoch 7072/10000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 19.9381 - val_loss: 108.9446\n",
      "Epoch 7073/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 16.8439 - val_loss: 110.4565\n",
      "Epoch 7074/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 23.3380 - val_loss: 88.6632\n",
      "Epoch 7075/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 19.8740 - val_loss: 100.3709\n",
      "Epoch 7076/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 18.0076 - val_loss: 89.2545\n",
      "Epoch 7077/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 24.7896 - val_loss: 96.7348\n",
      "Epoch 7078/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 27.3530 - val_loss: 93.9439\n",
      "Epoch 7079/10000\n",
      "96/96 [==============================] - 0s 496us/step - loss: 19.6389 - val_loss: 110.6613\n",
      "Epoch 7080/10000\n",
      "96/96 [==============================] - 0s 498us/step - loss: 23.6070 - val_loss: 94.5156\n",
      "Epoch 7081/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 17.7565 - val_loss: 98.0632\n",
      "Epoch 7082/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 23.5979 - val_loss: 100.1953\n",
      "Epoch 7083/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 17.0956 - val_loss: 99.0655\n",
      "Epoch 7084/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 16.4584 - val_loss: 105.0736\n",
      "Epoch 7085/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 21.3360 - val_loss: 91.9103\n",
      "Epoch 7086/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 19.1432 - val_loss: 76.7306\n",
      "Epoch 7087/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 17.3499 - val_loss: 95.5758\n",
      "Epoch 7088/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 18.4657 - val_loss: 97.3835\n",
      "Epoch 7089/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 22.9357 - val_loss: 105.2748\n",
      "Epoch 7090/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 16.8668 - val_loss: 94.1086\n",
      "Epoch 7091/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 20.4293 - val_loss: 105.6558\n",
      "Epoch 7092/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 18.6517 - val_loss: 91.1998\n",
      "Epoch 7093/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 21.4344 - val_loss: 108.9121\n",
      "Epoch 7094/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 21.6854 - val_loss: 99.1582\n",
      "Epoch 7095/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 22.4606 - val_loss: 96.4733\n",
      "Epoch 7096/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 13.4398 - val_loss: 104.6865\n",
      "Epoch 7097/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 15.0599 - val_loss: 89.1949\n",
      "Epoch 7098/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 17.4736 - val_loss: 103.6301\n",
      "Epoch 7099/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 20.1775 - val_loss: 100.8262\n",
      "Epoch 7100/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 21.5046 - val_loss: 101.7503\n",
      "Epoch 7101/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 17.0048 - val_loss: 101.5323\n",
      "Epoch 7102/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 16.5082 - val_loss: 104.4460\n",
      "Epoch 7103/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 15.6646 - val_loss: 108.7527\n",
      "Epoch 7104/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 18.9861 - val_loss: 103.0148\n",
      "Epoch 7105/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 19.1011 - val_loss: 114.3223\n",
      "Epoch 7106/10000\n",
      "96/96 [==============================] - 0s 487us/step - loss: 17.7539 - val_loss: 108.8006\n",
      "Epoch 7107/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 19.3005 - val_loss: 112.9844\n",
      "Epoch 7108/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 19.5325 - val_loss: 92.1649\n",
      "Epoch 7109/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 14.3699 - val_loss: 84.7631\n",
      "Epoch 7110/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 14.5321 - val_loss: 97.1113\n",
      "Epoch 7111/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 17.0432 - val_loss: 121.2312\n",
      "Epoch 7112/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 16.6111 - val_loss: 119.6120\n",
      "Epoch 7113/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 17.2339 - val_loss: 95.2021\n",
      "Epoch 7114/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 17.1892 - val_loss: 95.5890\n",
      "Epoch 7115/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 17.3617 - val_loss: 109.0918\n",
      "Epoch 7116/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 15.9896 - val_loss: 95.0779\n",
      "Epoch 7117/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 26.2774 - val_loss: 107.4598\n",
      "Epoch 7118/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 21.1169 - val_loss: 101.4450\n",
      "Epoch 7119/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 27.7343 - val_loss: 95.6611\n",
      "Epoch 7120/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 44.5899 - val_loss: 105.4456\n",
      "Epoch 7121/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 45.8469 - val_loss: 107.1980\n",
      "Epoch 7122/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 47.2371 - val_loss: 112.9609\n",
      "Epoch 7123/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 30.4432 - val_loss: 96.6404\n",
      "Epoch 7124/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 27.1074 - val_loss: 88.6625\n",
      "Epoch 7125/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 29.3178 - val_loss: 83.4135\n",
      "Epoch 7126/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 30.2322 - val_loss: 69.7003\n",
      "Epoch 7127/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 28.5848 - val_loss: 84.8218\n",
      "Epoch 7128/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 28.0970 - val_loss: 88.4932\n",
      "Epoch 7129/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 25.0164 - val_loss: 100.2017\n",
      "Epoch 7130/10000\n",
      "96/96 [==============================] - 0s 481us/step - loss: 21.7686 - val_loss: 101.3130\n",
      "Epoch 7131/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 24.7913 - val_loss: 98.2451\n",
      "Epoch 7132/10000\n",
      "96/96 [==============================] - 0s 500us/step - loss: 40.6218 - val_loss: 84.5672\n",
      "Epoch 7133/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 23.1580 - val_loss: 97.2377\n",
      "Epoch 7134/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 30.3133 - val_loss: 105.9372\n",
      "Epoch 7135/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 31.3006 - val_loss: 112.3994\n",
      "Epoch 7136/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 22.7372 - val_loss: 94.4353\n",
      "Epoch 7137/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 22.3150 - val_loss: 89.9624\n",
      "Epoch 7138/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 25.1514 - val_loss: 95.7010\n",
      "Epoch 7139/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 17.1289 - val_loss: 112.5208\n",
      "Epoch 7140/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 32.1303 - val_loss: 90.6969\n",
      "Epoch 7141/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 21.1895 - val_loss: 83.3556\n",
      "Epoch 7142/10000\n",
      "96/96 [==============================] - 0s 356us/step - loss: 25.2003 - val_loss: 104.7152\n",
      "Epoch 7143/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 19.8098 - val_loss: 91.1370\n",
      "Epoch 7144/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 20.9199 - val_loss: 86.2765\n",
      "Epoch 7145/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 18.1866 - val_loss: 87.6331\n",
      "Epoch 7146/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 17.8566 - val_loss: 104.8132\n",
      "Epoch 7147/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 21.3867 - val_loss: 92.2256\n",
      "Epoch 7148/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 17.4529 - val_loss: 104.0529\n",
      "Epoch 7149/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 23.9076 - val_loss: 104.6131\n",
      "Epoch 7150/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 20.2480 - val_loss: 106.4416\n",
      "Epoch 7151/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 18.7326 - val_loss: 115.3187\n",
      "Epoch 7152/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 23.6708 - val_loss: 117.8189\n",
      "Epoch 7153/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 22.9635 - val_loss: 88.2383\n",
      "Epoch 7154/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 20.7956 - val_loss: 103.5584\n",
      "Epoch 7155/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 80.9550 - val_loss: 109.7063\n",
      "Epoch 7156/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 24.7500 - val_loss: 115.4221\n",
      "Epoch 7157/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 22.7110 - val_loss: 114.9022\n",
      "Epoch 7158/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 16.3984 - val_loss: 97.5958\n",
      "Epoch 7159/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 22.1992 - val_loss: 98.5868\n",
      "Epoch 7160/10000\n",
      "96/96 [==============================] - 0s 475us/step - loss: 15.5543 - val_loss: 115.6469\n",
      "Epoch 7161/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 19.5740 - val_loss: 102.4540\n",
      "Epoch 7162/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 20.7057 - val_loss: 106.4666\n",
      "Epoch 7163/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 17.7859 - val_loss: 111.9611\n",
      "Epoch 7164/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 24.5981 - val_loss: 106.5154\n",
      "Epoch 7165/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 17.3684 - val_loss: 106.5346\n",
      "Epoch 7166/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 19.9489 - val_loss: 89.6598\n",
      "Epoch 7167/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 15.7085 - val_loss: 104.9762\n",
      "Epoch 7168/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 17.5365 - val_loss: 107.6092\n",
      "Epoch 7169/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 17.9634 - val_loss: 109.8013\n",
      "Epoch 7170/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 14.3137 - val_loss: 107.2197\n",
      "Epoch 7171/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 18.8516 - val_loss: 100.3619\n",
      "Epoch 7172/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 15.6905 - val_loss: 113.2091\n",
      "Epoch 7173/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 14.9957 - val_loss: 94.7245\n",
      "Epoch 7174/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 18.6843 - val_loss: 101.7809\n",
      "Epoch 7175/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 20.9338 - val_loss: 107.2332\n",
      "Epoch 7176/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 405us/step - loss: 17.8984 - val_loss: 80.4117\n",
      "Epoch 7177/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 15.6529 - val_loss: 91.2321\n",
      "Epoch 7178/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 17.0678 - val_loss: 84.4645\n",
      "Epoch 7179/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 16.5691 - val_loss: 91.9948\n",
      "Epoch 7180/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 19.5334 - val_loss: 105.0173\n",
      "Epoch 7181/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 18.6321 - val_loss: 108.9404\n",
      "Epoch 7182/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 18.1025 - val_loss: 99.1318\n",
      "Epoch 7183/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 17.5941 - val_loss: 103.2172\n",
      "Epoch 7184/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 17.6963 - val_loss: 99.2288\n",
      "Epoch 7185/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 15.6981 - val_loss: 96.7066\n",
      "Epoch 7186/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 14.5860 - val_loss: 97.5520\n",
      "Epoch 7187/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 20.0071 - val_loss: 102.1882\n",
      "Epoch 7188/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 14.8814 - val_loss: 108.4718\n",
      "Epoch 7189/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 23.0946 - val_loss: 104.5183\n",
      "Epoch 7190/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 36.1779 - val_loss: 91.2473\n",
      "Epoch 7191/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 22.6084 - val_loss: 111.7485\n",
      "Epoch 7192/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 98.5888 - val_loss: 105.8066\n",
      "Epoch 7193/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 17.4821 - val_loss: 99.3955\n",
      "Epoch 7194/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 31.0111 - val_loss: 97.9812\n",
      "Epoch 7195/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 40.3804 - val_loss: 82.3181\n",
      "Epoch 7196/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 37.7244 - val_loss: 92.8031\n",
      "Epoch 7197/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 35.8610 - val_loss: 94.1722\n",
      "Epoch 7198/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 38.5534 - val_loss: 92.1624\n",
      "Epoch 7199/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 27.5894 - val_loss: 85.9931\n",
      "Epoch 7200/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 26.7597 - val_loss: 102.5184\n",
      "Epoch 7201/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 24.1399 - val_loss: 89.0071\n",
      "Epoch 7202/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 25.4243 - val_loss: 100.4400\n",
      "Epoch 7203/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 42.8076 - val_loss: 93.1882\n",
      "Epoch 7204/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 30.7048 - val_loss: 103.5456\n",
      "Epoch 7205/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 25.0129 - val_loss: 101.6964\n",
      "Epoch 7206/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 29.4055 - val_loss: 99.7357\n",
      "Epoch 7207/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 28.3674 - val_loss: 102.9292\n",
      "Epoch 7208/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 23.5446 - val_loss: 81.5914\n",
      "Epoch 7209/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 26.4712 - val_loss: 93.8421\n",
      "Epoch 7210/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 23.6613 - val_loss: 101.4799\n",
      "Epoch 7211/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 21.6496 - val_loss: 92.9917\n",
      "Epoch 7212/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 25.2078 - val_loss: 98.4736\n",
      "Epoch 7213/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 17.2160 - val_loss: 112.8466\n",
      "Epoch 7214/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 22.1281 - val_loss: 108.1254\n",
      "Epoch 7215/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 18.8134 - val_loss: 116.6150\n",
      "Epoch 7216/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 22.9435 - val_loss: 104.0558\n",
      "Epoch 7217/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 19.9650 - val_loss: 89.0235\n",
      "Epoch 7218/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 18.8788 - val_loss: 103.0375\n",
      "Epoch 7219/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 20.9664 - val_loss: 84.2446\n",
      "Epoch 7220/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 19.6393 - val_loss: 101.0860\n",
      "Epoch 7221/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 22.4162 - val_loss: 97.1395\n",
      "Epoch 7222/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 19.5868 - val_loss: 100.6136\n",
      "Epoch 7223/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 17.2800 - val_loss: 90.4245\n",
      "Epoch 7224/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 18.6067 - val_loss: 102.0976\n",
      "Epoch 7225/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 40.3503 - val_loss: 89.7804\n",
      "Epoch 7226/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 26.7682 - val_loss: 112.2695\n",
      "Epoch 7227/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 36.0831 - val_loss: 103.4783\n",
      "Epoch 7228/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 35.2684 - val_loss: 111.2484\n",
      "Epoch 7229/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 105.7414 - val_loss: 97.5037\n",
      "Epoch 7230/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 26.0995 - val_loss: 80.8681\n",
      "Epoch 7231/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 27.8336 - val_loss: 109.2966\n",
      "Epoch 7232/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 24.9683 - val_loss: 100.3488\n",
      "Epoch 7233/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 23.1001 - val_loss: 99.8750\n",
      "Epoch 7234/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 19.5419 - val_loss: 102.7431\n",
      "Epoch 7235/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 20.3881 - val_loss: 81.7659\n",
      "Epoch 7236/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 23.5676 - val_loss: 102.9193\n",
      "Epoch 7237/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 19.9578 - val_loss: 95.3990\n",
      "Epoch 7238/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 19.8653 - val_loss: 88.7546\n",
      "Epoch 7239/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 17.8344 - val_loss: 101.7824\n",
      "Epoch 7240/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 16.2769 - val_loss: 101.9937\n",
      "Epoch 7241/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 20.0731 - val_loss: 98.6095\n",
      "Epoch 7242/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 18.6174 - val_loss: 97.2928\n",
      "Epoch 7243/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 19.8479 - val_loss: 88.4134\n",
      "Epoch 7244/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 18.5895 - val_loss: 96.7495\n",
      "Epoch 7245/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 18.7661 - val_loss: 89.0836\n",
      "Epoch 7246/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 19.0439 - val_loss: 109.1098\n",
      "Epoch 7247/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 16.0828 - val_loss: 88.5938\n",
      "Epoch 7248/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 19.4716 - val_loss: 109.9541\n",
      "Epoch 7249/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 15.7437 - val_loss: 98.5061\n",
      "Epoch 7250/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 16.9429 - val_loss: 96.0173\n",
      "Epoch 7251/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 17.5552 - val_loss: 92.2704\n",
      "Epoch 7252/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 13.9623 - val_loss: 107.4190\n",
      "Epoch 7253/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 14.6109 - val_loss: 99.5772\n",
      "Epoch 7254/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 20.0693 - val_loss: 83.2419\n",
      "Epoch 7255/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 15.2048 - val_loss: 101.2537\n",
      "Epoch 7256/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 14.6655 - val_loss: 93.3702\n",
      "Epoch 7257/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 17.0001 - val_loss: 82.9802\n",
      "Epoch 7258/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 18.1279 - val_loss: 94.3078\n",
      "Epoch 7259/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 18.5653 - val_loss: 101.9485\n",
      "Epoch 7260/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 14.7143 - val_loss: 103.5765\n",
      "Epoch 7261/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 17.6715 - val_loss: 100.6438\n",
      "Epoch 7262/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 23.7878 - val_loss: 101.8109\n",
      "Epoch 7263/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 16.4614 - val_loss: 105.4300\n",
      "Epoch 7264/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 18.1041 - val_loss: 92.9618\n",
      "Epoch 7265/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 17.1375 - val_loss: 100.0688\n",
      "Epoch 7266/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 17.7147 - val_loss: 93.7189\n",
      "Epoch 7267/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 15.7059 - val_loss: 105.3875\n",
      "Epoch 7268/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 16.6458 - val_loss: 94.2657\n",
      "Epoch 7269/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 18.5291 - val_loss: 101.3196\n",
      "Epoch 7270/10000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 18.7142 - val_loss: 97.6193\n",
      "Epoch 7271/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 64.0964 - val_loss: 91.7889\n",
      "Epoch 7272/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 52.0315 - val_loss: 95.5857\n",
      "Epoch 7273/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 20.2196 - val_loss: 95.2785\n",
      "Epoch 7274/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 66.9600 - val_loss: 99.3377\n",
      "Epoch 7275/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 22.4914 - val_loss: 94.0837\n",
      "Epoch 7276/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 26.9911 - val_loss: 77.8761\n",
      "Epoch 7277/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 26.2665 - val_loss: 100.0309\n",
      "Epoch 7278/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 23.8045 - val_loss: 101.5611\n",
      "Epoch 7279/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 21.8031 - val_loss: 81.4188\n",
      "Epoch 7280/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 16.9186 - val_loss: 93.9248\n",
      "Epoch 7281/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 18.8245 - val_loss: 91.9144\n",
      "Epoch 7282/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 20.1714 - val_loss: 90.4984\n",
      "Epoch 7283/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 19.9844 - val_loss: 97.6254\n",
      "Epoch 7284/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 14.4658 - val_loss: 85.2519\n",
      "Epoch 7285/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 17.0580 - val_loss: 95.8860\n",
      "Epoch 7286/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 17.2294 - val_loss: 94.1778\n",
      "Epoch 7287/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 17.5978 - val_loss: 95.6679\n",
      "Epoch 7288/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 28.5154 - val_loss: 93.4293\n",
      "Epoch 7289/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 39.7648 - val_loss: 98.9302\n",
      "Epoch 7290/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 20.9197 - val_loss: 91.3376\n",
      "Epoch 7291/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 26.3137 - val_loss: 95.7380\n",
      "Epoch 7292/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 28.2321 - val_loss: 98.4743\n",
      "Epoch 7293/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 75.9661 - val_loss: 99.6633\n",
      "Epoch 7294/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 51.5758 - val_loss: 96.5000\n",
      "Epoch 7295/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 34.6816 - val_loss: 105.7643\n",
      "Epoch 7296/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 29.3251 - val_loss: 95.8197\n",
      "Epoch 7297/10000\n",
      "96/96 [==============================] - 0s 482us/step - loss: 29.1970 - val_loss: 99.2074\n",
      "Epoch 7298/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 20.9813 - val_loss: 102.9113\n",
      "Epoch 7299/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 28.4297 - val_loss: 92.2066\n",
      "Epoch 7300/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 23.6878 - val_loss: 67.4957\n",
      "Epoch 7301/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 29.1745 - val_loss: 86.6756\n",
      "Epoch 7302/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 29.5084 - val_loss: 94.2299\n",
      "Epoch 7303/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 25.7506 - val_loss: 80.5933\n",
      "Epoch 7304/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 20.7312 - val_loss: 84.0455\n",
      "Epoch 7305/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 19.4346 - val_loss: 81.1850\n",
      "Epoch 7306/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 20.8915 - val_loss: 89.3815\n",
      "Epoch 7307/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 18.7116 - val_loss: 91.7247\n",
      "Epoch 7308/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 56.0356 - val_loss: 90.2773\n",
      "Epoch 7309/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 19.8654 - val_loss: 97.3251\n",
      "Epoch 7310/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 22.4635 - val_loss: 105.7041\n",
      "Epoch 7311/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 26.4013 - val_loss: 103.5656\n",
      "Epoch 7312/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 22.3312 - val_loss: 106.2766\n",
      "Epoch 7313/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 27.4660 - val_loss: 107.1950\n",
      "Epoch 7314/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 31.3301 - val_loss: 98.3956\n",
      "Epoch 7315/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 23.7306 - val_loss: 99.8428\n",
      "Epoch 7316/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 18.5181 - val_loss: 76.7232\n",
      "Epoch 7317/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 16.0571 - val_loss: 95.3676\n",
      "Epoch 7318/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 18.6424 - val_loss: 81.9430\n",
      "Epoch 7319/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 21.2167 - val_loss: 99.9684\n",
      "Epoch 7320/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 16.3325 - val_loss: 91.9260\n",
      "Epoch 7321/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 18.0141 - val_loss: 105.9605\n",
      "Epoch 7322/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 16.6090 - val_loss: 91.3251\n",
      "Epoch 7323/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 16.9363 - val_loss: 97.6189\n",
      "Epoch 7324/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 15.9803 - val_loss: 88.2703\n",
      "Epoch 7325/10000\n",
      "96/96 [==============================] - 0s 490us/step - loss: 17.6740 - val_loss: 101.7802\n",
      "Epoch 7326/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 16.3869 - val_loss: 104.8896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7327/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 16.0431 - val_loss: 97.0151\n",
      "Epoch 7328/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 15.8920 - val_loss: 82.2988\n",
      "Epoch 7329/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 16.1500 - val_loss: 99.8375\n",
      "Epoch 7330/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 13.8354 - val_loss: 103.2109\n",
      "Epoch 7331/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 17.4693 - val_loss: 104.7161\n",
      "Epoch 7332/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 18.0022 - val_loss: 95.0026\n",
      "Epoch 7333/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 21.3035 - val_loss: 101.7526\n",
      "Epoch 7334/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 18.7333 - val_loss: 114.0435\n",
      "Epoch 7335/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 16.4104 - val_loss: 98.0518\n",
      "Epoch 7336/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 19.9834 - val_loss: 101.8850\n",
      "Epoch 7337/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 15.1701 - val_loss: 102.6971\n",
      "Epoch 7338/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 13.9813 - val_loss: 99.0198\n",
      "Epoch 7339/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 17.9511 - val_loss: 99.7870\n",
      "Epoch 7340/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 13.9706 - val_loss: 99.1775\n",
      "Epoch 7341/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 15.5144 - val_loss: 97.2922\n",
      "Epoch 7342/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 17.6780 - val_loss: 107.6743\n",
      "Epoch 7343/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 18.2372 - val_loss: 99.5070\n",
      "Epoch 7344/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 17.5275 - val_loss: 96.7379\n",
      "Epoch 7345/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 15.9381 - val_loss: 100.6977\n",
      "Epoch 7346/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 14.5432 - val_loss: 106.3915\n",
      "Epoch 7347/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 18.3052 - val_loss: 95.1561\n",
      "Epoch 7348/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 14.4978 - val_loss: 113.9982\n",
      "Epoch 7349/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 18.1634 - val_loss: 90.1391\n",
      "Epoch 7350/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 14.8178 - val_loss: 103.5905\n",
      "Epoch 7351/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 16.7536 - val_loss: 104.8079\n",
      "Epoch 7352/10000\n",
      "96/96 [==============================] - 0s 502us/step - loss: 15.6667 - val_loss: 102.8996\n",
      "Epoch 7353/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 13.6735 - val_loss: 105.4753\n",
      "Epoch 7354/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 12.8328 - val_loss: 107.4887\n",
      "Epoch 7355/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 12.5749 - val_loss: 108.0142\n",
      "Epoch 7356/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 23.4212 - val_loss: 99.9249\n",
      "Epoch 7357/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 16.3226 - val_loss: 92.5258\n",
      "Epoch 7358/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 17.3352 - val_loss: 99.0414\n",
      "Epoch 7359/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 40.3666 - val_loss: 94.8267\n",
      "Epoch 7360/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 18.6828 - val_loss: 115.8411\n",
      "Epoch 7361/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 18.5757 - val_loss: 101.4158\n",
      "Epoch 7362/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 17.9046 - val_loss: 79.3697\n",
      "Epoch 7363/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 16.9091 - val_loss: 87.7439\n",
      "Epoch 7364/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 18.9170 - val_loss: 111.6614\n",
      "Epoch 7365/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 16.0538 - val_loss: 105.3421\n",
      "Epoch 7366/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 18.2862 - val_loss: 98.4776\n",
      "Epoch 7367/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 16.9552 - val_loss: 93.4049\n",
      "Epoch 7368/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 15.0118 - val_loss: 96.3141\n",
      "Epoch 7369/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 16.3540 - val_loss: 96.8290\n",
      "Epoch 7370/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 18.0861 - val_loss: 95.6625\n",
      "Epoch 7371/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 13.6349 - val_loss: 82.8392\n",
      "Epoch 7372/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 16.7057 - val_loss: 109.9653\n",
      "Epoch 7373/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 20.6430 - val_loss: 109.0731\n",
      "Epoch 7374/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 15.7193 - val_loss: 84.8419\n",
      "Epoch 7375/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 14.5894 - val_loss: 111.6899\n",
      "Epoch 7376/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 12.4235 - val_loss: 102.7221\n",
      "Epoch 7377/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 13.3598 - val_loss: 100.3279\n",
      "Epoch 7378/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 15.3529 - val_loss: 100.3903\n",
      "Epoch 7379/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 25.2554 - val_loss: 103.6280\n",
      "Epoch 7380/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 15.2144 - val_loss: 92.5226\n",
      "Epoch 7381/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 18.5975 - val_loss: 99.4936\n",
      "Epoch 7382/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 17.2929 - val_loss: 82.3964\n",
      "Epoch 7383/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 20.2853 - val_loss: 92.3425\n",
      "Epoch 7384/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 16.9890 - val_loss: 80.7764\n",
      "Epoch 7385/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 13.8916 - val_loss: 96.5856\n",
      "Epoch 7386/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 14.1778 - val_loss: 105.1108\n",
      "Epoch 7387/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 17.8085 - val_loss: 88.7712\n",
      "Epoch 7388/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 17.8439 - val_loss: 90.9303\n",
      "Epoch 7389/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 13.9153 - val_loss: 95.7873\n",
      "Epoch 7390/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 16.1855 - val_loss: 91.7531\n",
      "Epoch 7391/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 14.9128 - val_loss: 97.8430\n",
      "Epoch 7392/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 13.5606 - val_loss: 95.8439\n",
      "Epoch 7393/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 14.5187 - val_loss: 102.8486\n",
      "Epoch 7394/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 14.4208 - val_loss: 83.1976\n",
      "Epoch 7395/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 17.3098 - val_loss: 97.6203\n",
      "Epoch 7396/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 19.9447 - val_loss: 92.5995\n",
      "Epoch 7397/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 14.0151 - val_loss: 108.4302\n",
      "Epoch 7398/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 15.6894 - val_loss: 91.9969\n",
      "Epoch 7399/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 24.7835 - val_loss: 101.0301\n",
      "Epoch 7400/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 20.2222 - val_loss: 84.6144\n",
      "Epoch 7401/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 12.8214 - val_loss: 83.9102\n",
      "Epoch 7402/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 17.7115 - val_loss: 98.6889\n",
      "Epoch 7403/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 15.7530 - val_loss: 92.2420\n",
      "Epoch 7404/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 19.9324 - val_loss: 94.4597\n",
      "Epoch 7405/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 14.4100 - val_loss: 95.4790\n",
      "Epoch 7406/10000\n",
      "96/96 [==============================] - 0s 488us/step - loss: 16.8634 - val_loss: 89.0233\n",
      "Epoch 7407/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 14.0313 - val_loss: 96.2437\n",
      "Epoch 7408/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 14.2350 - val_loss: 90.3644\n",
      "Epoch 7409/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 13.3084 - val_loss: 87.9106\n",
      "Epoch 7410/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 15.5462 - val_loss: 101.6293\n",
      "Epoch 7411/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 13.8040 - val_loss: 92.4782\n",
      "Epoch 7412/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 15.1355 - val_loss: 101.0950\n",
      "Epoch 7413/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 16.4911 - val_loss: 98.4805\n",
      "Epoch 7414/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 17.8040 - val_loss: 102.4167\n",
      "Epoch 7415/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 12.2493 - val_loss: 98.6964\n",
      "Epoch 7416/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 12.4635 - val_loss: 93.8752\n",
      "Epoch 7417/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 20.1615 - val_loss: 90.5016\n",
      "Epoch 7418/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 12.3919 - val_loss: 91.0498\n",
      "Epoch 7419/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 14.4081 - val_loss: 102.7339\n",
      "Epoch 7420/10000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 14.9869 - val_loss: 86.4010\n",
      "Epoch 7421/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 15.2215 - val_loss: 94.9700\n",
      "Epoch 7422/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 13.8918 - val_loss: 101.4381\n",
      "Epoch 7423/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 14.3994 - val_loss: 105.2503\n",
      "Epoch 7424/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 16.1679 - val_loss: 91.0614\n",
      "Epoch 7425/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 16.4402 - val_loss: 99.6061\n",
      "Epoch 7426/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 16.1011 - val_loss: 99.4043\n",
      "Epoch 7427/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 14.2384 - val_loss: 88.8131\n",
      "Epoch 7428/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 15.1691 - val_loss: 101.6017\n",
      "Epoch 7429/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 12.8587 - val_loss: 97.9264\n",
      "Epoch 7430/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 22.2774 - val_loss: 97.1966\n",
      "Epoch 7431/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 15.4636 - val_loss: 88.4887\n",
      "Epoch 7432/10000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 13.9381 - val_loss: 94.8512\n",
      "Epoch 7433/10000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 11.4224 - val_loss: 89.9538\n",
      "Epoch 7434/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 14.9066 - val_loss: 92.7490\n",
      "Epoch 7435/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 10.5177 - val_loss: 93.1436\n",
      "Epoch 7436/10000\n",
      "96/96 [==============================] - 0s 481us/step - loss: 17.0626 - val_loss: 103.6298\n",
      "Epoch 7437/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 12.4259 - val_loss: 93.2399\n",
      "Epoch 7438/10000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 12.0276 - val_loss: 114.9463\n",
      "Epoch 7439/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 12.1023 - val_loss: 106.4377\n",
      "Epoch 7440/10000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 14.3642 - val_loss: 91.1976\n",
      "Epoch 7441/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 11.9428 - val_loss: 83.0072\n",
      "Epoch 7442/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 12.9476 - val_loss: 91.2290\n",
      "Epoch 7443/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 20.6463 - val_loss: 81.9032\n",
      "Epoch 7444/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 14.6534 - val_loss: 87.6248\n",
      "Epoch 7445/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 19.9953 - val_loss: 86.5312\n",
      "Epoch 7446/10000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 13.9916 - val_loss: 97.6795\n",
      "Epoch 7447/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 18.2926 - val_loss: 105.0265\n",
      "Epoch 7448/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 11.9256 - val_loss: 95.7888\n",
      "Epoch 7449/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 13.2014 - val_loss: 89.8894\n",
      "Epoch 7450/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 12.8756 - val_loss: 102.6180\n",
      "Epoch 7451/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 10.7067 - val_loss: 101.8190\n",
      "Epoch 7452/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 13.4896 - val_loss: 97.4822\n",
      "Epoch 7453/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 15.1448 - val_loss: 116.3238\n",
      "Epoch 7454/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 13.0150 - val_loss: 91.8873\n",
      "Epoch 7455/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 13.6724 - val_loss: 100.3538\n",
      "Epoch 7456/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 12.3901 - val_loss: 113.6023\n",
      "Epoch 7457/10000\n",
      "96/96 [==============================] - 0s 487us/step - loss: 18.3211 - val_loss: 85.7158\n",
      "Epoch 7458/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 14.0558 - val_loss: 97.7115\n",
      "Epoch 7459/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 14.3790 - val_loss: 112.7257\n",
      "Epoch 7460/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 15.6311 - val_loss: 95.9947\n",
      "Epoch 7461/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 15.4220 - val_loss: 98.9179\n",
      "Epoch 7462/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 36.6684 - val_loss: 120.9380\n",
      "Epoch 7463/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 12.8068 - val_loss: 119.3004\n",
      "Epoch 7464/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 15.0334 - val_loss: 105.7338\n",
      "Epoch 7465/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 20.4588 - val_loss: 101.0841\n",
      "Epoch 7466/10000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 16.0680 - val_loss: 84.9319\n",
      "Epoch 7467/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 16.4074 - val_loss: 106.8410\n",
      "Epoch 7468/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 14.3979 - val_loss: 102.6190\n",
      "Epoch 7469/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 22.0147 - val_loss: 90.8829\n",
      "Epoch 7470/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 14.2243 - val_loss: 105.5263\n",
      "Epoch 7471/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 15.4939 - val_loss: 87.1146\n",
      "Epoch 7472/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 16.6699 - val_loss: 97.7529\n",
      "Epoch 7473/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 13.2514 - val_loss: 108.6784\n",
      "Epoch 7474/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 15.3653 - val_loss: 97.3907\n",
      "Epoch 7475/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 13.5558 - val_loss: 114.4892\n",
      "Epoch 7476/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 14.2529 - val_loss: 103.5818\n",
      "Epoch 7477/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 12.8447 - val_loss: 106.3409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7478/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 19.2737 - val_loss: 102.2819\n",
      "Epoch 7479/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 17.2601 - val_loss: 110.2496\n",
      "Epoch 7480/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 17.4933 - val_loss: 107.3363\n",
      "Epoch 7481/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 14.9347 - val_loss: 99.5367\n",
      "Epoch 7482/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 24.2953 - val_loss: 109.3428\n",
      "Epoch 7483/10000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 23.7157 - val_loss: 79.6584\n",
      "Epoch 7484/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 29.4988 - val_loss: 93.3624\n",
      "Epoch 7485/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 32.1770 - val_loss: 117.3997\n",
      "Epoch 7486/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 23.9173 - val_loss: 102.5794\n",
      "Epoch 7487/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 12.9150 - val_loss: 104.1825\n",
      "Epoch 7488/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 11.3369 - val_loss: 106.0729\n",
      "Epoch 7489/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 12.0068 - val_loss: 111.0013\n",
      "Epoch 7490/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 14.8286 - val_loss: 89.5904\n",
      "Epoch 7491/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 16.3800 - val_loss: 102.1181\n",
      "Epoch 7492/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 11.9755 - val_loss: 106.5686\n",
      "Epoch 7493/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 12.6211 - val_loss: 111.4688\n",
      "Epoch 7494/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 13.0527 - val_loss: 111.2332\n",
      "Epoch 7495/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 14.1635 - val_loss: 98.7683\n",
      "Epoch 7496/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 15.2434 - val_loss: 110.5638\n",
      "Epoch 7497/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 12.8875 - val_loss: 114.1896\n",
      "Epoch 7498/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 14.8545 - val_loss: 89.0010\n",
      "Epoch 7499/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 15.4069 - val_loss: 101.4100\n",
      "Epoch 7500/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 11.2663 - val_loss: 100.7996\n",
      "Epoch 7501/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 13.7973 - val_loss: 102.8506\n",
      "Epoch 7502/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 13.6915 - val_loss: 96.4767\n",
      "Epoch 7503/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 13.1260 - val_loss: 113.0684\n",
      "Epoch 7504/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 11.7244 - val_loss: 104.5975\n",
      "Epoch 7505/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 14.7028 - val_loss: 108.7261\n",
      "Epoch 7506/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 11.9686 - val_loss: 109.5878\n",
      "Epoch 7507/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 10.4384 - val_loss: 92.7603\n",
      "Epoch 7508/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 10.2785 - val_loss: 110.8267\n",
      "Epoch 7509/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 11.3246 - val_loss: 109.9403\n",
      "Epoch 7510/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 11.5974 - val_loss: 106.7705\n",
      "Epoch 7511/10000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 16.4581 - val_loss: 95.5020\n",
      "Epoch 7512/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 12.3968 - val_loss: 106.0095\n",
      "Epoch 7513/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 11.3322 - val_loss: 100.7953\n",
      "Epoch 7514/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 12.4339 - val_loss: 107.2411\n",
      "Epoch 7515/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 11.1169 - val_loss: 101.8255\n",
      "Epoch 7516/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 15.5770 - val_loss: 112.6910\n",
      "Epoch 7517/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 12.5822 - val_loss: 107.1473\n",
      "Epoch 7518/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 13.1151 - val_loss: 93.2961\n",
      "Epoch 7519/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 10.2112 - val_loss: 107.0304\n",
      "Epoch 7520/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 12.6104 - val_loss: 108.3022\n",
      "Epoch 7521/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 25.2315 - val_loss: 90.0498\n",
      "Epoch 7522/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 14.5622 - val_loss: 85.6734\n",
      "Epoch 7523/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 20.7412 - val_loss: 87.4717\n",
      "Epoch 7524/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 46.3430 - val_loss: 76.0571\n",
      "Epoch 7525/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 24.5349 - val_loss: 98.0421\n",
      "Epoch 7526/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 22.8069 - val_loss: 103.1943\n",
      "Epoch 7527/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 30.1492 - val_loss: 103.8679\n",
      "Epoch 7528/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 26.0255 - val_loss: 111.7544\n",
      "Epoch 7529/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 21.4870 - val_loss: 85.2964\n",
      "Epoch 7530/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 18.3554 - val_loss: 102.6763\n",
      "Epoch 7531/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 22.2903 - val_loss: 78.4165\n",
      "Epoch 7532/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 28.8278 - val_loss: 97.6927\n",
      "Epoch 7533/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 21.3903 - val_loss: 99.4813\n",
      "Epoch 7534/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 18.3821 - val_loss: 108.2711\n",
      "Epoch 7535/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 20.3996 - val_loss: 112.1577\n",
      "Epoch 7536/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 17.5197 - val_loss: 83.9934\n",
      "Epoch 7537/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 19.5622 - val_loss: 93.6655\n",
      "Epoch 7538/10000\n",
      "96/96 [==============================] - 0s 510us/step - loss: 17.1133 - val_loss: 101.9201\n",
      "Epoch 7539/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 25.8736 - val_loss: 89.0868\n",
      "Epoch 7540/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 15.2688 - val_loss: 102.6616\n",
      "Epoch 7541/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 20.5136 - val_loss: 94.9727\n",
      "Epoch 7542/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 13.6190 - val_loss: 106.1716\n",
      "Epoch 7543/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 15.5207 - val_loss: 106.5277\n",
      "Epoch 7544/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 14.7934 - val_loss: 103.6435\n",
      "Epoch 7545/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 13.9295 - val_loss: 100.6901\n",
      "Epoch 7546/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 13.5666 - val_loss: 105.9530\n",
      "Epoch 7547/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 25.1497 - val_loss: 101.4654\n",
      "Epoch 7548/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 16.6686 - val_loss: 100.1278\n",
      "Epoch 7549/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 22.2278 - val_loss: 98.7525\n",
      "Epoch 7550/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 13.7790 - val_loss: 95.5474\n",
      "Epoch 7551/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 20.2880 - val_loss: 108.2516\n",
      "Epoch 7552/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 15.5850 - val_loss: 94.6527\n",
      "Epoch 7553/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 14.3820 - val_loss: 102.2225\n",
      "Epoch 7554/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 14.7941 - val_loss: 102.2433\n",
      "Epoch 7555/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 14.1796 - val_loss: 107.2268\n",
      "Epoch 7556/10000\n",
      "96/96 [==============================] - 0s 484us/step - loss: 16.7612 - val_loss: 101.3695\n",
      "Epoch 7557/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 13.9101 - val_loss: 102.9193\n",
      "Epoch 7558/10000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 16.0873 - val_loss: 99.9134\n",
      "Epoch 7559/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 11.4706 - val_loss: 91.4572\n",
      "Epoch 7560/10000\n",
      "96/96 [==============================] - 0s 485us/step - loss: 14.6038 - val_loss: 106.6523\n",
      "Epoch 7561/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 11.1954 - val_loss: 110.5501\n",
      "Epoch 7562/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 18.9543 - val_loss: 109.8893\n",
      "Epoch 7563/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 11.6734 - val_loss: 95.0166\n",
      "Epoch 7564/10000\n",
      "96/96 [==============================] - 0s 579us/step - loss: 18.9090 - val_loss: 107.1389\n",
      "Epoch 7565/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 15.2891 - val_loss: 109.3846\n",
      "Epoch 7566/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 16.5150 - val_loss: 108.8492\n",
      "Epoch 7567/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 12.0079 - val_loss: 87.8864\n",
      "Epoch 7568/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 16.6805 - val_loss: 87.2280\n",
      "Epoch 7569/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 16.2352 - val_loss: 103.3534\n",
      "Epoch 7570/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 15.4868 - val_loss: 88.9539\n",
      "Epoch 7571/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 14.0631 - val_loss: 112.1268\n",
      "Epoch 7572/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 11.8268 - val_loss: 95.6957\n",
      "Epoch 7573/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 11.5260 - val_loss: 111.4234\n",
      "Epoch 7574/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 12.4077 - val_loss: 93.0334\n",
      "Epoch 7575/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 10.0667 - val_loss: 111.2335\n",
      "Epoch 7576/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 13.2594 - val_loss: 100.7190\n",
      "Epoch 7577/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 16.0472 - val_loss: 107.3699\n",
      "Epoch 7578/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 13.8858 - val_loss: 101.7331\n",
      "Epoch 7579/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 12.5301 - val_loss: 97.0105\n",
      "Epoch 7580/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 12.7464 - val_loss: 107.1366\n",
      "Epoch 7581/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 12.5084 - val_loss: 100.7108\n",
      "Epoch 7582/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 10.9794 - val_loss: 106.8521\n",
      "Epoch 7583/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 14.4105 - val_loss: 95.8150\n",
      "Epoch 7584/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 31.8929 - val_loss: 103.6602\n",
      "Epoch 7585/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 15.3971 - val_loss: 104.1060\n",
      "Epoch 7586/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 24.0535 - val_loss: 82.0146\n",
      "Epoch 7587/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 23.0995 - val_loss: 94.5724\n",
      "Epoch 7588/10000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 25.0727 - val_loss: 86.0840\n",
      "Epoch 7589/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 25.9071 - val_loss: 80.9347\n",
      "Epoch 7590/10000\n",
      "96/96 [==============================] - 0s 500us/step - loss: 20.4172 - val_loss: 96.5883\n",
      "Epoch 7591/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 19.2707 - val_loss: 91.2263\n",
      "Epoch 7592/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 20.3978 - val_loss: 99.0370\n",
      "Epoch 7593/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 18.0548 - val_loss: 89.2635\n",
      "Epoch 7594/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 19.1421 - val_loss: 80.5025\n",
      "Epoch 7595/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 14.7092 - val_loss: 94.7294\n",
      "Epoch 7596/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 15.1922 - val_loss: 100.5675\n",
      "Epoch 7597/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 27.9847 - val_loss: 90.7926\n",
      "Epoch 7598/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 19.8027 - val_loss: 99.0191\n",
      "Epoch 7599/10000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 26.1824 - val_loss: 110.6422\n",
      "Epoch 7600/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 21.8726 - val_loss: 104.2412\n",
      "Epoch 7601/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 16.1532 - val_loss: 97.8877\n",
      "Epoch 7602/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 22.8696 - val_loss: 95.2159\n",
      "Epoch 7603/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 18.9185 - val_loss: 99.2656\n",
      "Epoch 7604/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 20.6917 - val_loss: 79.6063\n",
      "Epoch 7605/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 18.3965 - val_loss: 83.1261\n",
      "Epoch 7606/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 19.2793 - val_loss: 98.1587\n",
      "Epoch 7607/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 20.6611 - val_loss: 101.4240\n",
      "Epoch 7608/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 17.5911 - val_loss: 85.6728\n",
      "Epoch 7609/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 18.0365 - val_loss: 100.5905\n",
      "Epoch 7610/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 18.7113 - val_loss: 102.4861\n",
      "Epoch 7611/10000\n",
      "96/96 [==============================] - 0s 493us/step - loss: 14.0150 - val_loss: 102.5402\n",
      "Epoch 7612/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 13.0610 - val_loss: 94.9201\n",
      "Epoch 7613/10000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 13.4205 - val_loss: 99.7527\n",
      "Epoch 7614/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 13.6293 - val_loss: 89.9559\n",
      "Epoch 7615/10000\n",
      "96/96 [==============================] - 0s 525us/step - loss: 13.8365 - val_loss: 96.4225\n",
      "Epoch 7616/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 14.6920 - val_loss: 97.0300\n",
      "Epoch 7617/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 12.9024 - val_loss: 97.2855\n",
      "Epoch 7618/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 16.3438 - val_loss: 100.3520\n",
      "Epoch 7619/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 12.3693 - val_loss: 107.8400\n",
      "Epoch 7620/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 17.4465 - val_loss: 116.0104\n",
      "Epoch 7621/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 13.6069 - val_loss: 98.4805\n",
      "Epoch 7622/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 13.5998 - val_loss: 108.7278\n",
      "Epoch 7623/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 11.8539 - val_loss: 104.1956\n",
      "Epoch 7624/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 11.6253 - val_loss: 101.0259\n",
      "Epoch 7625/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 15.5668 - val_loss: 115.0941\n",
      "Epoch 7626/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 22.2286 - val_loss: 80.0336\n",
      "Epoch 7627/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 10.1870 - val_loss: 96.5043\n",
      "Epoch 7628/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 415us/step - loss: 11.0109 - val_loss: 87.6367\n",
      "Epoch 7629/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 15.0423 - val_loss: 79.9051\n",
      "Epoch 7630/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 16.3916 - val_loss: 85.8360\n",
      "Epoch 7631/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 15.9616 - val_loss: 99.7623\n",
      "Epoch 7632/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 19.8658 - val_loss: 86.5310\n",
      "Epoch 7633/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 14.9690 - val_loss: 100.5379\n",
      "Epoch 7634/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 18.8842 - val_loss: 86.2320\n",
      "Epoch 7635/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 17.4297 - val_loss: 108.0182\n",
      "Epoch 7636/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 15.3165 - val_loss: 93.9075\n",
      "Epoch 7637/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 12.8536 - val_loss: 105.8213\n",
      "Epoch 7638/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 14.2975 - val_loss: 99.3538\n",
      "Epoch 7639/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 13.3897 - val_loss: 103.7732\n",
      "Epoch 7640/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 12.2616 - val_loss: 107.5788\n",
      "Epoch 7641/10000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 18.1577 - val_loss: 109.2344\n",
      "Epoch 7642/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 12.3480 - val_loss: 96.5113\n",
      "Epoch 7643/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 17.9282 - val_loss: 105.4019\n",
      "Epoch 7644/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 11.4148 - val_loss: 109.4860\n",
      "Epoch 7645/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 10.8235 - val_loss: 110.4792\n",
      "Epoch 7646/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 11.8748 - val_loss: 107.6706\n",
      "Epoch 7647/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 10.9313 - val_loss: 113.0200\n",
      "Epoch 7648/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 11.1127 - val_loss: 104.9207\n",
      "Epoch 7649/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 14.0642 - val_loss: 96.4854\n",
      "Epoch 7650/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 13.0749 - val_loss: 101.1897\n",
      "Epoch 7651/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 16.8705 - val_loss: 107.4843\n",
      "Epoch 7652/10000\n",
      "96/96 [==============================] - 0s 349us/step - loss: 11.2672 - val_loss: 104.7760\n",
      "Epoch 7653/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 13.1865 - val_loss: 96.3920\n",
      "Epoch 7654/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 10.7048 - val_loss: 96.5991\n",
      "Epoch 7655/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 13.9428 - val_loss: 109.7958\n",
      "Epoch 7656/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 14.0306 - val_loss: 104.2594\n",
      "Epoch 7657/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 21.0668 - val_loss: 104.7754\n",
      "Epoch 7658/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 16.4160 - val_loss: 100.4316\n",
      "Epoch 7659/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 20.9846 - val_loss: 95.6452\n",
      "Epoch 7660/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 18.7573 - val_loss: 89.2384\n",
      "Epoch 7661/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 19.6353 - val_loss: 87.8901\n",
      "Epoch 7662/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 16.0156 - val_loss: 100.7422\n",
      "Epoch 7663/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 12.1639 - val_loss: 98.2962\n",
      "Epoch 7664/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 17.1921 - val_loss: 102.2331\n",
      "Epoch 7665/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 16.3517 - val_loss: 78.1235\n",
      "Epoch 7666/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 16.4088 - val_loss: 98.8808\n",
      "Epoch 7667/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 14.4187 - val_loss: 99.0700\n",
      "Epoch 7668/10000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 24.0422 - val_loss: 94.6806\n",
      "Epoch 7669/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 13.0338 - val_loss: 96.6294\n",
      "Epoch 7670/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 16.9004 - val_loss: 98.3284\n",
      "Epoch 7671/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 20.1143 - val_loss: 107.3313\n",
      "Epoch 7672/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 13.8014 - val_loss: 89.8514\n",
      "Epoch 7673/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 15.2046 - val_loss: 102.6890\n",
      "Epoch 7674/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 14.4904 - val_loss: 99.5835\n",
      "Epoch 7675/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 18.1680 - val_loss: 98.0212\n",
      "Epoch 7676/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 12.7467 - val_loss: 104.9479\n",
      "Epoch 7677/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 17.2057 - val_loss: 101.5661\n",
      "Epoch 7678/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 17.9198 - val_loss: 102.6488\n",
      "Epoch 7679/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 13.3842 - val_loss: 72.7371\n",
      "Epoch 7680/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 15.6700 - val_loss: 97.1093\n",
      "Epoch 7681/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 15.9892 - val_loss: 87.0407\n",
      "Epoch 7682/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 23.6701 - val_loss: 100.1686\n",
      "Epoch 7683/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 14.6152 - val_loss: 88.4537\n",
      "Epoch 7684/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 28.9625 - val_loss: 100.8340\n",
      "Epoch 7685/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 13.1691 - val_loss: 97.0218\n",
      "Epoch 7686/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 26.3691 - val_loss: 101.7860\n",
      "Epoch 7687/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 21.3267 - val_loss: 111.5585\n",
      "Epoch 7688/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 20.4188 - val_loss: 105.5029\n",
      "Epoch 7689/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 15.7523 - val_loss: 106.2367\n",
      "Epoch 7690/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 16.1548 - val_loss: 92.0375\n",
      "Epoch 7691/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 15.3165 - val_loss: 87.5630\n",
      "Epoch 7692/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 16.9677 - val_loss: 85.7517\n",
      "Epoch 7693/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 15.1397 - val_loss: 89.2030\n",
      "Epoch 7694/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 41.1306 - val_loss: 104.0497\n",
      "Epoch 7695/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 28.4473 - val_loss: 100.2395\n",
      "Epoch 7696/10000\n",
      "96/96 [==============================] - 0s 494us/step - loss: 31.1257 - val_loss: 109.6414\n",
      "Epoch 7697/10000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 31.9691 - val_loss: 108.2356\n",
      "Epoch 7698/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 20.4933 - val_loss: 109.3927\n",
      "Epoch 7699/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 20.5543 - val_loss: 87.6062\n",
      "Epoch 7700/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 23.5243 - val_loss: 92.9689\n",
      "Epoch 7701/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 18.9023 - val_loss: 82.4288\n",
      "Epoch 7702/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 19.4181 - val_loss: 82.9394\n",
      "Epoch 7703/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 17.0357 - val_loss: 107.5098\n",
      "Epoch 7704/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 16.2164 - val_loss: 106.5936\n",
      "Epoch 7705/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 15.8562 - val_loss: 109.3309\n",
      "Epoch 7706/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 18.2891 - val_loss: 100.9327\n",
      "Epoch 7707/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 16.0727 - val_loss: 100.7897\n",
      "Epoch 7708/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 13.2630 - val_loss: 107.9633\n",
      "Epoch 7709/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 18.8355 - val_loss: 107.4260\n",
      "Epoch 7710/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 14.4801 - val_loss: 98.5654\n",
      "Epoch 7711/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 16.8183 - val_loss: 103.3150\n",
      "Epoch 7712/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 13.9277 - val_loss: 106.3227\n",
      "Epoch 7713/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 12.1066 - val_loss: 105.5195\n",
      "Epoch 7714/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 14.3233 - val_loss: 86.1602\n",
      "Epoch 7715/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 17.6950 - val_loss: 103.4729\n",
      "Epoch 7716/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 13.4072 - val_loss: 75.7557\n",
      "Epoch 7717/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 14.2738 - val_loss: 100.0410\n",
      "Epoch 7718/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 12.7403 - val_loss: 109.5100\n",
      "Epoch 7719/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 11.8630 - val_loss: 103.9188\n",
      "Epoch 7720/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 13.6088 - val_loss: 88.0181\n",
      "Epoch 7721/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 15.5219 - val_loss: 97.0832\n",
      "Epoch 7722/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 16.5723 - val_loss: 101.6567\n",
      "Epoch 7723/10000\n",
      "96/96 [==============================] - 0s 481us/step - loss: 20.7532 - val_loss: 95.9807\n",
      "Epoch 7724/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 13.0302 - val_loss: 89.8524\n",
      "Epoch 7725/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 17.1013 - val_loss: 106.9987\n",
      "Epoch 7726/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 14.5443 - val_loss: 99.0335\n",
      "Epoch 7727/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 14.0933 - val_loss: 84.3348\n",
      "Epoch 7728/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 17.5015 - val_loss: 87.9822\n",
      "Epoch 7729/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 9.5330 - val_loss: 111.0318\n",
      "Epoch 7730/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 13.8196 - val_loss: 89.2406\n",
      "Epoch 7731/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 11.1744 - val_loss: 84.8625\n",
      "Epoch 7732/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 13.2349 - val_loss: 94.6318\n",
      "Epoch 7733/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 13.5623 - val_loss: 91.1779\n",
      "Epoch 7734/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 10.5904 - val_loss: 100.9891\n",
      "Epoch 7735/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 13.0501 - val_loss: 102.2929\n",
      "Epoch 7736/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 14.6805 - val_loss: 94.3431\n",
      "Epoch 7737/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 15.4879 - val_loss: 91.5042\n",
      "Epoch 7738/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 13.8773 - val_loss: 91.7047\n",
      "Epoch 7739/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 13.4719 - val_loss: 95.6493\n",
      "Epoch 7740/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 11.5719 - val_loss: 97.4233\n",
      "Epoch 7741/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 12.7650 - val_loss: 95.5040\n",
      "Epoch 7742/10000\n",
      "96/96 [==============================] - 0s 357us/step - loss: 13.6258 - val_loss: 104.8600\n",
      "Epoch 7743/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 10.8952 - val_loss: 86.0211\n",
      "Epoch 7744/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 10.5879 - val_loss: 84.4916\n",
      "Epoch 7745/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 12.9047 - val_loss: 92.0623\n",
      "Epoch 7746/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 11.1765 - val_loss: 101.9289\n",
      "Epoch 7747/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 12.2445 - val_loss: 93.6206\n",
      "Epoch 7748/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 15.0883 - val_loss: 94.4695\n",
      "Epoch 7749/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 12.1358 - val_loss: 98.6266\n",
      "Epoch 7750/10000\n",
      "96/96 [==============================] - 0s 490us/step - loss: 13.8164 - val_loss: 102.4165\n",
      "Epoch 7751/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 13.6752 - val_loss: 97.2892\n",
      "Epoch 7752/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 12.7391 - val_loss: 97.5008\n",
      "Epoch 7753/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 11.7558 - val_loss: 97.5703\n",
      "Epoch 7754/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 13.6306 - val_loss: 96.0519\n",
      "Epoch 7755/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 12.0083 - val_loss: 100.3616\n",
      "Epoch 7756/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 11.8938 - val_loss: 105.7212\n",
      "Epoch 7757/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 17.9575 - val_loss: 85.2834\n",
      "Epoch 7758/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 15.1434 - val_loss: 98.2380\n",
      "Epoch 7759/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 26.9970 - val_loss: 79.9959\n",
      "Epoch 7760/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 21.4817 - val_loss: 80.6054\n",
      "Epoch 7761/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 25.8046 - val_loss: 85.7518\n",
      "Epoch 7762/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 35.4045 - val_loss: 93.9827\n",
      "Epoch 7763/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 27.1038 - val_loss: 89.1156\n",
      "Epoch 7764/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 29.9128 - val_loss: 97.6099\n",
      "Epoch 7765/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 26.7948 - val_loss: 112.8019\n",
      "Epoch 7766/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 36.8163 - val_loss: 88.9230\n",
      "Epoch 7767/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 30.3112 - val_loss: 95.3577\n",
      "Epoch 7768/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 20.2000 - val_loss: 67.9552\n",
      "Epoch 7769/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 20.7708 - val_loss: 93.5812\n",
      "Epoch 7770/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 23.4869 - val_loss: 77.1912\n",
      "Epoch 7771/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 29.7390 - val_loss: 79.8960\n",
      "Epoch 7772/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 19.1645 - val_loss: 79.9125\n",
      "Epoch 7773/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 17.8385 - val_loss: 81.9016\n",
      "Epoch 7774/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 21.6640 - val_loss: 91.4329\n",
      "Epoch 7775/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 18.6660 - val_loss: 97.6252\n",
      "Epoch 7776/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 20.8725 - val_loss: 93.8675\n",
      "Epoch 7777/10000\n",
      "96/96 [==============================] - 0s 509us/step - loss: 22.2792 - val_loss: 79.7934\n",
      "Epoch 7778/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 18.7764 - val_loss: 87.3276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7779/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 17.8449 - val_loss: 94.0987\n",
      "Epoch 7780/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 18.6092 - val_loss: 75.6833\n",
      "Epoch 7781/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 20.1352 - val_loss: 96.5282\n",
      "Epoch 7782/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 19.5234 - val_loss: 72.6389\n",
      "Epoch 7783/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 15.4332 - val_loss: 101.0476\n",
      "Epoch 7784/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 15.0947 - val_loss: 101.5079\n",
      "Epoch 7785/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 14.9771 - val_loss: 96.6847\n",
      "Epoch 7786/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 16.8065 - val_loss: 91.1886\n",
      "Epoch 7787/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 15.7276 - val_loss: 102.2497\n",
      "Epoch 7788/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 17.9940 - val_loss: 101.3156\n",
      "Epoch 7789/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 14.3679 - val_loss: 93.9026\n",
      "Epoch 7790/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 14.0154 - val_loss: 103.3889\n",
      "Epoch 7791/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 12.6925 - val_loss: 103.5625\n",
      "Epoch 7792/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 16.9895 - val_loss: 90.9317\n",
      "Epoch 7793/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 17.6193 - val_loss: 84.8112\n",
      "Epoch 7794/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 12.1477 - val_loss: 82.0522\n",
      "Epoch 7795/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 16.4797 - val_loss: 92.5511\n",
      "Epoch 7796/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 13.0212 - val_loss: 95.3599\n",
      "Epoch 7797/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 20.0829 - val_loss: 97.3264\n",
      "Epoch 7798/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 10.8322 - val_loss: 98.2117\n",
      "Epoch 7799/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 17.2375 - val_loss: 95.4894\n",
      "Epoch 7800/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 22.1629 - val_loss: 102.7402\n",
      "Epoch 7801/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 29.0928 - val_loss: 106.0064\n",
      "Epoch 7802/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 23.5750 - val_loss: 91.7085\n",
      "Epoch 7803/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 22.0001 - val_loss: 96.3545\n",
      "Epoch 7804/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 20.1376 - val_loss: 79.3951\n",
      "Epoch 7805/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 20.4301 - val_loss: 93.6830\n",
      "Epoch 7806/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 17.3243 - val_loss: 99.4147\n",
      "Epoch 7807/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 17.5293 - val_loss: 91.2313\n",
      "Epoch 7808/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 16.3056 - val_loss: 87.5926\n",
      "Epoch 7809/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 19.7787 - val_loss: 116.8959\n",
      "Epoch 7810/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 16.4519 - val_loss: 108.4212\n",
      "Epoch 7811/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 13.6689 - val_loss: 95.0228\n",
      "Epoch 7812/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 13.1819 - val_loss: 111.2675\n",
      "Epoch 7813/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 13.3959 - val_loss: 94.5079\n",
      "Epoch 7814/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 16.9990 - val_loss: 116.9892\n",
      "Epoch 7815/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 15.6824 - val_loss: 99.1887\n",
      "Epoch 7816/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 11.4796 - val_loss: 107.1232\n",
      "Epoch 7817/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 12.6748 - val_loss: 110.9658\n",
      "Epoch 7818/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 12.1406 - val_loss: 105.0282\n",
      "Epoch 7819/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 11.1667 - val_loss: 100.9037\n",
      "Epoch 7820/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 13.2525 - val_loss: 111.6072\n",
      "Epoch 7821/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 10.2619 - val_loss: 107.5999\n",
      "Epoch 7822/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 13.0597 - val_loss: 99.2066\n",
      "Epoch 7823/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 10.6996 - val_loss: 112.5252\n",
      "Epoch 7824/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 15.1850 - val_loss: 115.0683\n",
      "Epoch 7825/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 11.3034 - val_loss: 91.0537\n",
      "Epoch 7826/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 13.5085 - val_loss: 104.7673\n",
      "Epoch 7827/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 13.1634 - val_loss: 108.2091\n",
      "Epoch 7828/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 10.9929 - val_loss: 105.2689\n",
      "Epoch 7829/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 12.4366 - val_loss: 100.1441\n",
      "Epoch 7830/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 11.3779 - val_loss: 112.6910\n",
      "Epoch 7831/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 11.4278 - val_loss: 118.0475\n",
      "Epoch 7832/10000\n",
      "96/96 [==============================] - 0s 492us/step - loss: 11.5634 - val_loss: 96.8327\n",
      "Epoch 7833/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 9.5153 - val_loss: 102.7446\n",
      "Epoch 7834/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 13.4675 - val_loss: 112.4762\n",
      "Epoch 7835/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 13.4895 - val_loss: 96.3575\n",
      "Epoch 7836/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 35.7936 - val_loss: 109.2837\n",
      "Epoch 7837/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 14.1088 - val_loss: 97.2516\n",
      "Epoch 7838/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 13.5793 - val_loss: 94.3102\n",
      "Epoch 7839/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 12.7787 - val_loss: 86.0838\n",
      "Epoch 7840/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 13.1915 - val_loss: 93.8707\n",
      "Epoch 7841/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 10.5266 - val_loss: 108.0344\n",
      "Epoch 7842/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 17.4397 - val_loss: 106.4078\n",
      "Epoch 7843/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 14.5220 - val_loss: 87.0607\n",
      "Epoch 7844/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 11.6052 - val_loss: 80.7448\n",
      "Epoch 7845/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 13.5186 - val_loss: 106.7394\n",
      "Epoch 7846/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 19.5696 - val_loss: 87.1442\n",
      "Epoch 7847/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 16.1679 - val_loss: 96.3194\n",
      "Epoch 7848/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 12.4953 - val_loss: 109.8167\n",
      "Epoch 7849/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 11.4856 - val_loss: 105.0094\n",
      "Epoch 7850/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 12.6393 - val_loss: 94.7663\n",
      "Epoch 7851/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 15.8995 - val_loss: 103.3248\n",
      "Epoch 7852/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 10.3974 - val_loss: 100.4345\n",
      "Epoch 7853/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 13.9042 - val_loss: 104.2804\n",
      "Epoch 7854/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 14.9230 - val_loss: 103.5027\n",
      "Epoch 7855/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 10.7393 - val_loss: 89.2068\n",
      "Epoch 7856/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 12.2224 - val_loss: 101.7542\n",
      "Epoch 7857/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 11.4990 - val_loss: 100.0224\n",
      "Epoch 7858/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 12.2451 - val_loss: 99.4988\n",
      "Epoch 7859/10000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 11.0481 - val_loss: 111.5086\n",
      "Epoch 7860/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 12.1343 - val_loss: 102.1219\n",
      "Epoch 7861/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 10.3437 - val_loss: 106.5304\n",
      "Epoch 7862/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 16.2236 - val_loss: 104.4941\n",
      "Epoch 7863/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 10.8581 - val_loss: 89.6818\n",
      "Epoch 7864/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 15.4798 - val_loss: 102.4291\n",
      "Epoch 7865/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 14.7300 - val_loss: 93.3288\n",
      "Epoch 7866/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 10.2268 - val_loss: 113.9193\n",
      "Epoch 7867/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 13.4133 - val_loss: 102.7991\n",
      "Epoch 7868/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 11.4921 - val_loss: 111.7251\n",
      "Epoch 7869/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 12.4315 - val_loss: 99.8205\n",
      "Epoch 7870/10000\n",
      "96/96 [==============================] - 0s 510us/step - loss: 11.8984 - val_loss: 102.6748\n",
      "Epoch 7871/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 9.8945 - val_loss: 106.1721\n",
      "Epoch 7872/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 12.5118 - val_loss: 87.3297\n",
      "Epoch 7873/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 11.4296 - val_loss: 110.6937\n",
      "Epoch 7874/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 21.4049 - val_loss: 91.0323\n",
      "Epoch 7875/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 13.7712 - val_loss: 108.2503\n",
      "Epoch 7876/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 11.2031 - val_loss: 110.3111\n",
      "Epoch 7877/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 12.3626 - val_loss: 113.0957\n",
      "Epoch 7878/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 10.6269 - val_loss: 92.1428\n",
      "Epoch 7879/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 14.1961 - val_loss: 105.1608\n",
      "Epoch 7880/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 14.6962 - val_loss: 103.2781\n",
      "Epoch 7881/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 10.9111 - val_loss: 94.1610\n",
      "Epoch 7882/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 8.9034 - val_loss: 102.2780\n",
      "Epoch 7883/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 11.3074 - val_loss: 100.9654\n",
      "Epoch 7884/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 15.6260 - val_loss: 90.5332\n",
      "Epoch 7885/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 10.2111 - val_loss: 119.8819\n",
      "Epoch 7886/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 13.2702 - val_loss: 97.8564\n",
      "Epoch 7887/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 11.8468 - val_loss: 102.0655\n",
      "Epoch 7888/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 11.9621 - val_loss: 86.9863\n",
      "Epoch 7889/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 10.9711 - val_loss: 104.6833\n",
      "Epoch 7890/10000\n",
      "96/96 [==============================] - 0s 357us/step - loss: 10.1633 - val_loss: 104.4495\n",
      "Epoch 7891/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 11.2111 - val_loss: 103.6466\n",
      "Epoch 7892/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 9.7888 - val_loss: 108.7862\n",
      "Epoch 7893/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 11.3664 - val_loss: 114.2703\n",
      "Epoch 7894/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 10.9762 - val_loss: 113.8917\n",
      "Epoch 7895/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 9.4556 - val_loss: 100.2220\n",
      "Epoch 7896/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 12.3212 - val_loss: 98.3847\n",
      "Epoch 7897/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 10.7057 - val_loss: 106.7335\n",
      "Epoch 7898/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 11.2540 - val_loss: 106.6858\n",
      "Epoch 7899/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 7.8808 - val_loss: 98.8072\n",
      "Epoch 7900/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 8.9698 - val_loss: 108.0118\n",
      "Epoch 7901/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 10.1751 - val_loss: 83.6951\n",
      "Epoch 7902/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 10.7351 - val_loss: 99.3961\n",
      "Epoch 7903/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 12.4676 - val_loss: 80.4708\n",
      "Epoch 7904/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 9.8814 - val_loss: 103.5207\n",
      "Epoch 7905/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 10.3548 - val_loss: 101.3529\n",
      "Epoch 7906/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 9.4077 - val_loss: 103.9022\n",
      "Epoch 7907/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 8.2802 - val_loss: 88.8072\n",
      "Epoch 7908/10000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 9.8687 - val_loss: 101.8416\n",
      "Epoch 7909/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 22.5842 - val_loss: 94.9221\n",
      "Epoch 7910/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 12.7704 - val_loss: 100.2974\n",
      "Epoch 7911/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 13.6559 - val_loss: 100.0416\n",
      "Epoch 7912/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 10.4937 - val_loss: 98.8161\n",
      "Epoch 7913/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 9.7384 - val_loss: 100.3547\n",
      "Epoch 7914/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 14.0458 - val_loss: 103.0751\n",
      "Epoch 7915/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 12.8996 - val_loss: 108.4225\n",
      "Epoch 7916/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 13.4087 - val_loss: 105.1670\n",
      "Epoch 7917/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 7.8517 - val_loss: 113.8580\n",
      "Epoch 7918/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 10.1538 - val_loss: 90.6460\n",
      "Epoch 7919/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 9.5982 - val_loss: 104.4353\n",
      "Epoch 7920/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 10.8869 - val_loss: 103.6690\n",
      "Epoch 7921/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 10.4662 - val_loss: 101.1670\n",
      "Epoch 7922/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 10.6627 - val_loss: 96.3749\n",
      "Epoch 7923/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 9.1445 - val_loss: 101.4930\n",
      "Epoch 7924/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 9.8733 - val_loss: 91.5104\n",
      "Epoch 7925/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 10.6170 - val_loss: 90.6463\n",
      "Epoch 7926/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 11.9987 - val_loss: 113.5046\n",
      "Epoch 7927/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 11.7680 - val_loss: 101.3716\n",
      "Epoch 7928/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 14.5498 - val_loss: 90.9763\n",
      "Epoch 7929/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 356us/step - loss: 13.5798 - val_loss: 103.6035\n",
      "Epoch 7930/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 15.9816 - val_loss: 93.2853\n",
      "Epoch 7931/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 13.4060 - val_loss: 94.4240\n",
      "Epoch 7932/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 16.0320 - val_loss: 102.5240\n",
      "Epoch 7933/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 23.4413 - val_loss: 80.9711\n",
      "Epoch 7934/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 14.7266 - val_loss: 94.1338\n",
      "Epoch 7935/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 14.1346 - val_loss: 106.5653\n",
      "Epoch 7936/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 11.8780 - val_loss: 98.4285\n",
      "Epoch 7937/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 17.0944 - val_loss: 105.8434\n",
      "Epoch 7938/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 14.8162 - val_loss: 92.3594\n",
      "Epoch 7939/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 17.5751 - val_loss: 91.5829\n",
      "Epoch 7940/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 23.2802 - val_loss: 68.8698\n",
      "Epoch 7941/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 27.5453 - val_loss: 73.7433\n",
      "Epoch 7942/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 31.6635 - val_loss: 71.6266\n",
      "Epoch 7943/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 21.0451 - val_loss: 91.2933\n",
      "Epoch 7944/10000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 19.6182 - val_loss: 84.3880\n",
      "Epoch 7945/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 21.7604 - val_loss: 101.3920\n",
      "Epoch 7946/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 21.3822 - val_loss: 85.7184\n",
      "Epoch 7947/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 22.6654 - val_loss: 89.5409\n",
      "Epoch 7948/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 18.8726 - val_loss: 84.9644\n",
      "Epoch 7949/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 17.6570 - val_loss: 72.5230\n",
      "Epoch 7950/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 20.6558 - val_loss: 81.1493\n",
      "Epoch 7951/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 15.5811 - val_loss: 82.3375\n",
      "Epoch 7952/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 20.7761 - val_loss: 101.2620\n",
      "Epoch 7953/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 19.3065 - val_loss: 113.4210\n",
      "Epoch 7954/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 13.3485 - val_loss: 97.7088\n",
      "Epoch 7955/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 16.1483 - val_loss: 87.3980\n",
      "Epoch 7956/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 18.3880 - val_loss: 90.6802\n",
      "Epoch 7957/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 17.9879 - val_loss: 104.6918\n",
      "Epoch 7958/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 16.1811 - val_loss: 95.6027\n",
      "Epoch 7959/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 17.2909 - val_loss: 89.9378\n",
      "Epoch 7960/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 13.4936 - val_loss: 94.6620\n",
      "Epoch 7961/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 25.7936 - val_loss: 97.7778\n",
      "Epoch 7962/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 16.1355 - val_loss: 82.5868\n",
      "Epoch 7963/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 16.6066 - val_loss: 92.9680\n",
      "Epoch 7964/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 13.8017 - val_loss: 95.7387\n",
      "Epoch 7965/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 13.2683 - val_loss: 93.0633\n",
      "Epoch 7966/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 20.6014 - val_loss: 93.6595\n",
      "Epoch 7967/10000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 13.7380 - val_loss: 88.1552\n",
      "Epoch 7968/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 19.4117 - val_loss: 94.8342\n",
      "Epoch 7969/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 16.2505 - val_loss: 98.7473\n",
      "Epoch 7970/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 12.1163 - val_loss: 85.3946\n",
      "Epoch 7971/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 14.6490 - val_loss: 95.7783\n",
      "Epoch 7972/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 12.3477 - val_loss: 79.4268\n",
      "Epoch 7973/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 16.1734 - val_loss: 82.8989\n",
      "Epoch 7974/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 13.2025 - val_loss: 99.6241\n",
      "Epoch 7975/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 14.8021 - val_loss: 90.8048\n",
      "Epoch 7976/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 16.1367 - val_loss: 105.5261\n",
      "Epoch 7977/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 11.4618 - val_loss: 103.1023\n",
      "Epoch 7978/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 11.1526 - val_loss: 83.8180\n",
      "Epoch 7979/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 12.8569 - val_loss: 89.9993\n",
      "Epoch 7980/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 12.0480 - val_loss: 96.3409\n",
      "Epoch 7981/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 16.3613 - val_loss: 89.2787\n",
      "Epoch 7982/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 13.6957 - val_loss: 88.1878\n",
      "Epoch 7983/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 14.0834 - val_loss: 99.3258\n",
      "Epoch 7984/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 10.4163 - val_loss: 101.7178\n",
      "Epoch 7985/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 10.7739 - val_loss: 98.0903\n",
      "Epoch 7986/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 15.7753 - val_loss: 103.9675\n",
      "Epoch 7987/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 14.1098 - val_loss: 95.2269\n",
      "Epoch 7988/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 16.3927 - val_loss: 91.9399\n",
      "Epoch 7989/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 12.3117 - val_loss: 87.5840\n",
      "Epoch 7990/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 14.1323 - val_loss: 92.8425\n",
      "Epoch 7991/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 13.0840 - val_loss: 98.9760\n",
      "Epoch 7992/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 18.4676 - val_loss: 84.3881\n",
      "Epoch 7993/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 16.8141 - val_loss: 95.7011\n",
      "Epoch 7994/10000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 15.4965 - val_loss: 87.9628\n",
      "Epoch 7995/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 15.4619 - val_loss: 99.8359\n",
      "Epoch 7996/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 12.6140 - val_loss: 99.2748\n",
      "Epoch 7997/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 12.4331 - val_loss: 91.1594\n",
      "Epoch 7998/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 12.2190 - val_loss: 104.2736\n",
      "Epoch 7999/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 12.1903 - val_loss: 98.8632\n",
      "Epoch 8000/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 12.9929 - val_loss: 92.9917\n",
      "Epoch 8001/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 10.3816 - val_loss: 103.4162\n",
      "Epoch 8002/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 13.7622 - val_loss: 92.1350\n",
      "Epoch 8003/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 10.1835 - val_loss: 102.5769\n",
      "Epoch 8004/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 11.1276 - val_loss: 99.7096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8005/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 11.6018 - val_loss: 100.8395\n",
      "Epoch 8006/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 11.1204 - val_loss: 94.6953\n",
      "Epoch 8007/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 9.4201 - val_loss: 100.9584\n",
      "Epoch 8008/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 10.1299 - val_loss: 90.2087\n",
      "Epoch 8009/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 12.0343 - val_loss: 86.4403\n",
      "Epoch 8010/10000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 10.4821 - val_loss: 99.9987\n",
      "Epoch 8011/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 15.8702 - val_loss: 94.3811\n",
      "Epoch 8012/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 12.0120 - val_loss: 94.8507\n",
      "Epoch 8013/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 13.1371 - val_loss: 91.5828\n",
      "Epoch 8014/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 10.4272 - val_loss: 104.3088\n",
      "Epoch 8015/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 12.5730 - val_loss: 82.5986\n",
      "Epoch 8016/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 13.7695 - val_loss: 85.1052\n",
      "Epoch 8017/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 10.8066 - val_loss: 99.6408\n",
      "Epoch 8018/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 12.7913 - val_loss: 100.8069\n",
      "Epoch 8019/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 17.5511 - val_loss: 91.7249\n",
      "Epoch 8020/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 11.9803 - val_loss: 92.7573\n",
      "Epoch 8021/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 12.2987 - val_loss: 99.7109\n",
      "Epoch 8022/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 10.6881 - val_loss: 90.2572\n",
      "Epoch 8023/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 11.7304 - val_loss: 108.8870\n",
      "Epoch 8024/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 8.9347 - val_loss: 102.8848\n",
      "Epoch 8025/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 12.1520 - val_loss: 99.0246\n",
      "Epoch 8026/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 12.3818 - val_loss: 114.1554\n",
      "Epoch 8027/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 12.4326 - val_loss: 93.5975\n",
      "Epoch 8028/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 10.9908 - val_loss: 95.0204\n",
      "Epoch 8029/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 12.5152 - val_loss: 104.3584\n",
      "Epoch 8030/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 11.3306 - val_loss: 104.1947\n",
      "Epoch 8031/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 11.2022 - val_loss: 89.7865\n",
      "Epoch 8032/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 10.6964 - val_loss: 96.7691\n",
      "Epoch 8033/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 12.5138 - val_loss: 100.8385\n",
      "Epoch 8034/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 10.1214 - val_loss: 76.6035\n",
      "Epoch 8035/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 15.2936 - val_loss: 90.7268\n",
      "Epoch 8036/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 10.1712 - val_loss: 88.0621\n",
      "Epoch 8037/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 13.0028 - val_loss: 90.4548\n",
      "Epoch 8038/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 13.0363 - val_loss: 97.0907\n",
      "Epoch 8039/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 14.0333 - val_loss: 87.3930\n",
      "Epoch 8040/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 16.3092 - val_loss: 95.0894\n",
      "Epoch 8041/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 16.1993 - val_loss: 87.4506\n",
      "Epoch 8042/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 13.0206 - val_loss: 82.0191\n",
      "Epoch 8043/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 13.9134 - val_loss: 93.3421\n",
      "Epoch 8044/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 12.7962 - val_loss: 100.8715\n",
      "Epoch 8045/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 12.2513 - val_loss: 110.9645\n",
      "Epoch 8046/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 10.8095 - val_loss: 91.4578\n",
      "Epoch 8047/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 13.7397 - val_loss: 101.1630\n",
      "Epoch 8048/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 15.6174 - val_loss: 100.4259\n",
      "Epoch 8049/10000\n",
      "96/96 [==============================] - 0s 482us/step - loss: 9.6053 - val_loss: 102.3959\n",
      "Epoch 8050/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 14.1353 - val_loss: 101.6997\n",
      "Epoch 8051/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 11.4923 - val_loss: 94.2966\n",
      "Epoch 8052/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 10.6731 - val_loss: 103.0162\n",
      "Epoch 8053/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 16.4613 - val_loss: 102.9514\n",
      "Epoch 8054/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 10.2169 - val_loss: 91.9752\n",
      "Epoch 8055/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 9.6157 - val_loss: 101.3743\n",
      "Epoch 8056/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 16.1114 - val_loss: 89.3442\n",
      "Epoch 8057/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 10.1323 - val_loss: 95.8628\n",
      "Epoch 8058/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 10.2479 - val_loss: 103.2231\n",
      "Epoch 8059/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 12.3319 - val_loss: 97.5354\n",
      "Epoch 8060/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 11.2872 - val_loss: 109.1083\n",
      "Epoch 8061/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 13.2802 - val_loss: 97.8440\n",
      "Epoch 8062/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 12.6019 - val_loss: 81.6342\n",
      "Epoch 8063/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 15.1325 - val_loss: 99.5745\n",
      "Epoch 8064/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 12.6738 - val_loss: 109.8850\n",
      "Epoch 8065/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 10.7028 - val_loss: 112.3940\n",
      "Epoch 8066/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 12.0787 - val_loss: 107.3569\n",
      "Epoch 8067/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 8.3485 - val_loss: 102.7721\n",
      "Epoch 8068/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 9.9320 - val_loss: 85.8160\n",
      "Epoch 8069/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 9.9851 - val_loss: 98.5375\n",
      "Epoch 8070/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 17.2758 - val_loss: 107.2604\n",
      "Epoch 8071/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 10.7076 - val_loss: 101.2930\n",
      "Epoch 8072/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 11.1298 - val_loss: 103.0357\n",
      "Epoch 8073/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 12.4605 - val_loss: 95.4086\n",
      "Epoch 8074/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 11.1999 - val_loss: 83.8441\n",
      "Epoch 8075/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 12.5763 - val_loss: 95.5197\n",
      "Epoch 8076/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 12.0444 - val_loss: 90.7679\n",
      "Epoch 8077/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 10.7151 - val_loss: 98.2713\n",
      "Epoch 8078/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 11.1445 - val_loss: 101.5840\n",
      "Epoch 8079/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 17.4173 - val_loss: 107.3836\n",
      "Epoch 8080/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 12.4280 - val_loss: 103.1860\n",
      "Epoch 8081/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 12.1013 - val_loss: 93.2339\n",
      "Epoch 8082/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 10.7034 - val_loss: 106.3394\n",
      "Epoch 8083/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 11.1534 - val_loss: 86.3349\n",
      "Epoch 8084/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 10.4695 - val_loss: 113.0197\n",
      "Epoch 8085/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 12.5354 - val_loss: 108.1735\n",
      "Epoch 8086/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 12.0845 - val_loss: 110.1508\n",
      "Epoch 8087/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 12.9120 - val_loss: 96.2450\n",
      "Epoch 8088/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 10.1839 - val_loss: 106.5188\n",
      "Epoch 8089/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 8.0088 - val_loss: 115.4960\n",
      "Epoch 8090/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 10.9282 - val_loss: 90.4323\n",
      "Epoch 8091/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 10.4440 - val_loss: 107.1003\n",
      "Epoch 8092/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 10.5402 - val_loss: 101.3852\n",
      "Epoch 8093/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 13.7465 - val_loss: 91.0695\n",
      "Epoch 8094/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 8.5370 - val_loss: 116.9976\n",
      "Epoch 8095/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 10.3148 - val_loss: 92.0760\n",
      "Epoch 8096/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 10.3868 - val_loss: 105.5606\n",
      "Epoch 8097/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 13.2268 - val_loss: 101.8693\n",
      "Epoch 8098/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 16.8160 - val_loss: 103.8257\n",
      "Epoch 8099/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 11.0061 - val_loss: 94.0688\n",
      "Epoch 8100/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 11.1040 - val_loss: 101.6807\n",
      "Epoch 8101/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 11.1060 - val_loss: 87.2008\n",
      "Epoch 8102/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 10.7736 - val_loss: 107.2285\n",
      "Epoch 8103/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 9.5880 - val_loss: 104.4443\n",
      "Epoch 8104/10000\n",
      "96/96 [==============================] - 0s 494us/step - loss: 13.9377 - val_loss: 92.4997\n",
      "Epoch 8105/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 9.4398 - val_loss: 111.6976\n",
      "Epoch 8106/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 10.3824 - val_loss: 96.7713\n",
      "Epoch 8107/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 14.8914 - val_loss: 86.7778\n",
      "Epoch 8108/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 18.0308 - val_loss: 94.2574\n",
      "Epoch 8109/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 16.1882 - val_loss: 101.8501\n",
      "Epoch 8110/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 16.4598 - val_loss: 99.8204\n",
      "Epoch 8111/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 21.8305 - val_loss: 98.2867\n",
      "Epoch 8112/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 15.0103 - val_loss: 100.3070\n",
      "Epoch 8113/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 12.1539 - val_loss: 99.5266\n",
      "Epoch 8114/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 12.2074 - val_loss: 91.0625\n",
      "Epoch 8115/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 13.6860 - val_loss: 98.5238\n",
      "Epoch 8116/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 11.6371 - val_loss: 81.8611\n",
      "Epoch 8117/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 11.4725 - val_loss: 86.6286\n",
      "Epoch 8118/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 11.3080 - val_loss: 105.7047\n",
      "Epoch 8119/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 11.2670 - val_loss: 90.2308\n",
      "Epoch 8120/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 11.0354 - val_loss: 100.7509\n",
      "Epoch 8121/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 9.0201 - val_loss: 102.0002\n",
      "Epoch 8122/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 12.0961 - val_loss: 101.6681\n",
      "Epoch 8123/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 13.7679 - val_loss: 93.5696\n",
      "Epoch 8124/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 9.9177 - val_loss: 100.2442\n",
      "Epoch 8125/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 9.1929 - val_loss: 103.1506\n",
      "Epoch 8126/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 11.6669 - val_loss: 95.0162\n",
      "Epoch 8127/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 11.4616 - val_loss: 110.7334\n",
      "Epoch 8128/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 12.2680 - val_loss: 97.4889\n",
      "Epoch 8129/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 8.9094 - val_loss: 97.0709\n",
      "Epoch 8130/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 15.5993 - val_loss: 101.6078\n",
      "Epoch 8131/10000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 16.5578 - val_loss: 95.1049\n",
      "Epoch 8132/10000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 12.3108 - val_loss: 96.3125\n",
      "Epoch 8133/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 11.4324 - val_loss: 107.7404\n",
      "Epoch 8134/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 13.1039 - val_loss: 94.8036\n",
      "Epoch 8135/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 26.8906 - val_loss: 109.7334\n",
      "Epoch 8136/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 13.6924 - val_loss: 98.3865\n",
      "Epoch 8137/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 16.8660 - val_loss: 111.7206\n",
      "Epoch 8138/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 13.3435 - val_loss: 106.4541\n",
      "Epoch 8139/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 11.6760 - val_loss: 81.9293\n",
      "Epoch 8140/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 12.7286 - val_loss: 72.2529\n",
      "Epoch 8141/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 14.6949 - val_loss: 102.0512\n",
      "Epoch 8142/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 10.7722 - val_loss: 94.1289\n",
      "Epoch 8143/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 13.3477 - val_loss: 112.9349\n",
      "Epoch 8144/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 12.0822 - val_loss: 115.6942\n",
      "Epoch 8145/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 12.1294 - val_loss: 103.4027\n",
      "Epoch 8146/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 12.1832 - val_loss: 112.4676\n",
      "Epoch 8147/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 11.4058 - val_loss: 91.2377\n",
      "Epoch 8148/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 10.8241 - val_loss: 103.3581\n",
      "Epoch 8149/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 13.2689 - val_loss: 115.2653\n",
      "Epoch 8150/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 10.4049 - val_loss: 108.7798\n",
      "Epoch 8151/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 11.5698 - val_loss: 96.8586\n",
      "Epoch 8152/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 9.9959 - val_loss: 106.5558\n",
      "Epoch 8153/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 9.2475 - val_loss: 96.0026\n",
      "Epoch 8154/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 8.9908 - val_loss: 107.7649\n",
      "Epoch 8155/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 379us/step - loss: 10.8423 - val_loss: 90.6786\n",
      "Epoch 8156/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 8.8037 - val_loss: 102.4953\n",
      "Epoch 8157/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 8.5142 - val_loss: 96.0313\n",
      "Epoch 8158/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 10.9696 - val_loss: 106.1183\n",
      "Epoch 8159/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 10.9299 - val_loss: 105.5089\n",
      "Epoch 8160/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 9.8714 - val_loss: 109.0030\n",
      "Epoch 8161/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 9.5584 - val_loss: 103.7132\n",
      "Epoch 8162/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 11.2867 - val_loss: 118.6823\n",
      "Epoch 8163/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 10.6807 - val_loss: 98.8580\n",
      "Epoch 8164/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 10.4818 - val_loss: 111.1143\n",
      "Epoch 8165/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 8.9391 - val_loss: 109.4198\n",
      "Epoch 8166/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 13.6561 - val_loss: 103.9426\n",
      "Epoch 8167/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 10.0822 - val_loss: 110.7538\n",
      "Epoch 8168/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 7.1268 - val_loss: 112.5681\n",
      "Epoch 8169/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 12.3552 - val_loss: 111.4230\n",
      "Epoch 8170/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 11.4426 - val_loss: 109.2707\n",
      "Epoch 8171/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 8.6725 - val_loss: 118.6323\n",
      "Epoch 8172/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 9.8754 - val_loss: 97.5728\n",
      "Epoch 8173/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 11.7006 - val_loss: 103.1985\n",
      "Epoch 8174/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 8.3713 - val_loss: 113.1336\n",
      "Epoch 8175/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 10.0893 - val_loss: 99.0388\n",
      "Epoch 8176/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 9.7626 - val_loss: 97.6954\n",
      "Epoch 8177/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 9.8986 - val_loss: 106.4625\n",
      "Epoch 8178/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 9.0088 - val_loss: 88.7576\n",
      "Epoch 8179/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 7.9653 - val_loss: 97.0091\n",
      "Epoch 8180/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 10.1531 - val_loss: 110.4425\n",
      "Epoch 8181/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 9.0938 - val_loss: 104.7933\n",
      "Epoch 8182/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 9.6968 - val_loss: 103.0752\n",
      "Epoch 8183/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 9.8315 - val_loss: 98.7495\n",
      "Epoch 8184/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 11.4515 - val_loss: 102.3485\n",
      "Epoch 8185/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 11.1403 - val_loss: 112.6444\n",
      "Epoch 8186/10000\n",
      "96/96 [==============================] - 0s 504us/step - loss: 11.2645 - val_loss: 105.9883\n",
      "Epoch 8187/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 12.0131 - val_loss: 99.9644\n",
      "Epoch 8188/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 14.1325 - val_loss: 89.7098\n",
      "Epoch 8189/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 9.1656 - val_loss: 92.5207\n",
      "Epoch 8190/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 10.0720 - val_loss: 104.6687\n",
      "Epoch 8191/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 15.6118 - val_loss: 89.5299\n",
      "Epoch 8192/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 10.2221 - val_loss: 117.9028\n",
      "Epoch 8193/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 8.7628 - val_loss: 96.6342\n",
      "Epoch 8194/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 9.9477 - val_loss: 92.8822\n",
      "Epoch 8195/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 14.0489 - val_loss: 104.7701\n",
      "Epoch 8196/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 38.8283 - val_loss: 83.4093\n",
      "Epoch 8197/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 32.0521 - val_loss: 97.6428\n",
      "Epoch 8198/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 21.2757 - val_loss: 102.1171\n",
      "Epoch 8199/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 18.7294 - val_loss: 105.5899\n",
      "Epoch 8200/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 21.2253 - val_loss: 107.5655\n",
      "Epoch 8201/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 21.9471 - val_loss: 105.4514\n",
      "Epoch 8202/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 14.6887 - val_loss: 107.2642\n",
      "Epoch 8203/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 47.3856 - val_loss: 109.4851\n",
      "Epoch 8204/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 33.5333 - val_loss: 82.4406\n",
      "Epoch 8205/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 22.6284 - val_loss: 76.4967\n",
      "Epoch 8206/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 35.6500 - val_loss: 86.6970\n",
      "Epoch 8207/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 26.3774 - val_loss: 77.4774\n",
      "Epoch 8208/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 75.3403 - val_loss: 85.0504\n",
      "Epoch 8209/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 25.2605 - val_loss: 83.6599\n",
      "Epoch 8210/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 26.0560 - val_loss: 103.2079\n",
      "Epoch 8211/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 23.3534 - val_loss: 81.8991\n",
      "Epoch 8212/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 22.8702 - val_loss: 107.9904\n",
      "Epoch 8213/10000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 21.1232 - val_loss: 103.3098\n",
      "Epoch 8214/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 17.7942 - val_loss: 100.9831\n",
      "Epoch 8215/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 19.6119 - val_loss: 83.1159\n",
      "Epoch 8216/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 20.1165 - val_loss: 97.5000\n",
      "Epoch 8217/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 19.6814 - val_loss: 106.1893\n",
      "Epoch 8218/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 20.7506 - val_loss: 86.1968\n",
      "Epoch 8219/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 18.3113 - val_loss: 88.3265\n",
      "Epoch 8220/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 17.1058 - val_loss: 90.4352\n",
      "Epoch 8221/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 17.2629 - val_loss: 100.3311\n",
      "Epoch 8222/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 18.5331 - val_loss: 105.5991\n",
      "Epoch 8223/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 15.2525 - val_loss: 98.1539\n",
      "Epoch 8224/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 15.2930 - val_loss: 107.6049\n",
      "Epoch 8225/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 15.5391 - val_loss: 94.3792\n",
      "Epoch 8226/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 13.6026 - val_loss: 80.8269\n",
      "Epoch 8227/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 24.0373 - val_loss: 106.7914\n",
      "Epoch 8228/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 12.3453 - val_loss: 88.0620\n",
      "Epoch 8229/10000\n",
      "96/96 [==============================] - 0s 550us/step - loss: 13.8423 - val_loss: 97.8495\n",
      "Epoch 8230/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 11.3189 - val_loss: 85.3579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8231/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 12.1286 - val_loss: 93.9262\n",
      "Epoch 8232/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 10.1911 - val_loss: 93.9427\n",
      "Epoch 8233/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 12.0395 - val_loss: 95.4178\n",
      "Epoch 8234/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 11.6002 - val_loss: 76.3315\n",
      "Epoch 8235/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 13.1618 - val_loss: 89.9360\n",
      "Epoch 8236/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 10.5602 - val_loss: 98.6544\n",
      "Epoch 8237/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 11.2472 - val_loss: 86.8838\n",
      "Epoch 8238/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 12.0486 - val_loss: 82.3557\n",
      "Epoch 8239/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 10.3883 - val_loss: 87.1414\n",
      "Epoch 8240/10000\n",
      "96/96 [==============================] - 0s 514us/step - loss: 15.2646 - val_loss: 96.0775\n",
      "Epoch 8241/10000\n",
      "96/96 [==============================] - 0s 504us/step - loss: 13.9114 - val_loss: 90.9787\n",
      "Epoch 8242/10000\n",
      "96/96 [==============================] - 0s 490us/step - loss: 10.9983 - val_loss: 106.7503\n",
      "Epoch 8243/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 13.7151 - val_loss: 93.7335\n",
      "Epoch 8244/10000\n",
      "96/96 [==============================] - 0s 573us/step - loss: 9.6719 - val_loss: 96.5041\n",
      "Epoch 8245/10000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 10.1113 - val_loss: 85.5250\n",
      "Epoch 8246/10000\n",
      "96/96 [==============================] - 0s 480us/step - loss: 10.0401 - val_loss: 79.7511\n",
      "Epoch 8247/10000\n",
      "96/96 [==============================] - 0s 499us/step - loss: 10.2255 - val_loss: 80.6283\n",
      "Epoch 8248/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 12.8743 - val_loss: 90.5352\n",
      "Epoch 8249/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 14.6144 - val_loss: 96.1279\n",
      "Epoch 8250/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 11.9893 - val_loss: 89.4079\n",
      "Epoch 8251/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 10.7546 - val_loss: 92.5827\n",
      "Epoch 8252/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 13.1561 - val_loss: 95.9074\n",
      "Epoch 8253/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 11.2016 - val_loss: 91.6843\n",
      "Epoch 8254/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 12.2959 - val_loss: 106.8959\n",
      "Epoch 8255/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 9.0697 - val_loss: 99.4325\n",
      "Epoch 8256/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 12.6856 - val_loss: 99.5781\n",
      "Epoch 8257/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 13.4108 - val_loss: 105.7385\n",
      "Epoch 8258/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 13.2932 - val_loss: 86.9704\n",
      "Epoch 8259/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 13.1891 - val_loss: 79.5849\n",
      "Epoch 8260/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 9.9085 - val_loss: 108.6024\n",
      "Epoch 8261/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 12.1613 - val_loss: 79.5626\n",
      "Epoch 8262/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 14.4368 - val_loss: 88.5332\n",
      "Epoch 8263/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 15.5102 - val_loss: 88.3684\n",
      "Epoch 8264/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 16.8943 - val_loss: 99.9179\n",
      "Epoch 8265/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 14.7053 - val_loss: 92.2388\n",
      "Epoch 8266/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 11.5551 - val_loss: 82.4791\n",
      "Epoch 8267/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 16.7247 - val_loss: 71.3066\n",
      "Epoch 8268/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 11.8632 - val_loss: 102.1185\n",
      "Epoch 8269/10000\n",
      "96/96 [==============================] - 0s 350us/step - loss: 12.2785 - val_loss: 81.4752\n",
      "Epoch 8270/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 19.2717 - val_loss: 83.3437\n",
      "Epoch 8271/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 15.4042 - val_loss: 98.1156\n",
      "Epoch 8272/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 11.7767 - val_loss: 92.0415\n",
      "Epoch 8273/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 11.1197 - val_loss: 92.1610\n",
      "Epoch 8274/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 14.5925 - val_loss: 84.0448\n",
      "Epoch 8275/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 13.0959 - val_loss: 107.4185\n",
      "Epoch 8276/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 13.6313 - val_loss: 95.3575\n",
      "Epoch 8277/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 13.1572 - val_loss: 80.0990\n",
      "Epoch 8278/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 14.6435 - val_loss: 97.8023\n",
      "Epoch 8279/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 17.4168 - val_loss: 80.3060\n",
      "Epoch 8280/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 13.6490 - val_loss: 93.9363\n",
      "Epoch 8281/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 15.0085 - val_loss: 79.5466\n",
      "Epoch 8282/10000\n",
      "96/96 [==============================] - 0s 486us/step - loss: 12.5036 - val_loss: 92.2545\n",
      "Epoch 8283/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 14.8321 - val_loss: 92.6317\n",
      "Epoch 8284/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 13.8282 - val_loss: 99.6302\n",
      "Epoch 8285/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 17.1656 - val_loss: 104.5610\n",
      "Epoch 8286/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 14.0550 - val_loss: 93.5829\n",
      "Epoch 8287/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 9.8894 - val_loss: 105.9887\n",
      "Epoch 8288/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 12.5297 - val_loss: 100.9761\n",
      "Epoch 8289/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 10.8444 - val_loss: 106.2410\n",
      "Epoch 8290/10000\n",
      "96/96 [==============================] - 0s 355us/step - loss: 11.3021 - val_loss: 91.4100\n",
      "Epoch 8291/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 10.5118 - val_loss: 107.7891\n",
      "Epoch 8292/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 12.3458 - val_loss: 102.6775\n",
      "Epoch 8293/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 11.7679 - val_loss: 95.7534\n",
      "Epoch 8294/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 11.7471 - val_loss: 99.5739\n",
      "Epoch 8295/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 11.3607 - val_loss: 108.3371\n",
      "Epoch 8296/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 11.5136 - val_loss: 101.2219\n",
      "Epoch 8297/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 11.7382 - val_loss: 90.3227\n",
      "Epoch 8298/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 9.6404 - val_loss: 103.3564\n",
      "Epoch 8299/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 10.3303 - val_loss: 100.2584\n",
      "Epoch 8300/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 12.2897 - val_loss: 105.5570\n",
      "Epoch 8301/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 10.4645 - val_loss: 99.8410\n",
      "Epoch 8302/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 9.7539 - val_loss: 100.2947\n",
      "Epoch 8303/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 10.0840 - val_loss: 100.7917\n",
      "Epoch 8304/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 9.7859 - val_loss: 95.4201\n",
      "Epoch 8305/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 10.0459 - val_loss: 99.9140\n",
      "Epoch 8306/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 9.5883 - val_loss: 107.6729\n",
      "Epoch 8307/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 9.0483 - val_loss: 104.2927\n",
      "Epoch 8308/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 12.4477 - val_loss: 102.0860\n",
      "Epoch 8309/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 10.4026 - val_loss: 99.0824\n",
      "Epoch 8310/10000\n",
      "96/96 [==============================] - 0s 581us/step - loss: 10.4987 - val_loss: 90.6617\n",
      "Epoch 8311/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 9.5076 - val_loss: 88.1067\n",
      "Epoch 8312/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 8.9641 - val_loss: 99.0819\n",
      "Epoch 8313/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 11.7026 - val_loss: 101.0181\n",
      "Epoch 8314/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 10.1796 - val_loss: 102.9806\n",
      "Epoch 8315/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 12.6471 - val_loss: 106.3251\n",
      "Epoch 8316/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 11.1895 - val_loss: 100.9737\n",
      "Epoch 8317/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 10.7429 - val_loss: 96.4496\n",
      "Epoch 8318/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 10.8961 - val_loss: 100.4629\n",
      "Epoch 8319/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 9.5598 - val_loss: 106.9261\n",
      "Epoch 8320/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 9.3942 - val_loss: 85.2163\n",
      "Epoch 8321/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 12.3564 - val_loss: 111.4674\n",
      "Epoch 8322/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 9.1711 - val_loss: 69.2303\n",
      "Epoch 8323/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 9.7842 - val_loss: 78.7442\n",
      "Epoch 8324/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 9.1726 - val_loss: 103.5808\n",
      "Epoch 8325/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 10.0411 - val_loss: 102.0757\n",
      "Epoch 8326/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 10.0125 - val_loss: 99.7992\n",
      "Epoch 8327/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 9.4323 - val_loss: 101.0238\n",
      "Epoch 8328/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 9.9592 - val_loss: 101.5132\n",
      "Epoch 8329/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 10.5972 - val_loss: 101.4411\n",
      "Epoch 8330/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 9.2282 - val_loss: 86.0688\n",
      "Epoch 8331/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 7.1657 - val_loss: 84.9261\n",
      "Epoch 8332/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 9.2866 - val_loss: 92.3984\n",
      "Epoch 8333/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 7.9993 - val_loss: 100.9330\n",
      "Epoch 8334/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 9.7147 - val_loss: 104.7927\n",
      "Epoch 8335/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 9.6313 - val_loss: 99.0448\n",
      "Epoch 8336/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 8.6604 - val_loss: 94.0535\n",
      "Epoch 8337/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 9.9632 - val_loss: 97.5641\n",
      "Epoch 8338/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 9.2444 - val_loss: 96.4429\n",
      "Epoch 8339/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 8.9751 - val_loss: 91.2102\n",
      "Epoch 8340/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 10.8740 - val_loss: 107.6310\n",
      "Epoch 8341/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 10.3008 - val_loss: 104.8242\n",
      "Epoch 8342/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 11.3047 - val_loss: 98.5315\n",
      "Epoch 8343/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 11.7146 - val_loss: 95.1645\n",
      "Epoch 8344/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 9.6465 - val_loss: 110.8629\n",
      "Epoch 8345/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 8.2194 - val_loss: 105.5130\n",
      "Epoch 8346/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 10.1396 - val_loss: 115.8214\n",
      "Epoch 8347/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 10.2108 - val_loss: 93.3270\n",
      "Epoch 8348/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 10.5867 - val_loss: 87.1494\n",
      "Epoch 8349/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 12.7452 - val_loss: 92.4077\n",
      "Epoch 8350/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 9.7203 - val_loss: 106.8639\n",
      "Epoch 8351/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 10.6044 - val_loss: 81.5066\n",
      "Epoch 8352/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 10.3098 - val_loss: 99.3337\n",
      "Epoch 8353/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 11.0063 - val_loss: 107.6051\n",
      "Epoch 8354/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 9.6250 - val_loss: 103.7757\n",
      "Epoch 8355/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 10.4287 - val_loss: 103.9912\n",
      "Epoch 8356/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 9.0179 - val_loss: 91.7237\n",
      "Epoch 8357/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 24.0252 - val_loss: 96.5102\n",
      "Epoch 8358/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 12.5918 - val_loss: 85.5072\n",
      "Epoch 8359/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 28.6115 - val_loss: 91.9320\n",
      "Epoch 8360/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 21.2430 - val_loss: 88.7750\n",
      "Epoch 8361/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 33.9055 - val_loss: 89.2652\n",
      "Epoch 8362/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 27.4382 - val_loss: 95.4037\n",
      "Epoch 8363/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 16.8952 - val_loss: 98.9081\n",
      "Epoch 8364/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 20.5386 - val_loss: 92.4514\n",
      "Epoch 8365/10000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 20.2630 - val_loss: 100.8472\n",
      "Epoch 8366/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 20.2008 - val_loss: 89.6786\n",
      "Epoch 8367/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 20.9797 - val_loss: 78.9395\n",
      "Epoch 8368/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 16.4894 - val_loss: 101.0053\n",
      "Epoch 8369/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 16.3553 - val_loss: 97.1116\n",
      "Epoch 8370/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 16.0838 - val_loss: 100.2737\n",
      "Epoch 8371/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 13.5516 - val_loss: 101.9439\n",
      "Epoch 8372/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 17.5464 - val_loss: 93.2542\n",
      "Epoch 8373/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 15.6476 - val_loss: 92.0805\n",
      "Epoch 8374/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 14.8834 - val_loss: 103.0109\n",
      "Epoch 8375/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 11.7510 - val_loss: 107.5313\n",
      "Epoch 8376/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 14.9618 - val_loss: 103.1593\n",
      "Epoch 8377/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 12.6186 - val_loss: 107.8172\n",
      "Epoch 8378/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 14.0979 - val_loss: 93.5625\n",
      "Epoch 8379/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 12.8889 - val_loss: 99.7374\n",
      "Epoch 8380/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 11.3992 - val_loss: 95.0209\n",
      "Epoch 8381/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 13.7806 - val_loss: 87.3882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8382/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 14.4588 - val_loss: 88.6824\n",
      "Epoch 8383/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 10.4249 - val_loss: 97.3643\n",
      "Epoch 8384/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 12.7808 - val_loss: 100.5417\n",
      "Epoch 8385/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 11.2783 - val_loss: 100.7791\n",
      "Epoch 8386/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 12.2068 - val_loss: 102.5113\n",
      "Epoch 8387/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 12.4074 - val_loss: 107.7373\n",
      "Epoch 8388/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 10.1708 - val_loss: 101.2888\n",
      "Epoch 8389/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 9.9884 - val_loss: 108.0343\n",
      "Epoch 8390/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 12.0621 - val_loss: 101.5796\n",
      "Epoch 8391/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 9.4798 - val_loss: 97.4959\n",
      "Epoch 8392/10000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 9.7634 - val_loss: 97.3514\n",
      "Epoch 8393/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 10.6968 - val_loss: 99.5909\n",
      "Epoch 8394/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 10.4764 - val_loss: 105.6888\n",
      "Epoch 8395/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 10.5606 - val_loss: 95.1871\n",
      "Epoch 8396/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 10.4461 - val_loss: 100.2235\n",
      "Epoch 8397/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 13.8285 - val_loss: 100.9178\n",
      "Epoch 8398/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 10.7622 - val_loss: 83.3477\n",
      "Epoch 8399/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 24.7416 - val_loss: 85.9529\n",
      "Epoch 8400/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 21.7861 - val_loss: 78.7553\n",
      "Epoch 8401/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 93.1236 - val_loss: 86.5335\n",
      "Epoch 8402/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 23.4011 - val_loss: 92.9712\n",
      "Epoch 8403/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 22.8922 - val_loss: 95.6758\n",
      "Epoch 8404/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 22.0770 - val_loss: 107.5491\n",
      "Epoch 8405/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 17.9156 - val_loss: 98.0153\n",
      "Epoch 8406/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 15.1228 - val_loss: 99.1242\n",
      "Epoch 8407/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 19.8978 - val_loss: 93.9787\n",
      "Epoch 8408/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 21.3405 - val_loss: 92.4427\n",
      "Epoch 8409/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 19.2184 - val_loss: 85.6897\n",
      "Epoch 8410/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 18.1795 - val_loss: 81.9547\n",
      "Epoch 8411/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 16.5538 - val_loss: 104.2407\n",
      "Epoch 8412/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 16.4328 - val_loss: 89.0415\n",
      "Epoch 8413/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 13.9078 - val_loss: 89.0277\n",
      "Epoch 8414/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 11.8011 - val_loss: 101.9812\n",
      "Epoch 8415/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 20.1811 - val_loss: 84.9314\n",
      "Epoch 8416/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 20.1394 - val_loss: 101.0158\n",
      "Epoch 8417/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 31.3013 - val_loss: 100.5731\n",
      "Epoch 8418/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 26.9092 - val_loss: 102.8911\n",
      "Epoch 8419/10000\n",
      "96/96 [==============================] - 0s 490us/step - loss: 33.3121 - val_loss: 79.5874\n",
      "Epoch 8420/10000\n",
      "96/96 [==============================] - 0s 497us/step - loss: 28.5338 - val_loss: 87.1409\n",
      "Epoch 8421/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 29.3472 - val_loss: 77.3057\n",
      "Epoch 8422/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 31.4386 - val_loss: 84.3973\n",
      "Epoch 8423/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 24.8132 - val_loss: 71.6435\n",
      "Epoch 8424/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 24.1738 - val_loss: 77.7419\n",
      "Epoch 8425/10000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 24.5416 - val_loss: 68.6179\n",
      "Epoch 8426/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 21.3602 - val_loss: 85.1910\n",
      "Epoch 8427/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 21.2235 - val_loss: 70.7469\n",
      "Epoch 8428/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 21.3684 - val_loss: 90.1160\n",
      "Epoch 8429/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 16.6048 - val_loss: 88.1727\n",
      "Epoch 8430/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 17.3712 - val_loss: 106.8038\n",
      "Epoch 8431/10000\n",
      "96/96 [==============================] - 0s 475us/step - loss: 14.4933 - val_loss: 105.5479\n",
      "Epoch 8432/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 13.0546 - val_loss: 82.0416\n",
      "Epoch 8433/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 14.5645 - val_loss: 99.6906\n",
      "Epoch 8434/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 13.9561 - val_loss: 87.8780\n",
      "Epoch 8435/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 14.3140 - val_loss: 96.9373\n",
      "Epoch 8436/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 14.7991 - val_loss: 98.3525\n",
      "Epoch 8437/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 13.7505 - val_loss: 91.7265\n",
      "Epoch 8438/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 12.2995 - val_loss: 105.4707\n",
      "Epoch 8439/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 11.4601 - val_loss: 97.5262\n",
      "Epoch 8440/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 12.8808 - val_loss: 102.8558\n",
      "Epoch 8441/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 10.0831 - val_loss: 91.0604\n",
      "Epoch 8442/10000\n",
      "96/96 [==============================] - 0s 363us/step - loss: 12.2290 - val_loss: 106.4279\n",
      "Epoch 8443/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 10.4303 - val_loss: 107.9255\n",
      "Epoch 8444/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 14.4324 - val_loss: 100.8439\n",
      "Epoch 8445/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 11.8039 - val_loss: 99.6718\n",
      "Epoch 8446/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 14.1055 - val_loss: 102.5297\n",
      "Epoch 8447/10000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 12.1072 - val_loss: 105.5252\n",
      "Epoch 8448/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 12.0575 - val_loss: 84.8598\n",
      "Epoch 8449/10000\n",
      "96/96 [==============================] - 0s 352us/step - loss: 13.3092 - val_loss: 96.8197\n",
      "Epoch 8450/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 10.7498 - val_loss: 90.9535\n",
      "Epoch 8451/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 10.5002 - val_loss: 106.4410\n",
      "Epoch 8452/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 14.0629 - val_loss: 100.4572\n",
      "Epoch 8453/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 13.3318 - val_loss: 95.4622\n",
      "Epoch 8454/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 11.4610 - val_loss: 98.5541\n",
      "Epoch 8455/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 12.0410 - val_loss: 94.4306\n",
      "Epoch 8456/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 17.5835 - val_loss: 80.7581\n",
      "Epoch 8457/10000\n",
      "96/96 [==============================] - 0s 354us/step - loss: 10.4984 - val_loss: 87.3914\n",
      "Epoch 8458/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 12.8308 - val_loss: 89.7212\n",
      "Epoch 8459/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 12.3811 - val_loss: 85.3777\n",
      "Epoch 8460/10000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 14.4704 - val_loss: 93.0708\n",
      "Epoch 8461/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 10.9896 - val_loss: 73.6735\n",
      "Epoch 8462/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 11.6394 - val_loss: 88.0588\n",
      "Epoch 8463/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 10.9890 - val_loss: 103.2327\n",
      "Epoch 8464/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 11.7320 - val_loss: 106.2643\n",
      "Epoch 8465/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 11.8059 - val_loss: 97.0365\n",
      "Epoch 8466/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 10.2212 - val_loss: 85.5073\n",
      "Epoch 8467/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 11.4060 - val_loss: 82.0976\n",
      "Epoch 8468/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 10.5074 - val_loss: 97.2104\n",
      "Epoch 8469/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 11.7338 - val_loss: 90.3027\n",
      "Epoch 8470/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 12.9374 - val_loss: 88.5874\n",
      "Epoch 8471/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 11.8470 - val_loss: 106.9267\n",
      "Epoch 8472/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 10.0340 - val_loss: 102.8173\n",
      "Epoch 8473/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 13.8349 - val_loss: 101.5553\n",
      "Epoch 8474/10000\n",
      "96/96 [==============================] - 0s 480us/step - loss: 12.7402 - val_loss: 96.2245\n",
      "Epoch 8475/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 9.5933 - val_loss: 111.1556\n",
      "Epoch 8476/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 8.6615 - val_loss: 92.0701\n",
      "Epoch 8477/10000\n",
      "96/96 [==============================] - 0s 352us/step - loss: 9.5880 - val_loss: 89.8580\n",
      "Epoch 8478/10000\n",
      "96/96 [==============================] - 0s 357us/step - loss: 10.7169 - val_loss: 93.1903\n",
      "Epoch 8479/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 13.0577 - val_loss: 94.3380\n",
      "Epoch 8480/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 10.8904 - val_loss: 97.7229\n",
      "Epoch 8481/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 11.7786 - val_loss: 91.7698\n",
      "Epoch 8482/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 9.8931 - val_loss: 84.7855\n",
      "Epoch 8483/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 10.3649 - val_loss: 93.2546\n",
      "Epoch 8484/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 10.6906 - val_loss: 96.9734\n",
      "Epoch 8485/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 11.9603 - val_loss: 98.8736\n",
      "Epoch 8486/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 8.5516 - val_loss: 107.3009\n",
      "Epoch 8487/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 10.4063 - val_loss: 88.4856\n",
      "Epoch 8488/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 9.6458 - val_loss: 100.4839\n",
      "Epoch 8489/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 12.0445 - val_loss: 101.5951\n",
      "Epoch 8490/10000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 10.4010 - val_loss: 85.8928\n",
      "Epoch 8491/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 8.7462 - val_loss: 95.2732\n",
      "Epoch 8492/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 9.7553 - val_loss: 103.5925\n",
      "Epoch 8493/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 10.9154 - val_loss: 91.0931\n",
      "Epoch 8494/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 9.1749 - val_loss: 96.1849\n",
      "Epoch 8495/10000\n",
      "96/96 [==============================] - 0s 350us/step - loss: 8.4009 - val_loss: 99.8254\n",
      "Epoch 8496/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 8.3223 - val_loss: 96.2411\n",
      "Epoch 8497/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 10.8191 - val_loss: 100.0495\n",
      "Epoch 8498/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 12.7076 - val_loss: 98.2138\n",
      "Epoch 8499/10000\n",
      "96/96 [==============================] - 0s 553us/step - loss: 8.2606 - val_loss: 99.7284\n",
      "Epoch 8500/10000\n",
      "96/96 [==============================] - 0s 626us/step - loss: 9.1750 - val_loss: 97.2114\n",
      "Epoch 8501/10000\n",
      "96/96 [==============================] - 0s 732us/step - loss: 10.0915 - val_loss: 87.2725\n",
      "Epoch 8502/10000\n",
      "96/96 [==============================] - 0s 567us/step - loss: 8.6212 - val_loss: 88.7453\n",
      "Epoch 8503/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 9.5868 - val_loss: 104.6682\n",
      "Epoch 8504/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 10.6613 - val_loss: 101.8490\n",
      "Epoch 8505/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 8.7615 - val_loss: 80.4606\n",
      "Epoch 8506/10000\n",
      "96/96 [==============================] - 0s 475us/step - loss: 8.1819 - val_loss: 90.2840\n",
      "Epoch 8507/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 11.3988 - val_loss: 96.3174\n",
      "Epoch 8508/10000\n",
      "96/96 [==============================] - 0s 526us/step - loss: 9.0462 - val_loss: 98.0884\n",
      "Epoch 8509/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 11.7405 - val_loss: 107.9795\n",
      "Epoch 8510/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 9.7291 - val_loss: 96.3603\n",
      "Epoch 8511/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 11.4798 - val_loss: 86.3302\n",
      "Epoch 8512/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 12.9878 - val_loss: 83.1232\n",
      "Epoch 8513/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 12.8380 - val_loss: 101.3342\n",
      "Epoch 8514/10000\n",
      "96/96 [==============================] - 0s 323us/step - loss: 11.5611 - val_loss: 96.2257\n",
      "Epoch 8515/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 13.1980 - val_loss: 93.9312\n",
      "Epoch 8516/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 15.7770 - val_loss: 98.0160\n",
      "Epoch 8517/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 24.8490 - val_loss: 89.3654\n",
      "Epoch 8518/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 14.9326 - val_loss: 94.4502\n",
      "Epoch 8519/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 20.9396 - val_loss: 83.7897\n",
      "Epoch 8520/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 22.0953 - val_loss: 93.5019\n",
      "Epoch 8521/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 16.4146 - val_loss: 95.7731\n",
      "Epoch 8522/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 16.3721 - val_loss: 92.8715\n",
      "Epoch 8523/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 18.7177 - val_loss: 95.7645\n",
      "Epoch 8524/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 18.7602 - val_loss: 93.7058\n",
      "Epoch 8525/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 46.4234 - val_loss: 99.5565\n",
      "Epoch 8526/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 20.2992 - val_loss: 94.4121\n",
      "Epoch 8527/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 18.9843 - val_loss: 105.4800\n",
      "Epoch 8528/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 17.2106 - val_loss: 99.0824\n",
      "Epoch 8529/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 18.8243 - val_loss: 114.2922\n",
      "Epoch 8530/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 20.8294 - val_loss: 106.4499\n",
      "Epoch 8531/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 16.6716 - val_loss: 112.2871\n",
      "Epoch 8532/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 16.8118 - val_loss: 116.8301\n",
      "Epoch 8533/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 451us/step - loss: 18.4538 - val_loss: 114.4697\n",
      "Epoch 8534/10000\n",
      "96/96 [==============================] - 0s 511us/step - loss: 17.6901 - val_loss: 104.9001\n",
      "Epoch 8535/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 15.7640 - val_loss: 106.7897\n",
      "Epoch 8536/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 16.4821 - val_loss: 104.3787\n",
      "Epoch 8537/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 18.9900 - val_loss: 92.1161\n",
      "Epoch 8538/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 19.4940 - val_loss: 107.1023\n",
      "Epoch 8539/10000\n",
      "96/96 [==============================] - 0s 482us/step - loss: 14.3244 - val_loss: 111.2420\n",
      "Epoch 8540/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 13.3400 - val_loss: 96.8600\n",
      "Epoch 8541/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 16.4427 - val_loss: 115.8915\n",
      "Epoch 8542/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 16.7359 - val_loss: 93.3810\n",
      "Epoch 8543/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 12.6373 - val_loss: 98.9234\n",
      "Epoch 8544/10000\n",
      "96/96 [==============================] - 0s 360us/step - loss: 13.1700 - val_loss: 106.3985\n",
      "Epoch 8545/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 11.3149 - val_loss: 110.0592\n",
      "Epoch 8546/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 12.5199 - val_loss: 109.8764\n",
      "Epoch 8547/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 13.5480 - val_loss: 89.5694\n",
      "Epoch 8548/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 11.1225 - val_loss: 104.7062\n",
      "Epoch 8549/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 13.2558 - val_loss: 101.0158\n",
      "Epoch 8550/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 11.1025 - val_loss: 93.3823\n",
      "Epoch 8551/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 12.7012 - val_loss: 93.7607\n",
      "Epoch 8552/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 15.1843 - val_loss: 94.4012\n",
      "Epoch 8553/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 11.9937 - val_loss: 97.3083\n",
      "Epoch 8554/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 14.2485 - val_loss: 102.7499\n",
      "Epoch 8555/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 11.1814 - val_loss: 94.0350\n",
      "Epoch 8556/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 12.3175 - val_loss: 106.8842\n",
      "Epoch 8557/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 12.8744 - val_loss: 100.4741\n",
      "Epoch 8558/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 10.9400 - val_loss: 103.4485\n",
      "Epoch 8559/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 10.3820 - val_loss: 103.2863\n",
      "Epoch 8560/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 13.1730 - val_loss: 119.2359\n",
      "Epoch 8561/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 12.0476 - val_loss: 106.9127\n",
      "Epoch 8562/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 10.0185 - val_loss: 106.5501\n",
      "Epoch 8563/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 10.7189 - val_loss: 102.9962\n",
      "Epoch 8564/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 10.9285 - val_loss: 81.4821\n",
      "Epoch 8565/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 12.8913 - val_loss: 101.6100\n",
      "Epoch 8566/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 8.7612 - val_loss: 88.4048\n",
      "Epoch 8567/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 8.9525 - val_loss: 92.8695\n",
      "Epoch 8568/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 10.5825 - val_loss: 95.2097\n",
      "Epoch 8569/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 9.7884 - val_loss: 99.5695\n",
      "Epoch 8570/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 10.0454 - val_loss: 106.1802\n",
      "Epoch 8571/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 10.5082 - val_loss: 102.5532\n",
      "Epoch 8572/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 11.1129 - val_loss: 81.9547\n",
      "Epoch 8573/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 8.8009 - val_loss: 97.6281\n",
      "Epoch 8574/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 11.8444 - val_loss: 104.0178\n",
      "Epoch 8575/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 9.4438 - val_loss: 108.2588\n",
      "Epoch 8576/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 10.6962 - val_loss: 94.2211\n",
      "Epoch 8577/10000\n",
      "96/96 [==============================] - 0s 349us/step - loss: 12.6924 - val_loss: 94.0462\n",
      "Epoch 8578/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 12.2385 - val_loss: 92.6220\n",
      "Epoch 8579/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 11.7320 - val_loss: 110.8141\n",
      "Epoch 8580/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 9.8873 - val_loss: 105.0423\n",
      "Epoch 8581/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 10.7362 - val_loss: 106.3925\n",
      "Epoch 8582/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 11.2093 - val_loss: 92.8104\n",
      "Epoch 8583/10000\n",
      "96/96 [==============================] - 0s 338us/step - loss: 11.5299 - val_loss: 104.3839\n",
      "Epoch 8584/10000\n",
      "96/96 [==============================] - 0s 343us/step - loss: 10.3045 - val_loss: 99.9070\n",
      "Epoch 8585/10000\n",
      "96/96 [==============================] - 0s 355us/step - loss: 9.9129 - val_loss: 105.7586\n",
      "Epoch 8586/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 10.8917 - val_loss: 93.7527\n",
      "Epoch 8587/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 9.8865 - val_loss: 89.2245\n",
      "Epoch 8588/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 16.4401 - val_loss: 97.8685\n",
      "Epoch 8589/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 10.0251 - val_loss: 102.6787\n",
      "Epoch 8590/10000\n",
      "96/96 [==============================] - 0s 356us/step - loss: 14.6526 - val_loss: 93.6042\n",
      "Epoch 8591/10000\n",
      "96/96 [==============================] - 0s 356us/step - loss: 11.7216 - val_loss: 99.9635\n",
      "Epoch 8592/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 11.0635 - val_loss: 105.7553\n",
      "Epoch 8593/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 10.9768 - val_loss: 91.9592\n",
      "Epoch 8594/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 11.6486 - val_loss: 101.6853\n",
      "Epoch 8595/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 9.2049 - val_loss: 103.8212\n",
      "Epoch 8596/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 9.5655 - val_loss: 87.9506\n",
      "Epoch 8597/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 9.0960 - val_loss: 98.0853\n",
      "Epoch 8598/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 11.4801 - val_loss: 82.1347\n",
      "Epoch 8599/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 12.0578 - val_loss: 88.3717\n",
      "Epoch 8600/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 7.9963 - val_loss: 90.6128\n",
      "Epoch 8601/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 12.2847 - val_loss: 96.5219\n",
      "Epoch 8602/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 9.5961 - val_loss: 99.0792\n",
      "Epoch 8603/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 10.7626 - val_loss: 116.3788\n",
      "Epoch 8604/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 10.8200 - val_loss: 81.9627\n",
      "Epoch 8605/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 11.0561 - val_loss: 92.2520\n",
      "Epoch 8606/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 11.4420 - val_loss: 110.7405\n",
      "Epoch 8607/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 11.1730 - val_loss: 92.5125\n",
      "Epoch 8608/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 11.3233 - val_loss: 100.5694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8609/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 9.4169 - val_loss: 106.9917\n",
      "Epoch 8610/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 7.8745 - val_loss: 103.9357\n",
      "Epoch 8611/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 10.7746 - val_loss: 95.8240\n",
      "Epoch 8612/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 10.5548 - val_loss: 102.0824\n",
      "Epoch 8613/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 7.2924 - val_loss: 116.3292\n",
      "Epoch 8614/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 10.1115 - val_loss: 99.4836\n",
      "Epoch 8615/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 8.5490 - val_loss: 97.2533\n",
      "Epoch 8616/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 9.0752 - val_loss: 86.0531\n",
      "Epoch 8617/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 8.6308 - val_loss: 95.6842\n",
      "Epoch 8618/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 8.8219 - val_loss: 95.9867\n",
      "Epoch 8619/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 7.4821 - val_loss: 109.1528\n",
      "Epoch 8620/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 7.7306 - val_loss: 98.8348\n",
      "Epoch 8621/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 10.7114 - val_loss: 104.3117\n",
      "Epoch 8622/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 7.7264 - val_loss: 101.7479\n",
      "Epoch 8623/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 16.5908 - val_loss: 93.5375\n",
      "Epoch 8624/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 11.5042 - val_loss: 85.0780\n",
      "Epoch 8625/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 16.2835 - val_loss: 79.8458\n",
      "Epoch 8626/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 13.4045 - val_loss: 88.9743\n",
      "Epoch 8627/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 10.6851 - val_loss: 76.6507\n",
      "Epoch 8628/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 11.0436 - val_loss: 75.1869\n",
      "Epoch 8629/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 14.0229 - val_loss: 80.3729\n",
      "Epoch 8630/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 13.4511 - val_loss: 84.3994\n",
      "Epoch 8631/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 17.4402 - val_loss: 77.8287\n",
      "Epoch 8632/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 21.2888 - val_loss: 76.0081\n",
      "Epoch 8633/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 20.6165 - val_loss: 66.8978\n",
      "Epoch 8634/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 15.9708 - val_loss: 98.5602\n",
      "Epoch 8635/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 15.0690 - val_loss: 90.3065\n",
      "Epoch 8636/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 16.8993 - val_loss: 99.6501\n",
      "Epoch 8637/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 12.1073 - val_loss: 90.5458\n",
      "Epoch 8638/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 14.1381 - val_loss: 77.4681\n",
      "Epoch 8639/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 13.6762 - val_loss: 85.7821\n",
      "Epoch 8640/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 13.0147 - val_loss: 88.7503\n",
      "Epoch 8641/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 10.5991 - val_loss: 87.5322\n",
      "Epoch 8642/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 15.5075 - val_loss: 77.9920\n",
      "Epoch 8643/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 13.2471 - val_loss: 97.3342\n",
      "Epoch 8644/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 11.4296 - val_loss: 84.5452\n",
      "Epoch 8645/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 8.3535 - val_loss: 84.1725\n",
      "Epoch 8646/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 12.8165 - val_loss: 96.1169\n",
      "Epoch 8647/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 15.2582 - val_loss: 94.8537\n",
      "Epoch 8648/10000\n",
      "96/96 [==============================] - 0s 521us/step - loss: 10.2595 - val_loss: 92.9708\n",
      "Epoch 8649/10000\n",
      "96/96 [==============================] - 0s 482us/step - loss: 10.1149 - val_loss: 90.9110\n",
      "Epoch 8650/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 12.8941 - val_loss: 93.1935\n",
      "Epoch 8651/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 12.4168 - val_loss: 101.5755\n",
      "Epoch 8652/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 11.2552 - val_loss: 101.4619\n",
      "Epoch 8653/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 10.5243 - val_loss: 95.4652\n",
      "Epoch 8654/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 11.6061 - val_loss: 105.3630\n",
      "Epoch 8655/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 9.0735 - val_loss: 102.4897\n",
      "Epoch 8656/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 8.8811 - val_loss: 100.4111\n",
      "Epoch 8657/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 10.7536 - val_loss: 103.9947\n",
      "Epoch 8658/10000\n",
      "96/96 [==============================] - 0s 362us/step - loss: 10.9417 - val_loss: 102.9024\n",
      "Epoch 8659/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 8.9257 - val_loss: 108.2547\n",
      "Epoch 8660/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 10.4998 - val_loss: 94.5872\n",
      "Epoch 8661/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 9.6201 - val_loss: 100.7548\n",
      "Epoch 8662/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 12.1680 - val_loss: 98.3241\n",
      "Epoch 8663/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 9.8306 - val_loss: 95.4939\n",
      "Epoch 8664/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 9.4976 - val_loss: 94.3202\n",
      "Epoch 8665/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 10.1346 - val_loss: 101.5532\n",
      "Epoch 8666/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 11.0885 - val_loss: 101.1995\n",
      "Epoch 8667/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 10.1548 - val_loss: 90.2027\n",
      "Epoch 8668/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 10.7452 - val_loss: 106.1687\n",
      "Epoch 8669/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 9.2208 - val_loss: 102.7009\n",
      "Epoch 8670/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 9.2847 - val_loss: 98.2540\n",
      "Epoch 8671/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 14.4287 - val_loss: 89.1684\n",
      "Epoch 8672/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 15.0708 - val_loss: 104.5507\n",
      "Epoch 8673/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 14.1736 - val_loss: 98.4504\n",
      "Epoch 8674/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 9.5353 - val_loss: 90.2887\n",
      "Epoch 8675/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 13.3386 - val_loss: 73.1832\n",
      "Epoch 8676/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 56.8018 - val_loss: 93.6045\n",
      "Epoch 8677/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 26.1326 - val_loss: 97.8119\n",
      "Epoch 8678/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 15.7582 - val_loss: 95.9890\n",
      "Epoch 8679/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 21.1943 - val_loss: 87.4343\n",
      "Epoch 8680/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 21.6889 - val_loss: 83.3958\n",
      "Epoch 8681/10000\n",
      "96/96 [==============================] - 0s 356us/step - loss: 19.5190 - val_loss: 85.9956\n",
      "Epoch 8682/10000\n",
      "96/96 [==============================] - 0s 359us/step - loss: 18.1268 - val_loss: 96.9233\n",
      "Epoch 8683/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 16.1943 - val_loss: 85.2668\n",
      "Epoch 8684/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 16.8767 - val_loss: 74.5432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8685/10000\n",
      "96/96 [==============================] - 0s 352us/step - loss: 13.8364 - val_loss: 93.9926\n",
      "Epoch 8686/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 20.4680 - val_loss: 93.2939\n",
      "Epoch 8687/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 11.7566 - val_loss: 85.2405\n",
      "Epoch 8688/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 12.5625 - val_loss: 90.1435\n",
      "Epoch 8689/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 12.0693 - val_loss: 83.7025\n",
      "Epoch 8690/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 9.6561 - val_loss: 94.1024\n",
      "Epoch 8691/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 11.1943 - val_loss: 92.6255\n",
      "Epoch 8692/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 9.4048 - val_loss: 98.1219\n",
      "Epoch 8693/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 11.0162 - val_loss: 93.9775\n",
      "Epoch 8694/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 10.1938 - val_loss: 83.8051\n",
      "Epoch 8695/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 9.9003 - val_loss: 76.4761\n",
      "Epoch 8696/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 12.3756 - val_loss: 87.8998\n",
      "Epoch 8697/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 11.7208 - val_loss: 95.8360\n",
      "Epoch 8698/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 11.8129 - val_loss: 94.0930\n",
      "Epoch 8699/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 10.5150 - val_loss: 91.0457\n",
      "Epoch 8700/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 8.1393 - val_loss: 86.6728\n",
      "Epoch 8701/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 9.1361 - val_loss: 82.0820\n",
      "Epoch 8702/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 9.5723 - val_loss: 92.8255\n",
      "Epoch 8703/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 11.2119 - val_loss: 90.3985\n",
      "Epoch 8704/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 8.5136 - val_loss: 91.4531\n",
      "Epoch 8705/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 9.2568 - val_loss: 86.8100\n",
      "Epoch 8706/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 8.7984 - val_loss: 87.1049\n",
      "Epoch 8707/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 9.1394 - val_loss: 89.7981\n",
      "Epoch 8708/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 7.6417 - val_loss: 86.9346\n",
      "Epoch 8709/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 9.5244 - val_loss: 77.9737\n",
      "Epoch 8710/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 9.3484 - val_loss: 95.3200\n",
      "Epoch 8711/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 9.3067 - val_loss: 89.2405\n",
      "Epoch 8712/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 10.0132 - val_loss: 91.1146\n",
      "Epoch 8713/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 8.0110 - val_loss: 91.1411\n",
      "Epoch 8714/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 11.9778 - val_loss: 89.0286\n",
      "Epoch 8715/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 7.7633 - val_loss: 91.3421\n",
      "Epoch 8716/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 8.3334 - val_loss: 92.3517\n",
      "Epoch 8717/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 8.6474 - val_loss: 96.0220\n",
      "Epoch 8718/10000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 10.2588 - val_loss: 87.7860\n",
      "Epoch 8719/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 8.6019 - val_loss: 96.2562\n",
      "Epoch 8720/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 32.4643 - val_loss: 103.4171\n",
      "Epoch 8721/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 14.4925 - val_loss: 82.2632\n",
      "Epoch 8722/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 18.2445 - val_loss: 73.8134\n",
      "Epoch 8723/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 14.5868 - val_loss: 87.5732\n",
      "Epoch 8724/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 13.0867 - val_loss: 78.0037\n",
      "Epoch 8725/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 14.7357 - val_loss: 72.6083\n",
      "Epoch 8726/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 12.3350 - val_loss: 78.5598\n",
      "Epoch 8727/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 12.5526 - val_loss: 98.0307\n",
      "Epoch 8728/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 13.1159 - val_loss: 89.5461\n",
      "Epoch 8729/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 12.0893 - val_loss: 104.4358\n",
      "Epoch 8730/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 11.4893 - val_loss: 78.5429\n",
      "Epoch 8731/10000\n",
      "96/96 [==============================] - 0s 488us/step - loss: 10.5606 - val_loss: 79.5325\n",
      "Epoch 8732/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 14.0238 - val_loss: 87.6805\n",
      "Epoch 8733/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 10.7909 - val_loss: 85.6775\n",
      "Epoch 8734/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 10.2148 - val_loss: 101.3895\n",
      "Epoch 8735/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 12.0273 - val_loss: 79.3796\n",
      "Epoch 8736/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 10.2836 - val_loss: 96.7051\n",
      "Epoch 8737/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 9.0444 - val_loss: 87.4081\n",
      "Epoch 8738/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 9.1845 - val_loss: 87.9560\n",
      "Epoch 8739/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 8.5375 - val_loss: 97.0736\n",
      "Epoch 8740/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 12.6433 - val_loss: 93.0623\n",
      "Epoch 8741/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 10.5634 - val_loss: 97.3310\n",
      "Epoch 8742/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 11.3614 - val_loss: 82.4666\n",
      "Epoch 8743/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 12.3624 - val_loss: 100.6572\n",
      "Epoch 8744/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 10.1273 - val_loss: 87.9030\n",
      "Epoch 8745/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 8.8337 - val_loss: 84.9617\n",
      "Epoch 8746/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 8.4499 - val_loss: 101.3654\n",
      "Epoch 8747/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 9.5997 - val_loss: 94.0449\n",
      "Epoch 8748/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 8.5707 - val_loss: 82.1185\n",
      "Epoch 8749/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 8.4763 - val_loss: 87.4956\n",
      "Epoch 8750/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 9.8856 - val_loss: 75.4233\n",
      "Epoch 8751/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 11.5306 - val_loss: 95.4685\n",
      "Epoch 8752/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 9.3922 - val_loss: 96.5669\n",
      "Epoch 8753/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 9.4620 - val_loss: 88.5011\n",
      "Epoch 8754/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 8.1878 - val_loss: 96.7082\n",
      "Epoch 8755/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 8.1794 - val_loss: 95.0501\n",
      "Epoch 8756/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 9.2520 - val_loss: 84.2737\n",
      "Epoch 8757/10000\n",
      "96/96 [==============================] - 0s 510us/step - loss: 8.8623 - val_loss: 90.4456\n",
      "Epoch 8758/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 8.6601 - val_loss: 103.4777\n",
      "Epoch 8759/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 9.3751 - val_loss: 83.6497\n",
      "Epoch 8760/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 11.0249 - val_loss: 90.7279\n",
      "Epoch 8761/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 387us/step - loss: 8.9841 - val_loss: 88.0648\n",
      "Epoch 8762/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 8.4534 - val_loss: 104.1372\n",
      "Epoch 8763/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 7.5600 - val_loss: 98.0544\n",
      "Epoch 8764/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 10.9781 - val_loss: 71.2364\n",
      "Epoch 8765/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 8.3536 - val_loss: 91.7443\n",
      "Epoch 8766/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 8.5666 - val_loss: 93.1328\n",
      "Epoch 8767/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 9.2124 - val_loss: 98.0428\n",
      "Epoch 8768/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 8.7513 - val_loss: 83.4745\n",
      "Epoch 8769/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 13.0710 - val_loss: 85.2326\n",
      "Epoch 8770/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 13.8419 - val_loss: 92.8216\n",
      "Epoch 8771/10000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 11.8941 - val_loss: 95.4942\n",
      "Epoch 8772/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 10.9861 - val_loss: 97.3089\n",
      "Epoch 8773/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 12.2320 - val_loss: 88.7867\n",
      "Epoch 8774/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 17.6073 - val_loss: 96.3530\n",
      "Epoch 8775/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 13.5587 - val_loss: 78.6382\n",
      "Epoch 8776/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 11.9505 - val_loss: 74.7681\n",
      "Epoch 8777/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 13.3834 - val_loss: 85.4332\n",
      "Epoch 8778/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 11.2852 - val_loss: 94.0869\n",
      "Epoch 8779/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 10.3735 - val_loss: 85.7781\n",
      "Epoch 8780/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 9.3679 - val_loss: 78.4456\n",
      "Epoch 8781/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 9.1800 - val_loss: 92.0591\n",
      "Epoch 8782/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 9.2918 - val_loss: 95.9162\n",
      "Epoch 8783/10000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 11.8245 - val_loss: 84.8363\n",
      "Epoch 8784/10000\n",
      "96/96 [==============================] - 0s 522us/step - loss: 12.4589 - val_loss: 92.0418\n",
      "Epoch 8785/10000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 9.5997 - val_loss: 83.5545\n",
      "Epoch 8786/10000\n",
      "96/96 [==============================] - 0s 531us/step - loss: 9.5731 - val_loss: 108.4421\n",
      "Epoch 8787/10000\n",
      "96/96 [==============================] - 0s 515us/step - loss: 10.7154 - val_loss: 105.0135\n",
      "Epoch 8788/10000\n",
      "96/96 [==============================] - 0s 572us/step - loss: 8.0901 - val_loss: 88.6664\n",
      "Epoch 8789/10000\n",
      "96/96 [==============================] - 0s 567us/step - loss: 10.9380 - val_loss: 85.7681\n",
      "Epoch 8790/10000\n",
      "96/96 [==============================] - 0s 606us/step - loss: 9.4815 - val_loss: 93.7076\n",
      "Epoch 8791/10000\n",
      "96/96 [==============================] - 0s 597us/step - loss: 10.6766 - val_loss: 94.4971\n",
      "Epoch 8792/10000\n",
      "96/96 [==============================] - 0s 526us/step - loss: 10.7338 - val_loss: 89.2276\n",
      "Epoch 8793/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 8.8465 - val_loss: 99.0936\n",
      "Epoch 8794/10000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 9.3624 - val_loss: 78.4401\n",
      "Epoch 8795/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 10.9421 - val_loss: 88.1929\n",
      "Epoch 8796/10000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 10.5248 - val_loss: 105.5137\n",
      "Epoch 8797/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 10.4281 - val_loss: 97.7814\n",
      "Epoch 8798/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 7.9712 - val_loss: 95.9734\n",
      "Epoch 8799/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 8.5455 - val_loss: 89.0139\n",
      "Epoch 8800/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 9.0971 - val_loss: 90.4960\n",
      "Epoch 8801/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 9.8493 - val_loss: 99.3532\n",
      "Epoch 8802/10000\n",
      "96/96 [==============================] - 0s 613us/step - loss: 12.3043 - val_loss: 96.1700\n",
      "Epoch 8803/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 11.1427 - val_loss: 88.2636\n",
      "Epoch 8804/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 8.7144 - val_loss: 100.4700\n",
      "Epoch 8805/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 8.3486 - val_loss: 96.8476\n",
      "Epoch 8806/10000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 8.4584 - val_loss: 72.4860\n",
      "Epoch 8807/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 9.0777 - val_loss: 100.5757\n",
      "Epoch 8808/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 8.6593 - val_loss: 91.8024\n",
      "Epoch 8809/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 9.8405 - val_loss: 94.3608\n",
      "Epoch 8810/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 12.7014 - val_loss: 86.4483\n",
      "Epoch 8811/10000\n",
      "96/96 [==============================] - 0s 503us/step - loss: 8.5063 - val_loss: 99.6102\n",
      "Epoch 8812/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 10.1107 - val_loss: 97.7038\n",
      "Epoch 8813/10000\n",
      "96/96 [==============================] - 0s 514us/step - loss: 10.6463 - val_loss: 82.4175\n",
      "Epoch 8814/10000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 9.8233 - val_loss: 93.1201\n",
      "Epoch 8815/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 10.1885 - val_loss: 85.2206\n",
      "Epoch 8816/10000\n",
      "96/96 [==============================] - 0s 485us/step - loss: 10.8449 - val_loss: 86.3983\n",
      "Epoch 8817/10000\n",
      "96/96 [==============================] - 0s 510us/step - loss: 8.9947 - val_loss: 88.7840\n",
      "Epoch 8818/10000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 9.2422 - val_loss: 86.1209\n",
      "Epoch 8819/10000\n",
      "96/96 [==============================] - 0s 493us/step - loss: 8.4042 - val_loss: 104.6183\n",
      "Epoch 8820/10000\n",
      "96/96 [==============================] - 0s 503us/step - loss: 10.7632 - val_loss: 98.0113\n",
      "Epoch 8821/10000\n",
      "96/96 [==============================] - 0s 586us/step - loss: 7.9626 - val_loss: 96.1914\n",
      "Epoch 8822/10000\n",
      "96/96 [==============================] - 0s 494us/step - loss: 7.6572 - val_loss: 86.4856\n",
      "Epoch 8823/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 8.9514 - val_loss: 80.8667\n",
      "Epoch 8824/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 8.8820 - val_loss: 79.5383\n",
      "Epoch 8825/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 11.4815 - val_loss: 99.8328\n",
      "Epoch 8826/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 9.3001 - val_loss: 87.2826\n",
      "Epoch 8827/10000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 7.2525 - val_loss: 93.4930\n",
      "Epoch 8828/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 7.8559 - val_loss: 101.5031\n",
      "Epoch 8829/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 8.4243 - val_loss: 98.1500\n",
      "Epoch 8830/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 9.2052 - val_loss: 94.1670\n",
      "Epoch 8831/10000\n",
      "96/96 [==============================] - 0s 482us/step - loss: 7.7394 - val_loss: 81.1969\n",
      "Epoch 8832/10000\n",
      "96/96 [==============================] - 0s 506us/step - loss: 8.2193 - val_loss: 88.1619\n",
      "Epoch 8833/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 8.9384 - val_loss: 79.4146\n",
      "Epoch 8834/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 9.5632 - val_loss: 98.0927\n",
      "Epoch 8835/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 8.4702 - val_loss: 89.9632\n",
      "Epoch 8836/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 8.7916 - val_loss: 89.7468\n",
      "Epoch 8837/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 7.2175 - val_loss: 93.0827\n",
      "Epoch 8838/10000\n",
      "96/96 [==============================] - 0s 516us/step - loss: 7.4313 - val_loss: 93.0535\n",
      "Epoch 8839/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 9.9904 - val_loss: 80.8111\n",
      "Epoch 8840/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 9.8835 - val_loss: 90.2951\n",
      "Epoch 8841/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 9.1969 - val_loss: 92.9077\n",
      "Epoch 8842/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 8.8230 - val_loss: 95.4928\n",
      "Epoch 8843/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 9.5898 - val_loss: 90.3186\n",
      "Epoch 8844/10000\n",
      "96/96 [==============================] - 0s 485us/step - loss: 8.3958 - val_loss: 101.0319\n",
      "Epoch 8845/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 8.2459 - val_loss: 96.5607\n",
      "Epoch 8846/10000\n",
      "96/96 [==============================] - 0s 527us/step - loss: 7.7159 - val_loss: 97.7281\n",
      "Epoch 8847/10000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 8.2174 - val_loss: 100.8665\n",
      "Epoch 8848/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 9.7427 - val_loss: 85.3829\n",
      "Epoch 8849/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 8.1663 - val_loss: 96.2246\n",
      "Epoch 8850/10000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 6.8407 - val_loss: 85.2556\n",
      "Epoch 8851/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 8.5162 - val_loss: 83.9702\n",
      "Epoch 8852/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 7.9721 - val_loss: 88.5975\n",
      "Epoch 8853/10000\n",
      "96/96 [==============================] - 0s 505us/step - loss: 8.5774 - val_loss: 86.1567\n",
      "Epoch 8854/10000\n",
      "96/96 [==============================] - 0s 506us/step - loss: 7.7293 - val_loss: 84.2382\n",
      "Epoch 8855/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 7.2096 - val_loss: 82.4289\n",
      "Epoch 8856/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 8.2365 - val_loss: 86.8176\n",
      "Epoch 8857/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 8.3552 - val_loss: 81.6921\n",
      "Epoch 8858/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 9.4619 - val_loss: 84.1160\n",
      "Epoch 8859/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 7.9536 - val_loss: 87.7567\n",
      "Epoch 8860/10000\n",
      "96/96 [==============================] - 0s 503us/step - loss: 11.5251 - val_loss: 105.9609\n",
      "Epoch 8861/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 7.7610 - val_loss: 103.1623\n",
      "Epoch 8862/10000\n",
      "96/96 [==============================] - 0s 689us/step - loss: 9.2902 - val_loss: 84.0199\n",
      "Epoch 8863/10000\n",
      "96/96 [==============================] - 0s 513us/step - loss: 7.3551 - val_loss: 90.3361\n",
      "Epoch 8864/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 8.9979 - val_loss: 91.1201\n",
      "Epoch 8865/10000\n",
      "96/96 [==============================] - 0s 649us/step - loss: 6.8471 - val_loss: 93.8228\n",
      "Epoch 8866/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 7.1391 - val_loss: 84.4991\n",
      "Epoch 8867/10000\n",
      "96/96 [==============================] - 0s 486us/step - loss: 8.4448 - val_loss: 90.2570\n",
      "Epoch 8868/10000\n",
      "96/96 [==============================] - 0s 495us/step - loss: 7.4191 - val_loss: 93.3483\n",
      "Epoch 8869/10000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 8.8220 - val_loss: 84.6405\n",
      "Epoch 8870/10000\n",
      "96/96 [==============================] - 0s 664us/step - loss: 8.2158 - val_loss: 95.6191\n",
      "Epoch 8871/10000\n",
      "96/96 [==============================] - 0s 698us/step - loss: 7.6123 - val_loss: 101.9252\n",
      "Epoch 8872/10000\n",
      "96/96 [==============================] - 0s 578us/step - loss: 8.9853 - val_loss: 88.1935\n",
      "Epoch 8873/10000\n",
      "96/96 [==============================] - 0s 600us/step - loss: 7.9986 - val_loss: 101.5572\n",
      "Epoch 8874/10000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 8.3351 - val_loss: 96.0525\n",
      "Epoch 8875/10000\n",
      "96/96 [==============================] - 0s 520us/step - loss: 8.0186 - val_loss: 90.2309\n",
      "Epoch 8876/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 9.0871 - val_loss: 92.4865\n",
      "Epoch 8877/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 9.9263 - val_loss: 98.9994\n",
      "Epoch 8878/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 7.6416 - val_loss: 90.1625\n",
      "Epoch 8879/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 7.6635 - val_loss: 88.6550\n",
      "Epoch 8880/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 10.9388 - val_loss: 82.0491\n",
      "Epoch 8881/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 9.5632 - val_loss: 100.1510\n",
      "Epoch 8882/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 10.6299 - val_loss: 84.4281\n",
      "Epoch 8883/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 8.4232 - val_loss: 97.0568\n",
      "Epoch 8884/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 9.4629 - val_loss: 84.5819\n",
      "Epoch 8885/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 10.2513 - val_loss: 89.7557\n",
      "Epoch 8886/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 8.4175 - val_loss: 97.3072\n",
      "Epoch 8887/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 8.8427 - val_loss: 76.2321\n",
      "Epoch 8888/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 9.5275 - val_loss: 84.8133\n",
      "Epoch 8889/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 10.7893 - val_loss: 82.3964\n",
      "Epoch 8890/10000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 7.7033 - val_loss: 84.0512\n",
      "Epoch 8891/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 7.6713 - val_loss: 75.4782\n",
      "Epoch 8892/10000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 8.1634 - val_loss: 111.0920\n",
      "Epoch 8893/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 6.4705 - val_loss: 90.7802\n",
      "Epoch 8894/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 6.9474 - val_loss: 93.8913\n",
      "Epoch 8895/10000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 8.4330 - val_loss: 90.6184\n",
      "Epoch 8896/10000\n",
      "96/96 [==============================] - 0s 493us/step - loss: 7.3376 - val_loss: 82.5751\n",
      "Epoch 8897/10000\n",
      "96/96 [==============================] - 0s 520us/step - loss: 7.8980 - val_loss: 87.7374\n",
      "Epoch 8898/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 8.4951 - val_loss: 94.6539\n",
      "Epoch 8899/10000\n",
      "96/96 [==============================] - 0s 510us/step - loss: 7.9725 - val_loss: 105.1199\n",
      "Epoch 8900/10000\n",
      "96/96 [==============================] - 0s 497us/step - loss: 7.7937 - val_loss: 100.7801\n",
      "Epoch 8901/10000\n",
      "96/96 [==============================] - 0s 565us/step - loss: 10.0835 - val_loss: 94.4366\n",
      "Epoch 8902/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 8.5689 - val_loss: 99.7111\n",
      "Epoch 8903/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 7.6457 - val_loss: 84.8702\n",
      "Epoch 8904/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 8.6444 - val_loss: 89.1878\n",
      "Epoch 8905/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 8.5212 - val_loss: 97.6019\n",
      "Epoch 8906/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 7.9392 - val_loss: 94.0553\n",
      "Epoch 8907/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 8.0432 - val_loss: 101.4755\n",
      "Epoch 8908/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 8.8998 - val_loss: 94.1009\n",
      "Epoch 8909/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 8.0501 - val_loss: 95.7608\n",
      "Epoch 8910/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 9.3941 - val_loss: 91.7432\n",
      "Epoch 8911/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 8.6017 - val_loss: 96.0210\n",
      "Epoch 8912/10000\n",
      "96/96 [==============================] - 0s 533us/step - loss: 6.9467 - val_loss: 101.8037\n",
      "Epoch 8913/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 456us/step - loss: 9.7179 - val_loss: 100.1195\n",
      "Epoch 8914/10000\n",
      "96/96 [==============================] - 0s 505us/step - loss: 8.8233 - val_loss: 102.9255\n",
      "Epoch 8915/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 9.7940 - val_loss: 95.6837\n",
      "Epoch 8916/10000\n",
      "96/96 [==============================] - 0s 508us/step - loss: 7.3201 - val_loss: 83.3721\n",
      "Epoch 8917/10000\n",
      "96/96 [==============================] - 0s 503us/step - loss: 7.5210 - val_loss: 91.5081\n",
      "Epoch 8918/10000\n",
      "96/96 [==============================] - 0s 495us/step - loss: 9.4946 - val_loss: 94.0730\n",
      "Epoch 8919/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 7.5287 - val_loss: 91.7079\n",
      "Epoch 8920/10000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 7.8139 - val_loss: 104.1153\n",
      "Epoch 8921/10000\n",
      "96/96 [==============================] - 0s 484us/step - loss: 9.7597 - val_loss: 90.9609\n",
      "Epoch 8922/10000\n",
      "96/96 [==============================] - 0s 480us/step - loss: 11.4127 - val_loss: 88.4925\n",
      "Epoch 8923/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 9.2914 - val_loss: 97.6324\n",
      "Epoch 8924/10000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 9.5138 - val_loss: 95.9151\n",
      "Epoch 8925/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 9.9765 - val_loss: 95.7589\n",
      "Epoch 8926/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 7.1775 - val_loss: 104.6702\n",
      "Epoch 8927/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 26.8679 - val_loss: 105.3413\n",
      "Epoch 8928/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 26.2804 - val_loss: 95.3062\n",
      "Epoch 8929/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 50.0559 - val_loss: 87.2438\n",
      "Epoch 8930/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 17.3301 - val_loss: 98.2839\n",
      "Epoch 8931/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 21.3223 - val_loss: 90.3804\n",
      "Epoch 8932/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 28.9611 - val_loss: 79.4006\n",
      "Epoch 8933/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 35.5702 - val_loss: 77.9386\n",
      "Epoch 8934/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 23.3035 - val_loss: 89.6207\n",
      "Epoch 8935/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 16.0075 - val_loss: 68.7612\n",
      "Epoch 8936/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 25.9645 - val_loss: 91.9736\n",
      "Epoch 8937/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 20.3974 - val_loss: 92.2852\n",
      "Epoch 8938/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 23.5733 - val_loss: 96.8560\n",
      "Epoch 8939/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 22.2288 - val_loss: 98.5684\n",
      "Epoch 8940/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 19.4876 - val_loss: 77.1535\n",
      "Epoch 8941/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 20.8305 - val_loss: 85.4597\n",
      "Epoch 8942/10000\n",
      "96/96 [==============================] - 0s 357us/step - loss: 34.5874 - val_loss: 82.3418\n",
      "Epoch 8943/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 28.0784 - val_loss: 90.4560\n",
      "Epoch 8944/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 27.3305 - val_loss: 82.0651\n",
      "Epoch 8945/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 22.3173 - val_loss: 83.2558\n",
      "Epoch 8946/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 18.8455 - val_loss: 82.6189\n",
      "Epoch 8947/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 15.1346 - val_loss: 87.3109\n",
      "Epoch 8948/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 13.7498 - val_loss: 81.1769\n",
      "Epoch 8949/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 13.1180 - val_loss: 72.9290\n",
      "Epoch 8950/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 12.8592 - val_loss: 71.9859\n",
      "Epoch 8951/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 10.7711 - val_loss: 83.7882\n",
      "Epoch 8952/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 14.0490 - val_loss: 80.1338\n",
      "Epoch 8953/10000\n",
      "96/96 [==============================] - 0s 486us/step - loss: 10.5181 - val_loss: 80.4992\n",
      "Epoch 8954/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 10.1262 - val_loss: 78.1484\n",
      "Epoch 8955/10000\n",
      "96/96 [==============================] - 0s 480us/step - loss: 13.5099 - val_loss: 78.0278\n",
      "Epoch 8956/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 17.7564 - val_loss: 75.8078\n",
      "Epoch 8957/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 14.5125 - val_loss: 81.6228\n",
      "Epoch 8958/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 14.6038 - val_loss: 74.7820\n",
      "Epoch 8959/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 12.8515 - val_loss: 68.3962\n",
      "Epoch 8960/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 11.2251 - val_loss: 87.5169\n",
      "Epoch 8961/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 10.4867 - val_loss: 83.7325\n",
      "Epoch 8962/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 11.2891 - val_loss: 88.6500\n",
      "Epoch 8963/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 9.6636 - val_loss: 89.9480\n",
      "Epoch 8964/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 9.7454 - val_loss: 85.8124\n",
      "Epoch 8965/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 11.5804 - val_loss: 96.3459\n",
      "Epoch 8966/10000\n",
      "96/96 [==============================] - 0s 396us/step - loss: 10.0420 - val_loss: 85.3930\n",
      "Epoch 8967/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 9.5035 - val_loss: 89.6856\n",
      "Epoch 8968/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 9.9477 - val_loss: 85.0637\n",
      "Epoch 8969/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 9.3091 - val_loss: 86.3091\n",
      "Epoch 8970/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 9.3221 - val_loss: 78.3945\n",
      "Epoch 8971/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 8.2575 - val_loss: 87.6981\n",
      "Epoch 8972/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 10.9483 - val_loss: 89.4305\n",
      "Epoch 8973/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 8.8843 - val_loss: 77.6761\n",
      "Epoch 8974/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 8.6076 - val_loss: 86.9569\n",
      "Epoch 8975/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 10.7750 - val_loss: 86.9741\n",
      "Epoch 8976/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 9.4556 - val_loss: 78.0363\n",
      "Epoch 8977/10000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 8.3839 - val_loss: 76.7845\n",
      "Epoch 8978/10000\n",
      "96/96 [==============================] - 0s 541us/step - loss: 7.5196 - val_loss: 76.2000\n",
      "Epoch 8979/10000\n",
      "96/96 [==============================] - 0s 520us/step - loss: 10.7777 - val_loss: 73.0233\n",
      "Epoch 8980/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 8.2343 - val_loss: 79.1572\n",
      "Epoch 8981/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 7.7480 - val_loss: 79.4615\n",
      "Epoch 8982/10000\n",
      "96/96 [==============================] - 0s 475us/step - loss: 8.0030 - val_loss: 83.4281\n",
      "Epoch 8983/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 9.5247 - val_loss: 84.8710\n",
      "Epoch 8984/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 8.7354 - val_loss: 76.6237\n",
      "Epoch 8985/10000\n",
      "96/96 [==============================] - 0s 484us/step - loss: 8.9801 - val_loss: 86.3363\n",
      "Epoch 8986/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 10.2808 - val_loss: 72.0877\n",
      "Epoch 8987/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 8.6257 - val_loss: 79.3723\n",
      "Epoch 8988/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 8.5550 - val_loss: 85.9738\n",
      "Epoch 8989/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 7.9529 - val_loss: 85.4764\n",
      "Epoch 8990/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 11.2565 - val_loss: 89.4190\n",
      "Epoch 8991/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 8.2131 - val_loss: 69.7052\n",
      "Epoch 8992/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 8.7490 - val_loss: 77.0641\n",
      "Epoch 8993/10000\n",
      "96/96 [==============================] - 0s 511us/step - loss: 9.9031 - val_loss: 77.2805\n",
      "Epoch 8994/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 10.0820 - val_loss: 72.1352\n",
      "Epoch 8995/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 13.1436 - val_loss: 80.1314\n",
      "Epoch 8996/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 11.4581 - val_loss: 91.7640\n",
      "Epoch 8997/10000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 8.3795 - val_loss: 78.9312\n",
      "Epoch 8998/10000\n",
      "96/96 [==============================] - 0s 485us/step - loss: 8.7227 - val_loss: 68.2608\n",
      "Epoch 8999/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 8.4234 - val_loss: 83.6353\n",
      "Epoch 9000/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 12.2303 - val_loss: 92.3412\n",
      "Epoch 9001/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 8.3714 - val_loss: 79.3811\n",
      "Epoch 9002/10000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 9.1404 - val_loss: 83.3145\n",
      "Epoch 9003/10000\n",
      "96/96 [==============================] - 0s 488us/step - loss: 8.7229 - val_loss: 93.2009\n",
      "Epoch 9004/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 8.4731 - val_loss: 79.1195\n",
      "Epoch 9005/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 8.3864 - val_loss: 82.4371\n",
      "Epoch 9006/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 7.4934 - val_loss: 91.9206\n",
      "Epoch 9007/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 7.5472 - val_loss: 83.8742\n",
      "Epoch 9008/10000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 9.0020 - val_loss: 96.6276\n",
      "Epoch 9009/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 10.7956 - val_loss: 90.2347\n",
      "Epoch 9010/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 8.2189 - val_loss: 84.2020\n",
      "Epoch 9011/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 8.9759 - val_loss: 100.0428\n",
      "Epoch 9012/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 9.4111 - val_loss: 87.7132\n",
      "Epoch 9013/10000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 8.1085 - val_loss: 76.7425\n",
      "Epoch 9014/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 10.4923 - val_loss: 93.3534\n",
      "Epoch 9015/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 8.3319 - val_loss: 97.0867\n",
      "Epoch 9016/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 11.4411 - val_loss: 90.3732\n",
      "Epoch 9017/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 10.2579 - val_loss: 77.9093\n",
      "Epoch 9018/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 8.3766 - val_loss: 78.6696\n",
      "Epoch 9019/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 10.6388 - val_loss: 78.8125\n",
      "Epoch 9020/10000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 10.8171 - val_loss: 80.2741\n",
      "Epoch 9021/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 9.6144 - val_loss: 89.7090\n",
      "Epoch 9022/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 12.3607 - val_loss: 83.4553\n",
      "Epoch 9023/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 7.4859 - val_loss: 77.0146\n",
      "Epoch 9024/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 7.2371 - val_loss: 86.8649\n",
      "Epoch 9025/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 7.7170 - val_loss: 94.4564\n",
      "Epoch 9026/10000\n",
      "96/96 [==============================] - 0s 587us/step - loss: 8.4787 - val_loss: 94.5590\n",
      "Epoch 9027/10000\n",
      "96/96 [==============================] - 0s 559us/step - loss: 7.7218 - val_loss: 92.7977\n",
      "Epoch 9028/10000\n",
      "96/96 [==============================] - 0s 638us/step - loss: 8.8417 - val_loss: 91.5358\n",
      "Epoch 9029/10000\n",
      "96/96 [==============================] - 0s 552us/step - loss: 8.4305 - val_loss: 87.0633\n",
      "Epoch 9030/10000\n",
      "96/96 [==============================] - 0s 575us/step - loss: 8.4319 - val_loss: 82.0361\n",
      "Epoch 9031/10000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 7.2737 - val_loss: 92.5842\n",
      "Epoch 9032/10000\n",
      "96/96 [==============================] - 0s 493us/step - loss: 8.0274 - val_loss: 89.3871\n",
      "Epoch 9033/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 6.5825 - val_loss: 98.6143\n",
      "Epoch 9034/10000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 7.9602 - val_loss: 95.2380\n",
      "Epoch 9035/10000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 8.2211 - val_loss: 90.5741\n",
      "Epoch 9036/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 11.8520 - val_loss: 77.7094\n",
      "Epoch 9037/10000\n",
      "96/96 [==============================] - 0s 522us/step - loss: 9.0056 - val_loss: 92.2764\n",
      "Epoch 9038/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 10.4976 - val_loss: 92.7944\n",
      "Epoch 9039/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 8.5177 - val_loss: 84.9406\n",
      "Epoch 9040/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 12.2269 - val_loss: 89.4653\n",
      "Epoch 9041/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 9.3858 - val_loss: 92.3926\n",
      "Epoch 9042/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 7.9041 - val_loss: 92.7212\n",
      "Epoch 9043/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 6.9702 - val_loss: 99.7068\n",
      "Epoch 9044/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 8.6943 - val_loss: 95.6368\n",
      "Epoch 9045/10000\n",
      "96/96 [==============================] - 0s 368us/step - loss: 9.8139 - val_loss: 87.3058\n",
      "Epoch 9046/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 9.3511 - val_loss: 94.2685\n",
      "Epoch 9047/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 8.7183 - val_loss: 106.9660\n",
      "Epoch 9048/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 7.4623 - val_loss: 92.8490\n",
      "Epoch 9049/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 9.1014 - val_loss: 100.1726\n",
      "Epoch 9050/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 7.4135 - val_loss: 86.9605\n",
      "Epoch 9051/10000\n",
      "96/96 [==============================] - 0s 373us/step - loss: 10.4284 - val_loss: 95.1645\n",
      "Epoch 9052/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 9.9378 - val_loss: 90.5145\n",
      "Epoch 9053/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 8.8436 - val_loss: 91.6581\n",
      "Epoch 9054/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 8.4442 - val_loss: 81.4561\n",
      "Epoch 9055/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 7.6798 - val_loss: 91.0753\n",
      "Epoch 9056/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 7.8531 - val_loss: 90.0990\n",
      "Epoch 9057/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 8.5383 - val_loss: 77.4456\n",
      "Epoch 9058/10000\n",
      "96/96 [==============================] - 0s 511us/step - loss: 6.7289 - val_loss: 95.0163\n",
      "Epoch 9059/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 7.0705 - val_loss: 81.8060\n",
      "Epoch 9060/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 9.2061 - val_loss: 76.8756\n",
      "Epoch 9061/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 7.7212 - val_loss: 85.5955\n",
      "Epoch 9062/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 7.9254 - val_loss: 83.4091\n",
      "Epoch 9063/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 8.2802 - val_loss: 100.9234\n",
      "Epoch 9064/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 9.0522 - val_loss: 83.3272\n",
      "Epoch 9065/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 397us/step - loss: 8.1167 - val_loss: 92.9841\n",
      "Epoch 9066/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 8.4923 - val_loss: 99.9166\n",
      "Epoch 9067/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 11.5034 - val_loss: 85.0974\n",
      "Epoch 9068/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 8.4434 - val_loss: 80.1050\n",
      "Epoch 9069/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 8.3061 - val_loss: 85.9491\n",
      "Epoch 9070/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 10.1864 - val_loss: 80.2465\n",
      "Epoch 9071/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 8.5250 - val_loss: 82.2344\n",
      "Epoch 9072/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 11.4548 - val_loss: 95.2908\n",
      "Epoch 9073/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 8.0635 - val_loss: 81.5917\n",
      "Epoch 9074/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 8.4057 - val_loss: 84.0888\n",
      "Epoch 9075/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 7.4020 - val_loss: 97.0888\n",
      "Epoch 9076/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 8.4080 - val_loss: 82.4881\n",
      "Epoch 9077/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 6.5058 - val_loss: 85.8965\n",
      "Epoch 9078/10000\n",
      "96/96 [==============================] - 0s 387us/step - loss: 10.7762 - val_loss: 86.4783\n",
      "Epoch 9079/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 7.9684 - val_loss: 101.4092\n",
      "Epoch 9080/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 9.3662 - val_loss: 84.0771\n",
      "Epoch 9081/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 6.4652 - val_loss: 92.8757\n",
      "Epoch 9082/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 8.6768 - val_loss: 91.0933\n",
      "Epoch 9083/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 7.0766 - val_loss: 89.7961\n",
      "Epoch 9084/10000\n",
      "96/96 [==============================] - 0s 511us/step - loss: 7.4441 - val_loss: 85.4697\n",
      "Epoch 9085/10000\n",
      "96/96 [==============================] - 0s 522us/step - loss: 7.9053 - val_loss: 85.0000\n",
      "Epoch 9086/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 6.8680 - val_loss: 96.2883\n",
      "Epoch 9087/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 8.7247 - val_loss: 87.8114\n",
      "Epoch 9088/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 6.7011 - val_loss: 90.3354\n",
      "Epoch 9089/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 7.5617 - val_loss: 95.7039\n",
      "Epoch 9090/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 7.6301 - val_loss: 91.2966\n",
      "Epoch 9091/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 8.8375 - val_loss: 92.1761\n",
      "Epoch 9092/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 6.9882 - val_loss: 89.3750\n",
      "Epoch 9093/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 7.5763 - val_loss: 89.1943\n",
      "Epoch 9094/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 7.6187 - val_loss: 93.8860\n",
      "Epoch 9095/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 8.1736 - val_loss: 96.8561\n",
      "Epoch 9096/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 7.2079 - val_loss: 89.8681\n",
      "Epoch 9097/10000\n",
      "96/96 [==============================] - 0s 475us/step - loss: 5.8258 - val_loss: 84.6526\n",
      "Epoch 9098/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 6.0762 - val_loss: 84.1637\n",
      "Epoch 9099/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 7.4654 - val_loss: 88.1370\n",
      "Epoch 9100/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 7.5672 - val_loss: 91.8727\n",
      "Epoch 9101/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 7.7903 - val_loss: 94.9857\n",
      "Epoch 9102/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 7.4913 - val_loss: 90.9387\n",
      "Epoch 9103/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 7.7559 - val_loss: 86.2407\n",
      "Epoch 9104/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 7.8405 - val_loss: 110.1645\n",
      "Epoch 9105/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 7.1186 - val_loss: 100.7326\n",
      "Epoch 9106/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 7.7573 - val_loss: 89.6113\n",
      "Epoch 9107/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 9.3567 - val_loss: 92.3855\n",
      "Epoch 9108/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 8.6559 - val_loss: 91.4010\n",
      "Epoch 9109/10000\n",
      "96/96 [==============================] - 0s 526us/step - loss: 7.0231 - val_loss: 101.5549\n",
      "Epoch 9110/10000\n",
      "96/96 [==============================] - 0s 498us/step - loss: 11.0766 - val_loss: 84.7552\n",
      "Epoch 9111/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 10.2870 - val_loss: 86.5781\n",
      "Epoch 9112/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 7.6484 - val_loss: 84.4406\n",
      "Epoch 9113/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 9.4215 - val_loss: 103.4770\n",
      "Epoch 9114/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 10.6164 - val_loss: 84.7226\n",
      "Epoch 9115/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 7.7231 - val_loss: 94.7265\n",
      "Epoch 9116/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 6.7527 - val_loss: 88.9178\n",
      "Epoch 9117/10000\n",
      "96/96 [==============================] - 0s 480us/step - loss: 6.6877 - val_loss: 98.7035\n",
      "Epoch 9118/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 6.7814 - val_loss: 85.4753\n",
      "Epoch 9119/10000\n",
      "96/96 [==============================] - 0s 494us/step - loss: 8.2417 - val_loss: 85.2950\n",
      "Epoch 9120/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 7.5079 - val_loss: 87.9781\n",
      "Epoch 9121/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 7.3166 - val_loss: 110.4436\n",
      "Epoch 9122/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 7.9583 - val_loss: 90.1830\n",
      "Epoch 9123/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 6.4895 - val_loss: 81.5841\n",
      "Epoch 9124/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 8.3002 - val_loss: 82.8092\n",
      "Epoch 9125/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 9.7491 - val_loss: 92.4136\n",
      "Epoch 9126/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 6.6012 - val_loss: 92.9206\n",
      "Epoch 9127/10000\n",
      "96/96 [==============================] - 0s 481us/step - loss: 7.2692 - val_loss: 88.0711\n",
      "Epoch 9128/10000\n",
      "96/96 [==============================] - 0s 490us/step - loss: 7.2372 - val_loss: 91.2826\n",
      "Epoch 9129/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 7.0235 - val_loss: 108.1512\n",
      "Epoch 9130/10000\n",
      "96/96 [==============================] - 0s 501us/step - loss: 7.2627 - val_loss: 79.3478\n",
      "Epoch 9131/10000\n",
      "96/96 [==============================] - 0s 525us/step - loss: 7.2333 - val_loss: 99.4718\n",
      "Epoch 9132/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 8.7925 - val_loss: 92.8611\n",
      "Epoch 9133/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 7.4601 - val_loss: 75.4865\n",
      "Epoch 9134/10000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 6.7469 - val_loss: 101.2897\n",
      "Epoch 9135/10000\n",
      "96/96 [==============================] - 0s 519us/step - loss: 8.5292 - val_loss: 96.1118\n",
      "Epoch 9136/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 8.5119 - val_loss: 99.1699\n",
      "Epoch 9137/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 10.2491 - val_loss: 82.7597\n",
      "Epoch 9138/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 9.5197 - val_loss: 91.8513\n",
      "Epoch 9139/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 9.5841 - val_loss: 83.0048\n",
      "Epoch 9140/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 8.4486 - val_loss: 87.3258\n",
      "Epoch 9141/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 9.1965 - val_loss: 89.6632\n",
      "Epoch 9142/10000\n",
      "96/96 [==============================] - 0s 526us/step - loss: 7.4528 - val_loss: 104.3632\n",
      "Epoch 9143/10000\n",
      "96/96 [==============================] - 0s 509us/step - loss: 8.0410 - val_loss: 85.6566\n",
      "Epoch 9144/10000\n",
      "96/96 [==============================] - 0s 485us/step - loss: 8.6005 - val_loss: 90.0346\n",
      "Epoch 9145/10000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 7.6971 - val_loss: 96.2312\n",
      "Epoch 9146/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 7.8397 - val_loss: 102.8092\n",
      "Epoch 9147/10000\n",
      "96/96 [==============================] - 0s 504us/step - loss: 7.9729 - val_loss: 89.2434\n",
      "Epoch 9148/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 6.7245 - val_loss: 106.9608\n",
      "Epoch 9149/10000\n",
      "96/96 [==============================] - 0s 516us/step - loss: 7.3929 - val_loss: 103.0570\n",
      "Epoch 9150/10000\n",
      "96/96 [==============================] - 0s 530us/step - loss: 6.7949 - val_loss: 107.6127\n",
      "Epoch 9151/10000\n",
      "96/96 [==============================] - 0s 493us/step - loss: 7.7531 - val_loss: 93.9088\n",
      "Epoch 9152/10000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 7.8838 - val_loss: 96.8300\n",
      "Epoch 9153/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 7.9602 - val_loss: 95.3475\n",
      "Epoch 9154/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 7.8228 - val_loss: 76.1741\n",
      "Epoch 9155/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 7.8951 - val_loss: 98.6369\n",
      "Epoch 9156/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 7.2779 - val_loss: 86.8852\n",
      "Epoch 9157/10000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 6.0573 - val_loss: 95.3530\n",
      "Epoch 9158/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 11.2105 - val_loss: 95.6511\n",
      "Epoch 9159/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 7.6466 - val_loss: 83.3787\n",
      "Epoch 9160/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 7.2596 - val_loss: 81.4599\n",
      "Epoch 9161/10000\n",
      "96/96 [==============================] - 0s 378us/step - loss: 7.4122 - val_loss: 82.4852\n",
      "Epoch 9162/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 8.7144 - val_loss: 105.5623\n",
      "Epoch 9163/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 9.5193 - val_loss: 84.6848\n",
      "Epoch 9164/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 7.9386 - val_loss: 95.3924\n",
      "Epoch 9165/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 7.8448 - val_loss: 95.0706\n",
      "Epoch 9166/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 7.5581 - val_loss: 83.5589\n",
      "Epoch 9167/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 6.6675 - val_loss: 97.9098\n",
      "Epoch 9168/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 6.6278 - val_loss: 90.3063\n",
      "Epoch 9169/10000\n",
      "96/96 [==============================] - 0s 346us/step - loss: 7.5008 - val_loss: 80.4381\n",
      "Epoch 9170/10000\n",
      "96/96 [==============================] - 0s 335us/step - loss: 6.3599 - val_loss: 88.4403\n",
      "Epoch 9171/10000\n",
      "96/96 [==============================] - 0s 328us/step - loss: 7.8566 - val_loss: 83.0302\n",
      "Epoch 9172/10000\n",
      "96/96 [==============================] - 0s 317us/step - loss: 7.4816 - val_loss: 90.0866\n",
      "Epoch 9173/10000\n",
      "96/96 [==============================] - 0s 317us/step - loss: 7.6358 - val_loss: 85.3849\n",
      "Epoch 9174/10000\n",
      "96/96 [==============================] - 0s 326us/step - loss: 8.0921 - val_loss: 95.2124\n",
      "Epoch 9175/10000\n",
      "96/96 [==============================] - 0s 326us/step - loss: 8.8663 - val_loss: 87.0821\n",
      "Epoch 9176/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 6.1767 - val_loss: 86.6116\n",
      "Epoch 9177/10000\n",
      "96/96 [==============================] - 0s 357us/step - loss: 6.7009 - val_loss: 91.5808\n",
      "Epoch 9178/10000\n",
      "96/96 [==============================] - 0s 341us/step - loss: 7.4543 - val_loss: 84.4155\n",
      "Epoch 9179/10000\n",
      "96/96 [==============================] - 0s 345us/step - loss: 7.4783 - val_loss: 94.9429\n",
      "Epoch 9180/10000\n",
      "96/96 [==============================] - 0s 343us/step - loss: 8.1866 - val_loss: 90.4776\n",
      "Epoch 9181/10000\n",
      "96/96 [==============================] - 0s 340us/step - loss: 7.5915 - val_loss: 79.8162\n",
      "Epoch 9182/10000\n",
      "96/96 [==============================] - 0s 357us/step - loss: 7.3699 - val_loss: 99.3234\n",
      "Epoch 9183/10000\n",
      "96/96 [==============================] - 0s 333us/step - loss: 7.2430 - val_loss: 91.5217\n",
      "Epoch 9184/10000\n",
      "96/96 [==============================] - 0s 319us/step - loss: 6.0137 - val_loss: 91.2938\n",
      "Epoch 9185/10000\n",
      "96/96 [==============================] - 0s 324us/step - loss: 7.6119 - val_loss: 86.7187\n",
      "Epoch 9186/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 7.1260 - val_loss: 82.0483\n",
      "Epoch 9187/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 8.6086 - val_loss: 89.5501\n",
      "Epoch 9188/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 8.6931 - val_loss: 78.8530\n",
      "Epoch 9189/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 6.5378 - val_loss: 85.1663\n",
      "Epoch 9190/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 7.1781 - val_loss: 99.1612\n",
      "Epoch 9191/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 7.0661 - val_loss: 89.6571\n",
      "Epoch 9192/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 6.5469 - val_loss: 87.5780\n",
      "Epoch 9193/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 11.9446 - val_loss: 93.6404\n",
      "Epoch 9194/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 8.9986 - val_loss: 81.0733\n",
      "Epoch 9195/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 9.8652 - val_loss: 82.2040\n",
      "Epoch 9196/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 8.6028 - val_loss: 91.9614\n",
      "Epoch 9197/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 9.1103 - val_loss: 85.0164\n",
      "Epoch 9198/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 9.3610 - val_loss: 97.7150\n",
      "Epoch 9199/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 6.9431 - val_loss: 96.4305\n",
      "Epoch 9200/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 6.8389 - val_loss: 86.4415\n",
      "Epoch 9201/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 8.1258 - val_loss: 94.7232\n",
      "Epoch 9202/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 10.3169 - val_loss: 95.2268\n",
      "Epoch 9203/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 7.8402 - val_loss: 89.7491\n",
      "Epoch 9204/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 7.0310 - val_loss: 85.7012\n",
      "Epoch 9205/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 8.6355 - val_loss: 87.8565\n",
      "Epoch 9206/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 7.4191 - val_loss: 86.6002\n",
      "Epoch 9207/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 5.5784 - val_loss: 87.2023\n",
      "Epoch 9208/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 7.5539 - val_loss: 97.4089\n",
      "Epoch 9209/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 6.9860 - val_loss: 104.6240\n",
      "Epoch 9210/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 10.2223 - val_loss: 98.0904\n",
      "Epoch 9211/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 8.0281 - val_loss: 100.1240\n",
      "Epoch 9212/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 10.9715 - val_loss: 93.4309\n",
      "Epoch 9213/10000\n",
      "96/96 [==============================] - 0s 507us/step - loss: 6.9600 - val_loss: 98.9144\n",
      "Epoch 9214/10000\n",
      "96/96 [==============================] - 0s 557us/step - loss: 9.4446 - val_loss: 92.7842\n",
      "Epoch 9215/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 6.9375 - val_loss: 95.4369\n",
      "Epoch 9216/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 8.0378 - val_loss: 84.9629\n",
      "Epoch 9217/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 427us/step - loss: 7.2862 - val_loss: 86.7790\n",
      "Epoch 9218/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 7.4907 - val_loss: 102.0614\n",
      "Epoch 9219/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 10.5673 - val_loss: 83.0494\n",
      "Epoch 9220/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 7.9038 - val_loss: 83.1189\n",
      "Epoch 9221/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 12.3884 - val_loss: 94.6100\n",
      "Epoch 9222/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 15.0748 - val_loss: 92.8638\n",
      "Epoch 9223/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 11.5137 - val_loss: 93.5050\n",
      "Epoch 9224/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 14.1272 - val_loss: 103.0558\n",
      "Epoch 9225/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 10.9206 - val_loss: 106.1815\n",
      "Epoch 9226/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 7.9101 - val_loss: 85.2505\n",
      "Epoch 9227/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 10.1481 - val_loss: 91.9987\n",
      "Epoch 9228/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 10.2152 - val_loss: 100.8455\n",
      "Epoch 9229/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 10.6386 - val_loss: 91.7870\n",
      "Epoch 9230/10000\n",
      "96/96 [==============================] - 0s 511us/step - loss: 7.5765 - val_loss: 99.3686\n",
      "Epoch 9231/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 8.9989 - val_loss: 87.6229\n",
      "Epoch 9232/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 10.0650 - val_loss: 90.4205\n",
      "Epoch 9233/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 9.0803 - val_loss: 84.5267\n",
      "Epoch 9234/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 8.3397 - val_loss: 83.4227\n",
      "Epoch 9235/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 8.4907 - val_loss: 99.1773\n",
      "Epoch 9236/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 9.6391 - val_loss: 95.4917\n",
      "Epoch 9237/10000\n",
      "96/96 [==============================] - 0s 522us/step - loss: 7.3622 - val_loss: 77.9738\n",
      "Epoch 9238/10000\n",
      "96/96 [==============================] - 0s 579us/step - loss: 6.1747 - val_loss: 87.0565\n",
      "Epoch 9239/10000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 7.9728 - val_loss: 102.9462\n",
      "Epoch 9240/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 8.9887 - val_loss: 93.5562\n",
      "Epoch 9241/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 7.9127 - val_loss: 101.9713\n",
      "Epoch 9242/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 7.9537 - val_loss: 92.4088\n",
      "Epoch 9243/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 7.6305 - val_loss: 86.4187\n",
      "Epoch 9244/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 8.6912 - val_loss: 105.8748\n",
      "Epoch 9245/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 10.0389 - val_loss: 104.6852\n",
      "Epoch 9246/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 7.4615 - val_loss: 93.1621\n",
      "Epoch 9247/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 9.3036 - val_loss: 98.3622\n",
      "Epoch 9248/10000\n",
      "96/96 [==============================] - 0s 490us/step - loss: 8.3532 - val_loss: 102.1596\n",
      "Epoch 9249/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 8.8309 - val_loss: 103.1242\n",
      "Epoch 9250/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 8.3210 - val_loss: 97.6282\n",
      "Epoch 9251/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 7.9600 - val_loss: 87.4206\n",
      "Epoch 9252/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 8.9889 - val_loss: 97.4124\n",
      "Epoch 9253/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 7.2696 - val_loss: 95.2453\n",
      "Epoch 9254/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 6.6825 - val_loss: 93.8155\n",
      "Epoch 9255/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 12.8518 - val_loss: 86.0755\n",
      "Epoch 9256/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 7.6801 - val_loss: 83.3824\n",
      "Epoch 9257/10000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 6.6218 - val_loss: 97.0478\n",
      "Epoch 9258/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 8.7273 - val_loss: 98.2577\n",
      "Epoch 9259/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 8.1290 - val_loss: 88.5045\n",
      "Epoch 9260/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 8.6068 - val_loss: 92.7627\n",
      "Epoch 9261/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 8.8335 - val_loss: 101.9742\n",
      "Epoch 9262/10000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 7.2376 - val_loss: 81.3519\n",
      "Epoch 9263/10000\n",
      "96/96 [==============================] - 0s 567us/step - loss: 7.5293 - val_loss: 98.3231\n",
      "Epoch 9264/10000\n",
      "96/96 [==============================] - 0s 480us/step - loss: 6.7920 - val_loss: 89.3878\n",
      "Epoch 9265/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 8.0613 - val_loss: 81.6597\n",
      "Epoch 9266/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 7.1901 - val_loss: 82.8251\n",
      "Epoch 9267/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 8.1098 - val_loss: 80.8051\n",
      "Epoch 9268/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 9.1361 - val_loss: 91.4364\n",
      "Epoch 9269/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 7.3518 - val_loss: 90.1966\n",
      "Epoch 9270/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 6.8493 - val_loss: 101.3166\n",
      "Epoch 9271/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 7.9000 - val_loss: 78.8659\n",
      "Epoch 9272/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 6.4448 - val_loss: 86.8947\n",
      "Epoch 9273/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 7.0743 - val_loss: 98.5491\n",
      "Epoch 9274/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 7.2731 - val_loss: 101.8570\n",
      "Epoch 9275/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 7.0388 - val_loss: 81.2408\n",
      "Epoch 9276/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 5.8625 - val_loss: 86.2630\n",
      "Epoch 9277/10000\n",
      "96/96 [==============================] - 0s 382us/step - loss: 10.5828 - val_loss: 96.0844\n",
      "Epoch 9278/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 21.0384 - val_loss: 83.4163\n",
      "Epoch 9279/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 62.5752 - val_loss: 85.5196\n",
      "Epoch 9280/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 16.1307 - val_loss: 97.5187\n",
      "Epoch 9281/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 30.1452 - val_loss: 91.3040\n",
      "Epoch 9282/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 129.0604 - val_loss: 79.6622\n",
      "Epoch 9283/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 30.4004 - val_loss: 62.3121\n",
      "Epoch 9284/10000\n",
      "96/96 [==============================] - 0s 367us/step - loss: 88.7969 - val_loss: 70.2774\n",
      "Epoch 9285/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 30.2329 - val_loss: 97.4713\n",
      "Epoch 9286/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 26.3301 - val_loss: 96.5986\n",
      "Epoch 9287/10000\n",
      "96/96 [==============================] - 0s 369us/step - loss: 25.3459 - val_loss: 92.3232\n",
      "Epoch 9288/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 21.8022 - val_loss: 98.6345\n",
      "Epoch 9289/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 32.2030 - val_loss: 74.1224\n",
      "Epoch 9290/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 23.2691 - val_loss: 78.1523\n",
      "Epoch 9291/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 24.9297 - val_loss: 78.0461\n",
      "Epoch 9292/10000\n",
      "96/96 [==============================] - 0s 370us/step - loss: 20.5827 - val_loss: 78.0599\n",
      "Epoch 9293/10000\n",
      "96/96 [==============================] - 0s 356us/step - loss: 21.3576 - val_loss: 77.6001\n",
      "Epoch 9294/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 19.3744 - val_loss: 70.8719\n",
      "Epoch 9295/10000\n",
      "96/96 [==============================] - 0s 526us/step - loss: 20.9399 - val_loss: 72.6385\n",
      "Epoch 9296/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 16.6373 - val_loss: 76.7212\n",
      "Epoch 9297/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 16.5172 - val_loss: 74.9742\n",
      "Epoch 9298/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 17.3565 - val_loss: 81.7623\n",
      "Epoch 9299/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 15.2269 - val_loss: 82.2498\n",
      "Epoch 9300/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 17.9041 - val_loss: 83.0606\n",
      "Epoch 9301/10000\n",
      "96/96 [==============================] - 0s 385us/step - loss: 12.9665 - val_loss: 80.5895\n",
      "Epoch 9302/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 15.5180 - val_loss: 79.5286\n",
      "Epoch 9303/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 15.3554 - val_loss: 71.6831\n",
      "Epoch 9304/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 14.9728 - val_loss: 81.0785\n",
      "Epoch 9305/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 12.5724 - val_loss: 81.9930\n",
      "Epoch 9306/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 12.5717 - val_loss: 88.4399\n",
      "Epoch 9307/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 13.9717 - val_loss: 82.0595\n",
      "Epoch 9308/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 12.2635 - val_loss: 84.1803\n",
      "Epoch 9309/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 14.2220 - val_loss: 93.9128\n",
      "Epoch 9310/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 11.5990 - val_loss: 72.3674\n",
      "Epoch 9311/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 13.7845 - val_loss: 78.4703\n",
      "Epoch 9312/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 11.5276 - val_loss: 83.0550\n",
      "Epoch 9313/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 19.7659 - val_loss: 76.4734\n",
      "Epoch 9314/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 12.5650 - val_loss: 80.5349\n",
      "Epoch 9315/10000\n",
      "96/96 [==============================] - 0s 366us/step - loss: 13.7796 - val_loss: 74.9385\n",
      "Epoch 9316/10000\n",
      "96/96 [==============================] - 0s 496us/step - loss: 15.9574 - val_loss: 80.1936\n",
      "Epoch 9317/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 20.1702 - val_loss: 66.7140\n",
      "Epoch 9318/10000\n",
      "96/96 [==============================] - 0s 384us/step - loss: 13.1334 - val_loss: 88.3152\n",
      "Epoch 9319/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 15.8023 - val_loss: 77.8120\n",
      "Epoch 9320/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 20.7201 - val_loss: 80.8616\n",
      "Epoch 9321/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 15.8213 - val_loss: 70.2544\n",
      "Epoch 9322/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 17.8941 - val_loss: 76.3822\n",
      "Epoch 9323/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 16.6820 - val_loss: 71.4851\n",
      "Epoch 9324/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 13.9498 - val_loss: 77.9067\n",
      "Epoch 9325/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 31.9683 - val_loss: 70.9346\n",
      "Epoch 9326/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 15.4513 - val_loss: 105.8099\n",
      "Epoch 9327/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 19.5879 - val_loss: 88.1237\n",
      "Epoch 9328/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 18.8711 - val_loss: 92.1738\n",
      "Epoch 9329/10000\n",
      "96/96 [==============================] - 0s 637us/step - loss: 14.1394 - val_loss: 83.2511\n",
      "Epoch 9330/10000\n",
      "96/96 [==============================] - 0s 581us/step - loss: 14.1541 - val_loss: 82.8389\n",
      "Epoch 9331/10000\n",
      "96/96 [==============================] - 0s 494us/step - loss: 20.2789 - val_loss: 83.2309\n",
      "Epoch 9332/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 42.5514 - val_loss: 90.3673\n",
      "Epoch 9333/10000\n",
      "96/96 [==============================] - 0s 487us/step - loss: 18.7733 - val_loss: 86.1153\n",
      "Epoch 9334/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 21.1416 - val_loss: 83.6362\n",
      "Epoch 9335/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 16.3995 - val_loss: 88.4628\n",
      "Epoch 9336/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 18.8754 - val_loss: 83.0577\n",
      "Epoch 9337/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 16.8519 - val_loss: 89.4194\n",
      "Epoch 9338/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 20.1844 - val_loss: 91.2813\n",
      "Epoch 9339/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 21.0780 - val_loss: 89.4623\n",
      "Epoch 9340/10000\n",
      "96/96 [==============================] - 0s 549us/step - loss: 17.4679 - val_loss: 82.8537\n",
      "Epoch 9341/10000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 16.1764 - val_loss: 102.7673\n",
      "Epoch 9342/10000\n",
      "96/96 [==============================] - 0s 481us/step - loss: 15.8271 - val_loss: 96.1238\n",
      "Epoch 9343/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 13.5960 - val_loss: 104.3391\n",
      "Epoch 9344/10000\n",
      "96/96 [==============================] - 0s 658us/step - loss: 12.6078 - val_loss: 89.1378\n",
      "Epoch 9345/10000\n",
      "96/96 [==============================] - 0s 611us/step - loss: 16.6045 - val_loss: 99.0320\n",
      "Epoch 9346/10000\n",
      "96/96 [==============================] - 0s 482us/step - loss: 16.4282 - val_loss: 84.1647\n",
      "Epoch 9347/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 10.6201 - val_loss: 102.2659\n",
      "Epoch 9348/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 11.7893 - val_loss: 92.6281\n",
      "Epoch 9349/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 13.5381 - val_loss: 97.2686\n",
      "Epoch 9350/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 12.9580 - val_loss: 80.0490\n",
      "Epoch 9351/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 11.8539 - val_loss: 85.2939\n",
      "Epoch 9352/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 12.7444 - val_loss: 94.9602\n",
      "Epoch 9353/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 11.6344 - val_loss: 90.1521\n",
      "Epoch 9354/10000\n",
      "96/96 [==============================] - 0s 482us/step - loss: 14.1982 - val_loss: 94.8001\n",
      "Epoch 9355/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 11.8781 - val_loss: 79.6991\n",
      "Epoch 9356/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 10.3245 - val_loss: 95.9447\n",
      "Epoch 9357/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 10.4335 - val_loss: 78.8378\n",
      "Epoch 9358/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 11.4072 - val_loss: 94.4963\n",
      "Epoch 9359/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 13.5458 - val_loss: 100.6874\n",
      "Epoch 9360/10000\n",
      "96/96 [==============================] - 0s 639us/step - loss: 10.8096 - val_loss: 88.2487\n",
      "Epoch 9361/10000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 8.7844 - val_loss: 91.5443\n",
      "Epoch 9362/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 9.5199 - val_loss: 93.3854\n",
      "Epoch 9363/10000\n",
      "96/96 [==============================] - 0s 501us/step - loss: 10.7225 - val_loss: 79.9738\n",
      "Epoch 9364/10000\n",
      "96/96 [==============================] - 0s 521us/step - loss: 10.3453 - val_loss: 98.5366\n",
      "Epoch 9365/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 9.4382 - val_loss: 91.9120\n",
      "Epoch 9366/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 10.7468 - val_loss: 87.9774\n",
      "Epoch 9367/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 9.5877 - val_loss: 75.6201\n",
      "Epoch 9368/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 9.6347 - val_loss: 92.7651\n",
      "Epoch 9369/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 452us/step - loss: 10.2627 - val_loss: 89.1610\n",
      "Epoch 9370/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 11.9850 - val_loss: 96.2560\n",
      "Epoch 9371/10000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 10.3209 - val_loss: 78.9721\n",
      "Epoch 9372/10000\n",
      "96/96 [==============================] - 0s 526us/step - loss: 8.7289 - val_loss: 72.6267\n",
      "Epoch 9373/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 9.7243 - val_loss: 105.0801\n",
      "Epoch 9374/10000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 10.6491 - val_loss: 91.4909\n",
      "Epoch 9375/10000\n",
      "96/96 [==============================] - 0s 502us/step - loss: 8.8250 - val_loss: 97.0783\n",
      "Epoch 9376/10000\n",
      "96/96 [==============================] - 0s 510us/step - loss: 10.1635 - val_loss: 92.1591\n",
      "Epoch 9377/10000\n",
      "96/96 [==============================] - 0s 507us/step - loss: 10.0544 - val_loss: 99.6903\n",
      "Epoch 9378/10000\n",
      "96/96 [==============================] - 0s 607us/step - loss: 10.5104 - val_loss: 88.6089\n",
      "Epoch 9379/10000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 9.4412 - val_loss: 92.8794\n",
      "Epoch 9380/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 7.7235 - val_loss: 87.5244\n",
      "Epoch 9381/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 8.9454 - val_loss: 106.1053\n",
      "Epoch 9382/10000\n",
      "96/96 [==============================] - 0s 592us/step - loss: 13.8962 - val_loss: 95.6774\n",
      "Epoch 9383/10000\n",
      "96/96 [==============================] - 0s 516us/step - loss: 10.4410 - val_loss: 95.0011\n",
      "Epoch 9384/10000\n",
      "96/96 [==============================] - 0s 522us/step - loss: 8.8128 - val_loss: 88.9109\n",
      "Epoch 9385/10000\n",
      "96/96 [==============================] - 0s 561us/step - loss: 9.9484 - val_loss: 85.5905\n",
      "Epoch 9386/10000\n",
      "96/96 [==============================] - 0s 475us/step - loss: 10.5323 - val_loss: 87.5499\n",
      "Epoch 9387/10000\n",
      "96/96 [==============================] - 0s 643us/step - loss: 8.5400 - val_loss: 89.3859\n",
      "Epoch 9388/10000\n",
      "96/96 [==============================] - 0s 521us/step - loss: 8.3121 - val_loss: 92.3934\n",
      "Epoch 9389/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 9.6635 - val_loss: 91.8612\n",
      "Epoch 9390/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 8.8561 - val_loss: 83.0342\n",
      "Epoch 9391/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 10.2433 - val_loss: 89.5727\n",
      "Epoch 9392/10000\n",
      "96/96 [==============================] - 0s 482us/step - loss: 9.8239 - val_loss: 87.0725\n",
      "Epoch 9393/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 8.4231 - val_loss: 96.0043\n",
      "Epoch 9394/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 8.5447 - val_loss: 88.7571\n",
      "Epoch 9395/10000\n",
      "96/96 [==============================] - 0s 480us/step - loss: 9.3643 - val_loss: 83.2936\n",
      "Epoch 9396/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 9.3883 - val_loss: 79.8489\n",
      "Epoch 9397/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 7.2564 - val_loss: 102.7987\n",
      "Epoch 9398/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 8.9286 - val_loss: 91.9466\n",
      "Epoch 9399/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 8.6034 - val_loss: 103.3645\n",
      "Epoch 9400/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 7.8044 - val_loss: 83.1794\n",
      "Epoch 9401/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 9.9985 - val_loss: 85.7669\n",
      "Epoch 9402/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 6.8942 - val_loss: 92.8595\n",
      "Epoch 9403/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 8.5486 - val_loss: 89.4674\n",
      "Epoch 9404/10000\n",
      "96/96 [==============================] - 0s 361us/step - loss: 8.8530 - val_loss: 98.7815\n",
      "Epoch 9405/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 6.6983 - val_loss: 86.6960\n",
      "Epoch 9406/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 9.0022 - val_loss: 91.0012\n",
      "Epoch 9407/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 10.4344 - val_loss: 92.7530\n",
      "Epoch 9408/10000\n",
      "96/96 [==============================] - 0s 493us/step - loss: 9.7258 - val_loss: 94.2361\n",
      "Epoch 9409/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 8.7653 - val_loss: 102.0464\n",
      "Epoch 9410/10000\n",
      "96/96 [==============================] - 0s 494us/step - loss: 8.3209 - val_loss: 86.9392\n",
      "Epoch 9411/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 8.9510 - val_loss: 82.8701\n",
      "Epoch 9412/10000\n",
      "96/96 [==============================] - 0s 513us/step - loss: 9.6869 - val_loss: 89.1431\n",
      "Epoch 9413/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 8.3340 - val_loss: 109.7838\n",
      "Epoch 9414/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 10.1211 - val_loss: 91.4863\n",
      "Epoch 9415/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 8.2054 - val_loss: 99.7688\n",
      "Epoch 9416/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 7.9471 - val_loss: 101.3182\n",
      "Epoch 9417/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 8.3540 - val_loss: 92.9481\n",
      "Epoch 9418/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 7.3205 - val_loss: 86.8474\n",
      "Epoch 9419/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 7.8688 - val_loss: 84.0296\n",
      "Epoch 9420/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 6.6753 - val_loss: 102.0486\n",
      "Epoch 9421/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 7.7926 - val_loss: 98.4454\n",
      "Epoch 9422/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 7.7771 - val_loss: 99.5192\n",
      "Epoch 9423/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 7.9311 - val_loss: 111.2033\n",
      "Epoch 9424/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 8.2849 - val_loss: 98.1068\n",
      "Epoch 9425/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 8.4509 - val_loss: 90.2154\n",
      "Epoch 9426/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 8.5077 - val_loss: 83.2992\n",
      "Epoch 9427/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 7.2548 - val_loss: 87.5074\n",
      "Epoch 9428/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 7.2433 - val_loss: 103.5921\n",
      "Epoch 9429/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 7.4187 - val_loss: 85.8843\n",
      "Epoch 9430/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 9.3892 - val_loss: 102.8399\n",
      "Epoch 9431/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 8.2338 - val_loss: 93.9477\n",
      "Epoch 9432/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 9.7421 - val_loss: 94.3446\n",
      "Epoch 9433/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 8.5922 - val_loss: 103.1633\n",
      "Epoch 9434/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 7.3052 - val_loss: 92.6757\n",
      "Epoch 9435/10000\n",
      "96/96 [==============================] - 0s 497us/step - loss: 7.5127 - val_loss: 98.0505\n",
      "Epoch 9436/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 7.5649 - val_loss: 90.8999\n",
      "Epoch 9437/10000\n",
      "96/96 [==============================] - 0s 568us/step - loss: 11.6746 - val_loss: 90.3566\n",
      "Epoch 9438/10000\n",
      "96/96 [==============================] - 0s 506us/step - loss: 7.2467 - val_loss: 92.0201\n",
      "Epoch 9439/10000\n",
      "96/96 [==============================] - 0s 487us/step - loss: 8.9755 - val_loss: 102.2838\n",
      "Epoch 9440/10000\n",
      "96/96 [==============================] - 0s 478us/step - loss: 6.1929 - val_loss: 79.4658\n",
      "Epoch 9441/10000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 7.5919 - val_loss: 93.9970\n",
      "Epoch 9442/10000\n",
      "96/96 [==============================] - 0s 526us/step - loss: 7.1991 - val_loss: 91.5336\n",
      "Epoch 9443/10000\n",
      "96/96 [==============================] - 0s 495us/step - loss: 7.7164 - val_loss: 92.9983\n",
      "Epoch 9444/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 10.5252 - val_loss: 93.8728\n",
      "Epoch 9445/10000\n",
      "96/96 [==============================] - 0s 485us/step - loss: 8.7277 - val_loss: 95.2270\n",
      "Epoch 9446/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 10.2396 - val_loss: 76.7320\n",
      "Epoch 9447/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 10.0494 - val_loss: 91.6939\n",
      "Epoch 9448/10000\n",
      "96/96 [==============================] - 0s 566us/step - loss: 11.0866 - val_loss: 113.9929\n",
      "Epoch 9449/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 10.3609 - val_loss: 108.9673\n",
      "Epoch 9450/10000\n",
      "96/96 [==============================] - 0s 487us/step - loss: 7.5489 - val_loss: 98.3121\n",
      "Epoch 9451/10000\n",
      "96/96 [==============================] - 0s 584us/step - loss: 11.4075 - val_loss: 88.7432\n",
      "Epoch 9452/10000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 11.0424 - val_loss: 108.2565\n",
      "Epoch 9453/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 6.4618 - val_loss: 98.2513\n",
      "Epoch 9454/10000\n",
      "96/96 [==============================] - 0s 524us/step - loss: 9.2982 - val_loss: 92.8875\n",
      "Epoch 9455/10000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 9.7101 - val_loss: 102.9577\n",
      "Epoch 9456/10000\n",
      "96/96 [==============================] - 0s 488us/step - loss: 21.7483 - val_loss: 103.4009\n",
      "Epoch 9457/10000\n",
      "96/96 [==============================] - 0s 569us/step - loss: 14.5178 - val_loss: 93.2120\n",
      "Epoch 9458/10000\n",
      "96/96 [==============================] - 0s 505us/step - loss: 15.3008 - val_loss: 84.5349\n",
      "Epoch 9459/10000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 18.5827 - val_loss: 68.0131\n",
      "Epoch 9460/10000\n",
      "96/96 [==============================] - 0s 523us/step - loss: 16.4557 - val_loss: 79.7895\n",
      "Epoch 9461/10000\n",
      "96/96 [==============================] - 0s 590us/step - loss: 15.9312 - val_loss: 80.7433\n",
      "Epoch 9462/10000\n",
      "96/96 [==============================] - 0s 521us/step - loss: 19.1804 - val_loss: 80.0887\n",
      "Epoch 9463/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 19.5874 - val_loss: 86.3829\n",
      "Epoch 9464/10000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 11.7602 - val_loss: 82.9131\n",
      "Epoch 9465/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 12.1166 - val_loss: 85.7066\n",
      "Epoch 9466/10000\n",
      "96/96 [==============================] - 0s 514us/step - loss: 12.3538 - val_loss: 72.5122\n",
      "Epoch 9467/10000\n",
      "96/96 [==============================] - 0s 516us/step - loss: 11.2664 - val_loss: 80.8091\n",
      "Epoch 9468/10000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 10.1559 - val_loss: 94.5321\n",
      "Epoch 9469/10000\n",
      "96/96 [==============================] - 0s 506us/step - loss: 10.6073 - val_loss: 88.9392\n",
      "Epoch 9470/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 10.6451 - val_loss: 91.9389\n",
      "Epoch 9471/10000\n",
      "96/96 [==============================] - 0s 495us/step - loss: 9.6081 - val_loss: 92.2605\n",
      "Epoch 9472/10000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 7.8769 - val_loss: 96.6578\n",
      "Epoch 9473/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 10.2381 - val_loss: 94.8689\n",
      "Epoch 9474/10000\n",
      "96/96 [==============================] - 0s 492us/step - loss: 9.0146 - val_loss: 102.3780\n",
      "Epoch 9475/10000\n",
      "96/96 [==============================] - 0s 501us/step - loss: 8.7289 - val_loss: 100.5390\n",
      "Epoch 9476/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 11.0862 - val_loss: 91.2766\n",
      "Epoch 9477/10000\n",
      "96/96 [==============================] - 0s 496us/step - loss: 10.8777 - val_loss: 94.8160\n",
      "Epoch 9478/10000\n",
      "96/96 [==============================] - 0s 496us/step - loss: 10.7213 - val_loss: 80.0377\n",
      "Epoch 9479/10000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 11.2846 - val_loss: 79.6079\n",
      "Epoch 9480/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 9.0268 - val_loss: 81.9416\n",
      "Epoch 9481/10000\n",
      "96/96 [==============================] - 0s 657us/step - loss: 9.4004 - val_loss: 86.0538\n",
      "Epoch 9482/10000\n",
      "96/96 [==============================] - 0s 601us/step - loss: 7.5703 - val_loss: 88.6681\n",
      "Epoch 9483/10000\n",
      "96/96 [==============================] - 0s 475us/step - loss: 8.7848 - val_loss: 90.5665\n",
      "Epoch 9484/10000\n",
      "96/96 [==============================] - 0s 505us/step - loss: 7.7623 - val_loss: 87.2271\n",
      "Epoch 9485/10000\n",
      "96/96 [==============================] - 0s 493us/step - loss: 8.3121 - val_loss: 93.9136\n",
      "Epoch 9486/10000\n",
      "96/96 [==============================] - 0s 517us/step - loss: 7.8857 - val_loss: 92.0181\n",
      "Epoch 9487/10000\n",
      "96/96 [==============================] - 0s 513us/step - loss: 8.5996 - val_loss: 105.5335\n",
      "Epoch 9488/10000\n",
      "96/96 [==============================] - 0s 520us/step - loss: 9.2910 - val_loss: 101.8229\n",
      "Epoch 9489/10000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 8.9275 - val_loss: 93.2245\n",
      "Epoch 9490/10000\n",
      "96/96 [==============================] - 0s 513us/step - loss: 8.5926 - val_loss: 94.4634\n",
      "Epoch 9491/10000\n",
      "96/96 [==============================] - 0s 560us/step - loss: 7.2080 - val_loss: 101.4701\n",
      "Epoch 9492/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 8.1407 - val_loss: 99.0308\n",
      "Epoch 9493/10000\n",
      "96/96 [==============================] - 0s 487us/step - loss: 8.7251 - val_loss: 110.8719\n",
      "Epoch 9494/10000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 7.2039 - val_loss: 103.3272\n",
      "Epoch 9495/10000\n",
      "96/96 [==============================] - 0s 388us/step - loss: 8.0990 - val_loss: 95.9632\n",
      "Epoch 9496/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 8.5491 - val_loss: 97.3095\n",
      "Epoch 9497/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 8.2870 - val_loss: 99.5629\n",
      "Epoch 9498/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 7.3399 - val_loss: 103.3627\n",
      "Epoch 9499/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 8.5783 - val_loss: 101.5599\n",
      "Epoch 9500/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 8.3267 - val_loss: 113.6724\n",
      "Epoch 9501/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 7.1731 - val_loss: 93.5550\n",
      "Epoch 9502/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 8.3164 - val_loss: 100.3964\n",
      "Epoch 9503/10000\n",
      "96/96 [==============================] - 0s 512us/step - loss: 7.4985 - val_loss: 114.4153\n",
      "Epoch 9504/10000\n",
      "96/96 [==============================] - 0s 488us/step - loss: 7.3359 - val_loss: 110.4291\n",
      "Epoch 9505/10000\n",
      "96/96 [==============================] - 0s 554us/step - loss: 9.6677 - val_loss: 96.1180\n",
      "Epoch 9506/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 5.9783 - val_loss: 108.5200\n",
      "Epoch 9507/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 6.1735 - val_loss: 83.5969\n",
      "Epoch 9508/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 8.0976 - val_loss: 108.8843\n",
      "Epoch 9509/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 6.9237 - val_loss: 101.6430\n",
      "Epoch 9510/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 5.6156 - val_loss: 102.8078\n",
      "Epoch 9511/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 7.0538 - val_loss: 98.5027\n",
      "Epoch 9512/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 7.9074 - val_loss: 107.3872\n",
      "Epoch 9513/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 7.5268 - val_loss: 95.3652\n",
      "Epoch 9514/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 14.1494 - val_loss: 87.1755\n",
      "Epoch 9515/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 6.6213 - val_loss: 98.4644\n",
      "Epoch 9516/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 12.3634 - val_loss: 97.4119\n",
      "Epoch 9517/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 9.2607 - val_loss: 83.6706\n",
      "Epoch 9518/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 8.7992 - val_loss: 82.7582\n",
      "Epoch 9519/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 8.5122 - val_loss: 93.7294\n",
      "Epoch 9520/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 9.7402 - val_loss: 103.3498\n",
      "Epoch 9521/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 420us/step - loss: 8.0026 - val_loss: 104.5451\n",
      "Epoch 9522/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 8.1961 - val_loss: 105.9716\n",
      "Epoch 9523/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 7.0891 - val_loss: 110.1330\n",
      "Epoch 9524/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 8.5170 - val_loss: 92.1620\n",
      "Epoch 9525/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 9.2663 - val_loss: 96.2996\n",
      "Epoch 9526/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 10.1810 - val_loss: 101.1348\n",
      "Epoch 9527/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 7.6346 - val_loss: 83.9640\n",
      "Epoch 9528/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 9.0956 - val_loss: 90.4103\n",
      "Epoch 9529/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 8.6463 - val_loss: 94.9217\n",
      "Epoch 9530/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 10.9050 - val_loss: 102.4778\n",
      "Epoch 9531/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 8.7671 - val_loss: 101.7495\n",
      "Epoch 9532/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 7.6976 - val_loss: 99.9307\n",
      "Epoch 9533/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 7.2417 - val_loss: 101.2545\n",
      "Epoch 9534/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 8.4011 - val_loss: 94.4875\n",
      "Epoch 9535/10000\n",
      "96/96 [==============================] - 0s 383us/step - loss: 8.6743 - val_loss: 99.9930\n",
      "Epoch 9536/10000\n",
      "96/96 [==============================] - 0s 406us/step - loss: 10.6878 - val_loss: 99.9967\n",
      "Epoch 9537/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 9.3921 - val_loss: 95.6380\n",
      "Epoch 9538/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 7.8462 - val_loss: 111.8347\n",
      "Epoch 9539/10000\n",
      "96/96 [==============================] - 0s 365us/step - loss: 8.7809 - val_loss: 97.6698\n",
      "Epoch 9540/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 9.0781 - val_loss: 99.4215\n",
      "Epoch 9541/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 6.6709 - val_loss: 113.1349\n",
      "Epoch 9542/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 7.6718 - val_loss: 98.4066\n",
      "Epoch 9543/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 7.9921 - val_loss: 101.7681\n",
      "Epoch 9544/10000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 15.2650 - val_loss: 95.0741\n",
      "Epoch 9545/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 8.8561 - val_loss: 90.5020\n",
      "Epoch 9546/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 12.7916 - val_loss: 93.3843\n",
      "Epoch 9547/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 10.5047 - val_loss: 98.1641\n",
      "Epoch 9548/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 12.6361 - val_loss: 90.5910\n",
      "Epoch 9549/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 8.3109 - val_loss: 109.4102\n",
      "Epoch 9550/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 7.8188 - val_loss: 99.7315\n",
      "Epoch 9551/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 6.7964 - val_loss: 82.4156\n",
      "Epoch 9552/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 9.0036 - val_loss: 109.2742\n",
      "Epoch 9553/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 6.9438 - val_loss: 93.5357\n",
      "Epoch 9554/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 6.2047 - val_loss: 94.4225\n",
      "Epoch 9555/10000\n",
      "96/96 [==============================] - 0s 506us/step - loss: 7.5583 - val_loss: 91.5378\n",
      "Epoch 9556/10000\n",
      "96/96 [==============================] - 0s 518us/step - loss: 7.2794 - val_loss: 98.3442\n",
      "Epoch 9557/10000\n",
      "96/96 [==============================] - 0s 446us/step - loss: 7.9932 - val_loss: 90.7209\n",
      "Epoch 9558/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 8.2738 - val_loss: 92.7197\n",
      "Epoch 9559/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 7.0519 - val_loss: 89.5429\n",
      "Epoch 9560/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 7.2384 - val_loss: 106.4221\n",
      "Epoch 9561/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 6.9943 - val_loss: 91.6522\n",
      "Epoch 9562/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 9.0657 - val_loss: 94.6655\n",
      "Epoch 9563/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 6.7295 - val_loss: 89.5931\n",
      "Epoch 9564/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 7.4303 - val_loss: 94.7548\n",
      "Epoch 9565/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 6.8972 - val_loss: 96.4370\n",
      "Epoch 9566/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 8.3268 - val_loss: 89.9988\n",
      "Epoch 9567/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 6.6648 - val_loss: 103.2024\n",
      "Epoch 9568/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 6.7858 - val_loss: 108.6528\n",
      "Epoch 9569/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 5.8447 - val_loss: 96.7082\n",
      "Epoch 9570/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 5.7692 - val_loss: 91.4782\n",
      "Epoch 9571/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 8.1287 - val_loss: 99.3301\n",
      "Epoch 9572/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 6.0526 - val_loss: 91.6441\n",
      "Epoch 9573/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 6.1449 - val_loss: 108.0788\n",
      "Epoch 9574/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 8.7807 - val_loss: 86.9021\n",
      "Epoch 9575/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 9.5021 - val_loss: 85.5439\n",
      "Epoch 9576/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 8.0643 - val_loss: 76.6069\n",
      "Epoch 9577/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 8.1187 - val_loss: 100.9159\n",
      "Epoch 9578/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 8.6380 - val_loss: 91.2535\n",
      "Epoch 9579/10000\n",
      "96/96 [==============================] - 0s 501us/step - loss: 7.8519 - val_loss: 89.5585\n",
      "Epoch 9580/10000\n",
      "96/96 [==============================] - 0s 540us/step - loss: 9.7216 - val_loss: 83.2346\n",
      "Epoch 9581/10000\n",
      "96/96 [==============================] - 0s 543us/step - loss: 6.5930 - val_loss: 82.9842\n",
      "Epoch 9582/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 7.1004 - val_loss: 86.6132\n",
      "Epoch 9583/10000\n",
      "96/96 [==============================] - 0s 511us/step - loss: 7.0505 - val_loss: 85.6528\n",
      "Epoch 9584/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 8.1303 - val_loss: 83.4621\n",
      "Epoch 9585/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 7.9909 - val_loss: 93.9335\n",
      "Epoch 9586/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 6.0975 - val_loss: 86.2241\n",
      "Epoch 9587/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 9.1975 - val_loss: 87.7132\n",
      "Epoch 9588/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 7.1522 - val_loss: 89.0514\n",
      "Epoch 9589/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 7.7034 - val_loss: 87.4795\n",
      "Epoch 9590/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 6.5901 - val_loss: 85.3440\n",
      "Epoch 9591/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 6.6119 - val_loss: 108.3923\n",
      "Epoch 9592/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 8.1609 - val_loss: 84.2704\n",
      "Epoch 9593/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 6.7739 - val_loss: 98.1876\n",
      "Epoch 9594/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 6.9663 - val_loss: 97.7154\n",
      "Epoch 9595/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 8.9324 - val_loss: 88.0653\n",
      "Epoch 9596/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 7.0485 - val_loss: 98.8166\n",
      "Epoch 9597/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 6.6917 - val_loss: 98.5478\n",
      "Epoch 9598/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 6.3271 - val_loss: 92.2588\n",
      "Epoch 9599/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 4.5746 - val_loss: 105.5579\n",
      "Epoch 9600/10000\n",
      "96/96 [==============================] - 0s 490us/step - loss: 9.3539 - val_loss: 92.0360\n",
      "Epoch 9601/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 4.8020 - val_loss: 88.9716\n",
      "Epoch 9602/10000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 6.5843 - val_loss: 108.6189\n",
      "Epoch 9603/10000\n",
      "96/96 [==============================] - 0s 422us/step - loss: 7.7172 - val_loss: 87.1676\n",
      "Epoch 9604/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 14.9130 - val_loss: 83.5144\n",
      "Epoch 9605/10000\n",
      "96/96 [==============================] - 0s 564us/step - loss: 19.1119 - val_loss: 72.8069\n",
      "Epoch 9606/10000\n",
      "96/96 [==============================] - 0s 555us/step - loss: 14.0859 - val_loss: 95.0072\n",
      "Epoch 9607/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 13.3151 - val_loss: 87.3465\n",
      "Epoch 9608/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 12.7650 - val_loss: 103.6140\n",
      "Epoch 9609/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 14.1453 - val_loss: 91.0763\n",
      "Epoch 9610/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 12.7136 - val_loss: 99.9189\n",
      "Epoch 9611/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 10.2273 - val_loss: 85.8506\n",
      "Epoch 9612/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 8.8538 - val_loss: 95.7750\n",
      "Epoch 9613/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 10.9579 - val_loss: 97.7474\n",
      "Epoch 9614/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 13.1492 - val_loss: 94.0577\n",
      "Epoch 9615/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 8.6572 - val_loss: 95.0137\n",
      "Epoch 9616/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 9.9106 - val_loss: 101.1680\n",
      "Epoch 9617/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 8.0407 - val_loss: 101.0368\n",
      "Epoch 9618/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 9.4177 - val_loss: 106.9605\n",
      "Epoch 9619/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 9.9337 - val_loss: 85.1365\n",
      "Epoch 9620/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 10.7713 - val_loss: 86.4120\n",
      "Epoch 9621/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 8.0097 - val_loss: 88.2348\n",
      "Epoch 9622/10000\n",
      "96/96 [==============================] - 0s 392us/step - loss: 8.6999 - val_loss: 91.0052\n",
      "Epoch 9623/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 7.1395 - val_loss: 85.2205\n",
      "Epoch 9624/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 6.5865 - val_loss: 96.9127\n",
      "Epoch 9625/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 7.5522 - val_loss: 94.0111\n",
      "Epoch 9626/10000\n",
      "96/96 [==============================] - 0s 401us/step - loss: 7.1749 - val_loss: 98.3161\n",
      "Epoch 9627/10000\n",
      "96/96 [==============================] - 0s 371us/step - loss: 6.3605 - val_loss: 94.8917\n",
      "Epoch 9628/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 8.7881 - val_loss: 88.9609\n",
      "Epoch 9629/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 6.7752 - val_loss: 101.6855\n",
      "Epoch 9630/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 6.3661 - val_loss: 101.1812\n",
      "Epoch 9631/10000\n",
      "96/96 [==============================] - 0s 456us/step - loss: 9.6808 - val_loss: 109.2216\n",
      "Epoch 9632/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 9.1054 - val_loss: 103.5167\n",
      "Epoch 9633/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 6.1746 - val_loss: 107.0556\n",
      "Epoch 9634/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 7.1343 - val_loss: 108.1525\n",
      "Epoch 9635/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 6.6400 - val_loss: 80.3695\n",
      "Epoch 9636/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 6.7585 - val_loss: 113.1150\n",
      "Epoch 9637/10000\n",
      "96/96 [==============================] - 0s 398us/step - loss: 7.2155 - val_loss: 109.9794\n",
      "Epoch 9638/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 6.8492 - val_loss: 96.5953\n",
      "Epoch 9639/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 8.6859 - val_loss: 92.6556\n",
      "Epoch 9640/10000\n",
      "96/96 [==============================] - 0s 358us/step - loss: 8.4358 - val_loss: 82.5637\n",
      "Epoch 9641/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 7.1386 - val_loss: 82.7037\n",
      "Epoch 9642/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 5.7477 - val_loss: 100.9311\n",
      "Epoch 9643/10000\n",
      "96/96 [==============================] - 0s 375us/step - loss: 6.1778 - val_loss: 96.5523\n",
      "Epoch 9644/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 5.4991 - val_loss: 86.1444\n",
      "Epoch 9645/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 5.1759 - val_loss: 83.8051\n",
      "Epoch 9646/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 9.0402 - val_loss: 84.2046\n",
      "Epoch 9647/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 9.2476 - val_loss: 79.1619\n",
      "Epoch 9648/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 7.3680 - val_loss: 88.9930\n",
      "Epoch 9649/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 5.9929 - val_loss: 90.2848\n",
      "Epoch 9650/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 6.7563 - val_loss: 88.6847\n",
      "Epoch 9651/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 6.0509 - val_loss: 94.6033\n",
      "Epoch 9652/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 7.5228 - val_loss: 90.2651\n",
      "Epoch 9653/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 7.8844 - val_loss: 91.0310\n",
      "Epoch 9654/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 5.6486 - val_loss: 93.6333\n",
      "Epoch 9655/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 6.5549 - val_loss: 103.6844\n",
      "Epoch 9656/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 6.3973 - val_loss: 85.8886\n",
      "Epoch 9657/10000\n",
      "96/96 [==============================] - 0s 394us/step - loss: 5.8312 - val_loss: 96.7111\n",
      "Epoch 9658/10000\n",
      "96/96 [==============================] - 0s 510us/step - loss: 6.9047 - val_loss: 83.1052\n",
      "Epoch 9659/10000\n",
      "96/96 [==============================] - 0s 521us/step - loss: 6.8152 - val_loss: 87.3632\n",
      "Epoch 9660/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 4.4608 - val_loss: 86.9511\n",
      "Epoch 9661/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 6.3604 - val_loss: 90.3815\n",
      "Epoch 9662/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 6.5304 - val_loss: 95.2018\n",
      "Epoch 9663/10000\n",
      "96/96 [==============================] - 0s 448us/step - loss: 5.9773 - val_loss: 95.4147\n",
      "Epoch 9664/10000\n",
      "96/96 [==============================] - 0s 523us/step - loss: 5.2830 - val_loss: 87.4953\n",
      "Epoch 9665/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 7.3680 - val_loss: 94.7829\n",
      "Epoch 9666/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 7.6312 - val_loss: 97.8934\n",
      "Epoch 9667/10000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 7.7256 - val_loss: 91.1944\n",
      "Epoch 9668/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 7.3268 - val_loss: 100.0486\n",
      "Epoch 9669/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 5.9027 - val_loss: 74.7919\n",
      "Epoch 9670/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 6.7522 - val_loss: 77.4817\n",
      "Epoch 9671/10000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 7.4186 - val_loss: 94.5487\n",
      "Epoch 9672/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 5.7302 - val_loss: 91.2953\n",
      "Epoch 9673/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 452us/step - loss: 9.0602 - val_loss: 93.1820\n",
      "Epoch 9674/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 6.0054 - val_loss: 76.7758\n",
      "Epoch 9675/10000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 4.6719 - val_loss: 84.2780\n",
      "Epoch 9676/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 6.8651 - val_loss: 88.8805\n",
      "Epoch 9677/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 5.7439 - val_loss: 103.4381\n",
      "Epoch 9678/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 7.1789 - val_loss: 84.5413\n",
      "Epoch 9679/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 6.9956 - val_loss: 95.7691\n",
      "Epoch 9680/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 7.4819 - val_loss: 87.5372\n",
      "Epoch 9681/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 9.6831 - val_loss: 87.7141\n",
      "Epoch 9682/10000\n",
      "96/96 [==============================] - 0s 512us/step - loss: 8.4077 - val_loss: 85.1097\n",
      "Epoch 9683/10000\n",
      "96/96 [==============================] - 0s 523us/step - loss: 7.5973 - val_loss: 95.8455\n",
      "Epoch 9684/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 6.6146 - val_loss: 81.7996\n",
      "Epoch 9685/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 8.8692 - val_loss: 86.6841\n",
      "Epoch 9686/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 8.0498 - val_loss: 87.6256\n",
      "Epoch 9687/10000\n",
      "96/96 [==============================] - 0s 497us/step - loss: 7.3025 - val_loss: 87.8513\n",
      "Epoch 9688/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 8.2501 - val_loss: 86.7621\n",
      "Epoch 9689/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 6.9966 - val_loss: 86.6567\n",
      "Epoch 9690/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 6.9451 - val_loss: 92.0195\n",
      "Epoch 9691/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 7.3191 - val_loss: 90.4550\n",
      "Epoch 9692/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 5.6768 - val_loss: 84.7835\n",
      "Epoch 9693/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 7.1946 - val_loss: 77.3617\n",
      "Epoch 9694/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 5.6099 - val_loss: 85.8338\n",
      "Epoch 9695/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 8.2532 - val_loss: 83.1110\n",
      "Epoch 9696/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 5.6594 - val_loss: 92.5242\n",
      "Epoch 9697/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 6.3659 - val_loss: 89.6688\n",
      "Epoch 9698/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 5.8063 - val_loss: 101.2707\n",
      "Epoch 9699/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 5.2488 - val_loss: 89.8125\n",
      "Epoch 9700/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 7.3634 - val_loss: 79.1184\n",
      "Epoch 9701/10000\n",
      "96/96 [==============================] - 0s 506us/step - loss: 5.1689 - val_loss: 89.8972\n",
      "Epoch 9702/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 6.0879 - val_loss: 98.8159\n",
      "Epoch 9703/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 5.8320 - val_loss: 85.9518\n",
      "Epoch 9704/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 6.0107 - val_loss: 87.3393\n",
      "Epoch 9705/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 5.1535 - val_loss: 76.6531\n",
      "Epoch 9706/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 60.7563 - val_loss: 90.8459\n",
      "Epoch 9707/10000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 18.5138 - val_loss: 85.4983\n",
      "Epoch 9708/10000\n",
      "96/96 [==============================] - 0s 551us/step - loss: 110.0815 - val_loss: 71.6453\n",
      "Epoch 9709/10000\n",
      "96/96 [==============================] - 0s 548us/step - loss: 21.4420 - val_loss: 76.4886\n",
      "Epoch 9710/10000\n",
      "96/96 [==============================] - 0s 487us/step - loss: 21.6764 - val_loss: 84.6377\n",
      "Epoch 9711/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 18.7324 - val_loss: 90.7328\n",
      "Epoch 9712/10000\n",
      "96/96 [==============================] - 0s 490us/step - loss: 16.2934 - val_loss: 104.4798\n",
      "Epoch 9713/10000\n",
      "96/96 [==============================] - 0s 536us/step - loss: 21.2257 - val_loss: 103.4060\n",
      "Epoch 9714/10000\n",
      "96/96 [==============================] - 0s 454us/step - loss: 19.0590 - val_loss: 89.9980\n",
      "Epoch 9715/10000\n",
      "96/96 [==============================] - 0s 496us/step - loss: 92.0543 - val_loss: 95.1797\n",
      "Epoch 9716/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 21.2914 - val_loss: 81.5479\n",
      "Epoch 9717/10000\n",
      "96/96 [==============================] - 0s 475us/step - loss: 17.6979 - val_loss: 82.1515\n",
      "Epoch 9718/10000\n",
      "96/96 [==============================] - 0s 511us/step - loss: 19.0354 - val_loss: 97.1867\n",
      "Epoch 9719/10000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 16.7862 - val_loss: 88.0748\n",
      "Epoch 9720/10000\n",
      "96/96 [==============================] - 0s 447us/step - loss: 19.7393 - val_loss: 89.9356\n",
      "Epoch 9721/10000\n",
      "96/96 [==============================] - 0s 524us/step - loss: 15.1296 - val_loss: 86.9639\n",
      "Epoch 9722/10000\n",
      "96/96 [==============================] - 0s 496us/step - loss: 16.5485 - val_loss: 86.5135\n",
      "Epoch 9723/10000\n",
      "96/96 [==============================] - 0s 508us/step - loss: 14.7511 - val_loss: 79.1499\n",
      "Epoch 9724/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 13.0964 - val_loss: 84.3169\n",
      "Epoch 9725/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 14.6990 - val_loss: 86.8141\n",
      "Epoch 9726/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 15.1666 - val_loss: 79.1579\n",
      "Epoch 9727/10000\n",
      "96/96 [==============================] - 0s 395us/step - loss: 14.1333 - val_loss: 81.9292\n",
      "Epoch 9728/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 12.2506 - val_loss: 80.6680\n",
      "Epoch 9729/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 13.9895 - val_loss: 78.4425\n",
      "Epoch 9730/10000\n",
      "96/96 [==============================] - 0s 574us/step - loss: 11.6660 - val_loss: 84.3527\n",
      "Epoch 9731/10000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 13.7570 - val_loss: 83.6863\n",
      "Epoch 9732/10000\n",
      "96/96 [==============================] - 0s 425us/step - loss: 12.4465 - val_loss: 75.0893\n",
      "Epoch 9733/10000\n",
      "96/96 [==============================] - 0s 431us/step - loss: 10.2025 - val_loss: 88.8399\n",
      "Epoch 9734/10000\n",
      "96/96 [==============================] - 0s 386us/step - loss: 11.3312 - val_loss: 93.5014\n",
      "Epoch 9735/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 8.3811 - val_loss: 87.3887\n",
      "Epoch 9736/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 12.3855 - val_loss: 95.0451\n",
      "Epoch 9737/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 9.8429 - val_loss: 87.1688\n",
      "Epoch 9738/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 9.8501 - val_loss: 71.7498\n",
      "Epoch 9739/10000\n",
      "96/96 [==============================] - 0s 443us/step - loss: 10.3810 - val_loss: 90.2444\n",
      "Epoch 9740/10000\n",
      "96/96 [==============================] - 0s 380us/step - loss: 10.0267 - val_loss: 79.8161\n",
      "Epoch 9741/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 9.6816 - val_loss: 95.3177\n",
      "Epoch 9742/10000\n",
      "96/96 [==============================] - 0s 419us/step - loss: 8.9269 - val_loss: 81.0070\n",
      "Epoch 9743/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 9.7042 - val_loss: 89.6107\n",
      "Epoch 9744/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 9.0535 - val_loss: 88.0289\n",
      "Epoch 9745/10000\n",
      "96/96 [==============================] - 0s 399us/step - loss: 10.3116 - val_loss: 90.8223\n",
      "Epoch 9746/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 7.5700 - val_loss: 74.0019\n",
      "Epoch 9747/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 11.8742 - val_loss: 79.1944\n",
      "Epoch 9748/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 7.9660 - val_loss: 82.2674\n",
      "Epoch 9749/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 8.4828 - val_loss: 89.4215\n",
      "Epoch 9750/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 9.1391 - val_loss: 100.0249\n",
      "Epoch 9751/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 9.2010 - val_loss: 70.2544\n",
      "Epoch 9752/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 8.0997 - val_loss: 76.7168\n",
      "Epoch 9753/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 8.3398 - val_loss: 81.4875\n",
      "Epoch 9754/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 7.3924 - val_loss: 92.5851\n",
      "Epoch 9755/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 7.6001 - val_loss: 81.8782\n",
      "Epoch 9756/10000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 8.7724 - val_loss: 90.2671\n",
      "Epoch 9757/10000\n",
      "96/96 [==============================] - 0s 537us/step - loss: 9.2382 - val_loss: 86.3435\n",
      "Epoch 9758/10000\n",
      "96/96 [==============================] - 0s 390us/step - loss: 9.2255 - val_loss: 93.6209\n",
      "Epoch 9759/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 7.7194 - val_loss: 86.2433\n",
      "Epoch 9760/10000\n",
      "96/96 [==============================] - 0s 426us/step - loss: 11.0061 - val_loss: 97.2307\n",
      "Epoch 9761/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 10.5278 - val_loss: 80.6117\n",
      "Epoch 9762/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 9.3241 - val_loss: 75.9025\n",
      "Epoch 9763/10000\n",
      "96/96 [==============================] - 0s 400us/step - loss: 10.6346 - val_loss: 90.0151\n",
      "Epoch 9764/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 7.6622 - val_loss: 74.0715\n",
      "Epoch 9765/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 6.6361 - val_loss: 89.5213\n",
      "Epoch 9766/10000\n",
      "96/96 [==============================] - 0s 417us/step - loss: 7.3350 - val_loss: 76.5805\n",
      "Epoch 9767/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 9.4258 - val_loss: 69.8736\n",
      "Epoch 9768/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 8.7602 - val_loss: 85.0882\n",
      "Epoch 9769/10000\n",
      "96/96 [==============================] - 0s 402us/step - loss: 8.0581 - val_loss: 93.2879\n",
      "Epoch 9770/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 8.9919 - val_loss: 91.1677\n",
      "Epoch 9771/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 10.2117 - val_loss: 86.0189\n",
      "Epoch 9772/10000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 9.1036 - val_loss: 99.5835\n",
      "Epoch 9773/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 6.5069 - val_loss: 71.9568\n",
      "Epoch 9774/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 7.6231 - val_loss: 85.8679\n",
      "Epoch 9775/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 7.4260 - val_loss: 107.3589\n",
      "Epoch 9776/10000\n",
      "96/96 [==============================] - 0s 518us/step - loss: 9.8885 - val_loss: 77.6618\n",
      "Epoch 9777/10000\n",
      "96/96 [==============================] - 0s 504us/step - loss: 7.7149 - val_loss: 74.7928\n",
      "Epoch 9778/10000\n",
      "96/96 [==============================] - 0s 526us/step - loss: 7.2579 - val_loss: 83.7952\n",
      "Epoch 9779/10000\n",
      "96/96 [==============================] - 0s 487us/step - loss: 7.9872 - val_loss: 66.7453\n",
      "Epoch 9780/10000\n",
      "96/96 [==============================] - 0s 475us/step - loss: 8.8931 - val_loss: 82.0987\n",
      "Epoch 9781/10000\n",
      "96/96 [==============================] - 0s 524us/step - loss: 8.7762 - val_loss: 83.4523\n",
      "Epoch 9782/10000\n",
      "96/96 [==============================] - 0s 539us/step - loss: 8.4431 - val_loss: 88.7442\n",
      "Epoch 9783/10000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 10.3054 - val_loss: 81.2469\n",
      "Epoch 9784/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 8.9372 - val_loss: 89.6040\n",
      "Epoch 9785/10000\n",
      "96/96 [==============================] - 0s 492us/step - loss: 7.7234 - val_loss: 76.6771\n",
      "Epoch 9786/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 7.4651 - val_loss: 75.5445\n",
      "Epoch 9787/10000\n",
      "96/96 [==============================] - 0s 545us/step - loss: 7.5258 - val_loss: 79.4549\n",
      "Epoch 9788/10000\n",
      "96/96 [==============================] - 0s 492us/step - loss: 8.9392 - val_loss: 98.8688\n",
      "Epoch 9789/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 7.4984 - val_loss: 91.6286\n",
      "Epoch 9790/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 8.9558 - val_loss: 94.4507\n",
      "Epoch 9791/10000\n",
      "96/96 [==============================] - 0s 469us/step - loss: 9.0635 - val_loss: 76.9306\n",
      "Epoch 9792/10000\n",
      "96/96 [==============================] - 0s 532us/step - loss: 7.6435 - val_loss: 82.4412\n",
      "Epoch 9793/10000\n",
      "96/96 [==============================] - 0s 465us/step - loss: 7.3087 - val_loss: 92.5403\n",
      "Epoch 9794/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 7.9123 - val_loss: 75.9266\n",
      "Epoch 9795/10000\n",
      "96/96 [==============================] - 0s 522us/step - loss: 6.9812 - val_loss: 87.4895\n",
      "Epoch 9796/10000\n",
      "96/96 [==============================] - 0s 471us/step - loss: 6.4876 - val_loss: 85.9211\n",
      "Epoch 9797/10000\n",
      "96/96 [==============================] - 0s 499us/step - loss: 7.2960 - val_loss: 109.6922\n",
      "Epoch 9798/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 5.7917 - val_loss: 95.1769\n",
      "Epoch 9799/10000\n",
      "96/96 [==============================] - 0s 441us/step - loss: 8.6286 - val_loss: 93.7879\n",
      "Epoch 9800/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 9.3018 - val_loss: 84.3503\n",
      "Epoch 9801/10000\n",
      "96/96 [==============================] - 0s 493us/step - loss: 5.5117 - val_loss: 84.6956\n",
      "Epoch 9802/10000\n",
      "96/96 [==============================] - 0s 496us/step - loss: 7.8568 - val_loss: 72.3587\n",
      "Epoch 9803/10000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 7.0472 - val_loss: 72.7225\n",
      "Epoch 9804/10000\n",
      "96/96 [==============================] - 0s 524us/step - loss: 6.4396 - val_loss: 82.5216\n",
      "Epoch 9805/10000\n",
      "96/96 [==============================] - 0s 592us/step - loss: 6.4247 - val_loss: 85.3316\n",
      "Epoch 9806/10000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 9.8132 - val_loss: 82.6003\n",
      "Epoch 9807/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 6.1682 - val_loss: 87.2349\n",
      "Epoch 9808/10000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 4.2094 - val_loss: 101.3074\n",
      "Epoch 9809/10000\n",
      "96/96 [==============================] - 0s 515us/step - loss: 6.2907 - val_loss: 91.1733\n",
      "Epoch 9810/10000\n",
      "96/96 [==============================] - 0s 472us/step - loss: 5.8347 - val_loss: 81.5121\n",
      "Epoch 9811/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 6.0060 - val_loss: 83.3364\n",
      "Epoch 9812/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 8.5160 - val_loss: 88.2635\n",
      "Epoch 9813/10000\n",
      "96/96 [==============================] - 0s 460us/step - loss: 9.3802 - val_loss: 86.2872\n",
      "Epoch 9814/10000\n",
      "96/96 [==============================] - 0s 513us/step - loss: 6.8737 - val_loss: 109.4334\n",
      "Epoch 9815/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 7.7176 - val_loss: 96.6085\n",
      "Epoch 9816/10000\n",
      "96/96 [==============================] - 0s 451us/step - loss: 6.2248 - val_loss: 86.8848\n",
      "Epoch 9817/10000\n",
      "96/96 [==============================] - 0s 483us/step - loss: 8.4218 - val_loss: 89.4168\n",
      "Epoch 9818/10000\n",
      "96/96 [==============================] - 0s 509us/step - loss: 7.9163 - val_loss: 104.6669\n",
      "Epoch 9819/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 7.1684 - val_loss: 87.9404\n",
      "Epoch 9820/10000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 6.5286 - val_loss: 85.9260\n",
      "Epoch 9821/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 7.4219 - val_loss: 87.0830\n",
      "Epoch 9822/10000\n",
      "96/96 [==============================] - 0s 467us/step - loss: 8.3439 - val_loss: 97.0159\n",
      "Epoch 9823/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 5.8087 - val_loss: 96.7931\n",
      "Epoch 9824/10000\n",
      "96/96 [==============================] - 0s 496us/step - loss: 6.1368 - val_loss: 86.4890\n",
      "Epoch 9825/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 486us/step - loss: 9.2460 - val_loss: 98.8840\n",
      "Epoch 9826/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 6.3603 - val_loss: 90.8772\n",
      "Epoch 9827/10000\n",
      "96/96 [==============================] - 0s 666us/step - loss: 5.3365 - val_loss: 95.2806\n",
      "Epoch 9828/10000\n",
      "96/96 [==============================] - 0s 596us/step - loss: 7.3477 - val_loss: 83.9263\n",
      "Epoch 9829/10000\n",
      "96/96 [==============================] - 0s 506us/step - loss: 8.5243 - val_loss: 96.2349\n",
      "Epoch 9830/10000\n",
      "96/96 [==============================] - 0s 486us/step - loss: 7.2735 - val_loss: 78.7317\n",
      "Epoch 9831/10000\n",
      "96/96 [==============================] - 0s 488us/step - loss: 6.9108 - val_loss: 95.2644\n",
      "Epoch 9832/10000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 9.9473 - val_loss: 93.9461\n",
      "Epoch 9833/10000\n",
      "96/96 [==============================] - 0s 452us/step - loss: 6.3857 - val_loss: 85.7349\n",
      "Epoch 9834/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 5.6470 - val_loss: 82.9374\n",
      "Epoch 9835/10000\n",
      "96/96 [==============================] - 0s 484us/step - loss: 6.3568 - val_loss: 89.2078\n",
      "Epoch 9836/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 8.6423 - val_loss: 81.3785\n",
      "Epoch 9837/10000\n",
      "96/96 [==============================] - 0s 413us/step - loss: 6.2178 - val_loss: 92.1257\n",
      "Epoch 9838/10000\n",
      "96/96 [==============================] - 0s 486us/step - loss: 6.5260 - val_loss: 76.0943\n",
      "Epoch 9839/10000\n",
      "96/96 [==============================] - 0s 430us/step - loss: 6.6820 - val_loss: 96.4127\n",
      "Epoch 9840/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 7.4779 - val_loss: 85.8751\n",
      "Epoch 9841/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 6.4937 - val_loss: 81.4491\n",
      "Epoch 9842/10000\n",
      "96/96 [==============================] - 0s 479us/step - loss: 5.7556 - val_loss: 87.9369\n",
      "Epoch 9843/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 5.9142 - val_loss: 101.9543\n",
      "Epoch 9844/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 5.5964 - val_loss: 95.4081\n",
      "Epoch 9845/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 7.6438 - val_loss: 96.0503\n",
      "Epoch 9846/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 6.5617 - val_loss: 88.3960\n",
      "Epoch 9847/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 7.2630 - val_loss: 98.9536\n",
      "Epoch 9848/10000\n",
      "96/96 [==============================] - 0s 374us/step - loss: 4.9704 - val_loss: 80.4106\n",
      "Epoch 9849/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 7.4681 - val_loss: 79.2242\n",
      "Epoch 9850/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 6.5909 - val_loss: 87.0770\n",
      "Epoch 9851/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 5.8106 - val_loss: 99.8708\n",
      "Epoch 9852/10000\n",
      "96/96 [==============================] - 0s 524us/step - loss: 9.1135 - val_loss: 104.7277\n",
      "Epoch 9853/10000\n",
      "96/96 [==============================] - 0s 509us/step - loss: 6.2977 - val_loss: 90.6449\n",
      "Epoch 9854/10000\n",
      "96/96 [==============================] - 0s 408us/step - loss: 7.8052 - val_loss: 78.1247\n",
      "Epoch 9855/10000\n",
      "96/96 [==============================] - 0s 364us/step - loss: 5.6432 - val_loss: 100.0959\n",
      "Epoch 9856/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 6.2344 - val_loss: 101.9170\n",
      "Epoch 9857/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 7.2507 - val_loss: 83.5887\n",
      "Epoch 9858/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 5.6325 - val_loss: 90.9055\n",
      "Epoch 9859/10000\n",
      "96/96 [==============================] - 0s 495us/step - loss: 7.5104 - val_loss: 96.2828\n",
      "Epoch 9860/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 6.4590 - val_loss: 84.6131\n",
      "Epoch 9861/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 6.7612 - val_loss: 104.7487\n",
      "Epoch 9862/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 7.8518 - val_loss: 84.9420\n",
      "Epoch 9863/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 5.2275 - val_loss: 93.5480\n",
      "Epoch 9864/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 6.4275 - val_loss: 93.4282\n",
      "Epoch 9865/10000\n",
      "96/96 [==============================] - 0s 434us/step - loss: 6.2015 - val_loss: 89.1942\n",
      "Epoch 9866/10000\n",
      "96/96 [==============================] - 0s 418us/step - loss: 5.5114 - val_loss: 93.3337\n",
      "Epoch 9867/10000\n",
      "96/96 [==============================] - 0s 435us/step - loss: 4.8791 - val_loss: 96.9406\n",
      "Epoch 9868/10000\n",
      "96/96 [==============================] - 0s 439us/step - loss: 5.9862 - val_loss: 90.6069\n",
      "Epoch 9869/10000\n",
      "96/96 [==============================] - 0s 490us/step - loss: 7.3043 - val_loss: 83.6385\n",
      "Epoch 9870/10000\n",
      "96/96 [==============================] - 0s 372us/step - loss: 5.9149 - val_loss: 100.2952\n",
      "Epoch 9871/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 6.4438 - val_loss: 109.7243\n",
      "Epoch 9872/10000\n",
      "96/96 [==============================] - 0s 455us/step - loss: 6.8264 - val_loss: 87.7698\n",
      "Epoch 9873/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 6.0733 - val_loss: 79.1904\n",
      "Epoch 9874/10000\n",
      "96/96 [==============================] - 0s 389us/step - loss: 6.7137 - val_loss: 92.9101\n",
      "Epoch 9875/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 11.7639 - val_loss: 83.0808\n",
      "Epoch 9876/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 6.0835 - val_loss: 78.9904\n",
      "Epoch 9877/10000\n",
      "96/96 [==============================] - 0s 516us/step - loss: 6.3641 - val_loss: 88.0859\n",
      "Epoch 9878/10000\n",
      "96/96 [==============================] - 0s 590us/step - loss: 4.5601 - val_loss: 90.2669\n",
      "Epoch 9879/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 6.0148 - val_loss: 91.7885\n",
      "Epoch 9880/10000\n",
      "96/96 [==============================] - 0s 496us/step - loss: 5.8737 - val_loss: 90.5555\n",
      "Epoch 9881/10000\n",
      "96/96 [==============================] - 0s 526us/step - loss: 6.0672 - val_loss: 97.1402\n",
      "Epoch 9882/10000\n",
      "96/96 [==============================] - 0s 510us/step - loss: 5.6759 - val_loss: 94.6321\n",
      "Epoch 9883/10000\n",
      "96/96 [==============================] - 0s 485us/step - loss: 7.1818 - val_loss: 91.6215\n",
      "Epoch 9884/10000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 8.0847 - val_loss: 82.0164\n",
      "Epoch 9885/10000\n",
      "96/96 [==============================] - 0s 499us/step - loss: 9.5138 - val_loss: 91.2817\n",
      "Epoch 9886/10000\n",
      "96/96 [==============================] - 0s 502us/step - loss: 8.1509 - val_loss: 77.1214\n",
      "Epoch 9887/10000\n",
      "96/96 [==============================] - 0s 487us/step - loss: 6.6823 - val_loss: 94.1548\n",
      "Epoch 9888/10000\n",
      "96/96 [==============================] - 0s 502us/step - loss: 6.7418 - val_loss: 97.6829\n",
      "Epoch 9889/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 6.3530 - val_loss: 97.1603\n",
      "Epoch 9890/10000\n",
      "96/96 [==============================] - 0s 464us/step - loss: 9.2257 - val_loss: 97.1612\n",
      "Epoch 9891/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 6.9767 - val_loss: 82.7139\n",
      "Epoch 9892/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 8.5369 - val_loss: 90.4859\n",
      "Epoch 9893/10000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 8.7480 - val_loss: 84.0762\n",
      "Epoch 9894/10000\n",
      "96/96 [==============================] - 0s 476us/step - loss: 9.0639 - val_loss: 84.2752\n",
      "Epoch 9895/10000\n",
      "96/96 [==============================] - 0s 488us/step - loss: 5.9835 - val_loss: 77.3638\n",
      "Epoch 9896/10000\n",
      "96/96 [==============================] - 0s 497us/step - loss: 4.7996 - val_loss: 88.3199\n",
      "Epoch 9897/10000\n",
      "96/96 [==============================] - 0s 521us/step - loss: 6.6473 - val_loss: 92.3385\n",
      "Epoch 9898/10000\n",
      "96/96 [==============================] - 0s 487us/step - loss: 5.9365 - val_loss: 86.6801\n",
      "Epoch 9899/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 5.5651 - val_loss: 96.7441\n",
      "Epoch 9900/10000\n",
      "96/96 [==============================] - 0s 525us/step - loss: 6.4819 - val_loss: 89.3879\n",
      "Epoch 9901/10000\n",
      "96/96 [==============================] - 0s 547us/step - loss: 5.3877 - val_loss: 78.0728\n",
      "Epoch 9902/10000\n",
      "96/96 [==============================] - 0s 501us/step - loss: 6.1844 - val_loss: 77.6281\n",
      "Epoch 9903/10000\n",
      "96/96 [==============================] - 0s 490us/step - loss: 4.0130 - val_loss: 88.0169\n",
      "Epoch 9904/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 6.8645 - val_loss: 95.6618\n",
      "Epoch 9905/10000\n",
      "96/96 [==============================] - 0s 391us/step - loss: 5.2028 - val_loss: 92.7746\n",
      "Epoch 9906/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 4.6300 - val_loss: 81.7177\n",
      "Epoch 9907/10000\n",
      "96/96 [==============================] - 0s 480us/step - loss: 7.1400 - val_loss: 91.3448\n",
      "Epoch 9908/10000\n",
      "96/96 [==============================] - 0s 507us/step - loss: 5.9382 - val_loss: 83.8148\n",
      "Epoch 9909/10000\n",
      "96/96 [==============================] - 0s 538us/step - loss: 5.2228 - val_loss: 95.9692\n",
      "Epoch 9910/10000\n",
      "96/96 [==============================] - 0s 499us/step - loss: 5.4687 - val_loss: 88.8639\n",
      "Epoch 9911/10000\n",
      "96/96 [==============================] - 0s 518us/step - loss: 6.0961 - val_loss: 85.3462\n",
      "Epoch 9912/10000\n",
      "96/96 [==============================] - 0s 492us/step - loss: 6.4425 - val_loss: 96.8574\n",
      "Epoch 9913/10000\n",
      "96/96 [==============================] - 0s 513us/step - loss: 5.0287 - val_loss: 81.5789\n",
      "Epoch 9914/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 4.6662 - val_loss: 88.0985\n",
      "Epoch 9915/10000\n",
      "96/96 [==============================] - 0s 474us/step - loss: 7.0745 - val_loss: 104.0382\n",
      "Epoch 9916/10000\n",
      "96/96 [==============================] - 0s 512us/step - loss: 6.0717 - val_loss: 102.1404\n",
      "Epoch 9917/10000\n",
      "96/96 [==============================] - 0s 505us/step - loss: 6.0979 - val_loss: 89.8280\n",
      "Epoch 9918/10000\n",
      "96/96 [==============================] - 0s 473us/step - loss: 7.1681 - val_loss: 88.2011\n",
      "Epoch 9919/10000\n",
      "96/96 [==============================] - ETA: 0s - loss: 13.91 - 0s 546us/step - loss: 7.1019 - val_loss: 87.4336\n",
      "Epoch 9920/10000\n",
      "96/96 [==============================] - 0s 480us/step - loss: 5.8490 - val_loss: 80.9013\n",
      "Epoch 9921/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 6.1660 - val_loss: 88.0741\n",
      "Epoch 9922/10000\n",
      "96/96 [==============================] - 0s 599us/step - loss: 5.9620 - val_loss: 94.8299\n",
      "Epoch 9923/10000\n",
      "96/96 [==============================] - 0s 581us/step - loss: 5.7522 - val_loss: 95.1379\n",
      "Epoch 9924/10000\n",
      "96/96 [==============================] - 0s 627us/step - loss: 5.8884 - val_loss: 88.4966\n",
      "Epoch 9925/10000\n",
      "96/96 [==============================] - 0s 499us/step - loss: 4.7784 - val_loss: 91.2741\n",
      "Epoch 9926/10000\n",
      "96/96 [==============================] - 0s 523us/step - loss: 6.0638 - val_loss: 85.2784\n",
      "Epoch 9927/10000\n",
      "96/96 [==============================] - 0s 477us/step - loss: 5.7264 - val_loss: 83.6199\n",
      "Epoch 9928/10000\n",
      "96/96 [==============================] - 0s 491us/step - loss: 6.3276 - val_loss: 89.7978\n",
      "Epoch 9929/10000\n",
      "96/96 [==============================] - 0s 495us/step - loss: 6.1855 - val_loss: 84.1973\n",
      "Epoch 9930/10000\n",
      "96/96 [==============================] - 0s 457us/step - loss: 6.8241 - val_loss: 80.3424\n",
      "Epoch 9931/10000\n",
      "96/96 [==============================] - 0s 525us/step - loss: 8.6851 - val_loss: 95.4720\n",
      "Epoch 9932/10000\n",
      "96/96 [==============================] - 0s 461us/step - loss: 7.6933 - val_loss: 95.0063\n",
      "Epoch 9933/10000\n",
      "96/96 [==============================] - 0s 463us/step - loss: 7.6791 - val_loss: 91.5942\n",
      "Epoch 9934/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 7.6027 - val_loss: 95.3001\n",
      "Epoch 9935/10000\n",
      "96/96 [==============================] - 0s 442us/step - loss: 5.5142 - val_loss: 88.6423\n",
      "Epoch 9936/10000\n",
      "96/96 [==============================] - 0s 563us/step - loss: 5.4026 - val_loss: 101.2447\n",
      "Epoch 9937/10000\n",
      "96/96 [==============================] - 0s 529us/step - loss: 7.6154 - val_loss: 90.5901\n",
      "Epoch 9938/10000\n",
      "96/96 [==============================] - 0s 475us/step - loss: 5.4936 - val_loss: 96.0191\n",
      "Epoch 9939/10000\n",
      "96/96 [==============================] - 0s 466us/step - loss: 5.9253 - val_loss: 86.1511\n",
      "Epoch 9940/10000\n",
      "96/96 [==============================] - 0s 462us/step - loss: 6.3660 - val_loss: 93.5857\n",
      "Epoch 9941/10000\n",
      "96/96 [==============================] - 0s 512us/step - loss: 5.8330 - val_loss: 102.8880\n",
      "Epoch 9942/10000\n",
      "96/96 [==============================] - 0s 514us/step - loss: 8.2508 - val_loss: 90.5277\n",
      "Epoch 9943/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 4.8224 - val_loss: 93.1539\n",
      "Epoch 9944/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 5.7728 - val_loss: 99.0815\n",
      "Epoch 9945/10000\n",
      "96/96 [==============================] - 0s 397us/step - loss: 9.0471 - val_loss: 101.1580\n",
      "Epoch 9946/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 5.7758 - val_loss: 87.7209\n",
      "Epoch 9947/10000\n",
      "96/96 [==============================] - 0s 424us/step - loss: 5.7057 - val_loss: 92.2788\n",
      "Epoch 9948/10000\n",
      "96/96 [==============================] - 0s 381us/step - loss: 6.9236 - val_loss: 88.7622\n",
      "Epoch 9949/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 5.1588 - val_loss: 80.7664\n",
      "Epoch 9950/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 7.9323 - val_loss: 87.7528\n",
      "Epoch 9951/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 5.3486 - val_loss: 76.6599\n",
      "Epoch 9952/10000\n",
      "96/96 [==============================] - 0s 432us/step - loss: 6.3791 - val_loss: 74.4637\n",
      "Epoch 9953/10000\n",
      "96/96 [==============================] - 0s 423us/step - loss: 6.2611 - val_loss: 97.4756\n",
      "Epoch 9954/10000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 5.5081 - val_loss: 86.4211\n",
      "Epoch 9955/10000\n",
      "96/96 [==============================] - 0s 377us/step - loss: 4.8415 - val_loss: 94.1664\n",
      "Epoch 9956/10000\n",
      "96/96 [==============================] - 0s 438us/step - loss: 4.2136 - val_loss: 91.8204\n",
      "Epoch 9957/10000\n",
      "96/96 [==============================] - 0s 428us/step - loss: 7.3528 - val_loss: 78.9142\n",
      "Epoch 9958/10000\n",
      "96/96 [==============================] - 0s 468us/step - loss: 6.6276 - val_loss: 83.8073\n",
      "Epoch 9959/10000\n",
      "96/96 [==============================] - 0s 450us/step - loss: 4.8451 - val_loss: 95.6594\n",
      "Epoch 9960/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 6.0080 - val_loss: 80.7279\n",
      "Epoch 9961/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 7.0835 - val_loss: 93.5601\n",
      "Epoch 9962/10000\n",
      "96/96 [==============================] - 0s 416us/step - loss: 5.6555 - val_loss: 105.7359\n",
      "Epoch 9963/10000\n",
      "96/96 [==============================] - 0s 415us/step - loss: 8.3856 - val_loss: 96.5437\n",
      "Epoch 9964/10000\n",
      "96/96 [==============================] - 0s 414us/step - loss: 7.5704 - val_loss: 69.0147\n",
      "Epoch 9965/10000\n",
      "96/96 [==============================] - 0s 427us/step - loss: 8.5957 - val_loss: 75.8735\n",
      "Epoch 9966/10000\n",
      "96/96 [==============================] - 0s 403us/step - loss: 8.3748 - val_loss: 85.7866\n",
      "Epoch 9967/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 9.1047 - val_loss: 91.1705\n",
      "Epoch 9968/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 6.2478 - val_loss: 90.0488\n",
      "Epoch 9969/10000\n",
      "96/96 [==============================] - 0s 379us/step - loss: 5.5416 - val_loss: 105.3909\n",
      "Epoch 9970/10000\n",
      "96/96 [==============================] - 0s 437us/step - loss: 6.1681 - val_loss: 86.7474\n",
      "Epoch 9971/10000\n",
      "96/96 [==============================] - 0s 459us/step - loss: 5.4704 - val_loss: 99.0730\n",
      "Epoch 9972/10000\n",
      "96/96 [==============================] - 0s 481us/step - loss: 5.3938 - val_loss: 97.2126\n",
      "Epoch 9973/10000\n",
      "96/96 [==============================] - 0s 544us/step - loss: 5.0088 - val_loss: 74.2844\n",
      "Epoch 9974/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 5.5088 - val_loss: 83.0719\n",
      "Epoch 9975/10000\n",
      "96/96 [==============================] - 0s 411us/step - loss: 4.7832 - val_loss: 96.0320\n",
      "Epoch 9976/10000\n",
      "96/96 [==============================] - 0s 421us/step - loss: 6.9697 - val_loss: 78.6094\n",
      "Epoch 9977/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 [==============================] - 0s 422us/step - loss: 6.7792 - val_loss: 91.4645\n",
      "Epoch 9978/10000\n",
      "96/96 [==============================] - 0s 420us/step - loss: 5.0351 - val_loss: 93.5662\n",
      "Epoch 9979/10000\n",
      "96/96 [==============================] - 0s 475us/step - loss: 5.4362 - val_loss: 85.1168\n",
      "Epoch 9980/10000\n",
      "96/96 [==============================] - 0s 410us/step - loss: 5.0429 - val_loss: 87.1638\n",
      "Epoch 9981/10000\n",
      "96/96 [==============================] - 0s 376us/step - loss: 4.9658 - val_loss: 94.0097\n",
      "Epoch 9982/10000\n",
      "96/96 [==============================] - 0s 409us/step - loss: 6.3488 - val_loss: 91.2295\n",
      "Epoch 9983/10000\n",
      "96/96 [==============================] - 0s 404us/step - loss: 5.6476 - val_loss: 94.7467\n",
      "Epoch 9984/10000\n",
      "96/96 [==============================] - 0s 412us/step - loss: 5.9224 - val_loss: 101.1191\n",
      "Epoch 9985/10000\n",
      "96/96 [==============================] - 0s 407us/step - loss: 4.6170 - val_loss: 84.2604\n",
      "Epoch 9986/10000\n",
      "96/96 [==============================] - 0s 429us/step - loss: 5.4191 - val_loss: 75.0766\n",
      "Epoch 9987/10000\n",
      "96/96 [==============================] - 0s 405us/step - loss: 5.4634 - val_loss: 108.1071\n",
      "Epoch 9988/10000\n",
      "96/96 [==============================] - 0s 393us/step - loss: 5.1044 - val_loss: 99.4924\n",
      "Epoch 9989/10000\n",
      "96/96 [==============================] - 0s 433us/step - loss: 4.7935 - val_loss: 73.6609\n",
      "Epoch 9990/10000\n",
      "96/96 [==============================] - 0s 458us/step - loss: 4.7620 - val_loss: 76.6641\n",
      "Epoch 9991/10000\n",
      "96/96 [==============================] - 0s 436us/step - loss: 5.2893 - val_loss: 84.8928\n",
      "Epoch 9992/10000\n",
      "96/96 [==============================] - 0s 470us/step - loss: 6.0115 - val_loss: 105.4894\n",
      "Epoch 9993/10000\n",
      "96/96 [==============================] - 0s 444us/step - loss: 29.3562 - val_loss: 78.2700\n",
      "Epoch 9994/10000\n",
      "96/96 [==============================] - 0s 453us/step - loss: 20.9320 - val_loss: 80.0539\n",
      "Epoch 9995/10000\n",
      "96/96 [==============================] - 0s 445us/step - loss: 11.6013 - val_loss: 79.4036\n",
      "Epoch 9996/10000\n",
      "96/96 [==============================] - 0s 449us/step - loss: 16.0682 - val_loss: 83.8679\n",
      "Epoch 9997/10000\n",
      "96/96 [==============================] - 0s 440us/step - loss: 22.1858 - val_loss: 88.5010\n",
      "Epoch 9998/10000\n",
      "96/96 [==============================] - 0s 509us/step - loss: 15.5516 - val_loss: 92.3069\n",
      "Epoch 9999/10000\n",
      "96/96 [==============================] - 0s 520us/step - loss: 25.9924 - val_loss: 90.9236\n",
      "Epoch 10000/10000\n",
      "96/96 [==============================] - 0s 489us/step - loss: 14.6624 - val_loss: 101.3517\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X[:-2], y[:-2], epochs=10000, validation_data=(X_test, y_test), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f09b843cc18>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f09bb06fb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(scaler.inverse_transform(y))\n",
    "plt.plot(scaler.inverse_transform(model.predict(X[:-2], batch_size=32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f09b83be1d0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f09b83be748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_hat = scaler.inverse_transform(model.predict(X_test, batch_size=32))\n",
    "y_real = scaler.inverse_transform(y_test[:-1])\n",
    "\n",
    "plt.plot(y_hat)\n",
    "plt.plot(y_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.673016860253062"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape(y_true=y_real, y_pred=y_hat[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f09b80c0a20>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f09b823a0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['loss'])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"best_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
